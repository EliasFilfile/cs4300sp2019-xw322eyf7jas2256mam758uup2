id,title,summary,link,,
0,An Optimal Control View of Adversarial Machine Learning,"I describe an optimal control view of adversarial machine learning, where thedynamical system is the machine learner, the input are adversarial actions, andthe control costs are defined by the adversary's goals to do harm and be hardto detect. This view encompasses many types of adversarial machine learning,including test-item attacks, training-data poisoning, and adversarial rewardshaping. The view encourages adversarial machine learning researcher to utilizeadvances in control theory and reinforcement learning.",http://arxiv.org/abs/1811.04422v1,,
1,"Minimax deviation strategies for machine learning and recognition with  short learning samples","The article is devoted to the problem of small learning samples in machinelearning. The flaws of maximum likelihood learning and minimax learning arelooked into and the concept of minimax deviation learning is introduced that isfree of those flaws.",http://arxiv.org/abs/1707.04849v1,,
2,Introduction to Machine Learning: Class Notes 67577,"Introduction to Machine learning covering Statistical Inference (Bayes, EM,ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",http://arxiv.org/abs/0904.3664v1,,
3,"A Unified Analytical Framework for Trustable Machine Learning and  Automation Running with Blockchain","Traditional machine learning algorithms use data from databases that aremutable, and therefore the data cannot be fully trusted. Also, the machinelearning process is difficult to automate. This paper proposes building atrustable machine learning system by using blockchain technology, which canstore data in a permanent and immutable way. In addition, smart contracts areused to automate the machine learning process. This paper makes threecontributions. First, it establishes a link between machine learning technologyand blockchain technology. Previously, machine learning and blockchain havebeen considered two independent technologies without an obvious link. Second,it proposes a unified analytical framework for trustable machine learning byusing blockchain technology. This unified framework solves both thetrustability and automation issues in machine learning. Third, it enables acomputer to translate core machine learning implementation from a single threadon a single machine to multiple threads on multiple machines running withblockchain by using a unified approach. The paper uses association rule miningas an example to demonstrate how trustable machine learning can be implementedwith blockchain, and it shows how this approach can be used to analyze opioidprescriptions to help combat the opioid crisis.",http://arxiv.org/abs/1903.08801v1,,
4,"MLBench: How Good Are Machine Learning Clouds for Binary Classification  Tasks on Structured Data?","We conduct an empirical study of machine learning functionalities provided bymajor cloud service providers, which we call machine learning clouds. Machinelearning clouds hold the promise of hiding all the sophistication of runninglarge-scale machine learning: Instead of specifying how to run a machinelearning task, users only specify what machine learning task to run and thecloud figures out the rest. Raising the level of abstraction, however, rarelycomes free - a performance penalty is possible. How good, then, are currentmachine learning clouds on real-world machine learning workloads?  We study this question with a focus on binary classication problems. Wepresent mlbench, a novel benchmark constructed by harvesting datasets fromKaggle competitions. We then compare the performance of the top winning codeavailable from Kaggle with that of running machine learning clouds from bothAzure and Amazon on mlbench. Our comparative study reveals the strength andweakness of existing machine learning clouds and points out potential futuredirections for improvement.",http://arxiv.org/abs/1707.09562v3,,
5,AutoCompete: A Framework for Machine Learning Competition,"In this paper, we propose AutoCompete, a highly automated machine learningframework for tackling machine learning competitions. This framework has beenlearned by us, validated and improved over a period of more than two years byparticipating in online machine learning competitions. It aims at minimizinghuman interference required to build a first useful predictive model and toassess the practical difficulty of a given machine learning challenge. Theproposed system helps in identifying data types, choosing a machine learn- ingmodel, tuning hyper-parameters, avoiding over-fitting and optimization for aprovided evaluation metric. We also observe that the proposed system producesbetter (or comparable) results with less runtime as compared to otherapproaches.",http://arxiv.org/abs/1507.02188v1,,
6,"Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in  Social Good Applications","This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learningin Social Good Applications, which was held on June 24, 2016 in New York.",http://arxiv.org/abs/1607.02450v2,,
7,Joint Training of Deep Boltzmann Machines,"We introduce a new method for training deep Boltzmann machines jointly. Priormethods require an initial learning pass that trains the deep Boltzmann machinegreedily, one layer at a time, or do not perform well on classifi- cationtasks.",http://arxiv.org/abs/1212.2686v1,,
8,A Primer on PAC-Bayesian Learning,"Generalized Bayesian learning algorithms are increasingly popular in machinelearning, due to their PAC generalization properties and flexibility. Thepresent paper aims at providing a self-contained survey on the resultingPAC-Bayes framework and some of its main theoretical and algorithmicdevelopments.",http://arxiv.org/abs/1901.05353v1,,
9,"Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of  learning relational order via reinforcement learning procedure?","In this article, we extend the conventional framework ofconvolutional-Restricted-Boltzmann-Machine to learn highly abstract featuresamong abitrary number of time related input maps by constructing a layer ofmultiplicative units, which capture the relations among inputs. In many cases,more than two maps are strongly related, so it is wise to make multiplicativeunit learn relations among more input maps, in other words, to find the optimalrelational-order of each unit. In order to enable our machine to learnrelational order, we developed a reinforcement-learning method whose optimalityis proven to train the network.",http://arxiv.org/abs/1706.08001v1,,
10,"Proceedings of the 29th International Conference on Machine Learning  (ICML-12)","This is an index to the papers that appear in the Proceedings of the 29thInternational Conference on Machine Learning (ICML-12). The conference was heldin Edinburgh, Scotland, June 27th - July 3rd, 2012.",http://arxiv.org/abs/1207.4676v2,,
11,Distributed Multi-Task Learning with Shared Representation,"We study the problem of distributed multi-task learning with sharedrepresentation, where each machine aims to learn a separate, but related, taskin an unknown shared low-dimensional subspaces, i.e. when the predictor matrixhas low rank. We consider a setting where each task is handled by a differentmachine, with samples for the task available locally on the machine, and studycommunication-efficient methods for exploiting the shared structure.",http://arxiv.org/abs/1603.02185v1,,
12,On-the-Fly Learning in a Perpetual Learning Machine,"Despite the promise of brain-inspired machine learning, deep neural networks(DNN) have frustratingly failed to bridge the deceptively large gap betweenlearning and memory. Here, we introduce a Perpetual Learning Machine; a newtype of DNN that is capable of brain-like dynamic 'on the fly' learning becauseit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.Thus, we provide the means to unify learning and memory within a machinelearning framework. We also explore the elegant duality of abstraction andsynthesis: the Yin and Yang of deep learning.",http://arxiv.org/abs/1509.00913v3,,
13,Distributed Multitask Learning,"We consider the problem of distributed multi-task learning, where eachmachine learns a separate, but related, task. Specifically, each machine learnsa linear predictor in high-dimensional space,where all tasks share the samesmall support. We present a communication-efficient estimator based on thedebiased lasso and show that it is comparable with the optimal centralizedmethod.",http://arxiv.org/abs/1510.00633v1,,
14,Distributed Stochastic Multi-Task Learning with Graph Regularization,"We propose methods for distributed graph-based multi-task learning that arebased on weighted averaging of messages from other machines. Uniform averagingor diminishing stepsize in these methods would yield consensus (single task)learning. We show how simply skewing the averaging weights or controlling thestepsize allows learning different, but related, tasks on the differentmachines.",http://arxiv.org/abs/1802.03830v1,,
15,"An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality  in Machine Learning","We propose a clustering-based iterative algorithm to solve certainoptimization problems in machine learning, where we start the algorithm byaggregating the original data, solving the problem on aggregated data, and thenin subsequent steps gradually disaggregate the aggregated data. We apply thealgorithm to common machine learning problems such as the least absolutedeviation regression problem, support vector machines, and semi-supervisedsupport vector machines. We derive model-specific data aggregation anddisaggregation procedures. We also show optimality, convergence, and theoptimality gap of the approximated solution in each iteration. A computationalstudy is provided.",http://arxiv.org/abs/1607.01400v1,,
16,Bayesian Optimization for Machine Learning : A Practical Guidebook,"The engineering of machine learning systems is still a nascent field; relyingon a seemingly daunting collection of quickly evolving tools and bestpractices. It is our hope that this guidebook will serve as a useful resourcefor machine learning practitioners looking to take advantage of Bayesianoptimization techniques. We outline four example machine learning problems thatcan be solved using open source machine learning libraries, and highlight thebenefits of using Bayesian optimization in the context of these common machinelearning applications.",http://arxiv.org/abs/1612.04858v1,,
17,Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge ofinterest in interpretable machine learning: systems that provide explanationfor their outputs. These explanations are often used to qualitatively assessother criteria such as safety or non-discrimination. However, despite theinterest in interpretability, there is very little consensus on whatinterpretable machine learning is and how it should be measured. In thisposition paper, we first define interpretability and describe wheninterpretability is needed (and when it is not). Next, we suggest a taxonomyfor rigorous evaluation and expose open questions towards a more rigorousscience of interpretable machine learning.",http://arxiv.org/abs/1702.08608v2,,
18,Infrastructure for Usable Machine Learning: The Stanford DAWN Project,"Despite incredible recent advances in machine learning, building machinelearning applications remains prohibitively time-consuming and expensive forall but the best-trained, best-funded engineering organizations. This expensecomes not from a need for new and improved statistical models but instead froma lack of systems and tools for supporting end-to-end machine learningapplication development, from data preparation and labeling toproductionization and monitoring. In this document, we outline opportunitiesfor infrastructure supporting usable, end-to-end machine learning applicationsin the context of the nascent DAWN (Data Analytics for What's Next) project atStanford.",http://arxiv.org/abs/1705.07538v2,,
19,"A review of possible effects of cognitive biases on interpretation of  rule-based machine learning models","This paper investigates to what extent cognitive biases may affect humanunderstanding of interpretable machine learning models, in particular of rulesdiscovered from data. Twenty cognitive biases are covered, as are possibledebiasing techniques that can be adopted by designers of machine learningalgorithms and software. Our review transfers results obtained in cognitivepsychology to the domain of machine learning, aiming to bridge the current gapbetween these two areas. It needs to be followed by empirical studiesspecifically aimed at the machine learning domain.",http://arxiv.org/abs/1804.02969v3,,
20,Techniques for Interpretable Machine Learning,"Interpretable machine learning tackles the important problem that humanscannot understand the behaviors of complex machine learning models and howthese models arrive at a particular decision. Although many approaches havebeen proposed, a comprehensive understanding of the achievements and challengesis still lacking. We provide a survey covering existing techniques to increasethe interpretability of machine learning models. We also discuss crucial issuesthat the community should consider in future work such as designinguser-friendly explanations and developing comprehensive evaluation metrics tofurther push forward the area of interpretable machine learning.",http://arxiv.org/abs/1808.00033v2,,
21,Optimization Models for Machine Learning: A Survey,"This paper surveys the machine learning literature and presents machinelearning as optimization models. Such models can benefit from the advancementof numerical optimization techniques which have already played a distinctiverole in several machine learning settings. Particularly, mathematicaloptimization models are presented for commonly used machine learning approachesfor regression, classification, clustering, and deep neural networks as wellnew emerging applications in machine teaching and empirical model learning. Thestrengths and the shortcomings of these models are discussed and potentialresearch directions are highlighted.",http://arxiv.org/abs/1901.05331v2,,
22,Meta-Learning: A Survey,"Meta-learning, or learning to learn, is the science of systematicallyobserving how different machine learning approaches perform on a wide range oflearning tasks, and then learning from this experience, or meta-data, to learnnew tasks much faster than otherwise possible. Not only does this dramaticallyspeed up and improve the design of machine learning pipelines or neuralarchitectures, it also allows us to replace hand-engineered algorithms withnovel approaches learned in a data-driven way. In this chapter, we provide anoverview of the state of the art in this fascinating and continuously evolvingfield.",http://arxiv.org/abs/1810.03548v1,,
23,Introduction to intelligent computing unit 1,"This brief note highlights some basic concepts required toward understandingthe evolution of machine learning and deep learning models. The note startswith an overview of artificial intelligence and its relationship to biologicalneuron that ultimately led to the evolution of todays intelligent models.",http://arxiv.org/abs/1711.06552v1,,
24,Compressive Classification (Machine Learning without learning),"Compressive learning is a framework where (so far unsupervised) learningtasks use not the entire dataset but a compressed summary (sketch) of it. Wepropose a compressive learning classification method, and a novel sketchfunction for images.",http://arxiv.org/abs/1812.01410v1,,
25,Machine Learning Interpretability: A Science rather than a tool,"The term ""interpretability"" is oftenly used by machine learning researcherseach with their own intuitive understanding of it. There is no universal wellagreed upon definition of interpretability in machine learning. As any type ofscience discipline is mainly driven by the set of formulated questions ratherthan by different tools in that discipline, e.g. astrophysics is the disciplinethat learns the composition of stars, not as the discipline that use thespectroscopes. Similarly, we propose that machine learning interpretabilityshould be a discipline that answers specific questions related tointerpretability. These questions can be of statistical, causal andcounterfactual nature. Therefore, there is a need to look into theinterpretability problem of machine learning in the context of questions thatneed to be addressed rather than different tools. We discuss about ahypothetical interpretability framework driven by a question based scientificapproach rather than some specific machine learning model. Using a questionbased notion of interpretability, we can step towards understanding the scienceof machine learning rather than its engineering. This notion will also help usunderstanding any specific problem more in depth rather than relying solely onmachine learning methods.",http://arxiv.org/abs/1807.06722v2,,
26,A Survey on Resilient Machine Learning,"Machine learning based system are increasingly being used for sensitive taskssuch as security surveillance, guiding autonomous vehicle, taking investmentdecisions, detecting and blocking network intrusion and malware etc. However,recent research has shown that machine learning models are venerable to attacksby adversaries at all phases of machine learning (eg, training data collection,training, operation). All model classes of machine learning systems can bemisled by providing carefully crafted inputs making them wrongly classifyinputs. Maliciously created input samples can affect the learning process of aML system by either slowing down the learning process, or affecting theperformance of the learned mode, or causing the system make error(s) only inattacker's planned scenario. Because of these developments, understandingsecurity of machine learning algorithms and systems is emerging as an importantresearch area among computer security and machine learning researchers andpractitioners. We present a survey of this emerging area in machine learning.",http://arxiv.org/abs/1707.03184v1,,
27,Discussion on Mechanical Learning and Learning Machine,"Mechanical learning is a computing system that is based on a set of simpleand fixed rules, and can learn from incoming data. A learning machine is asystem that realizes mechanical learning. Importantly, we emphasis that it isbased on a set of simple and fixed rules, contrasting to often called machinelearning that is sophisticated software based on very complicated mathematicaltheory, and often needs human intervene for software fine tune and manualadjustments. Here, we discuss some basic facts and principles of such system,and try to lay down a framework for further study. We propose 2 directions toapproach mechanical learning, just like Church-Turing pair: one is trying torealize a learning machine, another is trying to well describe the mechanicallearning.",http://arxiv.org/abs/1602.00198v1,,
28,Information Theory and its Relation to Machine Learning,"In this position paper, I first describe a new perspective on machinelearning (ML) by four basic problems (or levels), namely, ""What to learn?"",""How to learn?"", ""What to evaluate?"", and ""What to adjust?"". The paper stressesmore on the first level of ""What to learn?"", or ""Learning Target Selection"".Towards this primary problem within the four levels, I briefly review theexisting studies about the connection between information theoretical learning(ITL [1]) and machine learning. A theorem is given on the relation between theempirically-defined similarity measure and information measures. Finally, aconjecture is proposed for pursuing a unified mathematical interpretation tolearning target selection.",http://arxiv.org/abs/1501.04309v1,,
29,An Introduction to MM Algorithms for Machine Learning and Statistical,"MM (majorization--minimization) algorithms are an increasingly popular toolfor solving optimization problems in machine learning and statisticalestimation. This article introduces the MM algorithm framework in general andvia three popular example applications: Gaussian mixture regressions,multinomial logistic regressions, and support vector machines. Specificalgorithms for the three examples are derived and numerical demonstrations arepresented. Theoretical and practical aspects of MM algorithm design arediscussed.",http://arxiv.org/abs/1611.03969v1,,
30,"Some Requests for Machine Learning Research from the East African Tech  Scene","Based on 46 in-depth interviews with scientists, engineers, and CEOs, thisdocument presents a list of concrete machine research problems, progress onwhich would directly benefit tech ventures in East Africa.",http://arxiv.org/abs/1810.11383v2,,
31,Application of Machine Learning Techniques in Aquaculture,"In this paper we present applications of different machine learningalgorithms in aquaculture. Machine learning algorithms learn models fromhistorical data. In aquaculture historical data are obtained from farmpractices, yields, and environmental data sources. Associations between thesedifferent variables can be obtained by applying machine learning algorithms tohistorical data. In this paper we present applications of different machinelearning algorithms in aquaculture applications.",http://arxiv.org/abs/1405.1304v1,,
32,"TF.Learn: TensorFlow's High-level Module for Distributed Machine  Learning","TF.Learn is a high-level Python module for distributed machine learninginside TensorFlow. It provides an easy-to-use Scikit-learn style interface tosimplify the process of creating, configuring, training, evaluating, andexperimenting a machine learning model. TF.Learn integrates a wide range ofstate-of-art machine learning algorithms built on top of TensorFlow's low levelAPIs for small to large-scale supervised and unsupervised problems. This modulefocuses on bringing machine learning to non-specialists using a general-purposehigh-level language as well as researchers who want to implement, benchmark,and compare their new methods in a structured environment. Emphasis is put onease of use, performance, documentation, and API consistency.",http://arxiv.org/abs/1612.04251v1,,
33,Logistic Regression as Soft Perceptron Learning,"We comment on the fact that gradient ascent for logistic regression has aconnection with the perceptron learning algorithm. Logistic learning is the""soft"" variant of perceptron learning.",http://arxiv.org/abs/1708.07826v1,,
34,Artificial Neural Networks,"These are lecture notes for my course on Artificial Neural Networks that Ihave given at Chalmers and Gothenburg University. This course describes the useof neural networks in machine learning: deep learning, recurrent networks, andother supervised and unsupervised machine-learning algorithms.",http://arxiv.org/abs/1901.05639v2,,
35,Learning Theory and Support Vector Machines - a primer,"The main goal of statistical learning theory is to provide a fundamentalframework for the problem of decision making and model construction based onsets of data. Here, we present a brief introduction to the fundamentals ofstatistical learning theory, in particular the difference between empirical andstructural risk minimization, including one of its most prominentimplementations, i.e. the Support Vector Machine.",http://arxiv.org/abs/1902.04622v1,,
36,Quantum-enhanced machine learning,"The emerging field of quantum machine learning has the potential tosubstantially aid in the problems and scope of artificial intelligence. This isonly enhanced by recent successes in the field of classical machine learning.In this work we propose an approach for the systematic treatment of machinelearning, from the perspective of quantum information. Our approach is generaland covers all three main branches of machine learning: supervised,unsupervised and reinforcement learning. While quantum improvements insupervised and unsupervised learning have been reported, reinforcement learninghas received much less attention. Within our approach, we tackle the problem ofquantum enhancements in reinforcement learning as well, and propose asystematic scheme for providing improvements. As an example, we show thatquadratic improvements in learning efficiency, and exponential improvements inperformance over limited time periods, can be obtained for a broad class oflearning problems.",http://arxiv.org/abs/1610.08251v1,,
37,Learning Moore Machines from Input-Output Traces,"The problem of learning automata from example traces (but no equivalence ormembership queries) is fundamental in automata learning theory and practice. Inthis paper we study this problem for finite state machines with inputs andoutputs, and in particular for Moore machines. We develop three algorithms forsolving this problem: (1) the PTAP algorithm, which transforms a set ofinput-output traces into an incomplete Moore machine and then completes themachine with self-loops; (2) the PRPNI algorithm, which uses the well-knownRPNI algorithm for automata learning to learn a product of automata encoding aMoore machine; and (3) the MooreMI algorithm, which directly learns a Mooremachine using PTAP extended with state merging. We prove that MooreMI has thefundamental identification in the limit property. We also compare thealgorithms experimentally in terms of the size of the learned machine andseveral notions of accuracy, introduced in this paper. Finally, we compare withOSTIA, an algorithm that learns a more general class of transducers, and findthat OSTIA generally does not learn a Moore machine, even when fed with acharacteristic sample.",http://arxiv.org/abs/1605.07805v2,,
38,"How Developers Iterate on Machine Learning Workflows -- A Survey of the  Applied Machine Learning Literature","Machine learning workflow development is anecdotally regarded to be aniterative process of trial-and-error with humans-in-the-loop. However, we arenot aware of quantitative evidence corroborating this popular belief. Aquantitative characterization of iteration can serve as a benchmark for machinelearning workflow development in practice, and can aid the development ofhuman-in-the-loop machine learning systems. To this end, we conduct asmall-scale survey of the applied machine learning literature from fivedistinct application domains. We collect and distill statistics on the role ofiteration within machine learning workflow development, and report preliminarytrends and insights from our investigation, as a starting point towards thisbenchmark. Based on our findings, we finally describe desiderata for effectiveand versatile human-in-the-loop machine learning systems that can cater tousers in diverse domains.",http://arxiv.org/abs/1803.10311v2,,
39,"Generalization Guarantees for a Binary Classification Framework for  Two-Stage Multiple Kernel Learning","We present generalization bounds for the TS-MKL framework for two stagemultiple kernel learning. We also present bounds for sparse kernel learningformulations within the TS-MKL framework.",http://arxiv.org/abs/1302.0406v1,,
40,Diversity in Machine Learning,"Machine learning methods have achieved good performance and been widelyapplied in various real-world applications. It can learn the model adaptivelyand be better fit for special requirements of different tasks. Many factors canaffect the performance of the machine learning process, among which diversityof the machine learning is an important one. Generally, a good machine learningsystem is composed of plentiful training data, a good model training process,and an accurate inference. The diversity could help each procedure to guaranteea total good machine learning: diversity of the training data ensures the datacontain enough discriminative information, diversity of the learned model(diversity in parameters of each model or diversity in models) makes eachparameter/model capture unique or complement information and the diversity ininference can provide multiple choices each of which corresponds to a plausibleresult. However, there is no systematical analysis of the diversification inmachine learning system. In this paper, we systematically summarize the methodsto make data diversification, model diversification, and inferencediversification in machine learning process, respectively. In addition, thetypical applications where the diversity technology improved the machinelearning performances have been surveyed, including the remote sensing imagingtasks, machine translation, camera relocalization, image segmentation, objectdetection, topic modeling, and others. Finally, we discuss some challenges ofdiversity technology in machine learning and point out some directions infuture work. Our analysis provides a deeper understanding of the diversitytechnology in machine learning tasks, and hence can help design and learn moreeffective models for specific tasks.",http://arxiv.org/abs/1807.01477v1,,
41,Is 'Unsupervised Learning' a Misconceived Term?,"Is all of machine learning supervised to some degree? The field of machinelearning has traditionally been categorized pedagogically into$supervised~vs~unsupervised~learning$; where supervised learning has typicallyreferred to learning from labeled data, while unsupervised learning hastypically referred to learning from unlabeled data. In this paper, we assertthat all machine learning is in fact supervised to some degree, and that thescope of supervision is necessarily commensurate to the scope of learningpotential. In particular, we argue that clustering algorithms such as k-means,and dimensionality reduction algorithms such as principal component analysis,variational autoencoders, and deep belief networks are each internallysupervised by the data itself to learn their respective representations of itsfeatures. Furthermore, these algorithms are not capable of external inferenceuntil their respective outputs (clusters, principal components, orrepresentation codes) have been identified and externally labeled in effect. Assuch, they do not suffice as examples of unsupervised learning. We propose thatthe categorization `supervised vs unsupervised learning' be dispensed with, andinstead, learning algorithms be categorized as either$internally~or~externally~supervised$ (or both). We believe this change inperspective will yield new fundamental insights into the structure andcharacter of data and of learning algorithms.",http://arxiv.org/abs/1904.03259v1,,
42,"The Case for Meta-Cognitive Machine Learning: On Model Entropy and  Concept Formation in Deep Learning","Machine learning is usually defined in behaviourist terms, where externalvalidation is the primary mechanism of learning. In this paper, I argue for amore holistic interpretation in which finding more probable, efficient andabstract representations is as central to learning as performance. In otherwords, machine learning should be extended with strategies to reason over itsown learning process, leading to so-called meta-cognitive machine learning. Assuch, the de facto definition of machine learning should be reformulated inthese intrinsically multi-objective terms, taking into account not only thetask performance but also internal learning objectives. To this end, we suggesta ""model entropy function"" to be defined that quantifies the efficiency of theinternal learning processes. It is conjured that the minimization of this modelentropy leads to concept formation. Besides philosophical aspects, some initialillustrations are included to support the claims.",http://arxiv.org/abs/1711.01431v1,,
43,"Transfer Learning for Voice Activity Detection: A Denoising Deep Neural  Network Perspective","Mismatching problem between the source and target noisy corpora severelyhinder the practical use of the machine-learning-based voice activity detection(VAD). In this paper, we try to address this problem in the transfer learningprospective. Transfer learning tries to find a common learning machine or acommon feature subspace that is shared by both the source corpus and the targetcorpus. The denoising deep neural network is used as the learning machine.Three transfer techniques, which aim to learn common feature representations,are used for analysis. Experimental results demonstrate the effectiveness ofthe transfer learning schemes on the mismatch problem.",http://arxiv.org/abs/1303.2104v1,,
44,Learnable: Theory vs Applications,"Two different views on machine learning problem: Applied learning (machinelearning with business applications) and Agnostic PAC learning are formalizedand compared here. I show that, under some conditions, the theory of PACLearnable provides a way to solve the Applied learning problem. However, thetheory requires to have the training sets so large, that it would make thelearning practically useless. I suggest shedding some theoreticalmisconceptions about learning to make the theory more aligned with the needsand experience of practitioners.",http://arxiv.org/abs/1807.10681v1,,
45,"Deep Learning and Its Applications to Machine Health Monitoring: A  Survey","Since 2006, deep learning (DL) has become a rapidly growing researchdirection, redefining state-of-the-art performances in a wide range of areassuch as object recognition, image segmentation, speech recognition and machinetranslation. In modern manufacturing systems, data-driven machine healthmonitoring is gaining in popularity due to the widespread deployment oflow-cost sensors and their connection to the Internet. Meanwhile, deep learningprovides useful tools for processing and analyzing these big machinery data.The main purpose of this paper is to review and summarize the emerging researchwork of deep learning on machine health monitoring. After the briefintroduction of deep learning techniques, the applications of deep learning inmachine health monitoring systems are reviewed mainly from the followingaspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines andits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).Finally, some new trends of DL-based machine health monitoring methods arediscussed.",http://arxiv.org/abs/1612.07640v1,,
46,Machine Learning: Basic Principles,"This tutorial is based on the lecture notes for, and the plentiful studentfeedback received from, the courses ""Machine Learning: Basic Principles"" and""Artificial Intelligence"", which I have co-taught since 2015 at AaltoUniversity. The aim is to provide an accessible introduction to some of themain concepts and methods within machine learning. Many of the current systemswhich are considered as (artificially) intelligent are based on combinations offew basic machine learning methods. After formalizing the main building blocksof a machine learning problem, some popular algorithmic design patterns formachine learning methods are discussed in some detail.",http://arxiv.org/abs/1805.05052v10,,
47,"Dex: Incremental Learning for Complex Environments in Deep Reinforcement  Learning","This paper introduces Dex, a reinforcement learning environment toolkitspecialized for training and evaluation of continual learning methods as wellas general reinforcement learning problems. We also present the novel continuallearning method of incremental learning, where a challenging environment issolved using optimal weight initialization learned from first solving a similareasier environment. We show that incremental learning can produce vastlysuperior results than standard methods by providing a strong baseline methodacross ten Dex environments. We finally develop a saliency method forqualitative analysis of reinforcement learning, which shows the impactincremental learning has on network attention.",http://arxiv.org/abs/1706.05749v1,,
48,A Comparison of First-order Algorithms for Machine Learning,"Using an optimization algorithm to solve a machine learning problem is one ofmainstreams in the field of science. In this work, we demonstrate acomprehensive comparison of some state-of-the-art first-order optimizationalgorithms for convex optimization problems in machine learning. We concentrateon several smooth and non-smooth machine learning problems with a loss functionplus a regularizer. The overall experimental results show the superiority ofprimal-dual algorithms in solving a machine learning problem from theperspectives of the ease to construct, running time and accuracy.",http://arxiv.org/abs/1404.6674v1,,
49,"Meaningful Models: Utilizing Conceptual Structure to Improve Machine  Learning Interpretability","The last decade has seen huge progress in the development of advanced machinelearning models; however, those models are powerless unless human users caninterpret them. Here we show how the mind's construction of concepts andmeaning can be used to create more interpretable machine learning models. Byproposing a novel method of classifying concepts, in terms of 'form' and'function', we elucidate the nature of meaning and offer proposals to improvemodel understandability. As machine learning begins to permeate daily life,interpretable models may serve as a bridge between domain-expert authors andnon-expert users.",http://arxiv.org/abs/1607.00279v1,,
50,"Theoretical Impediments to Machine Learning With Seven Sparks from the  Causal Revolution","Current machine learning systems operate, almost exclusively, in astatistical, or model-free mode, which entails severe theoretical limits ontheir power and performance. Such systems cannot reason about interventions andretrospection and, therefore, cannot serve as the basis for strong AI. Toachieve human level intelligence, learning machines need the guidance of amodel of reality, similar to the ones used in causal inference tasks. Todemonstrate the essential role of such models, I will present a summary ofseven tasks which are beyond reach of current machine learning systems andwhich have been accomplished using the tools of causal modeling.",http://arxiv.org/abs/1801.04016v1,,
51,"Optimizing for Generalization in Machine Learning with Cross-Validation  Gradients","Cross-validation is the workhorse of modern applied statistics and machinelearning, as it provides a principled framework for selecting the model thatmaximizes generalization performance. In this paper, we show that thecross-validation risk is differentiable with respect to the hyperparameters andtraining data for many common machine learning algorithms, including logisticregression, elastic-net regression, and support vector machines. Leveragingthis property of differentiability, we propose a cross-validation gradientmethod (CVGM) for hyperparameter optimization. Our method enables efficientoptimization in high-dimensional hyperparameter spaces of the cross-validationrisk, the best surrogate of the true generalization ability of our learningalgorithm.",http://arxiv.org/abs/1805.07072v1,,
52,Algebraic Expression of Subjective Spatial and Temporal Patterns,"Universal learning machine is a theory trying to study machine learning frommathematical point of view. The outside world is reflected inside an universallearning machine according to pattern of incoming data. This is subjectivepattern of learning machine. In [2,4], we discussed subjective spatial pattern,and established a powerful tool -- X-form, which is an algebraic expression forsubjective spatial pattern. However, as the initial stage of study, there weonly discussed spatial pattern. Here, we will discuss spatial and temporalpatterns, and algebraic expression for them.",http://arxiv.org/abs/1805.11959v2,,
53,Machine Learning in Official Statistics,"In the first half of 2018, the Federal Statistical Office of Germany(Destatis) carried out a ""Proof of Concept Machine Learning"" as part of itsDigital Agenda. A major component of this was surveys on the use of machinelearning methods in official statistics, which were conducted at selectednational and international statistical institutions and among the divisions ofDestatis. It was of particular interest to find out in which statistical areasand for which tasks machine learning is used and which methods are applied.This paper is intended to make the results of the surveys publicly accessible.",http://arxiv.org/abs/1812.10422v1,,
54,Seven Myths in Machine Learning Research,"We present seven myths commonly believed to be true in machine learningresearch, circa Feb 2019. This is an archival copy of the blog post athttps://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/  Myth 1: TensorFlow is a Tensor manipulation library  Myth 2: Image datasets are representative of real images found in the wild  Myth 3: Machine Learning researchers do not use the test set for validation  Myth 4: Every datapoint is used in training a neural network  Myth 5: We need (batch) normalization to train very deep residual networks  Myth 6: Attention $>$ Convolution  Myth 7: Saliency maps are robust ways to interpret neural networks",http://arxiv.org/abs/1902.06789v2,,
55,Optimal Algorithms for Ski Rental with Soft Machine-Learned Predictions,"We consider a variant of the classic Ski Rental online algorithm withapplications to machine learning. In our variant, we allow the skier access toa black-box machine-learning algorithm that provides an estimate of theprobability that there will be at most a threshold number of ski-days. Wederive a class of optimal randomized algorithms to determine the strategy thatminimizes the worst-case expected competitive ratio for the skier given aprediction from the machine learning algorithm,and analyze the performance androbustness of these algorithms.",http://arxiv.org/abs/1903.00092v2,,
56,Efficient Deep Learning on Multi-Source Private Data,"Machine learning models benefit from large and diverse datasets. Using suchdatasets, however, often requires trusting a centralized data aggregator. Forsensitive applications like healthcare and finance this is undesirable as itcould compromise patient privacy or divulge trade secrets. Recent advances insecure and privacy-preserving computation, including trusted hardware enclavesand differential privacy, offer a way for mutually distrusting parties toefficiently train a machine learning model without revealing the training data.In this work, we introduce Myelin, a deep learning framework which combinesthese privacy-preservation primitives, and use it to establish a baseline levelof performance for fully private machine learning.",http://arxiv.org/abs/1807.06689v1,,
57,Pymc-learn: Practical Probabilistic Machine Learning in Python,"$\textit{Pymc-learn}$ is a Python package providing a variety ofstate-of-the-art probabilistic models for supervised and unsupervised machinelearning. It is inspired by $\textit{scikit-learn}$ and focuses on bringingprobabilistic machine learning to non-specialists. It uses a general-purposehigh-level language that mimics $\textit{scikit-learn}$. Emphasis is put onease of use, productivity, flexibility, performance, documentation, and an APIconsistent with $\textit{scikit-learn}$. It depends on $\textit{scikit-learn}$and $\textit{pymc3}$ and is distributed under the new BSD-3 license,encouraging its use in both academia and industry. Source code, binaries, anddocumentation are available on http://github.com/pymc-learn/pymc-learn.",http://arxiv.org/abs/1811.00542v1,,
58,"ReinBo: Machine Learning pipeline search and configuration with Bayesian  Optimization embedded Reinforcement Learning","Machine learning pipeline potentially consists of several stages ofoperations like data preprocessing, feature engineering and machine learningmodel training. Each operation has a set of hyper-parameters, which can becomeirrelevant for the pipeline when the operation is not selected. This gives riseto a hierarchical conditional hyper-parameter space. To optimize this mixedcontinuous and discrete conditional hierarchical hyper-parameter space, wepropose an efficient pipeline search and configuration algorithm which combinesthe power of Reinforcement Learning and Bayesian Optimization. Empiricalresults show that our method performs favorably compared to state of the artmethods like Auto-sklearn , TPOT, Tree Parzen Window, and Random Search.",http://arxiv.org/abs/1904.05381v1,,
59,"A Learning Algorithm for Relational Logistic Regression: Preliminary  Results","Relational logistic regression (RLR) is a representation of conditionalprobability in terms of weighted formulae for modelling multi-relational data.In this paper, we develop a learning algorithm for RLR models. Learning an RLRmodel from data consists of two steps: 1- learning the set of formulae to beused in the model (a.k.a. structure learning) and learning the weight of eachformula (a.k.a. parameter learning). For structure learning, we deploy Schmidtand Murphy's hierarchical assumption: first we learn a model with simpleformulae, then more complex formulae are added iteratively only if all theirsub-formulae have proven effective in previous learned models. For parameterlearning, we convert the problem into a non-relational learning problem and usean off-the-shelf logistic regression learning algorithm from Weka, anopen-source machine learning tool, to learn the weights. We also indicate howhidden features about the individuals can be incorporated into RLR to boost thelearning performance. We compare our learning algorithm to other structure andparameter learning algorithms in the literature, and compare the performance ofRLR models to standard logistic regression and RDN-Boost on a modified versionof the MovieLens data-set.",http://arxiv.org/abs/1606.08531v1,,
60,"Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for  Complex Systems","This is the Proceedings of NIPS 2016 Workshop on Interpretable MachineLearning for Complex Systems, held in Barcelona, Spain on December 9, 2016",http://arxiv.org/abs/1611.09139v1,,
61,"Linear, Machine Learning and Probabilistic Approaches for Time Series  Analysis","In this paper we study different approaches for time series modeling. Theforecasting approaches using linear models, ARIMA alpgorithm, XGBoost machinelearning algorithm are described. Results of different model combinations areshown. For probabilistic modeling the approaches using copulas and Bayesianinference are considered.",http://arxiv.org/abs/1703.01977v1,,
62,"Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing  World","This is the Proceedings of NIPS 2017 Workshop on Machine Learning for theDeveloping World, held in Long Beach, California, USA on December 8, 2017",http://arxiv.org/abs/1711.09522v2,,
63,"Classifying medical notes into standard disease codes using Machine  Learning","We investigate the automatic classification of patient discharge notes intostandard disease labels. We find that Convolutional Neural Networks withAttention outperform previous algorithms used in this task, and suggest furtherareas for improvement.",http://arxiv.org/abs/1802.00382v1,,
64,Deep Learning for Classification Tasks on Geospatial Vector Polygons,"In this paper, we evaluate the accuracy of deep learning approaches ongeospatial vector geometry classification tasks. The purpose of this evaluationis to investigate the ability of deep learning models to learn from geometrycoordinates directly. Previous machine learning research applied to geospatialpolygon data did not use geometries directly, but derived properties thereof.These are produced by way of extracting geometry properties such as Fourierdescriptors. Instead, our introduced deep neural net architectures are able tolearn on sequences of coordinates mapped directly from polygons. In threeclassification tasks we show that the deep learning architectures arecompetitive with common learning algorithms that require extracted features.",http://arxiv.org/abs/1806.03857v1,,
65,Bridging belief function theory to modern machine learning,"Machine learning is a quickly evolving field which now looks really differentfrom what it was 15 years ago, when classification and clustering were majorissues. This document proposes several trends to explore the new questions ofmodern machine learning, with the strong afterthought that the belief functionframework has a major role to play.",http://arxiv.org/abs/1504.03874v1,,
66,Electre Tri-Machine Learning Approach to the Record Linkage Problem,"In this short paper, the Electre Tri-Machine Learning Method, generally usedto solve ordinal classification problems, is proposed for solving the RecordLinkage problem. Preliminary experimental results show that, using the ElectreTri method, high accuracy can be achieved and more than 99% of the matches andnonmatches were correctly identified by the procedure.",http://arxiv.org/abs/1505.06614v1,,
67,Theoretical Robopsychology: Samu Has Learned Turing Machines,"From the point of view of a programmer, the robopsychology is a synonym forthe activity is done by developers to implement their machine learningapplications. This robopsychological approach raises some fundamentaltheoretical questions of machine learning. Our discussion of these questions isconstrained to Turing machines. Alan Turing had given an algorithm (aka theTuring Machine) to describe algorithms. If it has been applied to describeitself then this brings us to Turing's notion of the universal machine. In thepresent paper, we investigate algorithms to write algorithms. From a pedagogypoint of view, this way of writing programs can be considered as a combinationof learning by listening and learning by doing due to it is based on applyingagent technology and machine learning. As the main result we introduce theproblem of learning and then we show that it cannot easily be handled inreality therefore it is reasonable to use machine learning algorithm forlearning Turing machines.",http://arxiv.org/abs/1606.02767v2,,
68,Model-Agnostic Interpretability of Machine Learning,"Understanding why machine learning models behave the way they do empowersboth system designers and end-users in many ways: in model selection, featureengineering, in order to trust and act upon the predictions, and in moreintuitive user interfaces. Thus, interpretability has become a vital concern inmachine learning, and work in the area of interpretable models has foundrenewed interest. In some applications, such models are as accurate asnon-interpretable ones, and thus are preferred for their transparency. Evenwhen they are not accurate, they may still be preferred when interpretabilityis of paramount importance. However, restricting machine learning tointerpretable models is often a severe limitation. In this paper we argue forexplaining machine learning predictions using model-agnostic approaches. Bytreating the machine learning models as black-box functions, these approachesprovide crucial flexibility in the choice of models, explanations, andrepresentations, improving debugging, comparison, and interfaces for a varietyof users and models. We also outline the main challenges for such methods, andreview a recently-introduced model-agnostic explanation approach (LIME) thataddresses these challenges.",http://arxiv.org/abs/1606.05386v1,,
69,"Proceedings of the 2016 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2016)","This is the Proceedings of the 2016 ICML Workshop on Human Interpretabilityin Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,and Hanna Wallach.",http://arxiv.org/abs/1607.02531v2,,
70,"The Top 10 Topics in Machine Learning Revisited: A Quantitative  Meta-Study","Which topics of machine learning are most commonly addressed in research?This question was initially answered in 2007 by doing a qualitative surveyamong distinguished researchers. In our study, we revisit this question from aquantitative perspective. Concretely, we collect 54K abstracts of paperspublished between 2007 and 2016 in leading machine learning journals andconferences. We then use machine learning in order to determine the top 10topics in machine learning. We not only include models, but provide a holisticview across optimization, data, features, etc. This quantitative approachallows reducing the bias of surveys. It reveals new and up-to-date insightsinto what the 10 most prolific topics in machine learning research are. Thisallows researchers to identify popular topics as well as new and rising topicsfor their research.",http://arxiv.org/abs/1703.10121v1,,
71,"On conditional parity as a notion of non-discrimination in machine  learning","We identify conditional parity as a general notion of non-discrimination inmachine learning. In fact, several recently proposed notions ofnon-discrimination, including a few counterfactual notions, are instances ofconditional parity. We show that conditional parity is amenable to statisticalanalysis by studying randomization as a general mechanism for achievingconditional parity and a kernel-based test of conditional parity.",http://arxiv.org/abs/1706.08519v1,,
72,"Proceedings of the 2017 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2017)","This is the Proceedings of the 2017 ICML Workshop on Human Interpretabilityin Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.",http://arxiv.org/abs/1708.02666v1,,
73,Privacy Preserving Machine Learning: Threats and Solutions,"For privacy concerns to be addressed adequately in current machine learningsystems, the knowledge gap between the machine learning and privacy communitiesmust be bridged. This article aims to provide an introduction to theintersection of both fields with special emphasis on the techniques used toprotect the data.",http://arxiv.org/abs/1804.11238v1,,
74,An $O(N)$ Sorting Algorithm: Machine Learning Sort,"We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method,which shows a huge potential sorting big data. This sorting algorithm can beapplied to parallel sorting and is suitable for GPU or TPU acceleration.Furthermore, we discuss the application of this algorithm to sparse hash table.",http://arxiv.org/abs/1805.04272v2,,
75,"Proceedings of the 2018 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2018)","This is the Proceedings of the 2018 ICML Workshop on Human Interpretabilityin Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14,2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, FernandaVi\'egas, and Martin Wattenberg.",http://arxiv.org/abs/1807.01308v1,,
76,TherML: Thermodynamics of Machine Learning,"In this work we offer a framework for reasoning about a wide class ofexisting objectives in machine learning. We develop a formal correspondencebetween this work and thermodynamics and discuss its implications.",http://arxiv.org/abs/1807.04162v3,,
77,"ML-Schema: Exposing the Semantics of Machine Learning with Schemas and  Ontologies","The ML-Schema, proposed by the W3C Machine Learning Schema Community Group,is a top-level ontology that provides a set of classes, properties, andrestrictions for representing and interchanging information on machine learningalgorithms, datasets, and experiments. It can be easily extended andspecialized and it is also mapped to other more domain-specific ontologiesdeveloped in the area of machine learning and data mining. In this paper weoverview existing state-of-the-art machine learning interchange formats andpresent the first release of ML-Schema, a canonical format resulted of morethan seven years of experience among different research institutions. We arguethat exposing semantics of machine learning algorithms, models, and experimentsthrough a canonical format may pave the way to better interpretability and torealistically achieve the full interoperability of experiments regardless ofplatform or adopted workflow solution.",http://arxiv.org/abs/1807.05351v1,,
78,Characterizing machine learning process: A maturity framework,"Academic literature on machine learning modeling fails to address how to makemachine learning models work for enterprises. For example, existing machinelearning processes cannot address how to define business use cases for an AIapplication, how to convert business requirements from offering managers intodata requirements for data scientists, and how to continuously improve AIapplications in term of accuracy and fairness, and how to customize generalpurpose machine learning models with industry, domain, and use case specificdata to make them more accurate for specific situations etc. Making AI work forenterprises requires special considerations, tools, methods and processes. Inthis paper we present a maturity framework for machine learning model lifecyclemanagement for enterprises. Our framework is a re-interpretation of thesoftware Capability Maturity Model (CMM) for machine learning model developmentprocess. We present a set of best practices from our personal experience ofbuilding large scale real-world machine learning models to help organizationsachieve higher levels of maturity independent of their starting point.",http://arxiv.org/abs/1811.04871v1,,
79,"Towards Identifying and Managing Sources of Uncertainty in AI and  Machine Learning Models - An Overview","Quantifying and managing uncertainties that occur when data-driven modelssuch as those provided by AI and machine learning methods are applied iscrucial. This whitepaper provides a brief motivation and first overview of thestate of the art in identifying and quantifying sources of uncertainty fordata-driven components as well as means for analyzing their impact.",http://arxiv.org/abs/1811.11669v1,,
80,"Proceedings of NeurIPS 2018 Workshop on Machine Learning for the  Developing World: Achieving Sustainable Impact","This is the Proceedings of NeurIPS 2018 Workshop on Machine Learning for theDeveloping World: Achieving Sustainable Impact, held in Montreal, Canada onDecember 8, 2018",http://arxiv.org/abs/1812.10398v2,,
81,"Radiological images and machine learning: trends, perspectives, and  prospects","The application of machine learning to radiological images is an increasinglyactive research area that is expected to grow in the next five to ten years.Recent advances in machine learning have the potential to recognize andclassify complex patterns from different radiological imaging modalities suchas x-rays, computed tomography, magnetic resonance imaging and positronemission tomography imaging. In many applications, machine learning basedsystems have shown comparable performance to human decision-making. Theapplications of machine learning are the key ingredients of future clinicaldecision making and monitoring systems. This review covers the fundamentalconcepts behind various machine learning techniques and their applications inseveral radiological imaging areas, such as medical image segmentation, brainfunction studies and neurological disease diagnosis, as well as computer-aidedsystems, image registration, and content-based image retrieval systems.Synchronistically, we will briefly discuss current challenges and futuredirections regarding the application of machine learning in radiologicalimaging. By giving insight on how take advantage of machine learning poweredapplications, we expect that clinicians can prevent and diagnose diseases moreaccurately and efficiently.",http://arxiv.org/abs/1903.11726v1,,
82,A Game of Dice: Machine Learning and the Question Concerning Art,"We review some practical and philosophical questions raised by the use ofmachine learning in creative practice. Beyond the obvious problems regardingplagiarism and authorship, we argue that the novelty in AI Art relies mostly ona narrow machine learning contribution : manifold approximation. Nevertheless,this contribution creates a radical shift in the way we have to consider thismovement. Is this omnipotent tool a blessing or a curse for the artists?",http://arxiv.org/abs/1904.01957v1,,
83,Bounds for Vector-Valued Function Estimation,"We present a framework to derive risk bounds for vector-valued learning witha broad class of feature maps and loss functions. Multi-task learning andone-vs-all multi-category learning are treated as examples. We discuss indetail vector-valued functions with one hidden layer, and demonstrate that theconditions under which shared representations are beneficial for multi- tasklearning are equally applicable to multi-category learning.",http://arxiv.org/abs/1606.01487v1,,
84,Deep Learning: Computational Aspects,"In this article we review computational aspects of Deep Learning (DL). Deeplearning uses network architectures consisting of hierarchical layers of latentvariables to construct predictors for high-dimensional input-output models.Training a deep learning architecture is computationally intensive, andefficient linear algebra libraries is the key for training and inference.Stochastic gradient descent (SGD) optimization and batch sampling are used tolearn from massive data sets.",http://arxiv.org/abs/1808.08618v1,,
85,Concept-Oriented Deep Learning: Generative Concept Representations,"Generative concept representations have three major advantages overdiscriminative ones: they can represent uncertainty, they support integrationof learning and reasoning, and they are good for unsupervised andsemi-supervised learning. We discuss probabilistic and generative deeplearning, which generative concept representations are based on, and the use ofvariational autoencoders and generative adversarial networks for learninggenerative concept representations, particularly for concepts whose data aresequences, structured data or graphs.",http://arxiv.org/abs/1811.06622v1,,
86,The Preference Learning Toolbox,"Preference learning (PL) is a core area of machine learning that handlesdatasets with ordinal relations. As the number of generated data of ordinalnature is increasing, the importance and role of the PL field becomes centralwithin machine learning research and practice. This paper introduces an opensource, scalable, efficient and accessible preference learning toolbox thatsupports the key phases of the data training process incorporating variouspopular data preprocessing, feature selection and preference learning methods.",http://arxiv.org/abs/1506.01709v1,,
87,On Fundamental Limits of Robust Learning,"We consider the problems of robust PAC learning from distributed andstreaming data, which may contain malicious errors and outliers, and analyzetheir fundamental complexity questions. In particular, we establish lowerbounds on the communication complexity for distributed robust learningperformed on multiple machines, and on the space complexity for robust learningfrom streaming data on a single machine. These results demonstrate that gainingrobustness of learning algorithms is usually at the expense of increasedcomplexities. As far as we know, this work gives the first complexity resultsfor distributed and online robust PAC learning.",http://arxiv.org/abs/1703.10444v1,,
88,Seglearn: A Python Package for Learning Sequences and Time Series,"Seglearn is an open-source python package for machine learning time series orsequences using a sliding window segmentation approach. The implementationprovides a flexible pipeline for tackling classification, regression, andforecasting problems with multivariate sequence and contextual data. Thispackage is compatible with scikit-learn and is listed under scikit-learnRelated Projects. The package depends on numpy, scipy, and scikit-learn.Seglearn is distributed under the BSD 3-Clause License. Documentation includesa detailed API description, user guide, and examples. Unit tests provide a highdegree of code coverage.",http://arxiv.org/abs/1803.08118v3,,
89,Machine Learned Learning Machines,"There are two common approaches for optimizing the performance of a machine:genetic algorithms and machine learning. A genetic algorithm is applied overmany generations whereas machine learning works by applying feedback until thesystem meets a performance threshold. Though these are methods that typicallyoperate separately, we combine evolutionary adaptation and machine learninginto one approach. Our focus is on machines that can learn during theirlifetime, but instead of equipping them with a machine learning algorithm weaim to let them evolve their ability to learn by themselves. We use evolvablenetworks of probabilistic and deterministic logic gates, known as MarkovBrains, as our computational model organism. The ability of Markov Brains tolearn is augmented by a novel adaptive component that can change itscomputational behavior based on feedback. We show that Markov Brains can indeedevolve to incorporate these feedback gates to improve their adaptability tovariable environments. By combining these two methods, we now also implementeda computational model that can be used to study the evolution of learning.",http://arxiv.org/abs/1705.10201v2,,
90,Learning to Learn Neural Networks,"Meta-learning consists in learning learning algorithms. We use a Long ShortTerm Memory (LSTM) based network to learn to compute on-line updates of theparameters of another neural network. These parameters are stored in the cellstate of the LSTM. Our framework allows to compare learned algorithms tohand-made algorithms within the traditional train and test methodology. In anexperiment, we learn a learning algorithm for a one-hidden layer Multi-LayerPerceptron (MLP) on non-linearly separable datasets. The learned algorithm isable to update parameters of both layers and generalise well on similardatasets.",http://arxiv.org/abs/1610.06072v1,,
91,The Machine Learning Algorithm as Creative Musical Tool,"Machine learning is the capacity of a computational system to learnstructures from datasets in order to make predictions on newly seen data. Suchan approach offers a significant advantage in music scenarios in whichmusicians can teach the system to learn an idiosyncratic style, or can breakthe rules to explore the system's capacity in unexpected ways. In this chapterwe draw on music, machine learning, and human-computer interaction to elucidatean understanding of machine learning algorithms as creative tools for music andthe sonic arts. We motivate a new understanding of learning algorithms ashuman-computer interfaces. We show that, like other interfaces, learningalgorithms can be characterised by the ways their affordances intersect withgoals of human users. We also argue that the nature of interaction betweenusers and algorithms impacts the usability and usefulness of those algorithmsin profound ways. This human-centred view of machine learning motivates ourconcluding discussion of what it means to employ machine learning as a creativetool.",http://arxiv.org/abs/1611.00379v1,,
92,Machine Learning Automation Toolbox (MLaut),"In this paper we present MLaut (Machine Learning AUtomation Toolbox) for thepython data science ecosystem. MLaut automates large-scale evaluation andbenchmarking of machine learning algorithms on a large number of datasets.MLaut provides a high-level workflow interface to machine algorithm algorithms,implements a local back-end to a database of dataset collections, trainedalgorithms, and experimental results, and provides easy-to-use interfaces tothe scikit-learn and keras modelling libraries. Experiments are easy to set upwith default settings in a few lines of code, while remaining fullycustomizable to the level of hyper-parameter tuning, pipeline composition, ordeep learning architecture.  As a principal test case for MLaut, we conducted a large-scale supervisedclassification study in order to benchmark the performance of a number ofmachine learning algorithms - to our knowledge also the first larger-scalestudy on standard supervised learning data sets to include deep learningalgorithms. While corroborating a number of previous findings in literature, wefound (within the limitations of our study) that deep neural networks do notperform well on basic supervised learning, i.e., outside the more specialized,image-, audio-, or text-based tasks.",http://arxiv.org/abs/1901.03678v1,,
93,"Open Problems in Engineering Machine Learning Systems and the Quality  Model","Fatal accidents are a major issue hindering the wide acceptance ofsafety-critical systems that use machine learning and deep learning models,such as automated driving vehicles. To use machine learning in asafety-critical system, it is necessary to demonstrate the safety and securityof the system to society through the engineering process. However, there havebeen no such total concepts or frameworks established for these systems thathave been widely accepted, and needs or open problems are not organized in away researchers can select a theme and work on. The key to using a machinelearning model in a deductively engineered system, developed in a rigorousdevelopment lifecycle consisting of requirement, design, and verification, cf.V-Model, is decomposing the data-driven training of machine-learning modelsinto requirement, design, and verification, especially for machine learningmodels used in safety-critical systems. In this study, we identify, classify,and explore the open problems in engineering (safety-critical) machine learningsystems, i.e., requirement, design, and verification of machine learning modelsand systems, as well as related works and research directions, using automateddriving vehicles as an example. We also discuss the introduction ofmachine-learning models into a conventional system quality model such as SQuAREto study the quality model for machine learning systems.",http://arxiv.org/abs/1904.00001v1,,
94,A note on active learning for smooth problems,"We show that the disagreement coefficient of certain smooth hypothesisclasses is $O(m)$, where $m$ is the dimension of the hypothesis space, therebyanswering a question posed in \cite{friedman09}.",http://arxiv.org/abs/1103.3095v1,,
95,A Tutorial on Probabilistic Latent Semantic Analysis,"In this tutorial, I will discuss the details about how Probabilistic LatentSemantic Analysis (PLSA) is formalized and how different learning algorithmsare proposed to learn the model.",http://arxiv.org/abs/1212.3900v2,,
96,Falsifiable implies Learnable,"The paper demonstrates that falsifiability is fundamental to learning. Weprove the following theorem for statistical learning and sequential prediction:If a theory is falsifiable then it is learnable -- i.e. admits a strategy thatpredicts optimally. An analogous result is shown for universal induction.",http://arxiv.org/abs/1408.6618v1,,
97,"A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov  Models","A simple linear algebraic explanation of the algorithm in ""A SpectralAlgorithm for Learning Hidden Markov Models"" (COLT 2009). Most of the contentis in Figure 2; the text just makes everything precise in four nearly-trivialclaims.",http://arxiv.org/abs/1204.2477v1,,
98,Learning Reductions that Really Work,"We provide a summary of the mathematical and computational techniques thathave enabled learning reductions to effectively address a wide class ofproblems, and show that this approach to solving machine learning problems canbe broadly useful.",http://arxiv.org/abs/1502.02704v1,,
99,Structural query-by-committee,"In this work, we describe a framework that unifies many different interactivelearning tasks. We present a generalization of the {\it query-by-committee}active learning algorithm for this setting, and we study its consistency andrate of convergence, both theoretically and empirically, with and withoutnoise.",http://arxiv.org/abs/1803.06586v1,,
100,Automatic Rule Learning for Autonomous Driving Using Semantic Memory,"This paper presents a novel approach for automatic rule learning applicableto an autonomous driving system using real driving data.",http://arxiv.org/abs/1809.07904v2,,
101,Learning SMaLL Predictors,"We present a new machine learning technique for training smallresource-constrained predictors. Our algorithm, the Sparse MultiprototypeLinear Learner (SMaLL), is inspired by the classic machine learning problem oflearning $k$-DNF Boolean formulae. We present a formal derivation of ouralgorithm and demonstrate the benefits of our approach with a detailedempirical study.",http://arxiv.org/abs/1803.02388v1,,
102,Descriptions of Objectives and Processes of Mechanical Learning,"In [1], we introduced mechanical learning and proposed 2 approaches tomechanical learning. Here, we follow one such approach to well describe theobjects and the processes of learning. We discuss 2 kinds of patterns:objective and subjective pattern. Subjective pattern is crucial for learningmachine. We prove that for any objective pattern we can find a propersubjective pattern based upon least base patterns to express the objectivepattern well. X-form is algebraic expression for subjective pattern. Collectionof X-forms form internal representation space, which is center of learningmachine. We discuss learning by teaching and without teaching. We define datasufficiency by X-form. We then discussed some learning strategies. We show, ineach strategy, with sufficient data, and with certain capabilities, learningmachine indeed can learn any pattern (universal learning machine). In appendix,with knowledge of learning machine, we try to view deep learning from adifferent angle, i.e. its internal representation space and its learningdynamics.",http://arxiv.org/abs/1706.00066v1,,
103,Transductive Boltzmann Machines,"We present transductive Boltzmann machines (TBMs), which firstly achievetransductive learning of the Gibbs distribution. While exact learning of theGibbs distribution is impossible by the family of existing Boltzmann machinesdue to combinatorial explosion of the sample space, TBMs overcome the problemby adaptively constructing the minimum required sample space from data to avoidunnecessary generalization. We theoretically provide bias-variancedecomposition of the KL divergence in TBMs to analyze its learnability, andempirically demonstrate that TBMs are superior to the fully visible Boltzmannmachines and popularly used restricted Boltzmann machines in terms ofefficiency and effectiveness.",http://arxiv.org/abs/1805.07938v1,,
104,"Restricted Boltzmann Machine with Multivalued Hidden Variables: a model  suppressing over-fitting","Generalization is one of the most important issues in machine learningproblems. In this paper, we consider the generalization in restricted Boltzmannmachines. We propose a restricted Boltzmann machine with multivalued hiddenvariables, which is a simple extension of conventional restricted Boltzmannmachines. We demonstrate that our model is better than the conventional one vianumerical experiments: experiments for a contrastive divergence learning withartificial data and for a classification problem with MNIST.",http://arxiv.org/abs/1811.12587v1,,
105,Learning Feature Hierarchies with Centered Deep Boltzmann Machines,"Deep Boltzmann machines are in principle powerful models for extracting thehierarchical structure of data. Unfortunately, attempts to train layers jointly(without greedy layer-wise pretraining) have been largely unsuccessful. Wepropose a modification of the learning algorithm that initially recenters theoutput of the activation functions to zero. This modification leads to a betterconditioned Hessian and thus makes learning easier. We test the algorithm onreal data and demonstrate that our suggestion, the centered deep Boltzmannmachine, learns a hierarchy of increasingly abstract representations and abetter generative model of data.",http://arxiv.org/abs/1203.3783v1,,
106,Sliced Wasserstein Kernels for Probability Distributions,"Optimal transport distances, otherwise known as Wasserstein distances, haverecently drawn ample attention in computer vision and machine learning as apowerful discrepancy measure for probability distributions. The recentdevelopments on alternative formulations of the optimal transport have allowedfor faster solutions to the problem and has revamped its practical applicationsin machine learning. In this paper, we exploit the widely used kernel methodsand provide a family of provably positive definite kernels based on the SlicedWasserstein distance and demonstrate the benefits of these kernels in a varietyof learning tasks. Our work provides a new perspective on the application ofoptimal transport flavored distances through kernel methods in machine learningtasks.",http://arxiv.org/abs/1511.03198v1,,
107,Unifying distillation and privileged information,"Distillation (Hinton et al., 2015) and privileged information (Vapnik &Izmailov, 2015) are two techniques that enable machines to learn from othermachines. This paper unifies these two techniques into generalizeddistillation, a framework to learn from multiple machines and datarepresentations. We provide theoretical and causal insight about the innerworkings of generalized distillation, extend it to unsupervised, semisupervisedand multitask learning scenarios, and illustrate its efficacy on a variety ofnumerical simulations on both synthetic and real-world data.",http://arxiv.org/abs/1511.03643v3,,
108,Hyperparameter Search in Machine Learning,"We introduce the hyperparameter search problem in the field of machinelearning and discuss its main challenges from an optimization perspective.Machine learning methods attempt to build models that capture some element ofinterest based on given data. Most common learning algorithms feature a set ofhyperparameters that must be determined before training commences. The choiceof hyperparameters can significantly affect the resulting model's performance,but determining good values can be complex; hence a disciplined, theoreticallysound search strategy is essential.",http://arxiv.org/abs/1502.02127v2,,
109,Machine Learning for Machine Data from a CATI Network,"This is a machine learning application paper involving big data. We presenthigh-accuracy prediction methods of rare events in semi-structured machine logfiles, which are produced at high velocity and high volume by NORC'scomputer-assisted telephone interviewing (CATI) network for conducting surveys.We judiciously apply natural language processing (NLP) techniques anddata-mining strategies to train effective learning and prediction models forclassifying uncommon error messages in the log---without access to source code,updated documentation or dictionaries. In particular, our simple but effectiveapproach of features preallocation for learning from imbalanced data coupledwith naive Bayes classifiers can be conceivably generalized to supervised orsemi-supervised learning and prediction methods for other critical events suchas cyberattack detection.",http://arxiv.org/abs/1510.00772v1,,
110,Machine learning in protein engineering,"Machine learning-guided protein engineering is a new paradigm that enablesthe optimization of complex protein functions. Machine-learning methods usedata to predict protein function without requiring a detailed model of theunderlying physics or biological pathways. They accelerate protein engineeringby learning from information contained in all measured variants and using it toselect variants that are likely to be improved. In this review, we introducethe steps required to collect protein data, train machine-learning models, anduse trained models to guide engineering. We make recommendations at each stageand look to future opportunities for machine learning to enable the discoveryof new protein functions and uncover the relationship between protein sequenceand function.",http://arxiv.org/abs/1811.10775v1,,
111,"Learning Discriminative Features using Encoder-Decoder type Deep Neural  Nets","As machine learning is applied to an increasing variety of complex problems,which are defined by high dimensional and complex data sets, the necessity fortask oriented feature learning grows in importance. With the advancement ofDeep Learning algorithms, various successful feature learning techniques haveevolved. In this paper, we present a novel way of learning discriminativefeatures by training Deep Neural Nets which have Encoder or Decoder typearchitecture similar to an Autoencoder. We demonstrate that our approach canlearn discriminative features which can perform better at patternclassification tasks when the number of training samples is relatively small insize.",http://arxiv.org/abs/1607.01354v1,,
112,Deep Learning for Sentiment Analysis : A Survey,"Deep learning has emerged as a powerful machine learning technique thatlearns multiple layers of representations or features of the data and producesstate-of-the-art prediction results. Along with the success of deep learning inmany other application domains, deep learning is also popularly used insentiment analysis in recent years. This paper first gives an overview of deeplearning and then provides a comprehensive survey of its current applicationsin sentiment analysis.",http://arxiv.org/abs/1801.07883v2,,
113,"SeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural  Networks by Selective Network Augmentation","Lifelong learning aims to develop machine learning systems that can learn newtasks while preserving the performance on previous learned tasks. In this paperwe present a method to overcome catastrophic forgetting on convolutional neuralnetworks, that learns new tasks and preserves the performance on old taskswithout accessing the data of the original model, by selective networkaugmentation. The experiment results showed that SeNA-CNN, in some scenarios,outperforms the state-of-art Learning without Forgetting algorithm. Resultsalso showed that in some situations it is better to use SeNA-CNN instead oftraining a neural network using isolated learning.",http://arxiv.org/abs/1802.08250v2,,
114,An efficient quantum algorithm for generative machine learning,"A central task in the field of quantum computing is to find applicationswhere quantum computer could provide exponential speedup over any classicalcomputer. Machine learning represents an important field with broadapplications where quantum computer may offer significant speedup. Severalquantum algorithms for discriminative machine learning have been found based onefficient solving of linear algebraic problems, with potential exponentialspeedup in runtime under the assumption of effective input from a quantumrandom access memory. In machine learning, generative models represent anotherlarge class which is widely used for both supervised and unsupervised learning.Here, we propose an efficient quantum algorithm for machine learning based on aquantum generative model. We prove that our proposed model is exponentiallymore powerful to represent probability distributions compared with classicalgenerative models and has exponential speedup in training and inference atleast for some instances under a reasonable assumption in computationalcomplexity theory. Our result opens a new direction for quantum machinelearning and offers a remarkable example in which a quantum algorithm showsexponential improvement over any classical algorithm in an importantapplication field.",http://arxiv.org/abs/1711.02038v1,,
115,Stealing Hyperparameters in Machine Learning,"Hyperparameters are critical in machine learning, as differenthyperparameters often result in models with significantly differentperformance. Hyperparameters may be deemed confidential because of theircommercial value and the confidentiality of the proprietary algorithms that thelearner uses to learn them. In this work, we propose attacks on stealing thehyperparameters that are learned by a learner. We call our attackshyperparameter stealing attacks. Our attacks are applicable to a variety ofpopular machine learning algorithms such as ridge regression, logisticregression, support vector machine, and neural network. We evaluate theeffectiveness of our attacks both theoretically and empirically. For instance,we evaluate our attacks on Amazon Machine Learning. Our results demonstratethat our attacks can accurately steal hyperparameters. We also studycountermeasures. Our results highlight the need for new defenses against ourhyperparameter stealing attacks for certain machine learning algorithms.",http://arxiv.org/abs/1802.05351v2,,
116,"Machine Learning for Public Administration Research, with Application to  Organizational Reputation","Machine learning methods have gained a great deal of popularity in recentyears among public administration scholars and practitioners. These techniquesopen the door to the analysis of text, image and other types of data that allowus to test foundational theories of public administration and to develop newtheories. Despite the excitement surrounding machine learning methods, clarityregarding their proper use and potential pitfalls is lacking. This paperattempts to fill this gap in the literature through providing a machinelearning ""guide to practice"" for public administration scholars andpractitioners. Here, we take a foundational view of machine learning anddescribe how these methods can enrich public administration research andpractice through their ability develop new measures, tap into new sources ofdata and conduct statistical inference and causal inference in a principledmanner. We then turn our attention to the pitfalls of using these methods suchas unvalidated measures and lack of interpretability. Finally, we demonstratehow machine learning techniques can help us learn about organizationalreputation in federal agencies through an illustrated example using tweets from13 executive federal agencies.",http://arxiv.org/abs/1805.05409v2,,
117,"A Survey on Data Collection for Machine Learning: a Big Data - AI  Integration Perspective","Data collection is a major bottleneck in machine learning and an activeresearch topic in multiple communities. There are largely two reasons datacollection has recently become a critical issue. First, as machine learning isbecoming more widely-used, we are seeing new applications that do notnecessarily have enough labeled data. Second, unlike traditional machinelearning where feature engineering is the bottleneck, deep learning techniquesautomatically generate features, but instead require large amounts of labeleddata. Interestingly, recent research in data collection comes not only from themachine learning, natural language, and computer vision communities, but alsofrom the data management community due to the importance of handling largeamounts of data. In this survey, we perform a comprehensive study of datacollection from a data management point of view. Data collection largelyconsists of data acquisition, data labeling, and improvement of existing dataor models. We provide a research landscape of these operations, provideguidelines on which technique to use when, and identify interesting researchchallenges. The integration of machine learning and data management for datacollection is part of a larger trend of Big data and Artificial Intelligence(AI) integration and opens many opportunities for new research.",http://arxiv.org/abs/1811.03402v1,,
118,"Open Problems in Engineering and Quality Assurance of Safety Critical  Machine Learning Systems","Fatal accidents are a major issue hindering the wide acceptance ofsafety-critical systems using machine-learning and deep-learning models, suchas automated-driving vehicles. Quality assurance frameworks are required forsuch machine learning systems, but there are no widely accepted and establishedquality-assurance concepts and techniques. At the same time, open problems andthe relevant technical fields are not organized. To establish standard qualityassurance frameworks, it is necessary to visualize and organize these openproblems in an interdisciplinary way, so that the experts from many differenttechnical fields may discuss these problems in depth and develop solutions. Inthe present study, we identify, classify, and explore the open problems inquality assurance of safety-critical machine-learning systems, and theirrelevant corresponding industry and technological trends, usingautomated-driving vehicles as an example. Our results show that addressingthese open problems requires incorporating knowledge from several differenttechnological and industrial fields, including the automobile industry,statistics, software engineering, and machine learning.",http://arxiv.org/abs/1812.03057v1,,
119,Machine Learning for the Geosciences: Challenges and Opportunities,"Geosciences is a field of great societal relevance that requires solutions toseveral urgent problems facing our humanity and the planet. As geosciencesenters the era of big data, machine learning (ML) -- that has been widelysuccessful in commercial domains -- offers immense potential to contribute toproblems in geosciences. However, problems in geosciences have several uniquechallenges that are seldom found in traditional applications, requiring novelproblem formulations and methodologies in machine learning. This articleintroduces researchers in the machine learning (ML) community to thesechallenges offered by geoscience problems and the opportunities that exist foradvancing both machine learning and geosciences. We first highlight typicalsources of geoscience data and describe their properties that make itchallenging to use traditional machine learning techniques. We then describesome of the common categories of geoscience problems where machine learning canplay a role, and discuss some of the existing efforts and promising directionsfor methodological development in machine learning. We conclude by discussingsome of the emerging research themes in machine learning that are applicableacross all problems in the geosciences, and the importance of a deepcollaboration between machine learning and geosciences for synergisticadvancements in both disciplines.",http://arxiv.org/abs/1711.04708v1,,
120,"Modeling Stated Preference for Mobility-on-Demand Transit: A Comparison  of Machine Learning and Logit Models","Logit models are usually applied when studying individual travel behavior,i.e., to predict travel mode choice and to gain behavioral insights on travelerpreferences. Recently, some studies have applied machine learning to modeltravel mode choice and reported higher out-of-sample predictive accuracy thantraditional logit models (e.g., multinomial logit). However, little researchfocuses on comparing the interpretability of machine learning with logitmodels. In other words, how to draw behavioral insights from thehigh-performance ""black-box"" machine-learning models remains largely unsolvedin the field of travel behavior modeling.  This paper aims at providing a comprehensive comparison between the twoapproaches by examining the key similarities and differences in modeldevelopment, evaluation, and behavioral interpretation between logit andmachine-learning models for travel mode choice modeling. To complement thetheoretical discussions, the paper also empirically evaluates the twoapproaches on the stated-preference survey data for a new type of transitsystem integrating high-frequency fixed-route services and ridesourcing. Theresults show that machine learning can produce significantly higher predictiveaccuracy than logit models. Moreover, machine learning and logit models largelyagree on many aspects of behavioral interpretations. In addition, machinelearning can automatically capture the nonlinear relationship between the inputfeatures and choice outcomes. The paper concludes that there is great potentialin merging ideas from machine learning and conventional statistical methods todevelop refined models for travel behavior research and suggests some newresearch directions.",http://arxiv.org/abs/1811.01315v2,,
121,Multi-task Learning for Continuous Control,"Reliable and effective multi-task learning is a prerequisite for thedevelopment of robotic agents that can quickly learn to accomplish related,everyday tasks. However, in the reinforcement learning domain, multi-tasklearning has not exhibited the same level of success as in other domains, suchas computer vision. In addition, most reinforcement learning research onmulti-task learning has been focused on discrete action spaces, which are notused for robotic control in the real-world. In this work, we apply multi-tasklearning methods to continuous action spaces and benchmark their performance ona series of simulated continuous control tasks. Most notably, we show thatmulti-task learning outperforms our baselines and alternative knowledge sharingmethods.",http://arxiv.org/abs/1802.01034v1,,
122,Machine Learning with World Knowledge: The Position and Survey,"Machine learning has become pervasive in multiple domains, impacting a widevariety of applications, such as knowledge discovery and data mining, naturallanguage processing, information retrieval, computer vision, social and healthinformatics, ubiquitous computing, etc. Two essential problems of machinelearning are how to generate features and how to acquire labels for machines tolearn. Particularly, labeling large amount of data for each domain-specificproblem can be very time consuming and costly. It has become a key obstacle inmaking learning protocols realistic in applications. In this paper, we willdiscuss how to use the existing general-purpose world knowledge to enhancemachine learning processes, by enriching the features or reducing the labelingwork. We start from the comparison of world knowledge with domain-specificknowledge, and then introduce three key problems in using world knowledge inlearning processes, i.e., explicit and implicit feature representation,inference for knowledge linking and disambiguation, and learning with direct orindirect supervision. Finally we discuss the future directions of this researchtopic.",http://arxiv.org/abs/1705.02908v1,,
123,Generative Adversarial Learning for Spectrum Sensing,"A novel approach of training data augmentation and domain adaptation ispresented to support machine learning applications for cognitive radio. Machinelearning provides effective tools to automate cognitive radio functionalitiesby reliably extracting and learning intrinsic spectrum dynamics. However, thereare two important challenges to overcome, in order to fully utilize the machinelearning benefits with cognitive radios. First, machine learning requiressignificant amount of truthed data to capture complex channel and emittercharacteristics, and train the underlying algorithm (e.g., a classifier).Second, the training data that has been identified for one spectrum environmentcannot be used for another one (e.g., after channel and emitter conditionschange). To address these challenges, a generative adversarial network (GAN)with deep learning structures is used to 1)~generate additional synthetictraining data to improve classifier accuracy, and 2) adapt training data tospectrum dynamics. This approach is applied to spectrum sensing by assumingonly limited training data without knowledge of spectrum statistics. Machinelearning classifiers are trained with limited, augmented and adapted trainingdata to detect signals. Results show that training data augmentation increasesthe classifier accuracy significantly and this increase is sustained withdomain adaptation as spectrum conditions change.",http://arxiv.org/abs/1804.00709v1,,
124,A review on Neural Turing Machine,"One of the major objectives of Artificial Intelligence is to design learningalgorithms that are executed on a general purposes computational machines suchas human brain. Neural Turing Machine (NTM) is a step towards realizing such acomputational machine. The attempt is made here to run a systematic review onNeural Turing Machine. First, the mind-map and taxonomy of machine learning,neural networks, and Turing machine are introduced. Next, NTM is inspected interms of concepts, structure, variety of versions, implemented tasks,comparisons, etc. Finally, the paper discusses on issues and ends up withseveral future works.",http://arxiv.org/abs/1904.05061v1,,
125,Temporal Autoencoding Restricted Boltzmann Machine,"Much work has been done refining and characterizing the receptive fieldslearned by deep learning algorithms. A lot of this work has focused on thedevelopment of Gabor-like filters learned when enforcing sparsity constraintson a natural image dataset. Little work however has investigated how thesefilters might expand to the temporal domain, namely through training on naturalmovies. Here we investigate exactly this problem in established temporal deeplearning algorithms as well as a new learning paradigm suggested here, theTemporal Autoencoding Restricted Boltzmann Machine (TARBM).",http://arxiv.org/abs/1210.8353v1,,
126,A new approach in machine learning,"In this technical report we presented a novel approach to machine learning.Once the new framework is presented, we will provide a simple and yet verypowerful learning algorithm which will be benchmark on various dataset.  The framework we proposed is based on booleen circuits; more specifically theclassifier produced by our algorithm have that form. Using bits and booleangates instead of real numbers and multiplication enable the the learningalgorithm and classifier to use very efficient boolean vector operations. Thisenable both the learning algorithm and classifier to be extremely efficient.The accuracy of the classifier we obtain with our framework compares veryfavorably those produced by conventional techniques, both in terms ofefficiency and accuracy.",http://arxiv.org/abs/1409.4044v1,,
127,IoT Security Techniques Based on Machine Learning,"Internet of things (IoT) that integrate a variety of devices into networks toprovide advanced and intelligent services have to protect user privacy andaddress attacks such as spoofing attacks, denial of service attacks, jammingand eavesdropping. In this article, we investigate the attack model for IoTsystems, and review the IoT security solutions based on machine learningtechniques including supervised learning, unsupervised learning andreinforcement learning. We focus on the machine learning based IoTauthentication, access control, secure offloading and malware detection schemesto protect data privacy. In this article, we discuss the challenges that needto be addressed to implement these machine learning based security schemes inpractical IoT systems.",http://arxiv.org/abs/1801.06275v1,,
128,Continual Learning in Practice,"This paper describes a reference architecture for self-maintaining systemsthat can learn continually, as data arrives. In environments where dataevolves, we need architectures that manage Machine Learning (ML) models inproduction, adapt to shifting data distributions, cope with outliers, retrainwhen necessary, and adapt to new tasks. This represents continual AutoML orAutomatically Adaptive Machine Learning. We describe the challenges andproposes a reference architecture.",http://arxiv.org/abs/1903.05202v2,,
129,Transformative Machine Learning,"The key to success in machine learning (ML) is the use of effective datarepresentations. Traditionally, data representations were hand-crafted.Recently it has been demonstrated that, given sufficient data, deep neuralnetworks can learn effective implicit representations from simple inputrepresentations. However, for most scientific problems, the use of deeplearning is not appropriate as the amount of available data is limited, and/orthe output models must be explainable. Nevertheless, many scientific problemsdo have significant amounts of data available on related tasks, which makesthem amenable to multi-task learning, i.e. learning many related problemssimultaneously. Here we propose a novel and general representation learningapproach for multi-task learning that works successfully with small amounts ofdata. The fundamental new idea is to transform an input intrinsic datarepresentation (i.e., handcrafted features), to an extrinsic representationbased on what a pre-trained set of models predict about the examples. Thistransformation has the dual advantages of producing significantly more accuratepredictions, and providing explainable models. To demonstrate the utility ofthis transformative learning approach, we have applied it to three real-worldscientific problems: drug-design (quantitative structure activity relationshiplearning), predicting human gene expression (across different tissue types anddrug treatments), and meta-learning for machine learning (predicting whichmachine learning methods work best for a given problem). In all three problems,transformative machine learning significantly outperforms the best intrinsicrepresentation.",http://arxiv.org/abs/1811.03392v1,,
130,"Meta-QSAR: a large-scale application of meta-learning to drug design and  discovery","We investigate the learning of quantitative structure activity relationships(QSARs) as a case-study of meta-learning. This application area is of thehighest societal importance, as it is a key step in the development of newmedicines. The standard QSAR learning problem is: given a target (usually aprotein) and a set of chemical compounds (small molecules) with associatedbioactivities (e.g. inhibition of the target), learn a predictive mapping frommolecular representation to activity. Although almost every type of machinelearning method has been applied to QSAR learning there is no agreed singlebest way of learning QSARs, and therefore the problem area is well-suited tometa-learning. We first carried out the most comprehensive ever comparison ofmachine learning methods for QSAR learning: 18 regression methods, 6 molecularrepresentations, applied to more than 2,700 QSAR problems. (These results havebeen made publicly available on OpenML and represent a valuable resource fortesting novel meta-learning methods.) We then investigated the utility ofalgorithm selection for QSAR problems. We found that this meta-learningapproach outperformed the best individual QSAR learning method (random forestsusing a molecular fingerprint representation) by up to 13%, on average. Weconclude that meta-learning outperforms base-learning methods for QSARlearning, and as this investigation is one of the most extensive evercomparisons of base and meta-learning methods ever made, it provides evidencefor the general effectiveness of meta-learning over base-learning.",http://arxiv.org/abs/1709.03854v1,,
131,Deep Bayesian Active Learning with Image Data,"Even though active learning forms an important pillar of machine learning,deep learning tools are not prevalent within it. Deep learning poses severaldifficulties when used in an active learning setting. First, active learning(AL) methods generally rely on being able to learn and update models from smallamounts of data. Recent advances in deep learning, on the other hand, arenotorious for their dependence on large amounts of data. Second, many ALacquisition functions rely on model uncertainty, yet deep learning methodsrarely represent such model uncertainty. In this paper we combine recentadvances in Bayesian deep learning into the active learning framework in apractical way. We develop an active learning framework for high dimensionaldata, a task which has been extremely challenging so far, with very sparseexisting literature. Taking advantage of specialised models such as Bayesianconvolutional neural networks, we demonstrate our active learning techniqueswith image data, obtaining a significant improvement on existing activelearning approaches. We demonstrate this on both the MNIST dataset, as well asfor skin cancer diagnosis from lesion images (ISIC2016 task).",http://arxiv.org/abs/1703.02910v1,,
132,"Machine Learning of Phonologically Conditioned Noun Declensions For  Tamil Morphological Generators","This paper presents machine learning solutions to a practical problem ofNatural Language Generation (NLG), particularly the word formation inagglutinative languages like Tamil, in a supervised manner. The morphologicalgenerator is an important component of Natural Language Processing inArtificial Intelligence. It generates word forms given a root and affixes. Themorphophonemic changes like addition, deletion, alternation etc., occur whentwo or more morphemes or words joined together. The Sandhi rules should beexplicitly specified in the rule based morphological analyzers and generators.In machine learning framework, these rules can be learned automatically by thesystem from the training samples and subsequently be applied for new inputs. Inthis paper we proposed the machine learning models which learn themorphophonemic rules for noun declensions from the given training data. Thesemodels are trained to learn sandhi rules using various learning algorithms andthe performance of those algorithms are presented. From this we conclude thatmachine learning of morphological processing such as word form generation canbe successfully learned in a supervised manner, without explicit description ofrules. The performance of Decision trees and Bayesian machine learningalgorithms on noun declensions are discussed.",http://arxiv.org/abs/1402.3382v1,,
133,Universal Learning of Repeated Matrix Games,"We study and compare the learning dynamics of two universal learningalgorithms, one based on Bayesian learning and the other on prediction withexpert advice. Both approaches have strong asymptotic performance guarantees.When confronted with the task of finding good long-term strategies in repeated2x2 matrix games, they behave quite differently.",http://arxiv.org/abs/cs/0508073v1,,
134,Learning in Riemannian Orbifolds,"Learning in Riemannian orbifolds is motivated by existing machine learningalgorithms that directly operate on finite combinatorial structures such aspoint patterns, trees, and graphs. These methods, however, lack statisticaljustification. This contribution derives consistency results for learningproblems in structured domains and thereby generalizes learning in vectorspaces and manifolds.",http://arxiv.org/abs/1204.4294v1,,
135,Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE,"We propose a number of new algorithms for learning deep energy models anddemonstrate their properties. We show that our SteinCD performs well in term oftest likelihood, while SteinGAN performs well in terms of generating realisticlooking images. Our results suggest promising directions for learning bettermodels by combining GAN-style methods with traditional energy-based learning.",http://arxiv.org/abs/1707.00797v1,,
136,Machine Teaching: A New Paradigm for Building Machine Learning Systems,"The current processes for building machine learning systems requirepractitioners with deep knowledge of machine learning. This significantlylimits the number of machine learning systems that can be created and has ledto a mismatch between the demand for machine learning systems and the abilityfor organizations to build them. We believe that in order to meet this growingdemand for machine learning systems we must significantly increase the numberof individuals that can teach machines. We postulate that we can achieve thisgoal by making the process of teaching machines easy, fast and above all,universally accessible.  While machine learning focuses on creating new algorithms and improving theaccuracy of ""learners"", the machine teaching discipline focuses on the efficacyof the ""teachers"". Machine teaching as a discipline is a paradigm shift thatfollows and extends principles of software engineering and programminglanguages. We put a strong emphasis on the teacher and the teacher'sinteraction with data, as well as crucial components such as techniques anddesign principles of interaction and visualization.  In this paper, we present our position regarding the discipline of machineteaching and articulate fundamental machine teaching principles. We alsodescribe how, by decoupling knowledge about machine learning algorithms fromthe process of teaching, we can accelerate innovation and empower millions ofnew uses for machine learning models.",http://arxiv.org/abs/1707.06742v3,,
137,"Evaluating Conditional Cash Transfer Policies with Machine Learning  Methods","This paper presents an out-of-sample prediction comparison between majormachine learning models and the structural econometric model. Over the pastdecade, machine learning has established itself as a powerful tool in manyprediction applications, but this approach is still not widely adopted inempirical economic studies. To evaluate the benefits of this approach, I usethe most common machine learning algorithms, CART, C4.5, LASSO, random forest,and adaboost, to construct prediction models for a cash transfer experimentconducted by the Progresa program in Mexico, and I compare the predictionresults with those of a previous structural econometric study. Two predictiontasks are performed in this paper: the out-of-sample forecast and the long-termwithin-sample simulation. For the out-of-sample forecast, both the meanabsolute error and the root mean square error of the school attendance ratesfound by all machine learning models are smaller than those found by thestructural model. Random forest and adaboost have the highest accuracy for theindividual outcomes of all subgroups. For the long-term within-samplesimulation, the structural model has better performance than do all of themachine learning models. The poor within-sample fitness of the machine learningmodel results from the inaccuracy of the income and pregnancy predictionmodels. The result shows that the machine learning model performs better thandoes the structural model when there are many data to learn; however, when thedata are limited, the structural model offers a more sensible prediction. Thefindings of this paper show promise for adopting machine learning in economicpolicy analyses in the era of big data.",http://arxiv.org/abs/1803.06401v1,,
138,A Kernel for Hierarchical Parameter Spaces,"We define a family of kernels for mixed continuous/discrete hierarchicalparameter spaces and show that they are positive definite.",http://arxiv.org/abs/1310.5738v1,,
139,Indexing Cost Sensitive Prediction,"Predictive models are often used for real-time decision making. However,typical machine learning techniques ignore feature evaluation cost, and focussolely on the accuracy of the machine learning models obtained utilizing allthe features available. We develop algorithms and indexes to supportcost-sensitive prediction, i.e., making decisions using machine learning modelstaking feature evaluation cost into account. Given an item and a onlinecomputation cost (i.e., time) budget, we present two approaches to return anappropriately chosen machine learning model that will run within the specifiedtime on the given item. The first approach returns the optimal machine learningmodel, i.e., one with the highest accuracy, that runs within the specifiedtime, but requires significant up-front precomputation time. The secondapproach returns a possibly sub- optimal machine learning model, but requireslittle up-front precomputation time. We study these two algorithms in detailand characterize the scenarios (using real and synthetic data) in which eachperforms well. Unlike prior work that focuses on a narrow domain or a specificalgorithm, our techniques are very general: they apply to any cost-sensitiveprediction scenario on any machine learning algorithm.",http://arxiv.org/abs/1408.4072v1,,
140,BigDB: Automatic Machine Learning Optimizer,"In this short vision paper, we introduce a machine learning optimizer fordata management and describe its architecture and main functionality.",http://arxiv.org/abs/1301.1575v1,,
141,Piecewise Linear Multilayer Perceptrons and Dropout,"We propose a new type of hidden layer for a multilayer perceptron, anddemonstrate that it obtains the best reported performance for an MLP on theMNIST dataset.",http://arxiv.org/abs/1301.5088v1,,
142,"Identifying and Harnessing the Building Blocks of Machine Learning  Pipelines for Sensible Initialization of a Data Science Automation Tool","As data science continues to grow in popularity, there will be an increasingneed to make data science tools more scalable, flexible, and accessible. Inparticular, automated machine learning (AutoML) systems seek to automate theprocess of designing and optimizing machine learning pipelines. In thischapter, we present a genetic programming-based AutoML system called TPOT thatoptimizes a series of feature preprocessors and machine learning models withthe goal of maximizing classification accuracy on a supervised classificationproblem. Further, we analyze a large database of pipelines that were previouslyused to solve various supervised classification problems and identify 100 shortseries of machine learning operations that appear the most frequently, which wecall the building blocks of machine learning pipelines. We harness thesebuilding blocks to initialize TPOT with promising solutions, and find that thissensible initialization method significantly improves TPOT's performance on onebenchmark at no cost of significantly degrading performance on the others.Thus, sensible initialization with machine learning pipeline building blocksshows promise for GP-based AutoML systems, and should be further refined infuture work.",http://arxiv.org/abs/1607.08878v1,,
143,A Note on Kaldi's PLDA Implementation,"Some explanations to Kaldi's PLDA implementation to make formula derivationeasier to catch.",http://arxiv.org/abs/1804.00403v1,,
144,"Exponential Convergence of the Deep Neural Network Approximation for  Analytic Functions","We prove that for analytic functions in low dimension, the convergence rateof the deep neural network approximation is exponential.",http://arxiv.org/abs/1807.00297v1,,
145,RuleMatrix: Visualizing and Understanding Classifiers with Rules,"With the growing adoption of machine learning techniques, there is a surge ofresearch interest towards making machine learning systems more transparent andinterpretable. Various visualizations have been developed to help modeldevelopers understand, diagnose, and refine machine learning models. However, alarge number of potential but neglected users are the domain experts withlittle knowledge of machine learning but are expected to work with machinelearning systems. In this paper, we present an interactive visualizationtechnique to help users with little expertise in machine learning tounderstand, explore and validate predictive models. By viewing the model as ablack box, we extract a standardized rule-based knowledge representation fromits input-output behavior. We design RuleMatrix, a matrix-based visualizationof rules to help users navigate and verify the rules and the black-box model.We evaluate the effectiveness of RuleMatrix via two use cases and a usabilitystudy.",http://arxiv.org/abs/1807.06228v1,,
146,"Application of Machine Learning in Rock Facies Classification with  Physics-Motivated Feature Augmentation","With recent progress in algorithms and the availability of massive amounts ofcomputation power, application of machine learning techniques is becoming a hottopic in the oil and gas industry. One of the most promising aspects to applymachine learning to the upstream field is the rock facies classification inreservoir characterization, which is crucial in determining the net paythickness of reservoirs, thus a definitive factor in drilling decision makingprocess. For complex machine learning tasks like facies classification, featureengineering is often critical. This paper shows the inclusion ofphysics-motivated feature interaction in feature augmentation can furtherimprove the capability of machine learning in rock facies classification. Wedemonstrate this approach with the SEG 2016 machine learning contest datasetand the top winning algorithms. The improvement is roboust and can be $\sim5\%$better than current existing best F-1 score, where F-1 is an evaluation metricused to quantify average prediction accuracy.",http://arxiv.org/abs/1808.09856v1,,
147,A conjugate prior for the Dirichlet distribution,"This note investigates a conjugate class for the Dirichlet distribution classin the exponential family.",http://arxiv.org/abs/1811.05266v1,,
148,A Framework for Implementing Machine Learning on Omics Data,"The potential benefits of applying machine learning methods to -omics dataare becoming increasingly apparent, especially in clinical settings. However,the unique characteristics of these data are not always well suited to machinelearning techniques. These data are often generated across differenttechnologies in different labs, and frequently with high dimensionality. Inthis paper we present a framework for combining -omics data sets, and forhandling high dimensional data, making -omics research more accessible tomachine learning applications. We demonstrate the success of this frameworkthrough integration and analysis of multi-analyte data for a set of 3,533breast cancers. We then use this data-set to predict breast cancer patientsurvival for individuals at risk of an impending event, with higher accuracyand lower variance than methods trained on individual data-sets. We hope thatour pipelines for data-set generation and transformation will open up -omicsdata to machine learning researchers. We have made these freely available fornoncommercial use at www.ccg.ai.",http://arxiv.org/abs/1811.10455v1,,
149,Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?,No.,http://arxiv.org/abs/1902.02322v1,,
150,Machine Learning: A Dark Side of Cancer Computing,"Cancer analysis and prediction is the utmost important research field forwell-being of humankind. The Cancer data are analyzed and predicted usingmachine learning algorithms. Most of the researcher claims the accuracy of thepredicted results within 99%. However, we show that machine learning algorithmscan easily predict with an accuracy of 100% on Wisconsin Diagnostic BreastCancer dataset. We show that the method of gaining accuracy is an unethicalapproach that we can easily mislead the algorithms. In this paper, we exploitthe weakness of Machine Learning algorithms. We perform extensive experimentsfor the correctness of our results to exploit the weakness of machine learningalgorithms. The methods are rigorously evaluated to validate our claim. Inaddition, this paper focuses on correctness of accuracy. This paper reportthree key outcomes of the experiments, namely, correctness of accuracies,significance of minimum accuracy, and correctness of machine learningalgorithms.",http://arxiv.org/abs/1903.07167v1,,
151,Expert-Augmented Machine Learning,"Machine Learning is proving invaluable across disciplines. However, itssuccess is often limited by the quality and quantity of available data, whileits adoption by the level of trust that models afford users. Human vs. machineperformance is commonly compared empirically to decide whether a certain taskshould be performed by a computer or an expert. In reality, the optimallearning strategy may involve combining the complementary strengths of man andmachine. Here we present Expert-Augmented Machine Learning (EAML), an automatedmethod that guides the extraction of expert knowledge and its integration intomachine-learned models. We use a large dataset of intensive care patient datato predict mortality and show that we can extract expert knowledge using anonline platform, help reveal hidden confounders, improve generalizability on adifferent population and learn using less data. EAML presents a novel frameworkfor high performance and dependable machine learning in critical applications.",http://arxiv.org/abs/1903.09731v2,,
152,Generalization in Machine Learning via Analytical Learning Theory,"This paper introduces a novel measure-theoretic theory for machine learningthat does not require statistical assumptions. Based on this theory, a newregularization method in deep learning is derived and shown to outperformprevious methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposedtheory provides a theoretical basis for a family of practically successfulregularization methods in deep learning. We discuss several consequences of ourresults on one-shot learning, representation learning, deep learning, andcurriculum learning. Unlike statistical learning theory, the proposed learningtheory analyzes each problem instance individually via measure theory, ratherthan a set of problem instances via statistics. As a result, it providesdifferent types of results and insights when compared to statistical learningtheory.",http://arxiv.org/abs/1802.07426v3,,
153,Recent Advances in Deep Learning: An Overview,"Deep Learning is one of the newest trends in Machine Learning and ArtificialIntelligence research. It is also one of the most popular scientific researchtrends now-a-days. Deep learning methods have brought revolutionary advances incomputer vision and machine learning. Every now and then, new and new deeplearning techniques are being born, outperforming state-of-the-art machinelearning and even existing deep learning techniques. In recent years, the worldhas seen many major breakthroughs in this field. Since deep learning isevolving at a huge speed, its kind of hard to keep track of the regularadvances especially for new researchers. In this paper, we are going to brieflydiscuss about recent advances in Deep Learning for past few years.",http://arxiv.org/abs/1807.08169v1,,
154,A Learning Algorithm based on High School Teaching Wisdom,"A learning algorithm based on primary school teaching and learning ispresented. The methodology is to continuously evaluate a student and to givethem training on the examples for which they repeatedly fail, until, they cancorrectly answer all types of questions. This incremental learning procedureproduces better learning curves by demanding the student to optimally dedicatetheir learning time on the failed examples. When used in machine learning, thealgorithm is found to train a machine on a data with maximum variance in thefeature space so that the generalization ability of the network improves. Thealgorithm has interesting applications in data mining, model evaluations andrare objects discovery.",http://arxiv.org/abs/1008.1643v2,,
155,Interactive Learning from Multiple Noisy Labels,"Interactive learning is a process in which a machine learning algorithm isprovided with meaningful, well-chosen examples as opposed to randomly chosenexamples typical in standard supervised learning. In this paper, we propose anew method for interactive learning from multiple noisy labels where we exploitthe disagreement among annotators to quantify the easiness (or meaningfulness)of an example. We demonstrate the usefulness of this method in estimating theparameters of a latent variable classification model, and conduct experimentalanalyses on a range of synthetic and benchmark datasets. Furthermore, wetheoretically analyze the performance of perceptron in this interactivelearning framework.",http://arxiv.org/abs/1607.06988v1,,
156,"Learning Stochastic Differential Equations With Gaussian Processes  Without Gradient Matching","We introduce a novel paradigm for learning non-parametric drift and diffusionfunctions for stochastic differential equation (SDE). The proposed model learnsto simulate path distributions that match observations with non-uniform timeincrements and arbitrary sparseness, which is in contrast with gradientmatching that does not optimize simulated responses. We formulate sensitivityequations for learning and demonstrate that our general stochastic distributionoptimisation leads to robust and efficient learning of SDE systems.",http://arxiv.org/abs/1807.05748v2,,
157,Interpretable Reinforcement Learning with Ensemble Methods,"We propose to use boosted regression trees as a way to computehuman-interpretable solutions to reinforcement learning problems. Boostingcombines several regression trees to improve their accuracy withoutsignificantly reducing their inherent interpretability. Prior work has focusedindependently on reinforcement learning and on interpretable machine learning,but there has been little progress in interpretable reinforcement learning. Ourexperimental results show that boosted regression trees compute solutions thatare both interpretable and match the quality of leading reinforcement learningmethods.",http://arxiv.org/abs/1809.06995v1,,
158,"Machine Learning with Guarantees using Descriptive Complexity and SMT  Solvers","Machine learning is a thriving part of computer science. There are manyefficient approaches to machine learning that do not provide strong theoreticalguarantees, and a beautiful general learning theory. Unfortunately, machinelearning approaches that give strong theoretical guarantees have not beenefficient enough to be applicable. In this paper we introduce a logicalapproach to machine learning. Models are represented by tuples of logicalformulas and inputs and outputs are logical structures. We present ourframework together with several applications where we evaluate it using SAT andSMT solvers. We argue that this approach to machine learning is particularlysuited to bridge the gap between efficiency and theoretical soundness. Weexploit results from descriptive complexity theory to prove strong theoreticalguarantees for our approach. To show its applicability, we present experimentalresults including learning complexity-theoretic reductions rules for boardgames. We also explain how neural networks fit into our framework, although thecurrent implementation does not scale to provide guarantees for real-worldneural networks.",http://arxiv.org/abs/1609.02664v1,,
159,Feature Importance Measure for Non-linear Learning Algorithms,"Complex problems may require sophisticated, non-linear learning methods suchas kernel machines or deep neural networks to achieve state of the artprediction accuracies. However, high prediction accuracies are not the onlyobjective to consider when solving problems using machine learning. Instead,particular scientific applications require some explanation of the learnedprediction function. Unfortunately, most methods do not come with out of thebox straight forward interpretation. Even linear prediction functions are notstraight forward to explain if features exhibit complex correlation structure.  In this paper, we propose the Measure of Feature Importance (MFI). MFI isgeneral and can be applied to any arbitrary learning machine (including kernelmachines and deep learning). MFI is intrinsically non-linear and can detectfeatures that by itself are inconspicuous and only impact the predictionfunction through their interaction with other features. Lastly, MFI can be usedfor both --- model-based feature importance and instance-based featureimportance (i.e, measuring the importance of a feature for a particular datapoint).",http://arxiv.org/abs/1611.07567v1,,
160,"LoAdaBoost:Loss-Based AdaBoost Federated Machine Learning on medical  Data","Medical data are valuable for improvement of health care, policy making andmany other purposes. Vast amount of medical data are stored in differentlocations, on many different devices and in different data silos. Sharingmedical data among different sources is a big challenge due to regulatory,operational and security reasons. One potential solution is federated machinelearning ,which is a method that sends machine learning algorithmssimultaneously to all data sources, train models in each source and aggregatesthe learned models. This strategy allows utilization of valuable data withoutmoving them.One challenge in applying federated machine learning is theheterogeneity of data from different sources. To tackle this problem, weproposed an adaptive boosting method that increases the efficiency of federatedmachine learning. Using intensive care unit data from hospital, we showed thatLoAdaBoost federated learning outperformed baseline method and increasedcommunication efficiency at negligible additional cost.",http://arxiv.org/abs/1811.12629v2,,
161,Safe and Efficient Screening For Sparse Support Vector Machine,"Screening is an effective technique for speeding up the training process of asparse learning model by removing the features that are guaranteed to beinactive the process. In this paper, we present a efficient screening techniquefor sparse support vector machine based on variational inequality. Thetechnique is both efficient and safe.",http://arxiv.org/abs/1310.8320v1,,
162,Evaluation of Machine Learning Techniques for Green Energy Prediction,"We evaluate the following Machine Learning techniques for Green Energy (Wind,Solar) Prediction: Bayesian Inference, Neural Networks, Support VectorMachines, Clustering techniques (PCA). Our objective is to predict green energyusing weather forecasts, predict deviations from forecast green energy, findcorrelation amongst different weather parameters and green energy availability,recover lost or missing energy (/ weather) data. We use historical weather dataand weather forecasts for the same.",http://arxiv.org/abs/1406.3726v1,,
163,Towards learning-to-learn,"In good old-fashioned artificial intelligence (GOFAI), humans specifiedsystems that solved problems. Much of the recent progress in AI has come fromreplacing human insights by learning. However, learning itself is still usuallybuilt by humans -- specifically the choice that parameter updates should followthe gradient of a cost function. Yet, in analogy with GOFAI, there is no reasonto believe that humans are particularly good at defining such learning systems:we may expect learning itself to be better if we learn it. Recent research inmachine learning has started to realize the benefits of that strategy. Weshould thus expect this to be relevant for neuroscience: how could the correctlearning rules be acquired? Indeed, cognitive science has long shown thathumans learn-to-learn, which is potentially responsible for their impressivelearning abilities. Here we discuss ideas across machine learning,neuroscience, and cognitive science that matter for the principle oflearning-to-learn.",http://arxiv.org/abs/1811.00231v3,,
164,Kernel Machines With Missing Responses,"Missing responses is a missing data format in which outcomes are not alwaysobserved. In this work we develop kernel machines that can handle missingresponses. First, we propose a kernel machine family that uses mainly thecomplete cases. For the quadratic loss, we then propose a family ofdoubly-robust kernel machines. The proposed kernel-machine estimators can beapplied to both regression and classification problems. We prove oracleinequalities for the finite-sample differences between the kernel machine riskand Bayes risk. We use these oracle inequalities to prove consistency and tocalculate convergence rates. We demonstrate the performance of the two proposedkernel machine families using both a simulation study and a real-world dataanalysis.",http://arxiv.org/abs/1806.02865v1,,
165,"Activized Learning: Transforming Passive to Active with Improved Label  Complexity","We study the theoretical advantages of active learning over passive learning.Specifically, we prove that, in noise-free classifier learning for VC classes,any passive learning algorithm can be transformed into an active learningalgorithm with asymptotically strictly superior label complexity for allnontrivial target functions and distributions. We further provide a generalcharacterization of the magnitudes of these improvements in terms of a novelgeneralization of the disagreement coefficient. We also extend these results toactive learning in the presence of label noise, and find that even under broadclasses of noise distributions, we can typically guarantee strict improvementsover the known results for passive learning.",http://arxiv.org/abs/1108.1766v1,,
166,Continual Classification Learning Using Generative Models,"Continual learning is the ability to sequentially learn over time byaccommodating knowledge while retaining previously learned experiences. Neuralnetworks can learn multiple tasks when trained on them jointly, but cannotmaintain performance on previously learned tasks when tasks are presented oneat a time. This problem is called catastrophic forgetting. In this work, wepropose a classification model that learns continuously from sequentiallyobserved tasks, while preventing catastrophic forgetting. We build on thelifelong generative capabilities of [10] and extend it to the classificationsetting by deriving a new variational bound on the joint log likelihood, $\logp(x; y)$.",http://arxiv.org/abs/1810.10612v1,,
167,Considerations upon the Machine Learning Technologies,"Artificial intelligence offers superior techniques and methods by whichproblems from diverse domains may find an optimal solution. The MachineLearning technologies refer to the domain of artificial intelligence aiming todevelop the techniques allowing the computers to ""learn"". Some systems based onMachine Learning technologies tend to eliminate the necessity of the humanintelligence while the others adopt a man-machine collaborative approach.",http://arxiv.org/abs/0904.3667v1,,
168,Machine Learning: When and Where the Horses Went Astray?,"Machine Learning is usually defined as a subfield of AI, which is busy withinformation extraction from raw data sets. Despite of its common acceptance andwidespread recognition, this definition is wrong and groundless. Meaningfulinformation does not belong to the data that bear it. It belongs to theobservers of the data and it is a shared agreement and a convention among them.Therefore, this private information cannot be extracted from the data by anymeans. Therefore, all further attempts of Machine Learning apologists tojustify their funny business are inappropriate.",http://arxiv.org/abs/0911.1386v1,,
169,mlpy: Machine Learning Python,"mlpy is a Python Open Source Machine Learning library built on top ofNumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range ofstate-of-the-art machine learning methods for supervised and unsupervisedproblems and it is aimed at finding a reasonable compromise among modularity,maintainability, reproducibility, usability and efficiency. mlpy ismultiplatform, it works with Python 2 and 3 and it is distributed under GPL3 atthe website http://mlpy.fbk.eu.",http://arxiv.org/abs/1202.6548v2,,
170,Pylearn2: a machine learning research library,"Pylearn2 is a machine learning research library. This does not just mean thatit is a collection of machine learning algorithms that share a common API; itmeans that it has been designed for flexibility and extensibility in order tofacilitate research projects that involve new or unusual use cases. In thispaper we give a brief history of the library, an overview of its basicphilosophy, a summary of the library's architecture, and a description of howthe Pylearn2 community functions socially.",http://arxiv.org/abs/1308.4214v1,,
171,"Machine Learning for E-mail Spam Filtering: Review,Techniques and Trends","We present a comprehensive review of the most effective content-based e-mailspam filtering techniques. We focus primarily on Machine Learning-based spamfilters and their variants, and report on a broad review ranging from surveyingthe relevant ideas, efforts, effectiveness, and the current progress. Theinitial exposition of the background examines the basics of e-mail spamfiltering, the evolving nature of spam, spammers playing cat-and-mouse withe-mail service providers (ESPs), and the Machine Learning front in fightingspam. We conclude by measuring the impact of Machine Learning-based filters andexplore the promising offshoots of latest developments.",http://arxiv.org/abs/1606.01042v1,,
172,Using Visual Analytics to Interpret Predictive Machine Learning Models,"It is commonly believed that increasing the interpretability of a machinelearning model may decrease its predictive power. However, inspectinginput-output relationships of those models using visual analytics, whiletreating them as black-box, can help to understand the reasoning behindoutcomes without sacrificing predictive quality. We identify a space ofpossible solutions and provide two examples of where such techniques have beensuccessfully used in practice.",http://arxiv.org/abs/1606.05685v2,,
173,Machine Learning that Matters,"Much of current machine learning (ML) research has lost its connection toproblems of import to the larger world of science and society. From thisperspective, there exist glaring limitations in the data sets we investigate,the metrics we employ for evaluation, and the degree to which results arecommunicated back to their originating domains. What changes are needed to howwe conduct research to increase the impact that ML has? We present six ImpactChallenges to explicitly focus the field?s energy and attention, and we discussexisting obstacles that must be addressed. We aim to inspire ongoing discussionand focus on ML that matters.",http://arxiv.org/abs/1206.4656v1,,
174,"EnsembleSVM: A Library for Ensemble Learning Using Support Vector  Machines","EnsembleSVM is a free software package containing efficient routines toperform ensemble learning with support vector machine (SVM) base models. Itcurrently offers ensemble methods based on binary SVM models. Ourimplementation avoids duplicate storage and evaluation of support vectors whichare shared between constituent models. Experimental results show that usingensemble approaches can drastically reduce training complexity whilemaintaining high predictive accuracy. The EnsembleSVM software package isfreely available online at http://esat.kuleuven.be/stadius/ensemblesvm.",http://arxiv.org/abs/1403.0745v1,,
175,Creativity in Machine Learning,"Recent machine learning techniques can be modified to produce creativeresults. Those results did not exist before; it is not a trivial combination ofthe data which was fed into the machine learning system. The obtained resultscome in multiple forms: As images, as text and as audio.  This paper gives a high level overview of how they are created and gives someexamples. It is meant to be a summary of the current work and give people whoare new to machine learning some starting points.",http://arxiv.org/abs/1601.03642v1,,
176,"Encog: Library of Interchangeable Machine Learning Models for Java and  C#","This paper introduces the Encog library for Java and C#, a scalable,adaptable, multiplatform machine learning framework that was 1st released in2008. Encog allows a variety of machine learning models to be applied todatasets using regression, classification, and clustering. Various supportedmachine learning models can be used interchangeably with minimal recoding.Encog uses efficient multithreaded code to reduce training time by exploitingmodern multicore processors. The current version of Encog can be downloadedfrom http://www.encog.org.",http://arxiv.org/abs/1506.04776v1,,
177,"Machine Learning, Linear and Bayesian Models for Logistic Regression in  Failure Detection Problems","In this work, we study the use of logistic regression in manufacturingfailures detection. As a data set for the analysis, we used the data fromKaggle competition Bosch Production Line Performance. We considered the use ofmachine learning, linear and Bayesian models. For machine learning approach, weanalyzed XGBoost tree based classifier to obtain high scored classification.Using the generalized linear model for logistic regression makes it possible toanalyze the influence of the factors under study. The Bayesian approach forlogistic regression gives the statistical distribution for the parameters ofthe model. It can be useful in the probabilistic analysis, e.g. riskassessment.",http://arxiv.org/abs/1612.05740v1,,
178,Augmentor: An Image Augmentation Library for Machine Learning,"The generation of artificial data based on existing observations, known asdata augmentation, is a technique used in machine learning to improve modelaccuracy, generalisation, and to control overfitting. Augmentor is a softwarepackage, available in both Python and Julia versions, that provides a highlevel API for the expansion of image data using a stochastic, pipeline-basedapproach which effectively allows for images to be sampled from a distributionof augmented images at runtime. Augmentor provides methods for most standardaugmentation practices as well as several advanced features such aslabel-preserving, randomised elastic distortions, and provides many helperfunctions for typical augmentation tasks used in machine learning.",http://arxiv.org/abs/1708.04680v1,,
179,"On the Protection of Private Information in Machine Learning Systems:  Two Recent Approaches","The recent, remarkable growth of machine learning has led to intense interestin the privacy of the data on which machine learning relies, and to newtechniques for preserving privacy. However, older ideas about privacy may wellremain valid and useful. This note reviews two recent works on privacy in thelight of the wisdom of some of the early literature, in particular theprinciples distilled by Saltzer and Schroeder in the 1970s.",http://arxiv.org/abs/1708.08022v1,,
180,Gene Ontology (GO) Prediction using Machine Learning Methods,"We applied machine learning to predict whether a gene is involved in axonregeneration. We extracted 31 features from different databases and trainedfive machine learning models. Our optimal model, a Random Forest Classifierwith 50 submodels, yielded a test score of 85.71%, which is 4.1% higher thanthe baseline score. We concluded that our models have some predictivecapability. Similar methodology and features could be applied to predict otherGene Ontology (GO) terms.",http://arxiv.org/abs/1711.00001v1,,
181,"Bayesian Optimization Using Monotonicity Information and Its Application  in Machine Learning Hyperparameter","We propose an algorithm for a family of optimization problems where theobjective can be decomposed as a sum of functions with monotonicity properties.The motivating problem is optimization of hyperparameters of machine learningalgorithms, where we argue that the objective, validation error, can bedecomposed as monotonic functions of the hyperparameters. Our proposedalgorithm adapts Bayesian optimization methods to incorporate the monotonicityconstraints. We illustrate the advantages of exploiting monotonicity usingillustrative examples and demonstrate the improvements in optimizationefficiency for some machine learning hyperparameter tuning applications.",http://arxiv.org/abs/1802.03532v2,,
182,Extreme Learning Machine for Graph Signal Processing,"In this article, we improve extreme learning machines for regression tasksusing a graph signal processing based regularization. We assume that the targetsignal for prediction or regression is a graph signal. With this assumption, weuse the regularization to enforce that the output of an extreme learningmachine is smooth over a given graph. Simulation results with real data confirmthat such regularization helps significantly when the available training datais limited in size and corrupted by noise.",http://arxiv.org/abs/1803.04193v1,,
183,"Vanlearning: A Machine Learning SaaS Application for People Without  Programming Backgrounds","Although we have tons of machine learning tools to analyze data, most of themrequire users have some programming backgrounds. Here we introduce a SaaSapplication which allows users analyze their data without any coding and evenwithout any knowledge of machine learning. Users can upload, train, predict anddownload their data by simply clicks their mouses. Our system uses datapre-processor and validator to relieve the computational cost of our server.The simple architecture of Vanlearning helps developers can easily maintain andextend it.",http://arxiv.org/abs/1804.01382v1,,
184,Machine Learning in Compiler Optimisation,"In the last decade, machine learning based compilation has moved from an anobscure research niche to a mainstream activity. In this article, we describethe relationship between machine learning and compiler optimisation andintroduce the main concepts of features, models, training and deployment. Wethen provide a comprehensive survey and provide a road map for the wide varietyof different research areas. We conclude with a discussion on open issues inthe area and potential research directions. This paper provides both anaccessible introduction to the fast moving area of machine learning basedcompilation and a detailed bibliography of its main achievements.",http://arxiv.org/abs/1805.03441v1,,
185,The Roles of Supervised Machine Learning in Systems Neuroscience,"Over the last several years, the use of machine learning (ML) in neurosciencehas been rapidly increasing. Here, we review ML's contributions, both realizedand potential, across several areas of systems neuroscience. We describe fourprimary roles of ML within neuroscience: 1) creating solutions to engineeringproblems, 2) identifying predictive variables, 3) setting benchmarks for simplemodels of the brain, and 4) serving itself as a model for the brain. Thebreadth and ease of its applicability suggests that machine learning should bein the toolbox of most systems neuroscientists.",http://arxiv.org/abs/1805.08239v2,,
186,Opportunities in Machine Learning for Healthcare,"Healthcare is a natural arena for the application of machine learning,especially as modern electronic health records (EHRs) provide increasinglylarge amounts of data to answer clinically meaningful questions. However,clinical data and practice present unique challenges that complicate the use ofcommon methodologies. This article serves as a primer on addressing thesechallenges and highlights opportunities for members of the machine learning anddata science communities to contribute to this growing domain.",http://arxiv.org/abs/1806.00388v2,,
187,The CodRep Machine Learning on Source Code Competition,"CodRep is a machine learning competition on source code data. It is carefullydesigned so that anybody can enter the competition, whether professionalresearchers, students or independent scholars, without specific knowledge inmachine learning or program analysis. In particular, it aims at being a commonplayground on which the machine learning and the software engineering researchcommunities can interact. The competition has started on April 14th 2018 andhas ended on October 14th 2018. The CodRep data is hosted athttps://github.com/KTH/CodRep-competition/.",http://arxiv.org/abs/1807.03200v2,,
188,"Jensen: An Easily-Extensible C++ Toolkit for Production-Level Machine  Learning and Convex Optimization","This paper introduces Jensen, an easily extensible and scalable toolkit forproduction-level machine learning and convex optimization. Jensen implements aframework of convex (or loss) functions, convex optimization algorithms(including Gradient Descent, L-BFGS, Stochastic Gradient Descent, ConjugateGradient, etc.), and a family of machine learning classifiers and regressors(Logistic Regression, SVMs, Least Square Regression, etc.). This frameworkmakes it possible to deploy and train models with a few lines of code, and alsoextend and build upon this by integrating new loss functions and optimizationalgorithms.",http://arxiv.org/abs/1807.06574v1,,
189,Training Humans and Machines,"For many years, researchers in psychology, education, statistics, and machinelearning have been developing practical methods to improve learning speed,retention, and generalizability, and this work has been successful. Many ofthese methods are rooted in common underlying principles that seem to drivelearning and overlearning in both humans and machines. I present a review of asmall part of this work to point to potentially novel applications in bothmachine and human learning that may be worth exploring.",http://arxiv.org/abs/1807.08655v1,,
190,Multi-class Classification Model Inspired by Quantum Detection Theory,"Machine Learning has become very famous currently which assist in identifyingthe patterns from the raw data. Technological advancement has led tosubstantial improvement in Machine Learning which, thus helping to improveprediction. Current Machine Learning models are based on Classical Theory,which can be replaced by Quantum Theory to improve the effectiveness of themodel. In the previous work, we developed binary classifier inspired by QuantumDetection Theory. In this extended abstract, our main goal is to developmulti-class classifier. We generally use the terminology multinomialclassification or multi-class classification when we have a classificationproblem for classifying observations or instances into one of three or moreclasses.",http://arxiv.org/abs/1810.04491v1,,
191,"Entropic Variable Boosting for Explainability and Interpretability in  Machine Learning","In this paper, we present a new explainability formalism to make clear theimpact of each variable on the predictions given by black-box decision rules.Our method consists in evaluating the decision rules on test samples generatedin such a way that each variable is stressed incrementally while preserving theoriginal distribution of the machine learning problem. We then propose a newcomputation-ally efficient algorithm to stress the variables, which onlyreweights the reference observations and predictions. This makes ourmethodology scalable to large datasets. Results obtained on standard machinelearning datasets are presented and discussed.",http://arxiv.org/abs/1810.07924v1,,
192,Recent Research Advances on Interactive Machine Learning,"Interactive Machine Learning (IML) is an iterative learning process thattightly couples a human with a machine learner, which is widely used byresearchers and practitioners to effectively solve a wide variety of real-worldapplication problems. Although recent years have witnessed the proliferation ofIML in the field of visual analytics, most recent surveys either focus on aspecific area of IML or aim to summarize a visualization field that is toogeneric for IML. In this paper, we systematically review the recent literatureon IML and classify them into a task-oriented taxonomy built by us. We concludethe survey with a discussion of open challenges and research opportunities thatwe believe are inspiring for future work in IML.",http://arxiv.org/abs/1811.04548v1,,
193,"Prediction of Silicate Glasses' Stiffness by High-Throughput Molecular  Dynamics Simulations and Machine Learning","The development by machine learning of models predicting materials'properties usually requires the use of a large number of consistent data fortraining. However, quality experimental datasets are not always available orself-consistent. Here, as an alternative route, we combine machine learningwith high-throughput molecular dynamics simulations to predict the Young'smodulus of silicate glasses. We demonstrate that this combined approach offersexcellent predictions over the entire compositional domain. By comparing theperformance of select machine learning algorithms, we discuss the nature of thebalance between accuracy, simplicity, and interpretability in machine learning.",http://arxiv.org/abs/1901.09323v1,,
194,"Machine Learning for Data-Driven Movement Generation: a Review of the  State of the Art","The rise of non-linear and interactive media such as video games hasincreased the need for automatic movement animation generation. In this survey,we review and analyze different aspects of building automatic movementgeneration systems using machine learning techniques and motion capture data.We cover topics such as high-level movement characterization, training data,features representation, machine learning models, and evaluation methods. Weconclude by presenting a discussion of the reviewed literature and outliningthe research gaps and remaining challenges for future work.",http://arxiv.org/abs/1903.08356v1,,
195,"Comment on ""robustness and regularization of support vector machines"" by  H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510,  2009, arXiv:0803.3490)","This paper comments on the published work dealing with robustness andregularization of support vector machines (Journal of Machine LearningResearch, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. Theyproposed a theorem to show that it is possible to relate robustness in thefeature space and robustness in the sample space directly. In this paper, wepropose a counter example that rejects their theorem.",http://arxiv.org/abs/1308.3750v1,,
196,Learning ELM network weights using linear discriminant analysis,"We present an alternative to the pseudo-inverse method for determining thehidden to output weight values for Extreme Learning Machines performingclassification tasks. The method is based on linear discriminant analysis andprovides Bayes optimal single point estimates for the weight values.",http://arxiv.org/abs/1406.3100v1,,
197,Machine listening intelligence,"This manifesto paper will introduce machine listening intelligence, anintegrated research framework for acoustic and musical signals modelling, basedon signal processing, deep learning and computational musicology.",http://arxiv.org/abs/1706.09557v1,,
198,A Machine Learning Approach to Routing,"Can ideas and techniques from machine learning be leveraged to automaticallygenerate ""good"" routing configurations? We investigate the power of data-drivenrouting protocols. Our results suggest that applying ideas and techniques fromdeep reinforcement learning to this context yields high performance, motivatingfurther research along these lines.",http://arxiv.org/abs/1708.03074v2,,
199,Autoencoding topology,"The problem of learning a manifold structure on a dataset is framed in termsof a generative model, to which we use ideas behind autoencoders (namelyadversarial/Wasserstein autoencoders) to fit deep neural networks. From amachine learning perspective, the resulting structure, an atlas of a manifold,may be viewed as a combination of dimensionality reduction and ""fuzzy""clustering.",http://arxiv.org/abs/1803.00156v1,,
200,Machine Learning and Applied Linguistics,"This entry introduces the topic of machine learning and provides an overviewof its relevance for applied linguistics and language learning. The discussionwill focus on giving an introduction to the methods and applications of machinelearning in applied linguistics, and will provide references for further study.",http://arxiv.org/abs/1803.09103v1,,
201,Not quite unreasonable effectiveness of machine learning algorithms,"State-of-the-art machine learning algorithms demonstrate close to absoluteperformance in selected challenges. We provide arguments that the reason can bein low variability of the samples and high effectiveness in learning typicalpatterns. Due to this fact, standard performance metrics do not reveal modelcapacity and new metrics are required for the better understanding ofstate-of-the-art.",http://arxiv.org/abs/1804.02543v1,,
202,Kernel machines with two layers and multiple kernel learning,"In this paper, the framework of kernel machines with two layers isintroduced, generalizing classical kernel methods. The new learning methodologyprovide a formal connection between computational architectures with multiplelayers and the theme of kernel learning in standard regularization methods.First, a representer theorem for two-layer networks is presented, showing thatfinite linear combinations of kernels on each layer are optimal architectureswhenever the corresponding functions solve suitable variational problems inreproducing kernel Hilbert spaces (RKHS). The input-output map expressed bythese architectures turns out to be equivalent to a suitable single-layerkernel machines in which the kernel function is also learned from the data.Recently, the so-called multiple kernel learning methods have attractedconsiderable attention in the machine learning literature. In this paper,multiple kernel learning methods are shown to be specific cases of kernelmachines with two layers in which the second layer is linear. Finally, a simpleand effective multiple kernel learning method called RLS2 (regularized leastsquares with two layers) is introduced, and his performances on severallearning problems are extensively analyzed. An open source MATLAB toolbox totrain and validate RLS2 models with a Graphic User Interface is available.",http://arxiv.org/abs/1001.2709v1,,
203,"Contemporary machine learning: a guide for practitioners in the physical  sciences","Machine learning is finding increasingly broad application in the physicalsciences. This most often involves building a model relationship between adependent, measurable output and an associated set of controllable, butcomplicated, independent inputs. We present a tutorial on current techniques inmachine learning -- a jumping-off point for interested researchers to advancetheir work. We focus on deep neural networks with an emphasis on demystifyingdeep learning. We begin with background ideas in machine learning and someexample applications from current research in plasma physics. We discusssupervised learning techniques for modeling complicated functions, beginningwith familiar regression schemes, then advancing to more sophisticated deeplearning methods. We also address unsupervised learning and techniques forreducing the dimensionality of input spaces. Along the way, we describe methodsfor practitioners to help ensure that their models generalize from theirtraining data to as-yet-unseen test data. We describe classes of tasks --predicting scalars, handling images, fitting time-series -- and prepare thereader to choose an appropriate technique. We finally point out somelimitations to modern machine learning and speculate on some ways thatpractitioners from the physical sciences may be particularly suited to help.",http://arxiv.org/abs/1712.08523v1,,
204,"A New Perspective on Machine Learning: How to do Perfect Supervised  Learning","In this work, we introduce the concept of bandlimiting into the theory ofmachine learning because all physical processes are bandlimited by nature,including real-world machine learning tasks. After the bandlimiting constraintis taken into account, our theoretical analysis has shown that all practicalmachine learning tasks are asymptotically solvable in a perfect sense.Furthermore, the key towards this solvability almost solely relies on twofactors: i) a sufficiently large amount of training samples beyond a thresholddetermined by a difficulty measurement of the underlying task; ii) asufficiently complex and bandlimited model. Moreover, for some special cases,we have derived new error bounds for perfect learning, which can quantify thedifficulty of learning. These generalization bounds are not only asymptoticallyconvergent but also irrelevant to model complexity. Our new results ongeneralization have provided a new perspective to explain the recent successesof large-scale supervised learning using complex models like neural networks.",http://arxiv.org/abs/1901.02046v3,,
205,"On the Information Theoretic Distance Measures and Bidirectional  Helmholtz Machines","By establishing a connection between bi-directional Helmholtz machines andinformation theory, we propose a generalized Helmholtz machine. Theoretical andexperimental results show that given \textit{shallow} architectures, thegeneralized model outperforms the previous ones substantially.",http://arxiv.org/abs/1807.06054v1,,
206,Automatic differentiation in machine learning: a survey,"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous inmachine learning. Automatic differentiation (AD), also called algorithmicdifferentiation or simply ""autodiff"", is a family of techniques similar to butmore general than backpropagation for efficiently and accurately evaluatingderivatives of numeric functions expressed as computer programs. AD is a smallbut established field with applications in areas including computational fluiddynamics, atmospheric sciences, and engineering design optimization. Until veryrecently, the fields of machine learning and AD have largely been unaware ofeach other and, in some cases, have independently discovered each other'sresults. Despite its relevance, general-purpose AD has been missing from themachine learning toolbox, a situation slowly changing with its ongoing adoptionunder the names ""dynamic computational graphs"" and ""differentiableprogramming"". We survey the intersection of AD and machine learning, coverapplications where AD has direct relevance, and address the main implementationtechniques. By precisely defining the main differentiation techniques and theirinterrelationships, we aim to bring clarity to the usage of the terms""autodiff"", ""automatic differentiation"", and ""symbolic differentiation"" asthese are encountered more and more in machine learning settings.",http://arxiv.org/abs/1502.05767v4,,
207,"Progressive Sampling-Based Bayesian Optimization for Efficient and  Automatic Machine Learning Model Selection","Purpose: Machine learning is broadly used for clinical data analysis. Beforetraining a model, a machine learning algorithm must be selected. Also, thevalues of one or more model parameters termed hyper-parameters must be set.Selecting algorithms and hyper-parameter values requires advanced machinelearning knowledge and many labor-intensive manual iterations. To lower the barto machine learning, miscellaneous automatic selection methods for algorithmsand/or hyper-parameter values have been proposed. Existing automatic selectionmethods are inefficient on large data sets. This poses a challenge for usingmachine learning in the clinical big data era. Methods: To address thechallenge, this paper presents progressive sampling-based Bayesianoptimization, an efficient and automatic selection method for both algorithmsand hyper-parameter values. Results: We report an implementation of the method.We show that compared to a state of the art automatic selection method, ourmethod can significantly reduce search time, classification error rate, andstandard deviation of error rate due to randomization. Conclusions: This ismajor progress towards enabling fast turnaround in identifying high-qualitysolutions required by many machine learning-based clinical data analysis tasks.",http://arxiv.org/abs/1812.02855v1,,
208,The Management of Context-Sensitive Features: A Review of Strategies,"In this paper, we review five heuristic strategies for handlingcontext-sensitive features in supervised machine learning from examples. Wediscuss two methods for recovering lost (implicit) contextual information. Wemention some evidence that hybrid strategies can have a synergetic effect. Wethen show how the work of several machine learning researchers fits into thisframework. While we do not claim that these strategies exhaust thepossibilities, it appears that the framework includes all of the techniquesthat can be found in the published literature on contextsensitive learning.",http://arxiv.org/abs/cs/0212037v1,,
209,Quantum Learning Machine,"We propose a novel notion of a quantum learning machine for automaticallycontrolling quantum coherence and for developing quantum algorithms. A quantumlearning machine can be trained to learn a certain task with no a prioriknowledge on its algorithm. As an example, it is demonstrated that the quantumlearning machine learns Deutsch's task and finds itself a quantum algorithm,that is different from but equivalent to the original one.",http://arxiv.org/abs/0803.2976v2,,
210,Deep Multiple Kernel Learning,"Deep learning methods have predominantly been applied to large artificialneural networks. Despite their state-of-the-art performance, these largenetworks typically do not generalize well to datasets with limited samplesizes. In this paper, we take a different approach by learning multiple layersof kernels. We combine kernels at each layer and then optimize over an estimateof the support vector machine leave-one-out error rather than the dualobjective function. Our experiments on a variety of datasets show that eachlayer successively increases performance with only a few base kernels.",http://arxiv.org/abs/1310.3101v1,,
211,Equivalence of Learning Algorithms,"The purpose of this paper is to introduce a concept of equivalence betweenmachine learning algorithms. We define two notions of algorithmic equivalence,namely, weak and strong equivalence. These notions are of paramount importancefor identifying when learning prop erties from one learning algorithm can betransferred to another. Using regularized kernel machines as a case study, weillustrate the importance of the introduced equivalence concept by analyzingthe relation between kernel ridge regression (KRR) and m-power regularizedleast squares regression (M-RLSR) algorithms.",http://arxiv.org/abs/1406.2622v1,,
212,Learning to Reason With Adaptive Computation,"Multi-hop inference is necessary for machine learning systems to successfullysolve tasks such as Recognising Textual Entailment and Machine Reading. In thiswork, we demonstrate the effectiveness of adaptive computation for learning thenumber of inference steps required for examples of different complexity andthat learning the correct number of inference steps is difficult. We introducethe first model involving Adaptive Computation Time which provides a smallperformance benefit on top of a similar model without an adaptive component aswell as enabling considerable insight into the reasoning process of the model.",http://arxiv.org/abs/1610.07647v2,,
213,"Joint Structured Learning and Predictions under Logical Constraints in  Conditional Random Fields","This paper is concerned with structured machine learning, in a supervisedmachine learning context. It discusses how to make joint structured learning oninterdependent objects of different nature, as well as how to enforce logicalcon-straints when predicting labels. We explain how this need arose in aDocument Understanding task. We then discuss a general extension to ConditionalRandom Field (CRF) for this purpose and present the contributed open sourceimplementation on top of the open source PyStruct library. We evaluate itsperformance on a publicly available dataset.",http://arxiv.org/abs/1708.07644v1,,
214,"Counterfactual Learning for Machine Translation: Degeneracies and  Solutions","Counterfactual learning is a natural scenario to improve web-based machinetranslation services by offline learning from feedback logged during userinteractions. In order to avoid the risk of showing inferior translations tousers, in such scenarios mostly exploration-free deterministic logging policiesare in place. We analyze possible degeneracies of inverse and reweightedpropensity scoring estimators, in stochastic and deterministic settings, andrelate them to recently proposed techniques for counterfactual learning underdeterministic logging.",http://arxiv.org/abs/1711.08621v3,,
215,"Kalman Filter Modifier for Neural Networks in Non-stationary  Environments","Learning in a non-stationary environment is an inevitable problem whenapplying machine learning algorithm to real world environment. Learning newtasks without forgetting the previous knowledge is a challenge issue in machinelearning. We propose a Kalman Filter based modifier to maintain the performanceof Neural Network models under non-stationary environments. The result showsthat our proposed model can preserve the key information and adapts better tothe changes. The accuracy of proposed model decreases by 0.4% in ourexperiments, while the accuracy of conventional model decreases by 90% in thedrifts environment.",http://arxiv.org/abs/1811.02361v1,,
216,Machine Learning for Neuroimaging with Scikit-Learn,"Statistical machine learning methods are increasingly used for neuroimagingdata analysis. Their main virtue is their ability to model high-dimensionaldatasets, e.g. multivariate analysis of activation images or resting-state timeseries. Supervised learning is typically used in decoding or encoding settingsto relate brain images to behavioral or clinical observations, whileunsupervised learning can uncover hidden structures in sets of images (e.g.resting state functional MRI) or find sub-populations in large cohorts. Byconsidering different functional neuroimaging applications, we illustrate howscikit-learn, a Python machine learning library, can be used to perform somekey analysis steps. Scikit-learn contains a very large set of statisticallearning algorithms, both supervised and unsupervised, and its application toneuroimaging data provides a versatile tool to study the brain.",http://arxiv.org/abs/1412.3919v1,,
217,The Benefit of Multitask Representation Learning,"We discuss a general method to learn data representations from multipletasks. We provide a justification for this method in both settings of multitasklearning and learning-to-learn. The method is illustrated in detail in thespecial case of linear feature learning. Conditions on the theoreticaladvantage offered by multitask representation learning over independent tasklearning are established. In particular, focusing on the important example ofhalf-space learning, we derive the regime in which multitask representationlearning is beneficial over independent task learning, as a function of thesample size, the number of tasks and the intrinsic data dimensionality. Otherpotential applications of our results include multitask feature learning inreproducing kernel Hilbert spaces and multilayer, deep networks.",http://arxiv.org/abs/1505.06279v2,,
218,Provable benefits of representation learning,"There is general consensus that learning representations is useful for avariety of reasons, e.g. efficient use of labeled data (semi-supervisedlearning), transfer learning and understanding hidden structure of data.Popular techniques for representation learning include clustering, manifoldlearning, kernel-learning, autoencoders, Boltzmann machines, etc.  To study the relative merits of these techniques, it's essential to formalizethe definition and goals of representation learning, so that they are allbecome instances of the same definition. This paper introduces such a formalframework that also formalizes the utility of learning the representation. Itis related to previous Bayesian notions, but with some new twists. We show theusefulness of our framework by exhibiting simple and natural settings -- linearmixture models and loglinear models, where the power of representation learningcan be formally shown. In these examples, representation learning can beperformed provably and efficiently under plausible assumptions (despite beingNP-hard), and furthermore: (i) it greatly reduces the need for labeled data(semi-supervised learning) and (ii) it allows solving classification tasks whensimpler approaches like nearest neighbors require too much data (iii) it ismore powerful than manifold learning methods.",http://arxiv.org/abs/1706.04601v1,,
219,Opening the black box of deep learning,"The great success of deep learning shows that its technology containsprofound truth, and understanding its internal mechanism not only has importantimplications for the development of its technology and effective application invarious fields, but also provides meaningful insights into the understanding ofhuman brain mechanism. At present, most of the theoretical research on deeplearning is based on mathematics. This dissertation proposes that the neuralnetwork of deep learning is a physical system, examines deep learning fromthree different perspectives: microscopic, macroscopic, and physical worldviews, answers multiple theoretical puzzles in deep learning by using physicsprinciples. For example, from the perspective of quantum mechanics andstatistical physics, this dissertation presents the calculation methods forconvolution calculation, pooling, normalization, and Restricted BoltzmannMachine, as well as the selection of cost functions, explains why deep learningmust be deep, what characteristics are learned in deep learning, whyConvolutional Neural Networks do not have to be trained layer by layer, and thelimitations of deep learning, etc., and proposes the theoretical direction andbasis for the further development of deep learning now and in the future. Thebrilliance of physics flashes in deep learning, we try to establish the deeplearning technology based on the scientific theory of physics.",http://arxiv.org/abs/1805.08355v1,,
220,Small Sample Learning in Big Data Era,"As a promising area in artificial intelligence, a new learning paradigm,called Small Sample Learning (SSL), has been attracting prominent researchattention in the recent years. In this paper, we aim to present a survey tocomprehensively introduce the current techniques proposed on this topic.Specifically, current SSL techniques can be mainly divided into two categories.The first category of SSL approaches can be called ""concept learning"", whichemphasizes learning new concepts from only few related observations. Thepurpose is mainly to simulate human learning behaviors like recognition,generation, imagination, synthesis and analysis. The second category is called""experience learning"", which usually co-exists with the large sample learningmanner of conventional machine learning. This category mainly focuses onlearning with insufficient samples, and can also be called small data learningin some literatures. More extensive surveys on both categories of SSLtechniques are introduced and some neuroscience evidences are provided toclarify the rationality of the entire SSL regime, and the relationship withhuman learning process. Some discussions on the main challenges and possiblefuture research directions along this line are also presented.",http://arxiv.org/abs/1808.04572v3,,
221,Multi-View Factorization Machines,"For a learning task, data can usually be collected from different sources orbe represented from multiple views. For example, laboratory results fromdifferent medical examinations are available for disease diagnosis, and each ofthem can only reflect the health state of a person from a particularaspect/view. Therefore, different views provide complementary information forlearning tasks. An effective integration of the multi-view information isexpected to facilitate the learning performance. In this paper, we propose ageneral predictor, named multi-view machines (MVMs), that can effectivelyinclude all the possible interactions between features from multiple views. Ajoint factorization is embedded for the full-order interaction parameters whichallows parameter estimation under sparsity. Moreover, MVMs can work inconjunction with different loss functions for a variety of machine learningtasks. A stochastic gradient descent method is presented to learn the MVMmodel. We further illustrate the advantages of MVMs through comparison withother methods for multi-view classification, including support vector machines(SVMs), support tensor machines (STMs) and factorization machines (FMs).",http://arxiv.org/abs/1506.01110v2,,
222,A Shared Task on Bandit Learning for Machine Translation,"We introduce and describe the results of a novel shared task on banditlearning for machine translation. The task was organized jointly by Amazon andHeidelberg University for the first time at the Second Conference on MachineTranslation (WMT 2017). The goal of the task is to encourage research onlearning machine translation from weak user feedback instead of humanreferences or post-edits. On each of a sequence of rounds, a machinetranslation system is required to propose a translation for an input, andreceives a real-valued estimate of the quality of the proposed translation forlearning. This paper describes the shared task's learning and evaluation setup,using services hosted on Amazon Web Services (AWS), the data and evaluationmetrics, and the results of various machine translation architectures andlearning protocols.",http://arxiv.org/abs/1707.09050v1,,
223,"Performance Analysis of Machine Learning Techniques to Predict Diabetes  Mellitus","Diabetes mellitus is a common disease of human body caused by a group ofmetabolic disorders where the sugar levels over a prolonged period is veryhigh. It affects different organs of the human body which thus harm a largenumber of the body's system, in particular the blood veins and nerves. Earlyprediction in such disease can be controlled and save human life. To achievethe goal, this research work mainly explores various risk factors related tothis disease using machine learning techniques. Machine learning techniquesprovide efficient result to extract knowledge by constructing predicting modelsfrom diagnostic medical datasets collected from the diabetic patients.Extracting knowledge from such data can be useful to predict diabetic patients.In this work, we employ four popular machine learning algorithms, namelySupport Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbor (KNN) andC4.5 Decision Tree, on adult population data to predict diabetic mellitus. Ourexperimental results show that C4.5 decision tree achieved higher accuracycompared to other machine learning techniques.",http://arxiv.org/abs/1902.10028v1,,
224,Does data interpolation contradict statistical optimality?,"We show that learning methods interpolating the training data can achieveoptimal rates for the problems of nonparametric regression and prediction withsquare loss.",http://arxiv.org/abs/1806.09471v1,,
225,Stock Chart Pattern recognition with Deep Learning,"This study evaluates the performances of CNN and LSTM for recognizing commoncharts patterns in a stock historical data. It presents two common patterns,the method used to build the training set, the neural networks architecturesand the accuracies obtained.",http://arxiv.org/abs/1808.00418v1,,
226,Catastrophic Importance of Catastrophic Forgetting,"This paper describes some of the possibilities of artificial neural networksthat open up after solving the problem of catastrophic forgetting. A simplemodel and reinforcement learning applications of existing methods are alsoproposed.",http://arxiv.org/abs/1808.07049v1,,
227,"Study and Observation of the Variation of Accuracies of KNN, SVM, LMNN,  ENN Algorithms on Eleven Different Datasets from UCI Machine Learning  Repository","Machine learning qualifies computers to assimilate with data, without beingsolely programmed [1, 2]. Machine learning can be classified as supervised andunsupervised learning. In supervised learning, computers learn an objectivethat portrays an input to an output hinged on training input-output pairs [3].Most efficient and widely used supervised learning algorithms are K-NearestNeighbors (KNN), Support Vector Machine (SVM), Large Margin Nearest Neighbor(LMNN), and Extended Nearest Neighbor (ENN). The main contribution of thispaper is to implement these elegant learning algorithms on eleven differentdatasets from the UCI machine learning repository to observe the variation ofaccuracies for each of the algorithms on all datasets. Analyzing the accuracyof the algorithms will give us a brief idea about the relationship of themachine learning algorithms and the data dimensionality. All the algorithms aredeveloped in Matlab. Upon such accuracy observation, the comparison can bebuilt among KNN, SVM, LMNN, and ENN regarding their performances on eachdataset.",http://arxiv.org/abs/1809.06186v3,,
228,"Mutual learning in a tree parity machine and its application to  cryptography","Mutual learning of a pair of tree parity machines with continuous anddiscrete weight vectors is studied analytically. The analysis is based on amapping procedure that maps the mutual learning in tree parity machines ontomutual learning in noisy perceptrons. The stationary solution of the mutuallearning in the case of continuous tree parity machines depends on the learningrate where a phase transition from partial to full synchronization is observed.In the discrete case the learning process is based on a finite increment and afull synchronized state is achieved in a finite number of steps. Thesynchronization of discrete parity machines is introduced in order to constructan ephemeral key-exchange protocol. The dynamic learning of a third tree paritymachine (an attacker) that tries to imitate one of the two machines while thetwo still update their weight vectors is also analyzed. In particular, thesynchronization times of the naive attacker and the flipping attacker recentlyintroduced in [1] are analyzed. All analytical results are found to be in goodagreement with simulation results.",http://arxiv.org/abs/cond-mat/0209234v1,,
229,Benchmarking Machine Learning Technologies for Software Defect Detection,"Machine Learning approaches are good in solving problems that have lessinformation. In most cases, the software domain problems characterize as aprocess of learning that depend on the various circumstances and changesaccordingly. A predictive model is constructed by using machine learningapproaches and classified them into defective and non-defective modules.Machine learning techniques help developers to retrieve useful informationafter the classification and enable them to analyse data from differentperspectives. Machine learning techniques are proven to be useful in terms ofsoftware bug prediction. This study used public available data sets of softwaremodules and provides comparative performance analysis of different machinelearning techniques for software bug prediction. Results showed most of themachine learning methods performed well on software bug datasets.",http://arxiv.org/abs/1506.07563v1,,
230,"Multi-Sensor Data Pattern Recognition for Multi-Target Localization: A  Machine Learning Approach","Data-target pairing is an important step towards multi-target localizationfor the intelligent operation of unmanned systems. Target localization plays acrucial role in numerous applications, such as search, and rescue missions,traffic management and surveillance. The objective of this paper is to presentan innovative target location learning approach, where numerous machinelearning approaches, including K-means clustering and supported vector machines(SVM), are used to learn the data pattern across a list of spatiallydistributed sensors. To enable the accurate data association from differentsensors for accurate target localization, appropriate data pre-processing isessential, which is then followed by the application of different machinelearning algorithms to appropriately group data from different sensors for theaccurate localization of multiple targets. Through simulation examples, theperformance of these machine learning algorithms is quantified and compared.",http://arxiv.org/abs/1703.00084v1,,
231,Probabilistic Matrix Factorization for Automated Machine Learning,"In order to achieve state-of-the-art performance, modern machine learningtechniques require careful data pre-processing and hyperparameter tuning.Moreover, given the ever increasing number of machine learning models beingdeveloped, model selection is becoming increasingly important. Automating theselection and tuning of machine learning pipelines consisting of datapre-processing methods and machine learning models, has long been one of thegoals of the machine learning community. In this paper, we tackle thismeta-learning task by combining ideas from collaborative filtering and Bayesianoptimization. Using probabilistic matrix factorization techniques andacquisition functions from Bayesian optimization, we exploit experimentsperformed in hundreds of different datasets to guide the exploration of thespace of possible pipelines. In our experiments, we show that our approachquickly identifies high-performing pipelines across a wide range of datasets,significantly outperforming the current state-of-the-art.",http://arxiv.org/abs/1705.05355v2,,
232,Fairness-aware machine learning: a perspective,"Algorithms learned from data are increasingly used for deciding many aspectsin our life: from movies we see, to prices we pay, or medicine we get. Yetthere is growing evidence that decision making by inappropriately trainedalgorithms may unintentionally discriminate people. For example, in automatedmatching of candidate CVs with job descriptions, algorithms may capture andpropagate ethnicity related biases. Several repairs for selected algorithmshave already been proposed, but the underlying mechanisms how suchdiscrimination happens from the computational perspective are not yetscientifically understood. We need to develop theoretical understanding howalgorithms may become discriminatory, and establish fundamental machinelearning principles for prevention. We need to analyze machine learning processas a whole to systematically explain the roots of discrimination occurrence,which will allow to devise global machine learning optimization criteria forguaranteed prevention, as opposed to pushing empirical constraints intoexisting algorithms case-by-case. As a result, the state-of-the-art willadvance from heuristic repairing, to proactive and theoretically supportedprevention. This is needed not only because law requires to protect vulnerablepeople. Penetration of big data initiatives will only increase, and computerscience needs to provide solid explanations and accountability to the public,before public concerns lead to unnecessarily restrictive regulations againstmachine learning.",http://arxiv.org/abs/1708.00754v1,,
233,"MARVIN: An Open Machine Learning Corpus and Environment for Automated  Machine Learning Primitive Annotation and Execution","In this demo paper, we introduce the DARPA D3M program for automatic machinelearning (ML) and JPL's MARVIN tool that provides an environment to locate,annotate, and execute machine learning primitives for use in ML pipelines.MARVIN is a web-based application and associated back-end interface written inPython that enables composition of ML pipelines from hundreds of primitivesfrom the world of Scikit-Learn, Keras, DL4J and other widely used libraries.MARVIN allows for the creation of Docker containers that run on Kubernetesclusters within DARPA to provide an execution environment for automated machinelearning. MARVIN currently contains over 400 datasets and challenge problemsfrom a wide array of ML domains including routine classification and regressionto advanced video/image classification and remote sensing.",http://arxiv.org/abs/1808.03753v1,,
234,"Machine Learning for Combinatorial Optimization: a Methodological Tour  d'Horizon","This paper surveys the recent attempts, both from the machine learning andoperations research communities, at leveraging machine learning to solvecombinatorial optimization problems. Given the hard nature of these problems,state-of-the-art methodologies involve algorithmic decisions that eitherrequire too much computing time or are not mathematically well defined. Thus,machine learning looks like a promising candidate to effectively deal withthose decisions. We advocate for pushing further the integration of machinelearning and combinatorial optimization and detail methodology to do so. A mainpoint of the paper is seeing generic optimization problems as data points andinquiring what is the relevant distribution of problems to use for learning ona given task.",http://arxiv.org/abs/1811.06128v1,,
235,Machine learning in resting-state fMRI analysis,"Machine learning techniques have gained prominence for the analysis ofresting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, wepresent an overview of various unsupervised and supervised machine learningapplications to rs-fMRI. We present a methodical taxonomy of machine learningmethods in resting-state fMRI. We identify three major divisions ofunsupervised learning methods with regard to their applications to rs-fMRI,based on whether they discover principal modes of variation across space, timeor population. Next, we survey the algorithms and rs-fMRI featurerepresentations that have driven the success of supervised subject-levelpredictions. The goal is to provide a high-level overview of the burgeoningfield of rs-fMRI from the perspective of machine learning applications.",http://arxiv.org/abs/1812.11477v1,,
236,Ten ways to fool the masses with machine learning,"If you want to tell people the truth, make them laugh, otherwise they'll killyou. (source unclear)  Machine learning and deep learning are the technologies of the day fordeveloping intelligent automatic systems. However, a key hurdle for progress inthe field is the literature itself: we often encounter papers that reportresults that are difficult to reconstruct or reproduce, results thatmis-represent the performance of the system, or contain other biases that limittheir validity. In this semi-humorous article, we discuss issues that arise inrunning and reporting results of machine learning experiments. The purpose ofthe article is to provide a list of watch out points for researchers to beaware of when developing machine learning models or writing and reviewingmachine learning papers.",http://arxiv.org/abs/1901.01686v1,,
237,Adaptive Bayesian Linear Regression for Automated Machine Learning,"To solve a machine learning problem, one typically needs to perform datapreprocessing, modeling, and hyperparameter tuning, which is known as modelselection and hyperparameter optimization.The goal of automated machinelearning (AutoML) is to design methods that can automatically perform modelselection and hyperparameter optimization without human interventions for agiven dataset. In this paper, we propose a meta-learning method that can searchfor a high-performance machine learning pipeline from the predefined set ofcandidate pipelines for supervised classification datasets in an efficient wayby leveraging meta-data collected from previous experiments. More specifically,our method combines an adaptive Bayesian regression model with a neural networkbasis function and the acquisition function from Bayesian optimization. Theadaptive Bayesian regression model is able to capture knowledge from previousmeta-data and thus make predictions of the performances of machine learningpipelines on a new dataset. The acquisition function is then used to guide thesearch of possible pipelines based on the predictions.The experimentsdemonstrate that our approach can quickly identify high-performance pipelinesfor a range of test datasets and outperforms the baseline methods.",http://arxiv.org/abs/1904.00577v1,,
238,Unsupervised Meta-Learning for Reinforcement Learning,"Meta-learning is a powerful tool that builds on multi-task learning to learnhow to quickly adapt a model to new tasks. In the context of reinforcementlearning, meta-learning algorithms can acquire reinforcement learningprocedures to solve new problems more efficiently by meta-learning prior tasks.The performance of meta-learning algorithms critically depends on the tasksavailable for meta-training: in the same way that supervised learningalgorithms generalize best to test points drawn from the same distribution asthe training points, meta-learning methods generalize best to tasks from thesame distribution as the meta-training tasks. In effect, meta-reinforcementlearning offloads the design burden from algorithm design to task design. If wecan automate the process of task design as well, we can devise a meta-learningalgorithm that is truly automated. In this work, we take a step in thisdirection, proposing a family of unsupervised meta-learning algorithms forreinforcement learning. We describe a general recipe for unsupervisedmeta-reinforcement learning, and describe an effective instantiation of thisapproach based on a recently proposed unsupervised exploration technique andmodel-agnostic meta-learning. We also discuss practical and conceptualconsiderations for developing unsupervised meta-learning methods. Ourexperimental results demonstrate that unsupervised meta-reinforcement learningeffectively acquires accelerated reinforcement learning procedures without theneed for manual task design, significantly exceeds the performance of learningfrom scratch, and even matches performance of meta-learning methods that usehand-specified task distributions.",http://arxiv.org/abs/1806.04640v1,,
239,Enhanced version of AdaBoostM1 with J48 Tree learning method,"Machine Learning focuses on the construction and study of systems that canlearn from data. This is connected with the classification problem, whichusually is what Machine Learning algorithms are designed to solve. When amachine learning method is used by people with no special expertise in machinelearning, it is important that the method be robust in classification, in thesense that reasonable performance is obtained with minimal tuning of theproblem at hand. Algorithms are evaluated based on how robust they can classifythe given data. In this paper, we propose a quantifiable measure of robustness,and describe a particular learning method that is robust according to thismeasure in the context of classification problem. We proposed Adaptive Boosting(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold(P) and number of iterations (I) for boosting algorithm. To benchmark theperformance, we used the baseline classifier, AdaBoostM1 with Decision Stump asbase learner without tuning parameters. By tuning parameters and using J48 asbase learner, we are able to reduce the overall average error rate ratio(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2for evaluation sets of data.",http://arxiv.org/abs/1802.03522v1,,
240,Machine Learning for Spatiotemporal Sequence Forecasting: A Survey,"Spatiotemporal systems are common in the real-world. Forecasting themulti-step future of these spatiotemporal systems based on the pastobservations, or, Spatiotemporal Sequence Forecasting (STSF), is a significantand challenging problem. Although lots of real-world problems can be viewed asSTSF and many research works have proposed machine learning based methods forthem, no existing work has summarized and compared these methods from a unifiedperspective. This survey aims to provide a systematic review of machinelearning for STSF. In this survey, we define the STSF problem and classify itinto three subcategories: Trajectory Forecasting of Moving Point Cloud(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).We then introduce the two major challenges of STSF: 1) how to learn a model formulti-step forecasting and 2) how to adequately model the spatial and temporalstructures. After that, we review the existing works for solving thesechallenges, including the general learning strategies for multi-stepforecasting, the classical machine learning based methods for STSF, and thedeep learning based methods for STSF. We also compare these methods and pointout some potential research directions.",http://arxiv.org/abs/1808.06865v1,,
241,Security Matters: A Survey on Adversarial Machine Learning,"Adversarial machine learning is a fast growing research area, which considersthe scenarios when machine learning systems may face potential adversarialattackers, who intentionally synthesize input data to make a well-trained modelto make mistake. It always involves a defending side, usually a classifier, andan attacking side that aims to cause incorrect output. The earliest studies onthe adversarial examples for machine learning algorithms start from theinformation security area, which considers a much wider varieties of attackingmethods. But recent research focus that popularized by the deep learningcommunity places strong emphasis on how the ""imperceivable"" perturbations onthe normal inputs may cause dramatic mistakes by the deep learning withsupposed super-human accuracy. This paper serves to give a comprehensiveintroduction to a range of aspects of the adversarial deep learning topic,including its foundations, typical attacking and defending strategies, and someextended studies.",http://arxiv.org/abs/1810.07339v2,,
242,"FADL:Federated-Autonomous Deep Learning for Distributed Electronic  Health Record","Electronic health record (EHR) data is collected by individual institutionsand often stored across locations in silos. Getting access to these data isdifficult and slow due to security, privacy, regulatory, and operationalissues. We show, using ICU data from 58 different hospitals, that machinelearning models to predict patient mortality can be trained efficiently withoutmoving health data out of their silos using a distributed machine learningstrategy. We propose a new method, called Federated-Autonomous Deep Learning(FADL) that trains part of the model using all data sources in a distributedmanner and other parts using data from specific data sources. We observed thatFADL outperforms traditional federated learning strategy and conclude thatbalance between global and local training is an important factor to considerwhen design distributed machine learning methods , especially in healthcare.",http://arxiv.org/abs/1811.11400v2,,
243,Greedy Deep Dictionary Learning,"In this work we propose a new deep learning tool called deep dictionarylearning. Multi-level dictionaries are learnt in a greedy fashion, one layer ata time. This requires solving a simple (shallow) dictionary learning problem,the solution to this is well known. We apply the proposed technique on somebenchmark deep learning datasets. We compare our results with other deeplearning tools like stacked autoencoder and deep belief network; and state ofthe art supervised dictionary learning tools like discriminative KSVD and labelconsistent KSVD. Our method yields better results than all.",http://arxiv.org/abs/1602.00203v1,,
244,Co-Clustering for Multitask Learning,"This paper presents a new multitask learning framework that learns a sharedrepresentation among the tasks, incorporating both task and feature clusters.The jointly-induced clusters yield a shared latent subspace where taskrelationships are learned more effectively and more generally than instate-of-the-art multitask learning methods. The proposed general frameworkenables the derivation of more specific or restricted state-of-the-artmultitask methods. The paper also proposes a highly-scalable multitask learningalgorithm, based on the new framework, using conjugate gradient descent andgeneralized \textit{Sylvester equations}. Experimental results on synthetic andbenchmark datasets show that the proposed method systematically outperformsseveral state-of-the-art multitask learning methods.",http://arxiv.org/abs/1703.00994v1,,
245,Rethinking Convolutional Semantic Segmentation Learning,"Deep convolutional semantic segmentation (DCSS) learning doesn't converge toan optimal local minimum with random parameters initializations; a pre-trainedmodel on the same domain becomes necessary to achieve convergence.In this work,we propose a joint cooperative end-to-end learning method for DCSS. Itaddresses many drawbacks with existing deep semantic segmentation learning; theproposed approach simultaneously learn both segmentation and classification;taking away the essential need of the pre-trained model for learningconvergence. We present an improved inception based architecture with partialattention gating (PAG) over encoder information. The PAG also adds to achievefaster convergence and better accuracy for segmentation task. We will show theeffectiveness of this learning on a diabetic retinopathy classification andsegmentation dataset.",http://arxiv.org/abs/1710.07991v1,,
246,Expectation Learning for Adaptive Crossmodal Stimuli Association,"The human brain is able to learn, generalize, and predict crossmodal stimuli.Learning by expectation fine-tunes crossmodal processing at different levels,thus enhancing our power of generalization and adaptation in highly dynamicenvironments. In this paper, we propose a deep neural architecture trained byusing expectation learning accounting for unsupervised learning tasks. Ourlearning model exhibits a self-adaptable behavior, setting the first stepstowards the development of deep learning architectures for crossmodal stimuliassociation.",http://arxiv.org/abs/1801.07654v1,,
247,Hierarchical Reinforcement Learning with Hindsight,"Reinforcement Learning (RL) algorithms can suffer from poor sample efficiencywhen rewards are delayed and sparse. We introduce a solution that enablesagents to learn temporally extended actions at multiple levels of abstractionin a sample efficient and automated fashion. Our approach combines universalvalue functions and hindsight learning, allowing agents to learn policiesbelonging to different time scales in parallel. We show that our methodsignificantly accelerates learning in a variety of discrete and continuoustasks.",http://arxiv.org/abs/1805.08180v2,,
248,Reinforced Continual Learning,"Most artificial intelligence models have limiting ability to solve new tasksfaster, without forgetting previously acquired knowledge. The recently emergingparadigm of continual learning aims to solve this issue, in which the modellearns various tasks in a sequential fashion. In this work, a novel approachfor continual learning is proposed, which searches for the best neuralarchitecture for each coming task via sophisticatedly designed reinforcementlearning strategies. We name it as Reinforced Continual Learning. Our methodnot only has good performance on preventing catastrophic forgetting but alsofits new tasks well. The experiments on sequential classification tasks forvariants of MNIST and CIFAR-100 datasets demonstrate that the proposed approachoutperforms existing continual learning alternatives for deep networks.",http://arxiv.org/abs/1805.12369v1,,
249,Learning Beam Search Policies via Imitation Learning,"Beam search is widely used for approximate decoding in structured predictionproblems. Models often use a beam at test time but ignore its existence attrain time, and therefore do not explicitly learn how to use the beam. Wedevelop an unifying meta-algorithm for learning beam search policies usingimitation learning. In our setting, the beam is part of the model, and not justan artifact of approximate decoding. Our meta-algorithm captures existinglearning algorithms and suggests new ones. It also lets us show novel no-regretguarantees for learning beam search policies.",http://arxiv.org/abs/1811.00512v1,,
250,"Split learning for health: Distributed deep learning without sharing raw  patient data","Can health entities collaboratively train deep learning models withoutsharing sensitive raw data? This paper proposes several configurations of adistributed deep learning method called SplitNN to facilitate suchcollaborations. SplitNN does not share raw data or model details withcollaborating institutions. The proposed configurations of splitNN cater topractical settings of i) entities holding different modalities of patient data,ii) centralized and local health entities collaborating on multiple tasks andiii) learning without sharing labels. We compare performance and resourceefficiency trade-offs of splitNN and other distributed deep learning methodslike federated learning, large batch synchronous stochastic gradient descentand show highly encouraging results for splitNN.",http://arxiv.org/abs/1812.00564v1,,
251,Bandit Structured Prediction for Neural Sequence-to-Sequence Learning,"Bandit structured prediction describes a stochastic optimization frameworkwhere learning is performed from partial feedback. This feedback is received inthe form of a task loss evaluation to a predicted output structure, withouthaving access to gold standard structures. We advance this framework by liftinglinear bandit learning to neural sequence-to-sequence learning problems usingattention-based recurrent neural networks. Furthermore, we show how toincorporate control variates into our learning algorithms for variancereduction and improved generalization. We present an evaluation on a neuralmachine translation task that shows improvements of up to 5.89 BLEU points fordomain adaptation from simulated bandit feedback.",http://arxiv.org/abs/1704.06497v2,,
252,Robust Learning via Cause-Effect Models,"We consider the problem of function estimation in the case where the datadistribution may shift between training and test time, and additionalinformation about it may be available at test time. This relates to popularscenarios such as covariate shift, concept drift, transfer learning andsemi-supervised learning. This working paper discusses how these tasks could betackled depending on the kind of changes of the distributions. It argues thatknowledge of an underlying causal direction can facilitate several of thesetasks.",http://arxiv.org/abs/1112.2738v1,,
253,Kernel Alignment for Unsupervised Transfer Learning,"The ability of a human being to extrapolate previously gained knowledge toother domains inspired a new family of methods in machine learning calledtransfer learning. Transfer learning is often based on the assumption thatobjects in both target and source domains share some common feature and/or dataspace. In this paper, we propose a simple and intuitive approach that minimizesiteratively the distance between source and target task distributions byoptimizing the kernel target alignment (KTA). We show that this procedure issuitable for transfer learning by relating it to Hilbert-Schmidt IndependenceCriterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We runour method on benchmark computer vision data sets and show that it canoutperform some state-of-art methods.",http://arxiv.org/abs/1610.06434v1,,
254,Discriminative Gaifman Models,"We present discriminative Gaifman models, a novel family of relationalmachine learning models. Gaifman models learn feature representations bottom upfrom representations of locally connected and bounded-size regions of knowledgebases (KBs). Considering local and bounded-size neighborhoods of knowledgebases renders logical inference and learning tractable, mitigates the problemof overfitting, and facilitates weight sharing. Gaifman models sampleneighborhoods of knowledge bases so as to make the learned relational modelsmore robust to missing objects and relations which is a common situation inopen-world KBs. We present the core ideas of Gaifman models and apply them tolarge-scale relational learning problems. We also discuss the ways in whichGaifman models relate to some existing relational machine learning approaches.",http://arxiv.org/abs/1610.09369v1,,
255,Federated Multi-Task Learning,"Federated learning poses new statistical and systems challenges in trainingmachine learning models over distributed networks of devices. In this work, weshow that multi-task learning is naturally suited to handle the statisticalchallenges of this setting, and propose a novel systems-aware optimizationmethod, MOCHA, that is robust to practical systems issues. Our method andtheory for the first time consider issues of high communication cost,stragglers, and fault tolerance for distributed multi-task learning. Theresulting method achieves significant speedups compared to alternatives in thefederated setting, as we demonstrate through simulations on real-worldfederated datasets.",http://arxiv.org/abs/1705.10467v2,,
256,Visual Analytics for Explainable Deep Learning,"Recently, deep learning has been advancing the state of the art in artificialintelligence to a new level, and humans rely on artificial intelligencetechniques more than ever. However, even with such unprecedented advancements,the lack of explanation regarding the decisions made by deep learning modelsand absence of control over their internal processes act as major drawbacks incritical decision-making processes, such as precision medicine and lawenforcement. In response, efforts are being made to make deep learninginterpretable and controllable by humans. In this paper, we review visualanalytics, information visualization, and machine learning perspectivesrelevant to this aim, and discuss potential challenges and future researchdirections.",http://arxiv.org/abs/1804.02527v1,,
257,Deep Embedding Kernel,"In this paper, we propose a novel supervised learning method that is calledDeep Embedding Kernel (DEK). DEK combines the advantages of deep learning andkernel methods in a unified framework. More specifically, DEK is a learnablekernel represented by a newly designed deep architecture. Compared withpre-defined kernels, this kernel can be explicitly trained to map data to anoptimized high-level feature space where data may have favorable featurestoward the application. Compared with typical deep learning using SoftMax orlogistic regression as the top layer, DEK is expected to be more generalizableto new data. Experimental results show that DEK has superior performance thantypical machine learning methods in identity detection, classification,regression, dimension reduction, and transfer learning.",http://arxiv.org/abs/1804.05806v1,,
258,Low-rank geometric mean metric learning,"We propose a low-rank approach to learning a Mahalanobis metric from data.Inspired by the recent geometric mean metric learning (GMML) algorithm, wepropose a low-rank variant of the algorithm. This allows to jointly learn alow-dimensional subspace where the data reside and the Mahalanobis metric thatappropriately fits the data. Our results show that we compete effectively withGMML at lower ranks.",http://arxiv.org/abs/1806.05454v1,,
259,Deep Learning,"Deep learning (DL) is a high dimensional data reduction technique forconstructing high-dimensional predictors in input-output models. DL is a formof machine learning that uses hierarchical layers of latent features. In thisarticle, we review the state-of-the-art of deep learning from a modeling andalgorithmic perspective. We provide a list of successful areas of applicationsin Artificial Intelligence (AI), Image Processing, Robotics and Automation.Deep learning is predictive in its nature rather then inferential and can beviewed as a black-box methodology for high-dimensional function estimation.",http://arxiv.org/abs/1807.07987v2,,
260,ToriLLE: Learning Environment for Hand-to-Hand Combat,"We present Toribash Learning Environment (ToriLLE), a learning environmentfor machine learning agents based on the video game Toribash. Toribash is aMuJoCo-like environment of two humanoid character fighting each otherhand-to-hand, controlled by changing actuation modes of the joints. Competitivenature of Toribash as well its focused domain provide a platform for evaluatingself-play methods, and evaluating machine learning agents against humanplayers. In this paper we describe the environment with ToriLLE's capabilitiesand limitations, and experimentally show its applicability as a learningenvironment. The source code of the environment and conducted experiments canbe found at https://github.com/Miffyli/ToriLLE.",http://arxiv.org/abs/1807.10110v2,,
261,Discovering General-Purpose Active Learning Strategies,"We propose a general-purpose approach to discovering active learning (AL)strategies from data. These strategies are transferable from one domain toanother and can be used in conjunction with many machine learning models. Tothis end, we formalize the annotation process as a Markov decision process,design universal state and action spaces and introduce a new reward functionthat precisely model the AL objective of minimizing the annotation cost. Weseek to find an optimal (non-myopic) AL strategy using reinforcement learning.We evaluate the learned strategies on multiple unrelated domains and show thatthey consistently outperform state-of-the-art baselines.",http://arxiv.org/abs/1810.04114v2,,
262,Learning From Positive and Unlabeled Data: A Survey,"Learning from positive and unlabeled data or PU learning is the setting wherea learner only has access to positive examples and unlabeled data. Theassumption is that the unlabeled data can contain both positive and negativeexamples. This setting has attracted increasing interest within the machinelearning literature as this type of data naturally arises in applications suchas medical diagnosis and knowledge base completion. This article provides asurvey of the current state of the art in PU learning. It proposes seven keyresearch questions that commonly arise in this field and provides a broadoverview of how the field has tried to address them.",http://arxiv.org/abs/1811.04820v1,,
263,Structure Learning Using Forced Pruning,"Markov networks are widely used in many Machine Learning applicationsincluding natural language processing, computer vision, and bioinformatics .Learning Markov networks have many complications ranging from intractablecomputations involved to the possibility of learning a model with a huge numberof parameters. In this report, we provide a computationally tractable greedyheuristic for learning Markov networks structure. The proposed heuristicresults in a model with a limited predefined number of parameters. We ran ourmethod on 3 fully-observed real datasets, and we observed that our method isdoing comparably good to the state of the art methods.",http://arxiv.org/abs/1812.00975v1,,
264,Applied Federated Learning: Improving Google Keyboard Query Suggestions,"Federated learning is a distributed form of machine learning where both thetraining data and model training are decentralized. In this paper, we usefederated learning in a commercial, global-scale setting to train, evaluate anddeploy a model to improve virtual keyboard search suggestion quality withoutdirect access to the underlying user data. We describe our observations infederated training, compare metrics to live deployments, and present resultingquality increases. In whole, we demonstrate how federated learning can beapplied end-to-end to both improve user experiences and enhance user privacy.",http://arxiv.org/abs/1812.02903v1,,
265,Robust Learning from Untrusted Sources,"Modern machine learning methods often require more data for training than asingle expert can provide. Therefore, it has become a standard procedure tocollect data from external sources, e.g. via crowdsourcing. Unfortunately, thequality of these sources is not always guaranteed. As additional complications,the data might be stored in a distributed way, or might even have to remainprivate. In this work, we address the question of how to learn robustly in suchscenarios. Studying the problem through the lens of statistical learningtheory, we derive a procedure that allows for learning from all availablesources, yet automatically suppresses irrelevant or corrupted data. We show byextensive experiments that our method provides significant improvements overalternative approaches from robust statistics and distributed optimization.",http://arxiv.org/abs/1901.10310v1,,
266,PAC-Bayes Analysis of Sentence Representation,"Learning sentence vectors from an unlabeled corpus has attracted attentionbecause such vectors can represent sentences in a lower dimensional andcontinuous space. Simple heuristics using pre-trained word vectors are widelyapplied to machine learning tasks. However, they are not well understood from atheoretical perspective. We analyze learning sentence vectors from a transferlearning perspective by using a PAC-Bayes bound that enables us to understandexisting heuristics. We show that simple heuristics such as averaging andinverse document frequency weighted averaging are derived by our formulation.Moreover, we propose novel sentence vector learning algorithms on the basis ofour PAC-Bayes analysis.",http://arxiv.org/abs/1902.04247v2,,
267,One-Shot Federated Learning,"We present one-shot federated learning, where a central server learns aglobal model over a network of federated devices in a single round ofcommunication. Our approach - drawing on ensemble learning and knowledgeaggregation - achieves an average relative gain of 51.5% in AUC over localbaselines and comes within 90.1% of the (unattainable) global ideal. We discussthese methods and identify several promising directions of future work.",http://arxiv.org/abs/1902.11175v2,,
268,"A Review of Reinforcement Learning for Autonomous Building Energy  Management","The area of building energy management has received a significant amount ofinterest in recent years. This area is concerned with combining advancements insensor technologies, communications and advanced control algorithms to optimizeenergy utilization. Reinforcement learning is one of the most prominent machinelearning algorithms used for control problems and has had many successfulapplications in the area of building energy management. This research gives acomprehensive review of the literature relating to the application ofreinforcement learning to developing autonomous building energy managementsystems. The main direction for future research and challenges in reinforcementlearning are also outlined.",http://arxiv.org/abs/1903.05196v2,,
269,"On-line learning dynamics of ReLU neural networks using statistical  physics techniques","We introduce exact macroscopic on-line learning dynamics of two-layer neuralnetworks with ReLU units in the form of a system of differential equations,using techniques borrowed from statistical physics. For the first experiments,numerical solutions reveal similar behavior compared to sigmoidal activationresearched in earlier work. In these experiments the theoretical results showgood correspondence with simulations. In ove-rrealizable and unrealizablelearning scenarios, the learning behavior of ReLU networks shows distinctivecharacteristics compared to sigmoidal networks.",http://arxiv.org/abs/1903.07378v1,,
270,Universal Learning Theory,"This encyclopedic article gives a mini-introduction into the theory ofuniversal learning, founded by Ray Solomonoff in the 1960s and significantlydeveloped and extended in the last decade. It explains the spirit of universallearning, but necessarily glosses over technical subtleties.",http://arxiv.org/abs/1102.2467v1,,
271,"On Learnability, Complexity and Stability","We consider the fundamental question of learnability of a hypotheses class inthe supervised learning setting and in the general learning setting introducedby Vladimir Vapnik. We survey classic results characterizing learnability interm of suitable notions of complexity, as well as more recent results thatestablish the connection between learnability and stability of a learningalgorithm.",http://arxiv.org/abs/1303.5976v1,,
272,Scalable Nonlinear Learning with Adaptive Polynomial Expansions,"Can we effectively learn a nonlinear representation in time comparable tolinear learning? We describe a new algorithm that explicitly and adaptivelyexpands higher-order interaction features over base linear representations. Thealgorithm is designed for extreme computational efficiency, and an extensiveexperimental study shows that its computation/prediction tradeoff abilitycompares very favorably against strong baselines.",http://arxiv.org/abs/1410.0440v1,,
273,"Lateral Connections in Denoising Autoencoders Support Supervised  Learning","We show how a deep denoising autoencoder with lateral connections can be usedas an auxiliary unsupervised learning task to support supervised learning. Theproposed model is trained to minimize simultaneously the sum of supervised andunsupervised cost functions by back-propagation, avoiding the need forlayer-wise pretraining. It improves the state of the art significantly in thepermutation-invariant MNIST classification task.",http://arxiv.org/abs/1504.08215v1,,
274,"Bias Correction for Regularized Regression and its Application in  Learning with Streaming Data","We propose an approach to reduce the bias of ridge regression andregularization kernel network. When applied to a single data set the newalgorithms have comparable learning performance with the original ones. Whenapplied to incremental learning with block wise streaming data the newalgorithms are more efficient due to bias reduction. Both theoreticalcharacterizations and simulation studies are used to verify the effectivenessof these new algorithms.",http://arxiv.org/abs/1603.04882v1,,
275,Learning Mixtures of Ising Models using Pseudolikelihood,"Maximum pseudolikelihood method has been among the most important methods forlearning parameters of statistical physics models, such as Ising models. Inthis paper, we study how pseudolikelihood can be derived for learningparameters of a mixture of Ising models. The performance of the proposedapproach is demonstrated for Ising and Potts models on both synthetic and realdata.",http://arxiv.org/abs/1506.02510v1,,
276,A vector-contraction inequality for Rademacher complexities,"The contraction inequality for Rademacher averages is extended to Lipschitzfunctions with vector-valued domains, and it is also shown that in the boundingexpression the Rademacher variables can be replaced by arbitrary iid symmetricand sub-gaussian variables. Example applications are given for multi-categorylearning, K-means clustering and learning-to-learn.",http://arxiv.org/abs/1605.00251v1,,
277,RSSL: Semi-supervised Learning in R,"In this paper, we introduce a package for semi-supervised learning researchin the R programming language called RSSL. We cover the purpose of the package,the methods it includes and comment on their use and implementation. We thenshow, using several code examples, how the package can be used to replicatewell-known results from the semi-supervised learning literature.",http://arxiv.org/abs/1612.07993v1,,
278,Learning first-order definable concepts over structures of small degree,"We consider a declarative framework for machine learning where concepts andhypotheses are defined by formulas of a logic over some background structure.We show that within this framework, concepts defined by first-order formulasover a background structure of at most polylogarithmic degree can be learned inpolylogarithmic time in the ""probably approximately correct"" learning sense.",http://arxiv.org/abs/1701.05487v1,,
279,Diameter-Based Active Learning,"To date, the tightest upper and lower-bounds for the active learning ofgeneral concept classes have been in terms of a parameter of the learningproblem called the splitting index. We provide, for the first time, anefficient algorithm that is able to realize this upper bound, and weempirically demonstrate its good performance.",http://arxiv.org/abs/1702.08553v2,,
280,Exploration by Distributional Reinforcement Learning,"We propose a framework based on distributional reinforcement learning andrecent attempts to combine Bayesian parameter updates with deep reinforcementlearning. We show that our proposed framework conceptually unifies multipleprevious methods in exploration. We also derive a practical algorithm thatachieves efficient exploration on challenging control tasks.",http://arxiv.org/abs/1805.01907v2,,
281,Implicit Policy for Reinforcement Learning,"We introduce Implicit Policy, a general class of expressive policies that canflexibly represent complex action distributions in reinforcement learning, withefficient algorithms to compute entropy regularized policy gradients. Weempirically show that, despite its simplicity in implementation, entropyregularization combined with a rich policy class can attain desirableproperties displayed under maximum entropy reinforcement learning framework,such as robustness and multi-modality.",http://arxiv.org/abs/1806.06798v2,,
282,"Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try  After Deep Learning","We found an easy and quick post-learning method named ""Icing on the Cake"" toenhance a classification performance in deep learning. The method is that wetrain only the final classifier again after an ordinary training is done.",http://arxiv.org/abs/1807.06540v1,,
283,"Seq2Seq and Multi-Task Learning for joint intent and content extraction  for domain specific interpreters","This study evaluates the performances of an LSTM network for detecting andextracting the intent and content of com- mands for a financial chatbot. Itpresents two techniques, sequence to sequence learning and Multi-Task Learning,which might improve on the previous task.",http://arxiv.org/abs/1808.00423v1,,
284,Online Learning Algorithms for Statistical Arbitrage,"Statistical arbitrage is a class of financial trading strategies using meanreversion models. The corresponding techniques rely on a number of assumptionswhich may not hold for general non-stationary stochastic processes. This paperpresents an alternative technique for statistical arbitrage based on onlinelearning which does not require such assumptions and which benefits from stronglearning guarantees.",http://arxiv.org/abs/1811.00200v1,,
285,"VC Classes are Adversarially Robustly Learnable, but Only Improperly","We study the question of learning an adversarially robust predictor. We showthat any hypothesis class $\mathcal{H}$ with finite VC dimension is robustlyPAC learnable with an improper learning rule. The requirement of being improperis necessary as we exhibit examples of hypothesis classes $\mathcal{H}$ withfinite VC dimension that are not robustly PAC learnable with any properlearning rule.",http://arxiv.org/abs/1902.04217v1,,
286,Differentially Private Learning of Geometric Concepts,"We present differentially private efficient algorithms for learning union ofpolygons in the plane (which are not necessarily convex). Our algorithmsachieve $(\alpha,\beta)$-PAC learning and $(\epsilon,\delta)$-differentialprivacy using a sample of size $\tilde{O}\left(\frac{1}{\alpha\epsilon}k\logd\right)$, where the domain is $[d]\times[d]$ and $k$ is the number of edges inthe union of polygons.",http://arxiv.org/abs/1902.05017v1,,
287,"An exact mapping between the Variational Renormalization Group and Deep  Learning","Deep learning is a broad set of techniques that uses multiple layers ofrepresentation to automatically learn relevant features directly fromstructured data. Recently, such techniques have yielded record-breaking resultson a diverse set of difficult machine learning tasks in computer vision, speechrecognition, and natural language processing. Despite the enormous success ofdeep learning, relatively little is understood theoretically about why thesetechniques are so successful at feature learning and compression. Here, we showthat deep learning is intimately related to one of the most important andsuccessful techniques in theoretical physics, the renormalization group (RG).RG is an iterative coarse-graining scheme that allows for the extraction ofrelevant features (i.e. operators) as a physical system is examined atdifferent length scales. We construct an exact mapping from the variationalrenormalization group, first introduced by Kadanoff, and deep learningarchitectures based on Restricted Boltzmann Machines (RBMs). We illustratethese ideas using the nearest-neighbor Ising Model in one and two-dimensions.Our results suggests that deep learning algorithms may be employing ageneralized RG-like scheme to learn relevant features from data.",http://arxiv.org/abs/1410.3831v1,,
288,"On the Generalization Ability of Online Learning Algorithms for Pairwise  Loss Functions","In this paper, we study the generalization properties of online learningbased stochastic methods for supervised learning problems where the lossfunction is dependent on more than one training sample (e.g., metric learning,ranking). We present a generic decoupling technique that enables us to provideRademacher complexity-based generalization error bounds. Our bounds are ingeneral tighter than those obtained by Wang et al (COLT 2012) for the sameproblem. Using our decoupling technique, we are further able to obtain fastconvergence rates for strongly convex pairwise loss functions. We are also ableto analyze a class of memory efficient online learning algorithms for pairwiselearning problems that use only a bounded subset of past training samples toupdate the hypothesis at each step. Finally, in order to complement ourgeneralization bounds, we propose a novel memory efficient online learningalgorithm for higher order learning problems with bounded regret guarantees.",http://arxiv.org/abs/1305.2505v1,,
289,Geometric Understanding of Deep Learning,"Deep learning is the mainstream technique for many machine learning tasks,including image recognition, machine translation, speech recognition, and soon. It has outperformed conventional methods in various fields and achievedgreat successes. Unfortunately, the understanding on how it works remainsunclear. It has the central importance to lay down the theoretic foundation fordeep learning.  In this work, we give a geometric view to understand deep learning: we showthat the fundamental principle attributing to the success is the manifoldstructure in data, namely natural high dimensional data concentrates close to alow-dimensional manifold, deep learning learns the manifold and the probabilitydistribution on it.  We further introduce the concepts of rectified linear complexity for deepneural network measuring its learning capability, rectified linear complexityof an embedding manifold describing the difficulty to be learned. Then we showfor any deep neural network with fixed architecture, there exists a manifoldthat cannot be learned by the network. Finally, we propose to apply optimalmass transportation theory to control the probability distribution in thelatent space.",http://arxiv.org/abs/1805.10451v2,,
290,Machine learning based hyperspectral image analysis: A survey,"Hyperspectral sensors enable the study of the chemical properties of scenematerials remotely for the purpose of identification, detection, and chemicalcomposition analysis of objects in the environment. Hence, hyperspectral imagescaptured from earth observing satellites and aircraft have been increasinglyimportant in agriculture, environmental monitoring, urban planning, mining, anddefense. Machine learning algorithms due to their outstanding predictive powerhave become a key tool for modern hyperspectral image analysis. Therefore, asolid understanding of machine learning techniques have become essential forremote sensing researchers and practitioners. This paper reviews and comparesrecent machine learning-based hyperspectral image analysis methods published inliterature. We organize the methods by the image analysis task and by the typeof machine learning algorithm, and present a two-way mapping between the imageanalysis tasks and the types of machine learning algorithms that can be appliedto them. The paper is comprehensive in coverage of both hyperspectral imageanalysis tasks and machine learning algorithms. The image analysis tasksconsidered are land cover classification, target detection, unmixing, andphysical parameter estimation. The machine learning algorithms covered areGaussian models, linear regression, logistic regression, support vectormachines, Gaussian mixture model, latent linear models, sparse linear models,Gaussian mixture models, ensemble learning, directed graphical models,undirected graphical models, clustering, Gaussian processes, Dirichletprocesses, and deep learning. We also discuss the open challenges in the fieldof hyperspectral image analysis and explore possible future directions.",http://arxiv.org/abs/1802.08701v2,,
291,Limits of End-to-End Learning,"End-to-end learning refers to training a possibly complex learning system byapplying gradient-based learning to the system as a whole. End-to-end learningsystem is specifically designed so that all modules are differentiable. Ineffect, not only a central learning machine, but also all ""peripheral"" moduleslike representation learning and memory formation are covered by a holisticlearning process. The power of end-to-end learning has been demonstrated onmany tasks, like playing a whole array of Atari video games with a singlearchitecture. While pushing for solutions to more challenging tasks, networkarchitectures keep growing more and more complex.  In this paper we ask the question whether and to what extent end-to-endlearning is a future-proof technique in the sense of scaling to complex anddiverse data processing architectures. We point out potential inefficiencies,and we argue in particular that end-to-end learning does not make optimal useof the modular design of present neural networks. Our surprisingly simpleexperiments demonstrate these inefficiencies, up to the complete breakdown oflearning.",http://arxiv.org/abs/1704.08305v1,,
292,A General Theory for Training Learning Machine,"Though the deep learning is pushing the machine learning to a new stage,basic theories of machine learning are still limited. The principle oflearning, the role of the a prior knowledge, the role of neuron bias, and thebasis for choosing neural transfer function and cost function, etc., are stillfar from clear. In this paper, we present a general theoretical framework formachine learning. We classify the prior knowledge into common andproblem-dependent parts, and consider that the aim of learning is to maximallyincorporate them. The principle we suggested for maximizing the former is thedesign risk minimization principle, while the neural transfer function, thecost function, as well as pretreatment of samples, are endowed with the rolefor maximizing the latter. The role of the neuron bias is explained from adifferent angle. We develop a Monte Carlo algorithm to establish theinput-output responses, and we control the input-output sensitivity of alearning machine by controlling that of individual neurons. Applications offunction approaching and smoothing, pattern recognition and classification, areprovided to illustrate how to train general learning machines based on ourtheory and algorithm. Our method may in addition induce new applications, suchas the transductive inference.",http://arxiv.org/abs/1704.06885v1,,
293,Quantum machine learning: a classical perspective,"Recently, increased computational power and data availability, as well asalgorithmic advances, have led machine learning techniques to impressiveresults in regression, classification, data-generation and reinforcementlearning tasks. Despite these successes, the proximity to the physical limitsof chip fabrication alongside the increasing size of datasets are motivating agrowing number of researchers to explore the possibility of harnessing thepower of quantum computation to speed-up classical machine learning algorithms.Here we review the literature in quantum machine learning and discussperspectives for a mixed readership of classical machine learning and quantumcomputation experts. Particular emphasis will be placed on clarifying thelimitations of quantum algorithms, how they compare with their best classicalcounterparts and why quantum resources are expected to provide advantages forlearning problems. Learning in the presence of noise and certaincomputationally hard problems in machine learning are identified as promisingdirections for the field. Practical questions, like how to upload classicaldata into quantum form, will also be addressed.",http://arxiv.org/abs/1707.08561v3,,
294,"Thirty Years of Machine Learning:The Road to Pareto-Optimal  Next-Generation Wireless Networks","Next-generation wireless networks (NGWN) have a substantial potential interms of supporting a broad range of complex compelling applications both inmilitary and civilian fields, where the users are able to enjoy high-rate,low-latency, low-cost and reliable information services. Achieving thisambitious goal requires new radio techniques for adaptive learning andintelligent decision making because of the complex heterogeneous nature of thenetwork structures and wireless services. Machine learning algorithms havegreat success in supporting big data analytics, efficient parameter estimationand interactive decision making. Hence, in this article, we review thethirty-year history of machine learning by elaborating on supervised learning,unsupervised learning, reinforcement learning and deep learning, respectively.Furthermore, we investigate their employment in the compelling applications ofNGWNs, including heterogeneous networks (HetNets), cognitive radios (CR),Internet of things (IoT), machine to machine networks (M2M), and so on. Thisarticle aims for assisting the readers in clarifying the motivation andmethodology of the various machine learning algorithms, so as to invoke themfor hitherto unexplored services as well as scenarios of future wirelessnetworks.",http://arxiv.org/abs/1902.01946v1,,
295,Learning Invariant Representations with Local Transformations,"Learning invariant representations is an important problem in machinelearning and pattern recognition. In this paper, we present a novel frameworkof transformation-invariant feature learning by incorporating lineartransformations into the feature learning algorithms. For example, we presentthe transformation-invariant restricted Boltzmann machine that compactlyrepresents data by its weights and their transformations, which achievesinvariance of the feature representation via probabilistic max pooling. Inaddition, we show that our transformation-invariant feature learning frameworkcan also be extended to other unsupervised learning methods, such asautoencoders or sparse coding. We evaluate our method on several imageclassification benchmark datasets, such as MNIST variations, CIFAR-10, andSTL-10, and show competitive or superior classification performance whencompared to the state-of-the-art. Furthermore, our method achievesstate-of-the-art performance on phone classification tasks with the TIMITdataset, which demonstrates wide applicability of our proposed algorithms toother domains.",http://arxiv.org/abs/1206.6418v1,,
296,"Improving Decision Analytics with Deep Learning: The Case of Financial  Disclosures","Decision analytics commonly focuses on the text mining of financial newssources in order to provide managerial decision support and to predict stockmarket movements. Existing predictive frameworks almost exclusively applytraditional machine learning methods, whereas recent research indicates thattraditional machine learning methods are not sufficiently capable of extractingsuitable features and capturing the non-linear nature of complex tasks. As aremedy, novel deep learning models aim to overcome this issue by extendingtraditional neural network models with additional hidden layers. Indeed, deeplearning has been shown to outperform traditional methods in terms ofpredictive performance. In this paper, we adapt the novel deep learningtechnique to financial decision support. In this instance, we aim to predictthe direction of stock movements following financial disclosures. As a result,we show how deep learning can outperform the accuracy of random forests as abenchmark for machine learning by 5.66%.",http://arxiv.org/abs/1508.01993v2,,
297,PAC-learning is Undecidable,"The problem of attempting to learn the mapping between data and labels is thecrux of any machine learning task. It is, therefore, of interest to the machinelearning community on practical as well as theoretical counts to consider theexistence of a test or criterion for deciding the feasibility of attempting tolearn. We investigate the existence of such a criterion in the setting ofPAC-learning, basing the feasibility solely on whether the mapping to be learntlends itself to approximation by a given class of hypothesis functions. We showthat no such criterion exists, exposing a fundamental limitation in thedecidability of learning. In other words, we prove that testing forPAC-learnability is undecidable in the Turing sense. We also briefly discusssome of the probable implications of this result to the current practice ofmachine learning.",http://arxiv.org/abs/1808.06324v1,,
298,What Can Machine Learning Teach Us about Communications?,"Rapid improvements in machine learning over the past decade are beginning tohave far-reaching effects. For communications, engineers with limited domainexpertise can now use off-the-shelf learning packages to designhigh-performance systems based on simulations. Prior to the current revolutionin machine learning, the majority of communication engineers were quite awarethat system parameters (such as filter coefficients) could be learned usingstochastic gradient descent. It was not at all clear, however, that morecomplicated parts of the system architecture could be learned as well. In thispaper, we discuss the application of machine-learning techniques to twocommunications problems and focus on what can be learned from the resultingsystems. We were pleasantly surprised that the observed gains in one examplehave a simple explanation that only became clear in hindsight. In essence, deeplearning discovered a simple and effective strategy that had not beenconsidered earlier.",http://arxiv.org/abs/1901.07592v2,,
299,"The principles of adaptation in organisms and machines I: machine  learning, information theory, and thermodynamics","How do organisms recognize their environment by acquiring knowledge about theworld, and what actions do they take based on this knowledge? This articleexamines hypotheses about organisms' adaptation to the environment from machinelearning, information-theoretic, and thermodynamic perspectives. We start withconstructing a hierarchical model of the world as an internal model in thebrain, and review standard machine learning methods to infer causes byapproximately learning the model under the maximum likelihood principle. Thisin turn provides an overview of the free energy principle for an organism, ahypothesis to explain perception and action from the principle of leastsurprise. Treating this statistical learning as communication between the worldand brain, learning is interpreted as a process to maximize information aboutthe world. We investigate how the classical theories of perception such as theinfomax principle relates to learning the hierarchical model. We then presentan approach to the recognition and learning based on thermodynamics, showingthat adaptation by causal learning results in the second law of thermodynamicswhereas inference dynamics that fuses observation with prior knowledge forms athermodynamic process. These provide a unified view on the adaptation oforganisms to the environment.",http://arxiv.org/abs/1902.11233v1,,
300,The performance of the batch learner algorithm,"We analyze completely the convergence speed of the \emph{batch learningalgorithm}, and compare its speed to that of the memoryless learning algorithmand of learning with memory. We show that the batch learning algorithm is neverworse than the memoryless learning algorithm (at least asymptotically). Itsperformance \emph{vis-a-vis} learning with full memory is less clearcut, anddepends on certain probabilistic assumptions.",http://arxiv.org/abs/cs/0201009v1,,
301,Concept-Oriented Deep Learning,"Concepts are the foundation of human deep learning, understanding, andknowledge integration and transfer. We propose concept-oriented deep learning(CODL) which extends (machine) deep learning with concept representations andconceptual understanding capability. CODL addresses some of the majorlimitations of deep learning: interpretability, transferability, contextualadaptation, and requirement for lots of labeled training data. We discuss themajor aspects of CODL including concept graph, concept representations, conceptexemplars, and concept representation learning systems supporting incrementaland continual learning.",http://arxiv.org/abs/1806.01756v1,,
302,Thumbs up? Sentiment Classification using Machine Learning Techniques,"We consider the problem of classifying documents not by topic, but by overallsentiment, e.g., determining whether a review is positive or negative. Usingmovie reviews as data, we find that standard machine learning techniquesdefinitively outperform human-produced baselines. However, the three machinelearning methods we employed (Naive Bayes, maximum entropy classification, andsupport vector machines) do not perform as well on sentiment classification ason traditional topic-based categorization. We conclude by examining factorsthat make the sentiment classification problem more challenging.",http://arxiv.org/abs/cs/0205070v1,,
303,Boltzmann Machine Learning with the Latent Maximum Entropy Principle,"We present a new statistical learning paradigm for Boltzmann machines basedon a new inference principle we have proposed: the latent maximum entropyprinciple (LME). LME is different both from Jaynes maximum entropy principleand from standard maximum likelihood estimation.We demonstrate the LMEprinciple BY deriving new algorithms for Boltzmann machine parameterestimation, and show how robust and fast new variant of the EM algorithm can bedeveloped.Our experiments show that estimation based on LME generally yieldsbetter results than maximum likelihood estimation, particularly when inferringhidden units from small amounts of data.",http://arxiv.org/abs/1212.2514v1,,
304,"Regularization approaches for support vector machines with applications  to biomedical data","The support vector machine (SVM) is a widely used machine learning tool forclassification based on statistical learning theory. Given a set of trainingdata, the SVM finds a hyperplane that separates two different classes of datapoints by the largest distance. While the standard form of SVM uses L2-normregularization, other regularization approaches are particularly attractive forbiomedical datasets where, for example, sparsity and interpretability of theclassifier's coefficient values are highly desired features. Therefore, in thispaper we consider different types of regularization approaches for SVMs, andexplore them in both synthetic and real biomedical datasets.",http://arxiv.org/abs/1710.10600v1,,
305,Nowcasting Recessions using the SVM Machine Learning Algorithm,"We introduce a novel application of Support Vector Machines (SVM), animportant Machine Learning algorithm, to determine the beginning and end ofrecessions in real time. Nowcasting, ""forecasting"" a condition about thepresent time because the full information about it is not available untillater, is key for recessions, which are only determined months after the fact.We show that SVM has excellent predictive performance for this task, and weprovide implementation details to facilitate its use in similar problems ineconomics and finance.",http://arxiv.org/abs/1903.03202v1,,
306,TonY: An Orchestrator for Distributed Machine Learning Jobs,"Training machine learning (ML) models on large datasets requires considerablecomputing power. To speed up training, it is typical to distribute trainingacross several machines, often with specialized hardware like GPUs or TPUs.Managing a distributed training job is complex and requires dealing withresource contention, distributed configurations, monitoring, and faulttolerance. In this paper, we describe TonY, an open-source orchestrator fordistributed ML jobs built at LinkedIn to address these challenges.",http://arxiv.org/abs/1904.01631v1,,
307,Meta-SGD: Learning to Learn Quickly for Few-Shot Learning,"Few-shot learning is challenging for learning algorithms that learn each taskin isolation and from scratch. In contrast, meta-learning learns from manyrelated tasks a meta-learner that can learn a new task more accurately andfaster with fewer examples, where the choice of meta-learners is crucial. Inthis paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learnerthat can initialize and adapt any differentiable learner in just one step, onboth supervised learning and reinforcement learning. Compared to the popularmeta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, andcan be learned more efficiently. Compared to the latest meta-learner MAML,Meta-SGD has a much higher capacity by learning to learn not just the learnerinitialization, but also the learner update direction and learning rate, all ina single meta-learning process. Meta-SGD shows highly competitive performancefor few-shot learning on regression, classification, and reinforcementlearning.",http://arxiv.org/abs/1707.09835v2,,
308,"Reconciled Polynomial Machine: A Unified Representation of Shallow and  Deep Learning Models","In this paper, we aim at introducing a new machine learning model, namelyreconciled polynomial machine, which can provide a unified representation ofexisting shallow and deep machine learning models. Reconciled polynomialmachine predicts the output by computing the inner product of the featurekernel function and variable reconciling function. Analysis of several concretemodels, including Linear Models, FM, MVM, Perceptron, MLP and Deep NeuralNetworks, will be provided in this paper, which can all be reduced to thereconciled polynomial machine representations. Detailed analysis of thelearning error by these models will also be illustrated in this paper based ontheir reduced representations from the function approximation perspective.",http://arxiv.org/abs/1805.07507v1,,
309,"Performance Evaluation of Machine Learning Algorithms in Post-operative  Life Expectancy in the Lung Cancer Patients","The nature of clinical data makes it difficult to quickly select, tune andapply machine learning algorithms to clinical prognosis. As a result, a lot oftime is spent searching for the most appropriate machine learning algorithmsapplicable in clinical prognosis that contains either binary-valued ormulti-valued attributes. The study set out to identify and evaluate theperformance of machine learning classification schemes applied in clinicalprognosis of post-operative life expectancy in the lung cancer patients.Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to trainand test models on Thoracic Surgery datasets obtained from the University ofCalifornia Irvine machine learning repository. Stratified 10-foldcross-validation was used to evaluate baseline performance accuracy of theclassifiers. The comparative analysis shows that multilayer perceptronperformed best with classification accuracy of 82.3%, J48 came out second withclassification accuracy of 81.8%, and Naive Bayes came out the worst withclassification accuracy of 74.4%. The quality and outcome of the chosen machinelearning algorithms depends on the ingenuity of the clinical miner.",http://arxiv.org/abs/1504.04646v1,,
310,Optimization Methods for Large-Scale Machine Learning,"This paper provides a review and commentary on the past, present, and futureof numerical optimization algorithms in the context of machine learningapplications. Through case studies on text classification and the training ofdeep neural networks, we discuss how optimization problems arise in machinelearning and what makes them challenging. A major theme of our study is thatlarge-scale machine learning represents a distinctive setting in which thestochastic gradient (SG) method has traditionally played a central role whileconventional gradient-based nonlinear optimization techniques typically falter.Based on this viewpoint, we present a comprehensive theory of astraightforward, yet versatile SG algorithm, discuss its practical behavior,and highlight opportunities for designing algorithms with improved performance.This leads to a discussion about the next generation of optimization methodsfor large-scale machine learning, including an investigation of two mainstreams of research on techniques that diminish noise in the stochasticdirections and methods that make use of second-order derivative approximations.",http://arxiv.org/abs/1606.04838v3,,
311,Debugging Machine Learning Tasks,"Unlike traditional programs (such as operating systems or word processors)which have large amounts of code, machine learning tasks use programs withrelatively small amounts of code (written in machine learning libraries), butvoluminous amounts of data. Just like developers of traditional programs debugerrors in their code, developers of machine learning tasks debug and fix errorsin their data. However, algorithms and tools for debugging and fixing errors indata are less common, when compared to their counterparts for detecting andfixing errors in code. In this paper, we consider classification tasks whereerrors in training data lead to misclassifications in test points, and proposean automated method to find the root causes of such misclassifications. Ourroot cause analysis is based on Pearl's theory of causation, and uses Pearl'sPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,encodes the computation of PS as a probabilistic program, and uses recent workon probabilistic programs and transformations on probabilistic programs (alongwith gray-box models of machine learning algorithms) to efficiently compute PS.Psi is able to identify root causes of data errors in interesting data sets.",http://arxiv.org/abs/1603.07292v1,,
312,"MILJS : Brand New JavaScript Libraries for Matrix Calculation and  Machine Learning","MILJS is a collection of state-of-the-art, platform-independent, scalable,fast JavaScript libraries for matrix calculation and machine learning. Our corelibrary offering a matrix calculation is called Sushi, which exhibits farbetter performance than any other leading machine learning libraries written inJavaScript. Especially, our matrix multiplication is 177 times faster than thefastest JavaScript benchmark. Based on Sushi, a machine learning library calledTempura is provided, which supports various algorithms widely used in machinelearning research. We also provide Soba as a visualization library. Theimplementations of our libraries are clearly written, properly documented andthus can are easy to get started with, as long as there is a web browser. Theselibraries are available from http://mil-tokyo.github.io/ under the MIT license.",http://arxiv.org/abs/1502.06064v1,,
313,Partitioning Large Scale Deep Belief Networks Using Dropout,"Deep learning methods have shown great promise in many practicalapplications, ranging from speech recognition, visual object recognition, totext processing. However, most of the current deep learning methods suffer fromscalability problems for large-scale applications, forcing researchers or usersto focus on small-scale problems with fewer parameters.  In this paper, we consider a well-known machine learning model, deep beliefnetworks (DBNs) that have yielded impressive classification performance on alarge number of benchmark machine learning tasks. To scale up DBN, we proposean approach that can use the computing clusters in a distributed environment totrain large models, while the dense matrix computations within a single machineare sped up using graphics processors (GPU). When training a DBN, each machinerandomly drops out a portion of neurons in each hidden layer, for each trainingcase, making the remaining neurons only learn to detect features that aregenerally helpful for producing the correct answer. Within our approach, wehave developed four methods to combine outcomes from each machine to form aunified model. Our preliminary experiment on the mnst handwritten digitdatabase demonstrates that our approach outperforms the state of the art testerror rate.",http://arxiv.org/abs/1508.07096v1,,
314,OpenML: An R Package to Connect to the Machine Learning Platform OpenML,"OpenML is an online machine learning platform where researchers can easilyshare data, machine learning tasks and experiments as well as organize themonline to work and collaborate more efficiently. In this paper, we present an Rpackage to interface with the OpenML platform and illustrate its usage incombination with the machine learning R package mlr. We show how the OpenMLpackage allows R users to easily search, download and upload data sets andmachine learning tasks. Furthermore, we also show how to upload results ofexperiments, share them with others and download results from other users.Beyond ensuring reproducibility of results, the OpenML platform automates muchof the drudge work, speeds up research, facilitates collaboration and increasesthe users' visibility online.",http://arxiv.org/abs/1701.01293v2,,
315,Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning,"Governments and businesses increasingly rely on data analytics and machinelearning (ML) for improving their competitive edge in areas such as consumersatisfaction, threat intelligence, decision making, and product efficiency.However, by cleverly corrupting a subset of data used as input to a target's MLalgorithms, an adversary can perturb outcomes and compromise the effectivenessof ML technology. While prior work in the field of adversarial machine learninghas studied the impact of input manipulation on correct ML algorithms, weconsider the exploitation of bugs in ML implementations. In this paper, wecharacterize the attack surface of ML programs, and we show that maliciousinputs exploiting implementation bugs enable strictly more powerful attacksthan the classic adversarial machine learning techniques. We propose asemi-automated technique, called steered fuzzing, for exploring this attacksurface and for discovering exploitable bugs in machine learning programs, inorder to demonstrate the magnitude of this threat. As a result of our work, weresponsibly disclosed five vulnerabilities, established three new CVE-IDs, andilluminated a common insecure practice across many machine learning systems.Finally, we outline several research directions for further understanding andmitigating this threat.",http://arxiv.org/abs/1701.04739v1,,
316,Attacking Machine Learning models as part of a cyber kill chain,"Machine learning is gaining popularity in the network security domain as manymore network-enabled devices get connected, as malicious activities becomestealthier, and as new technologies like Software Defined Networking emerge.Compromising machine learning model is a desirable goal. In fact, spammers havebeen quite successful getting through machine learning enabled spam filters foryears. While previous works have been done on adversarial machine learning,none has been considered within a defense-in-depth environment, in whichcorrect classification alone may not be good enough. For the first time, thispaper proposes a cyber kill-chain for attacking machine learning modelstogether with a proof of concept. The intention is to provide a high levelattack model that inspire more secure processes inresearch/design/implementation of machine learning based security solutions.",http://arxiv.org/abs/1705.00564v2,,
317,Towards Machine Learning on data from Professional Cyclists,"Professional sports are developing towards increasingly scientific trainingmethods with increasing amounts of data being collected from laboratory tests,training sessions and competitions. In cycling, it is standard to equipbicycles with small computers recording data from sensors such as power-meters,in addition to heart-rate, speed, altitude etc. Recently, machine learningtechniques have provided huge success in a wide variety of areas where largeamounts of data (big data) is available. In this paper, we perform a pilotexperiment on machine learning to model physical response in elite cyclists. Asa first experiment, we show that it is possible to train a LSTM machinelearning algorithm to predict the heart-rate response of a cyclist during atraining session. This work is a promising first step towards developing moreelaborate models based on big data and machine learning to capture performanceaspects of athletes.",http://arxiv.org/abs/1808.00198v1,,
318,Helix: Accelerating Human-in-the-loop Machine Learning,"Data application developers and data scientists spend an inordinate amount oftime iterating on machine learning (ML) workflows -- by modifying the datapre-processing, model training, and post-processing steps -- viatrial-and-error to achieve the desired model performance. Existing work onaccelerating machine learning focuses on speeding up one-shot execution ofworkflows, failing to address the incremental and dynamic nature of typical MLdevelopment. We propose Helix, a declarative machine learning system thataccelerates iterative development by optimizing workflow execution end-to-endand across iterations. Helix minimizes the runtime per iteration via programanalysis and intelligent reuse of previous results, which are selectivelymaterialized -- trading off the cost of materialization for potential futurebenefits -- to speed up future iterations. Additionally, Helix offers agraphical interface to visualize workflow DAGs and compare versions tofacilitate iterative development. Through two ML applications, inclassification and in structured prediction, attendees will experience thesuccinctness of Helix programming interface and the speed and ease of iterativedevelopment using Helix. In our evaluations, Helix achieved up to an order ofmagnitude reduction in cumulative run time compared to state-of-the-art machinelearning tools.",http://arxiv.org/abs/1808.01095v1,,
319,"Interlacing Personal and Reference Genomes for Machine Learning  Disease-Variant Detection","DNA sequencing to identify genetic variants is becoming increasingly valuablein clinical settings. Assessment of variants in such sequencing data iscommonly implemented through Bayesian heuristic algorithms. Machine learninghas shown great promise in improving on these variant calls, but the input forthese is still a standardized ""pile-up"" image, which is not always best suited.In this paper, we present a novel method for generating images from DNAsequencing data, which interlaces the human reference genome with personalizedsequencing output, to maximize usage of sequencing reads and improve machinelearning algorithm performance. We demonstrate the success of this in improvingstandard germline variant calling. We also furthered this approach to includesomatic variant calling across tumor/normal data with Siamese networks. Theseapproaches can be used in machine learning applications on sequencing data withthe hope of improving clinical outcomes, and are freely available fornoncommercial use at www.ccg.ai.",http://arxiv.org/abs/1811.11674v1,,
320,Personalized explanation in machine learning,"Explanation in machine learning and related fields such as artificialintelligence aims at making machine learning models and their decisionsunderstandable to humans. Existing work suggests that personalizingexplanations might help to improve understandability. In this work, we derive aconceptualization of personalized explanation by defining and structuring theproblem based on prior work on machine learning explanation, personalization(in machine learning) and concepts and techniques from other domains such asprivacy and knowledge elicitation. We perform a categorization of explaineeinformation used in the process of personalization as well as describing meansto collect this information. We also identify three key explanation propertiesthat are amendable to personalization: complexity, decision information andpresentation. We also enhance existing work on explanation by introducingadditional desiderata and measures to quantify the quality of personalizedexplanations.",http://arxiv.org/abs/1901.00770v1,,
321,Applications of Machine Learning in Cryptography: A Survey,"Machine learning techniques have had a long list of applications in recentyears. However, the use of machine learning in information and network securityis not new. Machine learning and cryptography have many things in common. Themost apparent is the processing of large amounts of data and large searchspaces. In its varying techniques, machine learning has been an interestingfield of study with massive potential for application. In the past threedecades, machine learning techniques, whether supervised or unsupervised, havebeen applied in cryptographic algorithms, cryptanalysis, steganography, amongother data-security-related applications. This paper presents an updated surveyof applications of machine learning techniques in cryptography andcryptanalysis. The paper summarizes the research done in these areas andprovides suggestions for future directions in research.",http://arxiv.org/abs/1902.04109v1,,
322,"Unmasking Clever Hans Predictors and Assessing What Machines Really  Learn","Current learning machines have successfully solved hard application problems,reaching high accuracy and displaying seemingly ""intelligent"" behavior. Here weapply recent techniques for explaining decisions of state-of-the-art learningmachines and analyze various tasks from computer vision and arcade games. Thisshowcases a spectrum of problem-solving behaviors ranging from naive andshort-sighted, to well-informed and strategic. We observe that standardperformance evaluation metrics can be oblivious to distinguishing these diverseproblem solving behaviors. Furthermore, we propose our semi-automated SpectralRelevance Analysis that provides a practically effective way of characterizingand validating the behavior of nonlinear learning machines. This helps toassess whether a learned model indeed delivers reliably for the problem that itwas conceived for. Furthermore, our work intends to add a voice of caution tothe ongoing excitement about machine intelligence and pledges to evaluate andjudge some of these recent successes in a more nuanced manner.",http://arxiv.org/abs/1902.10178v1,,
323,The invisible power of fairness. How machine learning shapes democracy,"Many machine learning systems make extensive use of large amounts of dataregarding human behaviors. Several researchers have found variousdiscriminatory practices related to the use of human-related machine learningsystems, for example in the field of criminal justice, credit scoring andadvertising. Fair machine learning is therefore emerging as a new field ofstudy to mitigate biases that are inadvertently incorporated into algorithms.Data scientists and computer engineers are making various efforts to providedefinitions of fairness. In this paper, we provide an overview of the mostwidespread definitions of fairness in the field of machine learning, arguingthat the ideas highlighting each formalization are closely related to differentideas of justice and to different interpretations of democracy embedded in ourculture. This work intends to analyze the definitions of fairness that havebeen proposed to date to interpret the underlying criteria and to relate themto different ideas of democracy.",http://arxiv.org/abs/1903.09493v1,,
324,"Informed Machine Learning - Towards a Taxonomy of Explicit Integration  of Knowledge into Machine Learning","Despite the great successes of machine learning, it can have its limits whendealing with insufficient training data.A potential solution is to incorporateadditional knowledge into the training process which leads to the idea ofinformed machine learning. We present a research survey and structured overviewof various approaches in this field. We aim to establish a taxonomy which canserve as a classification framework that considers the kind of additionalknowledge, its representation,and its integration into the machine learningpipeline. The evaluation of numerous papers on the bases of the taxonomyuncovers key methods in this field.",http://arxiv.org/abs/1903.12394v1,,
325,Efficient Representations for Life-Long Learning and Autoencoding,"It has been a long-standing goal in machine learning, as well as in AI moregenerally, to develop life-long learning systems that learn many differenttasks over time, and reuse insights from tasks learned, ""learning to learn"" asthey do so. In this work we pose and provide efficient algorithms for severalnatural theoretical formulations of this goal. Specifically, we consider theproblem of learning many different target functions over time, that sharecertain commonalities that are initially unknown to the learning algorithm. Ouraim is to learn new internal representations as the algorithm learns new targetfunctions, that capture this commonality and allow subsequent learning tasks tobe solved more efficiently and from less data. We develop efficient algorithmsfor two very different kinds of commonalities that target functions mightshare: one based on learning common low-dimensional and unions oflow-dimensional subspaces and one based on learning nonlinear Booleancombinations of features. Our algorithms for learning Boolean featurecombinations additionally have a dual interpretation, and can be viewed asgiving an efficient procedure for constructing near-optimal sparse Booleanautoencoders under a natural ""anchor-set"" assumption.",http://arxiv.org/abs/1411.1490v2,,
326,Few-shot Learning with Meta Metric Learners,"Few-shot Learning aims to learn classifiers for new classes with only a fewtraining examples per class. Existing meta-learning or metric-learning basedfew-shot learning approaches are limited in handling diverse domains withvarious number of labels. The meta-learning approaches train a meta learner topredict weights of homogeneous-structured task-specific networks, requiring auniform number of classes across tasks. The metric-learning approaches learnone task-invariant metric for all the tasks, and they fail if the tasksdiverge. We propose to deal with these limitations with meta metric learning.Our meta metric learning approach consists of task-specific learners, thatexploit metric learning to handle flexible labels, and a meta learner, thatdiscovers good parameters and gradient decent to specify the metrics intask-specific learners. Thus the proposed model is able to handle unbalancedclasses as well as to generate task-specific metrics. We test our approach inthe `$k$-shot $N$-way' few-shot learning setting used in previous work and newrealistic few-shot setting with diverse multi-domain tasks and flexible labelnumbers. Experiments show that our approach attains superior performances inboth settings.",http://arxiv.org/abs/1901.09890v1,,
327,Hindsight Generative Adversarial Imitation Learning,"Compared to reinforcement learning, imitation learning (IL) is a powerfulparadigm for training agents to learn control policies efficiently from expertdemonstrations. However, in most cases, obtaining demonstration data is costlyand laborious, which poses a significant challenge in some scenarios. Apromising alternative is to train agent learning skills via imitation learningwithout expert demonstrations, which, to some extent, would extremely expandimitation learning areas. To achieve such expectation, in this paper, wepropose Hindsight Generative Adversarial Imitation Learning (HGAIL) algorithm,with the aim of achieving imitation learning satisfying no need ofdemonstrations. Combining hindsight idea with the generative adversarialimitation learning (GAIL) framework, we realize implementing imitation learningsuccessfully in cases of expert demonstration data are not available.Experiments show that the proposed method can train policies showing comparableperformance to current imitation learning methods. Further more, HGAILessentially endows curriculum learning mechanism which is critical for learningpolicies.",http://arxiv.org/abs/1903.07854v1,,
328,"Linear Classification of data with Support Vector Machines and  Generalized Support Vector Machines","In this paper, we study the support vector machine and introduced the notionof generalized support vector machine for classification of data. We show thatthe problem of generalized support vector machine is equivalent to the problemof generalized variational inequality and establish various results for theexistence of solutions. Moreover, we provide various examples to support ourresults.",http://arxiv.org/abs/1606.05664v1,,
329,Poor starting points in machine learning,"Poor (even random) starting points for learning/training/optimization arecommon in machine learning. In many settings, the method of Robbins and Monro(online stochastic gradient descent) is known to be optimal for good startingpoints, but may not be optimal for poor starting points -- indeed, for poorstarting points Nesterov acceleration can help during the initial iterations,even though Nesterov methods not designed for stochastic approximation couldhurt during later iterations. The common practice of training with nontrivialminibatches enhances the advantage of Nesterov acceleration.",http://arxiv.org/abs/1602.02823v1,,
330,A Hybrid Algorithm for Convex Semidefinite Optimization,"We present a hybrid algorithm for optimizing a convex, smooth function overthe cone of positive semidefinite matrices. Our algorithm converges to theglobal optimal solution and can be used to solve general large-scalesemidefinite programs and hence can be readily applied to a variety of machinelearning problems. We show experimental results on three machine learningproblems (matrix completion, metric learning, and sparse PCA) . Our approachoutperforms state-of-the-art algorithms.",http://arxiv.org/abs/1206.4608v1,,
331,Nonnegative/binary matrix factorization with a D-Wave quantum annealer,"D-Wave quantum annealers represent a novel computational architecture andhave attracted significant interest, but have been used for few real-worldcomputations. Machine learning has been identified as an area where quantumannealing may be useful. Here, we show that the D-Wave 2X can be effectivelyused as part of an unsupervised machine learning method. This method can beused to analyze large datasets. The D-Wave only limits the number of featuresthat can be extracted from the dataset. We apply this method to learn thefeatures from a set of facial images.",http://arxiv.org/abs/1704.01605v1,,
332,"Technical Report of Participation in Higgs Boson Machine Learning  Challenge","This report entails the detailed description of the approach andmethodologies taken as part of competing in the Higgs Boson Machine LearningCompetition hosted by Kaggle Inc. and organized by CERN et al. It brieflydescribes the theoretical background of the problem and the motivation fortaking part in the competition. Furthermore, the various machine learningmodels and algorithms analyzed and implemented during the 4 month period ofparticipation are discussed and compared. Special attention is paid to the DeepLearning techniques and architectures implemented from scratch using Python andNumPy for this competition.",http://arxiv.org/abs/1510.02674v1,,
333,Online Learning for Neural Machine Translation Post-editing,"Neural machine translation has meant a revolution of the field. Nevertheless,post-editing the outputs of the system is mandatory for tasks requiring hightranslation quality. Post-editing offers a unique opportunity for improvingneural machine translation systems, using online learning techniques andtreating the post-edited translations as new, fresh training data. We reviewclassical learning methods and propose a new optimization algorithm. Wethoroughly compare online learning algorithms in a post-editing scenario.Results show significant improvements in translation quality and effortreduction.",http://arxiv.org/abs/1706.03196v1,,
334,Machine Learning and Manycore Systems Design: A Serendipitous Symbiosis,"Tight collaboration between experts of machine learning and manycore systemdesign is necessary to create a data-driven manycore design framework thatintegrates both learning and expert knowledge. Such a framework will benecessary to address the rising complexity of designing large-scale manycoresystems and machine learning techniques.",http://arxiv.org/abs/1712.00076v1,,
335,Bridging Cognitive Programs and Machine Learning,"While great advances are made in pattern recognition and machine learning,the successes of such fields remain restricted to narrow applications and seemto break down when training data is scarce, a shift in domain occurs, or whenintelligent reasoning is required for rapid adaptation to new environments. Inthis work, we list several of the shortcomings of modern machine-learningsolutions, specifically in the contexts of computer vision and in reinforcementlearning and suggest directions to explore in order to try to ameliorate theseweaknesses.",http://arxiv.org/abs/1802.06091v1,,
336,Interpreting Complex Regression Models,"Interpretation of a machine learning induced models is critical for featureengineering, debugging, and, arguably, compliance. Yet, best of breed machinelearning models tend to be very complex. This paper presents a method for modelinterpretation which has the main benefit that the simple interpretations itprovides are always grounded in actual sets of learning examples. The method isvalidated on the task of interpreting a complex regression model in the contextof both an academic problem -- predicting the year in which a song was recordedand an industrial one -- predicting mail user churn.",http://arxiv.org/abs/1802.09225v1,,
337,Fast Counting in Machine Learning Applications,"We propose scalable methods to execute counting queries in machine learningapplications. To achieve memory and computational efficiency, we abstractcounting queries and their context such that the counts can be aggregated as astream. We demonstrate performance and scalability of the resulting approach onrandom queries, and through extensive experimentation using Bayesian networkslearning and association rule mining. Our methods significantly outperformcommonly used ADtrees and hash tables, and are practical alternatives forprocessing large-scale data.",http://arxiv.org/abs/1804.04640v3,,
338,Mapping Images to Psychological Similarity Spaces Using Neural Networks,"The cognitive framework of conceptual spaces bridges the gap between symbolicand subsymbolic AI by proposing an intermediate conceptual layer whereknowledge is represented geometrically. There are two main approaches forobtaining the dimensions of this conceptual similarity space: using similarityratings from psychological experiments and using machine learning techniques.In this paper, we propose a combination of both approaches by usingpsychologically derived similarity ratings to constrain the machine learningprocess. This way, a mapping from stimuli to conceptual spaces can be learnedthat is both supported by psychological data and allows generalization tounseen stimuli. The results of a first feasibility study support our proposedapproach.",http://arxiv.org/abs/1804.07758v1,,
339,"Tell Me Something New: A New Framework for Asynchronous Parallel  Learning","We present a novel approach for parallel computation in the context ofmachine learning that we call ""Tell Me Something New"" (TMSN). This approachinvolves a set of independent workers that use broadcast to update each otherwhen they observe ""something new"". TMSN does not require synchronization or ahead node and is highly resilient against failing machines or laggards. Wedemonstrate the utility of TMSN by applying it to learning boosted trees. Weshow that our implementation is 10 times faster than XGBoost and LightGBM onthe splice-site prediction problem.",http://arxiv.org/abs/1805.07483v2,,
340,Rethinking Machine Learning Development and Deployment for Edge Devices,"Machine learning (ML), especially deep learning is made possible by theavailability of big data, enormous compute power and, often overlooked,development tools or frameworks. As the algorithms become mature and efficient,more and more ML inference is moving out of datacenters/cloud and deployed onedge devices. This model deployment process can be challenging as thedeployment environment and requirements can be substantially different fromthose during model development. In this paper, we propose a new ML developmentand deployment approach that is specially designed and optimized forinference-only deployment on edge devices. We build a prototype and demonstratethat this approach can address all the deployment challenges and result in moreefficient and high-quality solutions.",http://arxiv.org/abs/1806.07846v1,,
341,A Boo(n) for Evaluating Architecture Performance,"We point out important problems with the common practice of using the bestsingle model performance for comparing deep learning architectures, and wepropose a method that corrects these flaws. Each time a model is trained, onegets a different result due to random factors in the training process, whichinclude random parameter initialization and random data shuffling. Reportingthe best single model performance does not appropriately address thisstochasticity. We propose a normalized expected best-out-of-$n$ performance($\text{Boo}_n$) as a way to correct these problems.",http://arxiv.org/abs/1807.01961v2,,
342,Machine Learning for semi linear PDEs,"Recent machine learning algorithms dedicated to solving semi-linear PDEs areimproved by using different neural network architectures and differentparameterizations. These algorithms are compared to a new one that solves afixed point problem by using deep learning techniques. This new algorithmappears to be competitive in terms of accuracy with the best existingalgorithms.",http://arxiv.org/abs/1809.07609v2,,
343,"Machine Learning Analysis of Heterogeneity in the Effect of Student  Mindset Interventions","We study heterogeneity in the effect of a mindset intervention onstudent-level performance through an observational dataset from the NationalStudy of Learning Mindsets (NSLM). Our analysis uses machine learning (ML) toaddress the following associated problems: assessing treatment group overlapand covariate balance, imputing conditional average treatment effects, andinterpreting imputed effects. By comparing several different model families weillustrate the flexibility of both off-the-shelf and purpose-built estimators.We find that the mindset intervention has a positive average effect of 0.26,95%-CI [0.22, 0.30], and that heterogeneity in the range of [0.1, 0.4] ismoderated by school-level achievement level, poverty concentration, urbanicity,and student prior expectations.",http://arxiv.org/abs/1811.05975v1,,
344,Computations in Stochastic Acceptors,"Machine learning provides algorithms that can learn from data and makeinferences or predictions on data. Stochastic acceptors or probabilisticautomata are stochastic automata without output that can model components inmachine learning scenarios. In this paper, we provide dynamic programmingalgorithms for the computation of input marginals and the acceptanceprobabilities in stochastic acceptors. Furthermore, we specify an algorithm forthe parameter estimation of the conditional probabilities using theexpectation-maximization technique and a more efficient implementation relatedto the Baum-Welch algorithm.",http://arxiv.org/abs/1812.09687v1,,
345,Advances of Machine Learning in Molecular Modeling and Simulation,"In this review, we highlight recent developments in the application ofmachine learning for molecular modeling and simulation. After giving a briefoverview of the foundations, components, and workflow of a typical supervisedlearning approach for chemical problems, we showcase areas and state-of-the-artexamples of their deployment. In this context, we discuss how machine learningrelates to, supports, and augments more traditional physics-based approaches incomputational research. We conclude by outlining challenges and future researchdirections that need to be addressed in order to make machine learning amainstream chemical engineering tool.",http://arxiv.org/abs/1902.00140v2,,
346,Fourier Transform Approach to Machine Learning,"We propose a supervised learning algorithm for machine learning applications.Contrary to the model developing in the classical methods, which treattraining, validation, and test as separate steps, in the presented approach,there is a unified training and evaluating procedure based on an iterative bandfiltering by the use of a fast Fourier transform. The presented approach doesnot apply the method of least squares, thus, basically typical ill-conditionedmatrices do not occur at all. The optimal model results from the convergence ofthe performance metric, which automatically prevents the usual underfitting andoverfitting problems. The algorithm capability is investigated for noisy data,and the obtained result demonstrates a reliable and powerful machine learningapproach beyond the typical limits of the classical methods.",http://arxiv.org/abs/1904.00368v2,,
347,Can Boltzmann Machines Discover Cluster Updates ?,"Boltzmann machines are physics informed generative models with wideapplications in machine learning. They can learn the probability distributionfrom an input dataset and generate new samples accordingly. Applying them backto physics, the Boltzmann machines are ideal recommender systems to accelerateMonte Carlo simulation of physical systems due to their flexibility andeffectiveness. More intriguingly, we show that the generative sampling of theBoltzmann Machines can even discover unknown cluster Monte Carlo algorithms.The creative power comes from the latent representation of the Boltzmannmachines, which learn to mediate complex interactions and identify clusters ofthe physical system. We demonstrate these findings with concrete examples ofthe classical Ising model with and without four spin plaquette interactions.Our results endorse a fresh research paradigm where intelligent machines aredesigned to create or inspire human discovery of innovative algorithms.",http://arxiv.org/abs/1702.08586v1,,
348,Structure Learning of Sparse GGMs over Multiple Access Networks,"A central machine is interested in estimating the underlying structure of asparse Gaussian Graphical Model (GGM) from datasets distributed across multiplelocal machines. The local machines can communicate with the central machinethrough a wireless multiple access channel. In this paper, we are interested indesigning effective strategies where reliable learning is feasible under powerand bandwidth limitations. Two approaches are proposed: Signs and Uncodedmethods. In Signs method, the local machines quantize their data into binaryvectors and an optimal channel coding scheme is used to reliably send thevectors to the central machine where the structure is learned from the receiveddata. In Uncoded method, data symbols are scaled and transmitted through thechannel. The central machine uses the received noisy symbols to recover thestructure. Theoretical results show that both methods can recover the structurewith high probability for large enough sample size. Experimental resultsindicate the superiority of Signs method over Uncoded method under severalcircumstances.",http://arxiv.org/abs/1812.10437v1,,
349,Distributed Training of Structured SVM,"Training structured prediction models is time-consuming. However, mostexisting approaches only use a single machine, thus, the advantage of computingpower and the capacity for larger data sets of multiple machines have not beenexploited. In this work, we propose an efficient algorithm for distributedlytraining structured support vector machines based on a distributedblock-coordinate descent method. Both theoretical and experimental resultsindicate that our method is efficient.",http://arxiv.org/abs/1506.02620v2,,
350,Learning Boltzmann Machine with EM-like Method,"We propose an expectation-maximization-like(EMlike) method to train Boltzmannmachine with unconstrained connectivity. It adopts Monte Carlo approximation inthe E-step, and replaces the intractable likelihood objective with efficientlycomputed objectives or directly approximates the gradient of likelihoodobjective in the M-step. The EM-like method is a modification of alternatingminimization. We prove that EM-like method will be the exactly same withcontrastive divergence in restricted Boltzmann machine if the M-step of thismethod adopts special approximation. We also propose a new measure to assessthe performance of Boltzmann machine as generative models of data, and itscomputational complexity is O(Rmn). Finally, we demonstrate the performance ofEM-like method using numerical experiments.",http://arxiv.org/abs/1609.01840v1,,
351,A Survey on Metric Learning for Feature Vectors and Structured Data,"The need for appropriate ways to measure the distance or similarity betweendata is ubiquitous in machine learning, pattern recognition and data mining,but handcrafting such good metrics for specific problems is generallydifficult. This has led to the emergence of metric learning, which aims atautomatically learning a metric from data and has attracted a lot of interestin machine learning and related fields for the past ten years. This surveypaper proposes a systematic review of the metric learning literature,highlighting the pros and cons of each approach. We pay particular attention toMahalanobis distance metric learning, a well-studied and successful framework,but additionally present a wide range of methods that have recently emerged aspowerful alternatives, including nonlinear metric learning, similarity learningand local metric learning. Recent trends and extensions, such assemi-supervised metric learning, metric learning for histogram data and thederivation of generalization guarantees, are also covered. Finally, this surveyaddresses metric learning for structured data, in particular edit distancelearning, and attempts to give an overview of the remaining challenges inmetric learning for the years to come.",http://arxiv.org/abs/1306.6709v4,,
352,Differential Privacy and Machine Learning: a Survey and Review,"The objective of machine learning is to extract useful information from data,while privacy is preserved by concealing information. Thus it seems hard toreconcile these competing interests. However, they frequently must be balancedwhen mining sensitive data. For example, medical research represents animportant application where it is necessary both to extract useful informationand protect patient privacy. One way to resolve the conflict is to extractgeneral characteristics of whole populations without disclosing the privateinformation of individuals.  In this paper, we consider differential privacy, one of the most popular andpowerful definitions of privacy. We explore the interplay between machinelearning and differential privacy, namely privacy-preserving machine learningalgorithms and learning-based data release mechanisms. We also describe sometheoretical results that address what can be learned differentially privatelyand upper bounds of loss functions for differentially private algorithms.  Finally, we present some open questions, including how to incorporate publicdata, how to deal with missing data in private datasets, and whether, as thenumber of observed samples grows arbitrarily large, differentially privatemachine learning algorithms can be achieved at no cost to utility as comparedto corresponding non-differentially private algorithms.",http://arxiv.org/abs/1412.7584v1,,
353,Revisiting Large Scale Distributed Machine Learning,"Nowadays, with the widespread of smartphones and other portable gadgetsequipped with a variety of sensors, data is ubiquitous available and the focusof machine learning has shifted from being able to infer from small trainingsamples to dealing with large scale high-dimensional data. In domains such aspersonal healthcare applications, which motivates this survey, distributedmachine learning is a promising line of research, both for scaling up learningalgorithms, but mostly for dealing with data which is inherently produced atdifferent locations. This report offers a thorough overview of andstate-of-the-art algorithms for distributed machine learning, for bothsupervised and unsupervised learning, ranging from simple linear logisticregression to graphical models and clustering. We propose future directions formost categories, specific to the potential personal healthcare applications.With this in mind, the report focuses on how security and low communicationoverhead can be assured in the specific case of a strictly client-serverarchitectural model. As particular directions we provides an exhaustivepresentation of an empirical clustering algorithm, k-windows, and proposed anasynchronous distributed machine learning algorithm that would scale well andalso would be computationally cheap and easy to implement.",http://arxiv.org/abs/1507.01461v1,,
354,A new boosting algorithm based on dual averaging scheme,"The fields of machine learning and mathematical optimization increasinglyintertwined. The special topic on supervised learning and convex optimizationexamines this interplay. The training part of most supervised learningalgorithms can usually be reduced to an optimization problem that minimizes aloss between model predictions and training data. While most optimizationtechniques focus on accuracy and speed of convergence, the qualities of goodoptimization algorithm from the machine learning perspective can be quitedifferent since machine learning is more than fitting the data. Betteroptimization algorithms that minimize the training loss can possibly give verypoor generalization performance. In this paper, we examine a particular kind ofmachine learning algorithm, boosting, whose training process can be viewed asfunctional coordinate descent on the exponential loss. We study the relationbetween optimization techniques and machine learning by implementing a newboosting algorithm. DABoost, based on dual-averaging scheme and study itsgeneralization performance. We show that DABoost, although slower in reducingthe training error, in general enjoys a better generalization error thanAdaBoost.",http://arxiv.org/abs/1507.03125v1,,
355,Interpretability via Model Extraction,"The ability to interpret machine learning models has become increasinglyimportant now that machine learning is used to inform consequential decisions.We propose an approach called model extraction for interpreting complex,blackbox models. Our approach approximates the complex model using a much moreinterpretable model; as long as the approximation quality is good, thenstatistical properties of the complex model are reflected in the interpretablemodel. We show how model extraction can be used to understand and debug randomforests and neural nets trained on several datasets from the UCI MachineLearning Repository, as well as control policies learned for several classicalreinforcement learning problems.",http://arxiv.org/abs/1706.09773v4,,
356,Modeling reverse thinking for machine learning,"Human inertial thinking schemes can be formed through learning, which arethen applied to quickly solve similar problems later. However, when problemsare significantly different, inertial thinking generally presents the solutionsthat are definitely imperfect. In such cases, people will apply creativethinking, such as reverse thinking, to solve problems. Similarly, machinelearning methods also form inertial thinking schemes through learning theknowledge from a large amount of data. However, when the testing data arevastly difference, the formed inertial thinking schemes will inevitablygenerate errors. This kind of inertial thinking is called illusion inertialthinking. Because all machine learning methods do not consider illusioninertial thinking, in this paper we propose a new method that uses reversethinking to correct illusion inertial thinking, which increases thegeneralization ability of machine learning methods. Experimental results onbenchmark datasets are used to validate the proposed method.",http://arxiv.org/abs/1803.00158v1,,
357,Towards Intelligent Vehicular Networks: A Machine Learning Framework,"As wireless networks evolve towards high mobility and providing bettersupport for connected vehicles, a number of new challenges arise due to theresulting high dynamics in vehicular environments and thus motive rethinking oftraditional wireless design methodologies. Future intelligent vehicles, whichare at the heart of high mobility networks, are increasingly equipped withmultiple advanced onboard sensors and keep generating large volumes of data.Machine learning, as an effective approach to artificial intelligence, canprovide a rich set of tools to exploit such data for the benefit of thenetworks. In this article, we first identify the distinctive characteristics ofhigh mobility vehicular networks and motivate the use of machine learning toaddress the resulting challenges. After a brief introduction of the majorconcepts of machine learning, we discuss its applications to learn the dynamicsof vehicular networks and make informed decisions to optimize networkperformance. In particular, we discuss in greater detail the application ofreinforcement learning in managing network resources as an alternative to theprevalent optimization approach. Finally, some open issues worth furtherinvestigation are highlighted.",http://arxiv.org/abs/1804.00338v2,,
358,"Why Interpretability in Machine Learning? An Answer Using Distributed  Detection and Data Fusion Theory","As artificial intelligence is increasingly affecting all parts of society andlife, there is growing recognition that human interpretability of machinelearning models is important. It is often argued that accuracy or other similargeneralization performance metrics must be sacrificed in order to gaininterpretability. Such arguments, however, fail to acknowledge that the overalldecision-making system is composed of two entities: the learned model and ahuman who fuses together model outputs with his or her own information. Assuch, the relevant performance criteria should be for the entire system, notjust for the machine learning component. In this work, we characterize theperformance of such two-node tandem data fusion systems using the theory ofdistributed detection. In doing so, we work in the population setting and modelinterpretable learned models as multi-level quantizers. We prove that under ourabstraction, the overall system of a human with an interpretable classifieroutperforms one with a black box classifier.",http://arxiv.org/abs/1806.09710v1,,
359,"Importance of the Mathematical Foundations of Machine Learning Methods  for Scientific and Engineering Applications","There has been a lot of recent interest in adopting machine learning methodsfor scientific and engineering applications. This has in large part beeninspired by recent successes and advances in the domains of Natural LanguageProcessing (NLP) and Image Classification (IC). However, scientific andengineering problems have their own unique characteristics and requirementsraising new challenges for effective design and deployment of machine learningapproaches. There is a strong need for further mathematical developments on thefoundations of machine learning methods to increase the level of rigor ofemployed methods and to ensure more reliable and interpretable results. Also asreported in the recent literature on state-of-the-art results and indicated bythe No Free Lunch Theorems of statistical learning theory incorporating someform of inductive bias and domain knowledge is essential to success.Consequently, even for existing and widely used methods there is a strong needfor further mathematical work to facilitate ways to incorporate priorscientific knowledge and related inductive biases into learning frameworks andalgorithms. We briefly discuss these topics and discuss some ideas proceedingin this direction.",http://arxiv.org/abs/1808.02213v1,,
360,"Natively Interpretable Machine Learning and Artificial Intelligence:  Preliminary Results and Future Directions","Machine learning models have become more and more complex in order to betterapproximate complex functions. Although fruitful in many domains, the addedcomplexity has come at the cost of model interpretability. The once populark-nearest neighbors (kNN) approach, which finds and uses the most similar datafor reasoning, has received much less attention in recent decades due tonumerous problems when compared to other techniques. We show that many of thesehistorical problems with kNN can be overcome, and our contribution hasapplications not only in machine learning but also in online learning, datasynthesis, anomaly detection, model compression, and reinforcement learning,without sacrificing interpretability. We introduce a synthesis between kNN andinformation theory that we hope will provide a clear path towards models thatare innately interpretable and auditable. Through this work we hope to gatherinterest in combining kNN with information theory as a promising path to fullyauditable machine learning and artificial intelligence.",http://arxiv.org/abs/1901.00246v2,,
361,Deep Reinforcement Learning,"We discuss deep reinforcement learning in an overview style. We draw a bigpicture, filled with details. We discuss six core elements, six importantmechanisms, and twelve applications, focusing on contemporary work, and inhistorical contexts. We start with background of artificial intelligence,machine learning, deep learning, and reinforcement learning (RL), withresources. Next we discuss RL core elements, including value function, policy,reward, model, exploration vs. exploitation, and representation. Then wediscuss important mechanisms for RL, including attention and memory,unsupervised learning, hierarchical RL, multi-agent RL, relational RL, andlearning to learn. After that, we discuss RL applications, including games,robotics, natural language processing (NLP), computer vision, finance, businessmanagement, healthcare, education, energy, transportation, computer systems,and, science, engineering, and art. Finally we summarize briefly, discusschallenges and opportunities, and close with an epilogue.",http://arxiv.org/abs/1810.06339v1,,
362,Real-time Context-aware Learning System for IoT Applications,"We propose a real-time context-aware learning system along with thearchitecture that runs on the mobile devices, provide services to the user andmanage the IoT devices. In this system, an application running on mobiledevices collected data from the sensors, learned about the user-definedcontext, made predictions in real-time and manage IoT devices accordingly.However, the computational power of the mobile devices makes it challenging torun machine learning algorithms with acceptable accuracy. To solve this issue,some authors have run machine learning algorithms on the server and transmittedthe results to the mobile devices. Although the context-aware predictions madeby the server are more accurate than their mobile counterpart, it heavilydepends on the network connection for the delivery of the results to thedevices, which negatively affects real-time context-learning. Therefore, inthis work, we describe a context-learning algorithm for mobile devices which isless demanding on the computational resources and maintains the accuracy of theprediction by updating itself from the learning parameters obtained from theserver periodically. Experimental results show that the proposed light-weightcontext-learning algorithm can achieve mean accuracy up to 97.51% while meanexecution time requires only 11ms.",http://arxiv.org/abs/1810.11295v1,,
363,"Machine Learning Molecular Dynamics for the Simulation of Infrared  Spectra","Machine learning has emerged as an invaluable tool in many research areas. Inthe present work, we harness this power to predict highly accurate molecularinfrared spectra with unprecedented computational efficiency. To account forvibrational anharmonic and dynamical effects -- typically neglected byconventional quantum chemistry approaches -- we base our machine learningstrategy on ab initio molecular dynamics simulations. While these simulationsare usually extremely time consuming even for small molecules, we overcomethese limitations by leveraging the power of a variety of machine learningtechniques, not only accelerating simulations by several orders of magnitude,but also greatly extending the size of systems that can be treated. To thisend, we develop a molecular dipole moment model based on environment dependentneural network charges and combine it with the neural network potentials ofBehler and Parrinello. Contrary to the prevalent big data philosophy, we areable to obtain very accurate machine learning models for the prediction ofinfrared spectra based on only a few hundreds of electronic structure referencepoints. This is made possible through the introduction of a fully automatedsampling scheme and the use of molecular forces during neural network potentialtraining. We demonstrate the power of our machine learning approach by applyingit to model the infrared spectra of a methanol molecule, n-alkanes containingup to 200 atoms and the protonated alanine tripeptide, which at the same timerepresents the first application of machine learning techniques to simulate thedynamics of a peptide. In all these case studies we find excellent agreementbetween the infrared spectra predicted via machine learning models and therespective theoretical and experimental spectra.",http://arxiv.org/abs/1705.05907v1,,
364,"Automatically Explaining Machine Learning Prediction Results: A  Demonstration on Type 2 Diabetes Risk Prediction","Background: Predictive modeling is a key component of solutions to manyhealthcare problems. Among all predictive modeling approaches, machine learningmethods often achieve the highest prediction accuracy, but suffer from along-standing open problem precluding their widespread use in healthcare. Mostmachine learning models give no explanation for their prediction results,whereas interpretability is essential for a predictive model to be adopted intypical healthcare settings. Methods: This paper presents the first completemethod for automatically explaining results for any machine learning predictivemodel without degrading accuracy. We did a computer coding implementation ofthe method. Using the electronic medical record data set from the PracticeFusion diabetes classification competition containing patient records from all50 states in the United States, we demonstrated the method on predicting type 2diabetes diagnosis within the next year. Results: For the champion machinelearning model of the competition, our method explained prediction results for87.4% of patients who were correctly predicted by the model to have type 2diabetes diagnosis within the next year. Conclusions: Our demonstration showedthe feasibility of automatically explaining results for any machine learningpredictive model without degrading accuracy.",http://arxiv.org/abs/1812.02852v1,,
365,Mean-Field Theory of Meta-Learning,"We discuss here the mean-field theory for a cellular automata model ofmeta-learning. The meta-learning is the process of combining outcomes ofindividual learning procedures in order to determine the final decision withhigher accuracy than any single learning method. Our method is constructed froman ensemble of interacting, learning agents, that acquire and process incominginformation using various types, or different versions of machine learningalgorithms. The abstract learning space, where all agents are located, isconstructed here using a fully connected model that couples all agents withrandom strength values. The cellular automata network simulates the higherlevel integration of information acquired from the independent learning trials.The final classification of incoming input data is therefore defined as thestationary state of the meta-learning system using simple majority rule, yetthe minority clusters that share opposite classification outcome can beobserved in the system. Therefore, the probability of selecting proper classfor a given input data, can be estimated even without the prior knowledge ofits affiliation. The fuzzy logic can be easily introduced into the system, evenif learning agents are build from simple binary classification machine learningalgorithms by calculating the percentage of agreeing agents.",http://arxiv.org/abs/0907.4643v2,,
366,Active Perceptual Similarity Modeling with Auxiliary Information,"Learning a model of perceptual similarity from a collection of objects is afundamental task in machine learning underlying numerous applications. A commonway to learn such a model is from relative comparisons in the form of triplets:responses to queries of the form ""Is object a more similar to b than it is toc?"". If no consideration is made in the determination of which queries to ask,existing similarity learning methods can require a prohibitively large numberof responses. In this work, we consider the problem of actively learning fromtriplets -finding which queries are most useful for learning. Different fromprevious active triplet learning approaches, we incorporate auxiliaryinformation into our similarity model and introduce an active learning schemeto find queries that are informative for quickly learning both the relevantaspects of auxiliary data and the directly-learned similarity components.Compared to prior approaches, we show that we can learn just as effectivelywith much fewer queries. For evaluation, we introduce a new dataset ofexhaustive triplet comparisons obtained from humans and demonstrate improvedperformance for different types of auxiliary information.",http://arxiv.org/abs/1511.02254v1,,
367,Observational Learning by Reinforcement Learning,"Observational learning is a type of learning that occurs as a function ofobserving, retaining and possibly replicating or imitating the behaviour ofanother agent. It is a core mechanism appearing in various instances of sociallearning and has been found to be employed in several intelligent species,including humans. In this paper, we investigate to what extent the explicitmodelling of other agents is necessary to achieve observational learningthrough machine learning. Especially, we argue that observational learning canemerge from pure Reinforcement Learning (RL), potentially coupled with memory.Through simple scenarios, we demonstrate that an RL agent can leverage theinformation provided by the observations of an other agent performing a task ina shared environment. The other agent is only observed through the effect ofits actions on the environment and never explicitly modeled. Two key aspectsare borrowed from observational learning: i) the observer behaviour needs tochange as a result of viewing a 'teacher' (another agent) and ii) the observerneeds to be motivated somehow to engage in making use of the other agent'sbehaviour. The later is naturally modeled by RL, by correlating the learningagent's reward with the teacher agent's behaviour.",http://arxiv.org/abs/1706.06617v1,,
368,"Fraternal Twins: Unifying Attacks on Machine Learning and Digital  Watermarking","Machine learning is increasingly used in security-critical applications, suchas autonomous driving, face recognition and malware detection. Most learningmethods, however, have not been designed with security in mind and thus arevulnerable to different types of attacks. This problem has motivated theresearch field of adversarial machine learning that is concerned with attackingand defending learning methods. Concurrently, a different line of research hastackled a very similar problem: In digital watermarking information areembedded in a signal in the presence of an adversary. As a consequence, thisresearch field has also extensively studied techniques for attacking anddefending watermarking methods.  The two research communities have worked in parallel so far, unnoticeablydeveloping similar attack and defense strategies. This paper is a first effortto bring these communities together. To this end, we present a unified notationof black-box attacks against machine learning and watermarking that reveals thesimilarity of both settings. To demonstrate the efficacy of this unified view,we apply concepts from watermarking to machine learning and vice versa. We showthat countermeasures from watermarking can mitigate recent model-extractionattacks and, similarly, that techniques for hardening machine learning can fendoff oracle attacks against watermarks. Our work provides a conceptual linkbetween two research fields and thereby opens novel directions for improvingthe security of both, machine learning and digital watermarking.",http://arxiv.org/abs/1703.05561v1,,
369,"Taking Human out of Learning Applications: A Survey on Automated Machine  Learning","Machine learning techniques have deeply rooted in our everyday life. However,since it is knowledge- and labor-intensive to pursue good learning performance,human experts are heavily involved in every aspect of machine learning. Inorder to make machine learning techniques easier to apply and reduce the demandfor experienced human experts, automated machine learning (AutoML) has emergedas a hot topic with both industrial and academic interest. In this paper, weprovide an up to date survey on AutoML. First, we introduce and define theAutoML problem, with inspiration from both realms of automation and machinelearning. Then, we propose a general AutoML framework that not only covers mostexisting approaches to date but also can guide the design for new methods.Subsequently, we categorize and review the existing works from two aspects,i.e., the problem setup and the employed techniques. Finally, we provide adetailed analysis of AutoML approaches and explain the reasons underneath theirsuccessful applications. We hope this survey can serve as not only aninsightful guideline for AutoML beginners but also an inspiration for futureresearch.",http://arxiv.org/abs/1810.13306v3,,
370,"On Human Predictions with Explanations and Predictions of Machine  Learning Models: A Case Study on Deception Detection","Humans are the final decision makers in critical tasks that involve ethicaland legal concerns, ranging from recidivism prediction, to medical diagnosis,to fighting against fake news. Although machine learning models can sometimesachieve impressive performance in these tasks, these tasks are not amenable tofull automation. To realize the potential of machine learning for improvinghuman decisions, it is important to understand how assistance from machinelearning models affects human performance and human agency.  In this paper, we use deception detection as a testbed and investigate how wecan harness explanations and predictions of machine learning models to improvehuman performance while retaining human agency. We propose a spectrum betweenfull human agency and full automation, and develop varying levels of machineassistance along the spectrum that gradually increase the influence of machinepredictions. We find that without showing predicted labels, explanations aloneslightly improve human performance in the end task. In comparison, humanperformance is greatly improved by showing predicted labels (>20% relativeimprovement) and can be further improved by explicitly suggesting strongmachine performance. Interestingly, when predicted labels are shown,explanations of machine predictions induce a similar level of accuracy as anexplicit statement of strong machine performance. Our results demonstrate atradeoff between human performance and human agency and show that explanationsof machine predictions can moderate this tradeoff.",http://arxiv.org/abs/1811.07901v4,,
371,Online Learning via Sequential Complexities,"We consider the problem of sequential prediction and provide tools to studythe minimax value of the associated game. Classical statistical learning theoryprovides several useful complexity measures to study learning with i.i.d. data.Our proposed sequential complexities can be seen as extensions of thesemeasures to the sequential setting. The developed theory is shown to yieldprecise learning guarantees for the problem of sequential prediction. Inparticular, we show necessary and sufficient conditions for online learnabilityin the setting of supervised learning. Several examples show the utility of ourframework: we can establish learnability without having to exhibit an explicitonline learning algorithm.",http://arxiv.org/abs/1006.1138v3,,
372,Example Selection For Dictionary Learning,"In unsupervised learning, an unbiased uniform sampling strategy is typicallyused, in order that the learned features faithfully encode the statisticalstructure of the training data. In this work, we explore whether active exampleselection strategies - algorithms that select which examples to use, based onthe current estimate of the features - can accelerate learning. Specifically,we investigate effects of heuristic and saliency-inspired selection algorithmson the dictionary learning task with sparse activations. We show that someselection algorithms do improve the speed of learning, and we speculate on whythey might work.",http://arxiv.org/abs/1412.6177v3,,
373,No More Pesky Learning Rates,"The performance of stochastic gradient descent (SGD) depends critically onhow learning rates are tuned and decreased over time. We propose a method toautomatically adjust multiple learning rates so as to minimize the expectederror at any one time. The method relies on local gradient variations acrosssamples. In our approach, learning rates can increase as well as decrease,making it suitable for non-stationary problems. Using a number of convex andnon-convex learning tasks, we show that the resulting algorithm matches theperformance of SGD or other adaptive approaches with their best settingsobtained through systematic search, and effectively removes the need forlearning rate tuning.",http://arxiv.org/abs/1206.1106v2,,
374,Efficient Decomposed Learning for Structured Prediction,"Structured prediction is the cornerstone of several machine learningapplications. Unfortunately, in structured prediction settings with expressiveinter-variable interactions, exact inference-based learning algorithms, e.g.Structural SVM, are often intractable. We present a new way, DecomposedLearning (DecL), which performs efficient learning by restricting the inferencestep to a limited part of the structured spaces. We provide characterizationsbased on the structure, target parameters, and gold labels, under which DecL isequivalent to exact learning. We then show that in real world settings, whereour theoretical assumptions may not completely hold, DecL-based algorithms aresignificantly more efficient and as accurate as exact learning.",http://arxiv.org/abs/1206.4630v1,,
375,Generative Adversarial Active Learning,"We propose a new active learning by query synthesis approach using GenerativeAdversarial Networks (GAN). Different from regular active learning, theresulting algorithm adaptively synthesizes training instances for querying toincrease learning speed. We generate queries according to the uncertaintyprinciple, but our idea can work with other active learning principles. Wereport results from various numerical experiments to demonstrate theeffectiveness the proposed approach. In some settings, the proposed algorithmoutperforms traditional pool-based approaches. To the best our knowledge, thisis the first active learning work using GAN.",http://arxiv.org/abs/1702.07956v5,,
376,Variational Continual Learning,"This paper develops variational continual learning (VCL), a simple butgeneral framework for continual learning that fuses online variationalinference (VI) and recent advances in Monte Carlo VI for neural networks. Theframework can successfully train both deep discriminative models and deepgenerative models in complex continual learning settings where existing tasksevolve over time and entirely new tasks emerge. Experimental results show thatVCL outperforms state-of-the-art continual learning methods on a variety oftasks, avoiding catastrophic forgetting in a fully automatic way.",http://arxiv.org/abs/1710.10628v3,,
377,Analysis of Dropout in Online Learning,"Deep learning is the state-of-the-art in fields such as visual objectrecognition and speech recognition. This learning uses a large number of layersand a huge number of units and connections. Therefore, overfitting is a seriousproblem with it, and the dropout which is a kind of regularization tool isused. However, in online learning, the effect of dropout is not well known.This paper presents our investigation on the effect of dropout in onlinelearning. We analyzed the effect of dropout on convergence speed near thesingular point. Our results indicated that dropout is effective in onlinelearning. Dropout tends to avoid the singular point for convergence speed nearthat point.",http://arxiv.org/abs/1711.03343v1,,
378,Gradient descent revisited via an adaptive online learning rate,"Any gradient descent optimization requires to choose a learning rate. Withdeeper and deeper models, tuning that learning rate can easily become tediousand does not necessarily lead to an ideal convergence. We propose a variationof the gradient descent algorithm in the which the learning rate is not fixed.Instead, we learn the learning rate itself, either by another gradient descent(first-order method), or by Newton's method (second-order). This way, gradientdescent for any machine learning algorithm can be optimized.",http://arxiv.org/abs/1801.09136v2,,
379,Towards Robust Evaluations of Continual Learning,"The experiments used in current continual learning research do not faithfullyassess fundamental challenges of learning continually. We examine standardevaluations and show why these evaluations make some types of continuallearning approaches look better than they are. In particular, currentevaluations are biased towards continual learning approaches that treatprevious models as a prior (e.g., EWC, VCL). We introduce desiderata forcontinual learning evaluations and explain why their absence creates misleadingcomparisons. Our analysis calls for a reprioritization of research effort bythe community.",http://arxiv.org/abs/1805.09733v2,,
380,Will it Blend? Composing Value Functions in Reinforcement Learning,"An important property for lifelong-learning agents is the ability to combineexisting skills to solve unseen tasks. In general, however, it is unclear howto compose skills in a principled way. We provide a ""recipe"" for optimal valuefunction composition in entropy-regularised reinforcement learning (RL) andthen extend this to the standard RL setting. Composition is demonstrated in avideo game environment, where an agent with an existing library of policies isable to solve new tasks without the need for further learning.",http://arxiv.org/abs/1807.04439v1,,
381,Transfer Learning and Organic Computing for Autonomous Vehicles,"Autonomous Vehicles(AV) are one of the brightest promises of the future whichwould help cut down fatalities and improve travel time while working inharmony. Autonomous vehicles will face with challenging situations andexperiences not seen before. These experiences should be converted to knowledgeand help the vehicle prepare better in the future. Online Transfer Learningwill help transferring prior knowledge to a new task and also keep theknowledge updated as the task evolves. This paper presents the differentmethods of transfer learning, online transfer learning and organic computingthat could be adapted to the domain of autonomous vehicles.",http://arxiv.org/abs/1808.05443v1,,
382,Learning Multi-Layer Transform Models,"Learned data models based on sparsity are widely used in signal processingand imaging applications. A variety of methods for learning synthesisdictionaries, sparsifying transforms, etc., have been proposed in recent years,often imposing useful structures or properties on the models. In this work, wefocus on sparsifying transform learning, which enjoys a number of advantages.We consider multi-layer or nested extensions of the transform model, andpropose efficient learning algorithms. Numerical experiments with image dataillustrate the behavior of the multi-layer transform learning algorithm and itsusefulness for image denoising. Multi-layer models provide better denoisingquality than single layer schemes.",http://arxiv.org/abs/1810.08323v1,,
383,Deep Active Learning with a Neural Architecture Search,"We consider active learning of deep neural networks. Most active learningworks in this context have focused on studying effective querying mechanismsand assumed that an appropriate network architecture is a priori known for theproblem at hand. We challenge this assumption and propose a novel activestrategy whereby the learning algorithm searches for effective architectures onthe fly, while actively learning. We apply our strategy using three knownquerying techniques (softmax response, MC-dropout, and coresets) and show thatthe proposed approach overwhelmingly outperforms active learning using fixedarchitectures.",http://arxiv.org/abs/1811.07579v1,,
384,"On the Transferability of Representations in Neural Networks Between  Datasets and Tasks","Deep networks, composed of multiple layers of hierarchical distributedrepresentations, tend to learn low-level features in initial layers andtransition to high-level features towards final layers. Paradigms such astransfer learning, multi-task learning, and continual learning leverage thisnotion of generic hierarchical distributed representations to share knowledgeacross datasets and tasks. Herein, we study the layer-wise transferability ofrepresentations in deep networks across a few datasets and tasks and note someinteresting empirical observations.",http://arxiv.org/abs/1811.12273v1,,
385,"Generalization Bounds For Unsupervised and Semi-Supervised Learning With  Autoencoders","Autoencoders are widely used for unsupervised learning and as aregularization scheme in semi-supervised learning. However, theoreticalunderstanding of their generalization properties and of the manner in whichthey can assist supervised learning has been lacking. We utilize recentadvances in the theory of deep learning generalization, together with a novelreconstruction loss, to provide generalization bounds for autoencoders. To thebest of our knowledge, this is the first such bound. We further show that,under appropriate assumptions, an autoencoder with good generalizationproperties can improve any semi-supervised learning scheme. We support ourtheoretical results with empirical demonstrations.",http://arxiv.org/abs/1902.01449v1,,
386,Dyna-AIL : Adversarial Imitation Learning by Planning,"Adversarial methods for imitation learning have been shown to perform well onvarious control tasks. However, they require a large number of environmentinteractions for convergence. In this paper, we propose an end-to-enddifferentiable adversarial imitation learning algorithm in a Dyna-likeframework for switching between model-based planning and model-free learningfrom expert data. Our results on both discrete and continuous environments showthat our approach of using model-based planning along with model-free learningconverges to an optimal policy with fewer number of environment interactions incomparison to the state-of-the-art learning methods.",http://arxiv.org/abs/1903.03234v1,,
387,Deep Learning for Forecasting Stock Returns in the Cross-Section,"Many studies have been undertaken by using machine learning techniques,including neural networks, to predict stock returns. Recently, a method knownas deep learning, which achieves high performance mainly in image recognitionand speech recognition, has attracted attention in the machine learning field.This paper implements deep learning to predict one-month-ahead stock returns inthe cross-section in the Japanese stock market and investigates the performanceof the method. Our results show that deep neural networks generally outperformshallow neural networks, and the best networks also outperform representativemachine learning models. These results indicate that deep learning showspromise as a skillful machine learning method to predict stock returns in thecross-section.",http://arxiv.org/abs/1801.01777v4,,
388,ReSet: Learning Recurrent Dynamic Routing in ResNet-like Neural Networks,"Neural Network is a powerful Machine Learning tool that shows outstandingperformance in Computer Vision, Natural Language Processing, and ArtificialIntelligence. In particular, recently proposed ResNet architecture and itsmodifications produce state-of-the-art results in image classificationproblems. ResNet and most of the previously proposed architectures have a fixedstructure and apply the same transformation to all input images. In this work,we develop a ResNet-based model that dynamically selects Computational Units(CU) for each input object from a learned set of transformations. Dynamicselection allows the network to learn a sequence of useful transformations andapply only required units to predict the image label. We compare our model toResNet-38 architecture and achieve better results than the original ResNet onCIFAR-10.1 test set. While examining the produced paths, we discovered that thenetwork learned different routes for images from different classes and similarroutes for similar images.",http://arxiv.org/abs/1811.04380v1,,
389,An Introduction to Deep Reinforcement Learning,"Deep reinforcement learning is the combination of reinforcement learning (RL)and deep learning. This field of research has been able to solve a wide rangeof complex decision-making tasks that were previously out of reach for amachine. Thus, deep RL opens up many new applications in domains such ashealthcare, robotics, smart grids, finance, and many more. This manuscriptprovides an introduction to deep reinforcement learning models, algorithms andtechniques. Particular focus is on the aspects related to generalization andhow deep RL can be used for practical applications. We assume the reader isfamiliar with basic machine learning concepts.",http://arxiv.org/abs/1811.12560v2,,
390,Generalization in anti-causal learning,"The ability to learn and act in novel situations is still a prerogative ofanimate intelligence, as current machine learning methods mostly fail whenmoving beyond the standard i.i.d. setting. What is the reason for thisdiscrepancy? Most machine learning tasks are anti-causal, i.e., we infer causes(labels) from effects (observations). Typically, in supervised learning webuild systems that try to directly invert causal mechanisms. Instead, in thispaper we argue that strong generalization capabilities crucially hinge onsearching and validating meaningful hypotheses, requiring access to a causalmodel. In such a framework, we want to find a cause that leads to the observedeffect. Anti-causal models are used to drive this search, but a causal model isrequired for validation. We investigate the fundamental differences betweencausal and anti-causal tasks, discuss implications for topics ranging fromadversarial attacks to disentangling factors of variation, and provideextensive evidence from the literature to substantiate our view. We advocatefor incorporating causal models in supervised learning to shift the paradigmfrom inference only, to search and validation.",http://arxiv.org/abs/1812.00524v1,,
391,"Optimization under Uncertainty in the Era of Big Data and Deep Learning:  When Machine Learning Meets Mathematical Programming","This paper reviews recent advances in the field of optimization underuncertainty via a modern data lens, highlights key research challenges andpromise of data-driven optimization that organically integrates machinelearning and mathematical programming for decision-making under uncertainty,and identifies potential research opportunities. A brief review of classicalmathematical programming techniques for hedging against uncertainty is firstpresented, along with their wide spectrum of applications in Process SystemsEngineering. A comprehensive review and classification of the relevantpublications on data-driven distributionally robust optimization, data-drivenchance constrained program, data-driven robust optimization, and data-drivenscenario-based optimization is then presented. This paper also identifiesfertile avenues for future research that focuses on a closed-loop data-drivenoptimization framework, which allows the feedback from mathematical programmingto machine learning, as well as scenario-based optimization leveraging thepower of deep learning techniques. Perspectives on online learning-baseddata-driven multistage optimization with a learning-while-optimizing scheme ispresented.",http://arxiv.org/abs/1904.01934v1,,
392,Hierarchical learning in polynomial Support Vector Machines,"We study the typical properties of polynomial Support Vector Machines withina Statistical Mechanics approach that allows us to analyze the effect ofdifferent normalizations of the features. If the normalization is adecuatelychosen, there is a hierarchical learning of features of increasing order as afunction of the training set size.",http://arxiv.org/abs/cond-mat/0010423v1,,
393,"Two Projection Pursuit Algorithms for Machine Learning under  Non-Stationarity","This thesis derives, tests and applies two linear projection algorithms formachine learning under non-stationarity. The first finds a direction in alinear space upon which a data set is maximally non-stationary. The second aimsto robustify two-way classification against non-stationarity. The algorithm istested on a key application scenario, namely Brain Computer Interfacing.",http://arxiv.org/abs/1110.0593v1,,
394,Fano schemes of generic intersections and machine learning,"We investigate Fano schemes of conditionally generic intersections, i.e. ofhypersurfaces in projective space chosen generically up to additionalconditions. Via a correspondence between generic properties of algebraicvarieties and events in probability spaces that occur with probability one, weuse the obtained results on Fano schemes to solve a problem in machinelearning.",http://arxiv.org/abs/1301.3078v1,,
395,Dissimilarity Clustering by Hierarchical Multi-Level Refinement,"We introduce in this paper a new way of optimizing the natural extension ofthe quantization error using in k-means clustering to dissimilarity data. Theproposed method is based on hierarchical clustering analysis combined withmulti-level heuristic refinement. The method is computationally efficient andachieves better quantization errors than the",http://arxiv.org/abs/1204.6509v1,,
396,Agglomerative Bregman Clustering,"This manuscript develops the theory of agglomerative clustering with Bregmandivergences. Geometric smoothing techniques are developed to deal withdegenerate clusters. To allow for cluster models based on exponential familieswith overcomplete representations, Bregman divergences are developed fornondifferentiable convex functions.",http://arxiv.org/abs/1206.6446v1,,
397,"What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes","The F-measure or F-score is one of the most commonly used single numbermeasures in Information Retrieval, Natural Language Processing and MachineLearning, but it is based on a mistake, and the flawed assumptions render itunsuitable for use in most contexts! Fortunately, there are betteralternatives.",http://arxiv.org/abs/1503.06410v1,,
398,Open science in machine learning,"We present OpenML and mldata, open science platforms that provides easyaccess to machine learning data, software and results to encourage furtherstudy and application. They go beyond the more traditional repositories fordata sets and software packages in that they allow researchers to also easilyshare the results they obtained in experiments and to compare their solutionswith those of others.",http://arxiv.org/abs/1402.6013v1,,
399,Extending Defensive Distillation,"Machine learning is vulnerable to adversarial examples: inputs carefullymodified to force misclassification. Designing defenses against such inputsremains largely an open problem. In this work, we revisit defensivedistillation---which is one of the mechanisms proposed to mitigate adversarialexamples---to address its limitations. We view our results not only as aneffective way of addressing some of the recently discovered attacks but also asreinforcing the importance of improved training techniques.",http://arxiv.org/abs/1705.05264v1,,
400,Descriptors for Machine Learning of Materials Data,"Descriptors, which are representations of compounds, play an essential rolein machine learning of materials data. Although many representations ofelements and structures of compounds are known, these representations aredifficult to use as descriptors in their unchanged forms. This chapter showshow compounds in a dataset can be represented as descriptors and applied tomachine-learning models for materials datasets.",http://arxiv.org/abs/1709.01666v1,,
401,Proceedings of NIPS 2017 Symposium on Interpretable Machine Learning,"This is the Proceedings of NIPS 2017 Symposium on Interpretable MachineLearning, held in Long Beach, California, USA on December 7, 2017",http://arxiv.org/abs/1711.09889v3,,
402,Clustering to Reduce Spatial Data Set Size,"Traditionally it had been a problem that researchers did not have access toenough spatial data to answer pressing research questions or build compellingvisualizations. Today, however, the problem is often that we have too muchdata. Spatially redundant or approximately redundant points may refer to asingle feature (plus noise) rather than many distinct spatial features. We usea machine learning approach with density-based clustering to compress suchspatial data into a set of representative features.",http://arxiv.org/abs/1803.08101v1,,
403,On the Robustness of Interpretability Methods,"We argue that robustness of explanations---i.e., that similar inputs shouldgive rise to similar explanations---is a key desideratum for interpretability.We introduce metrics to quantify robustness and demonstrate that currentmethods do not perform well according to these metrics. Finally, we proposeways that robustness can be enforced on existing interpretability approaches.",http://arxiv.org/abs/1806.08049v1,,
404,Breast Cancer Diagnosis via Classification Algorithms,"In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data usingMachine Learning classification techniques, such as the SVM, Bayesian LogisticRegression (Variational Approximation), and K-Nearest-Neighbors. We describeeach model, and compare their performance through different measures. Weconclude that SVM has the best performance among all other classifiers, whileit competes closely with the Bayesian Logistic Regression that is ranked secondbest method for this dataset.",http://arxiv.org/abs/1807.01334v1,,
405,Applying Machine Learning To Maize Traits Prediction,"Heterosis is the improved or increased function of any biological quality ina hybrid offspring. We have studied yet the largest maize SNP dataset fortraits prediction. We develop linear and non-linear models which considerrelationships between different hybrids as well as other effect. Speciallydesigned model proved to be efficient and robust in prediction maize's traits.",http://arxiv.org/abs/1808.06275v1,,
406,IoU is not submodular,"This short article aims at demonstrate that the Intersection over Union (orJaccard index) is not a submodular function. This mistake has been made in anarticle which is cited and used as a foundation in another article. TheIntersection of Union is widely used in machine learning as a cost functionespecially for imbalance data and semantic segmentation.",http://arxiv.org/abs/1809.00593v1,,
407,Fair lending needs explainable models for responsible recommendation,"The financial services industry has unique explainability and fairnesschallenges arising from compliance and ethical considerations in creditdecisioning. These challenges complicate the use of model machine learning andartificial intelligence methods in business decision processes.",http://arxiv.org/abs/1809.04684v1,,
408,Nonlinear Regression without i.i.d. Assumption,"In this paper, we consider a class of nonlinear regression problems withoutthe assumption of being independent and identically distributed. We propose acorrespondent mini-max problem for nonlinear regression and outline a numericalalgorithm. Such an algorithm can be applied in regression and machine learningproblems, and yield better results than traditional regression and machinelearning methods.",http://arxiv.org/abs/1811.09623v1,,
409,Machine Learning for removing EEG artifacts: Setting the benchmark,"Electroencephalograms (EEG) are often contaminated by artifacts which makeinterpreting them more challenging for clinicians. Hence, automated artifactrecognition systems have the potential to aid the clinical workflow. In thisabstract, we share the first results on applying various machine learningalgorithms to the recently released world's largest open-source artifactrecognition dataset. We envision that these results will serve as a benchmarkfor researchers who might work with this dataset in future.",http://arxiv.org/abs/1903.07825v1,,
410,"Convergence rates for the stochastic gradient descent method for  non-convex objective functions","We prove the local convergence to minima and estimates on the rate ofconvergence for the stochastic gradient descent method in the case of notnecessarily globally convex nor contracting objective functions. In particular,the results are applicable to simple objective functions arising in machinelearning.",http://arxiv.org/abs/1904.01517v1,,
411,An overview of deep learning in medical imaging focusing on MRI,"What has happened in machine learning lately, and what does it mean for thefuture of medical image analysis? Machine learning has witnessed a tremendousamount of attention over the last few years. The current boom started around2009 when so-called deep artificial neural networks began outperforming otherestablished models on a number of important benchmarks. Deep neural networksare now the state-of-the-art machine learning models across a variety of areas,from image analysis to natural language processing, and widely deployed inacademia and industry. These developments have a huge potential for medicalimaging technology, medical data analysis, medical diagnostics and healthcarein general, slowly being realized. We provide a short overview of recentadvances and some associated challenges in machine learning applied to medicalimage processing and image analysis. As this has become a very broad and fastexpanding field we will not survey the entire landscape of applications, butput particular focus on deep learning in MRI.  Our aim is threefold: (i) give a brief introduction to deep learning withpointers to core references; (ii) indicate how deep learning has been appliedto the entire MRI processing chain, from acquisition to image retrieval, fromsegmentation to disease prediction; (iii) provide a starting point for peopleinterested in experimenting and perhaps contributing to the field of machinelearning for medical imaging by pointing out good educational resources,state-of-the-art open-source code, and interesting sources of data and problemsrelated medical imaging.",http://arxiv.org/abs/1811.10052v2,,
412,"Statistical Learning Theory: Models, Concepts, and Results","Statistical learning theory provides the theoretical basis for many oftoday's machine learning algorithms. In this article we attempt to give agentle, non-technical overview over the key ideas and insights of statisticallearning theory. We target at a broad audience, not necessarily machinelearning researchers. This paper can serve as a starting point for people whowant to get an overview on the field before diving into technical details.",http://arxiv.org/abs/0810.4752v1,,
413,"Reproducing Kernel Banach Spaces with the l1 Norm II: Error Analysis for  Regularized Least Square Regression","A typical approach in estimating the learning rate of a regularized learningscheme is to bound the approximation error by the sum of the sampling error,the hypothesis error and the regularization error. Using a reproducing kernelspace that satisfies the linear representer theorem brings the advantage ofdiscarding the hypothesis error from the sum automatically. Following thisdirection, we illustrate how reproducing kernel Banach spaces with the l1 normcan be applied to improve the learning rate estimate of l1-regularization inmachine learning.",http://arxiv.org/abs/1101.4439v2,,
414,Learning from networked examples in a k-partite graph,"Many machine learning algorithms are based on the assumption that trainingexamples are drawn independently. However, this assumption does not holdanymore when learning from a networked sample where two or more trainingexamples may share common features. We propose an efficient weighting methodfor learning from networked examples and show the sample error bound which isbetter than previous work.",http://arxiv.org/abs/1306.0393v3,,
415,Domain Generalization via Invariant Feature Representation,"This paper investigates domain generalization: How to take knowledge acquiredfrom an arbitrary number of related domains and apply it to previously unseendomains? We propose Domain-Invariant Component Analysis (DICA), a kernel-basedoptimization algorithm that learns an invariant transformation by minimizingthe dissimilarity across domains, whilst preserving the functional relationshipbetween input and output variables. A learning-theoretic analysis shows thatreducing dissimilarity improves the expected generalization ability ofclassifiers on new domains, motivating the proposed algorithm. Experimentalresults on synthetic and real-world datasets demonstrate that DICA successfullylearns invariant features and improves classifier performance in practice.",http://arxiv.org/abs/1301.2115v1,,
416,Unsupervised Learning in Neuromemristive Systems,"Neuromemristive systems (NMSs) currently represent the most promisingplatform to achieve energy efficient neuro-inspired computation. However, sincethe research field is less than a decade old, there are still countlessalgorithms and design paradigms to be explored within these systems. Oneparticular domain that remains to be fully investigated within NMSs isunsupervised learning. In this work, we explore the design of an NMS forunsupervised clustering, which is a critical element of several machinelearning algorithms. Using a simple memristor crossbar architecture andlearning rule, we are able to achieve performance which is on par with MATLAB'sk-means clustering.",http://arxiv.org/abs/1601.07482v1,,
417,Learning Local Invariant Mahalanobis Distances,"For many tasks and data types, there are natural transformations to which thedata should be invariant or insensitive. For instance, in visual recognition,natural images should be insensitive to rotation and translation. Thisrequirement and its implications have been important in many machine learningapplications, and tolerance for image transformations was primarily achieved byusing robust feature vectors. In this paper we propose a novel andcomputationally efficient way to learn a local Mahalanobis metric per datum,and show how we can learn a local invariant metric to any transformation inorder to improve performance.",http://arxiv.org/abs/1502.01176v1,,
418,The Optimal Sample Complexity of PAC Learning,"This work establishes a new upper bound on the number of samples sufficientfor PAC learning in the realizable case. The bound matches known lower boundsup to numerical constant factors. This solves a long-standing open problem onthe sample complexity of PAC learning. The technique and analysis build on arecent breakthrough by Hans Simon.",http://arxiv.org/abs/1507.00473v4,,
419,Learning Personalized Optimal Control for Repeatedly Operated Systems,"We consider the problem of online learning of optimal control for repeatedlyoperated systems in the presence of parametric uncertainty. During each roundof operation, environment selects system parameters according to a fixed butunknown probability distribution. These parameters govern the dynamics of aplant. An agent chooses a control input to the plant and is then revealed thecost of the choice. In this setting, we design an agent that personalizes thecontrol input to this plant taking into account the stochasticity involved. Wedemonstrate the effectiveness of our approach on a simulated system.",http://arxiv.org/abs/1609.05536v1,,
420,Deep Exploration via Randomized Value Functions,"We study the use of randomized value functions to guide deep exploration inreinforcement learning. This offers an elegant means for synthesizingstatistically and computationally efficient exploration with common practicalapproaches to value function learning. We present several reinforcementlearning algorithms that leverage randomized value functions and demonstratetheir efficacy through computational studies. We also prove a regret bound thatestablishes statistical efficiency with a tabular representation.",http://arxiv.org/abs/1703.07608v4,,
421,An Overview of Multi-Task Learning in Deep Neural Networks,"Multi-task learning (MTL) has led to successes in many applications ofmachine learning, from natural language processing and speech recognition tocomputer vision and drug discovery. This article aims to give a generaloverview of MTL, particularly in deep neural networks. It introduces the twomost common methods for MTL in Deep Learning, gives an overview of theliterature, and discusses recent advances. In particular, it seeks to help MLpractitioners apply MTL by shedding light on how MTL works and providingguidelines for choosing appropriate auxiliary tasks.",http://arxiv.org/abs/1706.05098v1,,
422,nuts-flow/ml: data pre-processing for deep learning,"Data preprocessing is a fundamental part of any machine learning applicationand frequently the most time-consuming aspect when developing a machinelearning solution. Preprocessing for deep learning is characterized bypipelines that lazily load data and perform data transformation, augmentation,batching and logging. Many of these functions are common across applicationsbut require different arrangements for training, testing or inference. Here weintroduce a novel software framework named nuts-flow/ml that encapsulatescommon preprocessing operations as components, which can be flexibly arrangedto rapidly construct efficient preprocessing pipelines for deep learning.",http://arxiv.org/abs/1708.06046v2,,
423,"MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural  Networks","We introduce MinimalRNN, a new recurrent neural network architecture thatachieves comparable performance as the popular gated RNNs with a simplifiedstructure. It employs minimal updates within RNN, which not only leads toefficient learning and testing but more importantly better interpretability andtrainability. We demonstrate that by endorsing the more restrictive updaterule, MinimalRNN learns disentangled RNN states. We further examine thelearning dynamics of different RNN structures using input-output Jacobians, andshow that MinimalRNN is able to capture longer range dependencies than existingRNN architectures.",http://arxiv.org/abs/1711.06788v2,,
424,Quantum Circuit Learning,"We propose a classical-quantum hybrid algorithm for machine learning onnear-term quantum processors, which we call quantum circuit learning. A quantumcircuit driven by our framework learns a given task by tuning parametersimplemented on it. The iterative optimization of the parameters allows us tocircumvent the high-depth circuit. Theoretical investigation shows that aquantum circuit can approximate nonlinear functions, which is further confirmedby numerical simulations. Hybridizing a low-depth quantum circuit and aclassical computer for machine learning, the proposed framework paves the waytoward applications of near-term quantum devices for quantum machine learning.",http://arxiv.org/abs/1803.00745v2,,
425,Lipschitz Continuity in Model-based Reinforcement Learning,"We examine the impact of learning Lipschitz continuous models in the contextof model-based reinforcement learning. We provide a novel bound on multi-stepprediction error of Lipschitz models where we quantify the error using theWasserstein metric. We go on to prove an error bound for the value-functionestimate arising from Lipschitz models and show that the estimated valuefunction is itself Lipschitz. We conclude with empirical results that show thebenefits of controlling the Lipschitz constant of neural-network models.",http://arxiv.org/abs/1804.07193v3,,
426,Learning Kolmogorov Models for Binary Random Variables,"We summarize our recent findings, where we proposed a framework for learninga Kolmogorov model, for a collection of binary random variables. Morespecifically, we derive conditions that link outcomes of specific randomvariables, and extract valuable relations from the data. We also propose analgorithm for computing the model and show its first-order optimality, despitethe combinatorial nature of the learning problem. We apply the proposedalgorithm to recommendation systems, although it is applicable to otherscenarios. We believe that the work is a significant step toward interpretablemachine learning.",http://arxiv.org/abs/1806.02322v1,,
427,The Potential of the Return Distribution for Exploration in RL,"This paper studies the potential of the return distribution for explorationin deterministic reinforcement learning (RL) environments. We study networklosses and propagation mechanisms for Gaussian, Categorical and Gaussianmixture distributions. Combined with exploration policies that leverage thisreturn distribution, we solve, for example, a randomized Chain task of length100, which has not been reported before when learning with neural networks.",http://arxiv.org/abs/1806.04242v2,,
428,"Feature Learning and Classification in Neuroimaging: Predicting  Cognitive Impairment from Magnetic Resonance Imaging","Due to the rapid innovation of technology and the desire to find and employbiomarkers for neurodegenerative disease, high-dimensional data classificationproblems are routinely encountered in neuroimaging studies. To avoidover-fitting and to explore relationships between disease and potentialbiomarkers, feature learning and selection plays an important role inclassifier construction and is an important area in machine learning. In thisarticle, we review several important feature learning and selection techniquesincluding lasso-based methods, PCA, the two-sample t-test, and stackedauto-encoders. We compare these approaches using a numerical study involvingthe prediction of Alzheimer's disease from Magnetic Resonance Imaging.",http://arxiv.org/abs/1806.06415v1,,
429,On the Implicit Bias of Dropout,"Algorithmic approaches endow deep learning systems with implicit bias thathelps them generalize even in over-parametrized settings. In this paper, wefocus on understanding such a bias induced in learning through dropout, apopular technique to avoid overfitting in deep learning. For singlehidden-layer linear neural networks, we show that dropout tends to make thenorm of incoming/outgoing weight vectors of all the hidden nodes equal. Inaddition, we provide a complete characterization of the optimization landscapeinduced by dropout.",http://arxiv.org/abs/1806.09777v1,,
430,COLA: Decentralized Linear Learning,"Decentralized machine learning is a promising emerging paradigm in view ofglobal challenges of data ownership and privacy. We consider learning of linearclassification and regression models, in the setting where the training data isdecentralized over many user devices, and the learning algorithm must runon-device, on an arbitrary communication network, without a centralcoordinator. We propose COLA, a new decentralized training algorithm withstrong theoretical guarantees and superior practical performance. Our frameworkovercomes many limitations of existing methods, and achieves communicationefficiency, scalability, elasticity as well as resilience to changes in dataand participating devices.",http://arxiv.org/abs/1808.04883v3,,
431,Phenotype Inference with Semi-Supervised Mixed Membership Models,"Disease phenotyping algorithms process observational clinical data toidentify patients with specific diseases. Supervised phenotyping methodsrequire significant quantities of expert-labeled data, while unsupervisedmethods may learn non-disease phenotypes. To address these limitations, wepropose the Semi-Supervised Mixed Membership Model (SS3M) -- a probabilisticgraphical model for learning disease phenotypes from clinical data withrelatively few labels. We show SS3M can learn interpretable, disease-specificphenotypes which capture the clinical characteristics of the diseases specifiedby the labels provided.",http://arxiv.org/abs/1812.03222v2,,
432,"Unsupervised Scalable Representation Learning for Multivariate Time  Series","Time series constitute a challenging data type for machine learningalgorithms, due to their highly variable lengths and sparse labeling inpractice. In this paper, we tackle this challenge by proposing an unsupervisedmethod to learn universal embeddings of time series. Unlike previous works, itis scalable with respect to their length and we demonstrate the quality,transferability and practicability of the learned representations with thoroughexperiments and comparisons. To this end, we combine an encoder based on causaldilated convolutions with a triplet loss employing time-based negativesampling, obtaining general-purpose representations for variable length andmultivariate time series.",http://arxiv.org/abs/1901.10738v1,,
433,Differentially Private Continual Learning,"Catastrophic forgetting can be a significant problem for institutions thatmust delete historic data for privacy reasons. For example, hospitals might notbe able to retain patient data permanently. But neural networks trained onrecent data alone will tend to forget lessons learned on old data. We present adifferentially private continual learning framework based on variationalinference. We estimate the likelihood of past data given the current modelusing differentially private generative models of old datasets.",http://arxiv.org/abs/1902.06497v1,,
434,"Sparse Learning for Variable Selection with Structures and  Nonlinearities","In this thesis we discuss machine learning methods performing automatedvariable selection for learning sparse predictive models. There are multiplereasons for promoting sparsity in the predictive models. By relying on alimited set of input variables the models naturally counteract the overfittingproblem ubiquitous in learning from finite sets of training points. Sparsemodels are cheaper to use for predictions, they usually require lowercomputational resources and by relying on smaller sets of inputs can possiblyreduce costs for data collection and storage. Sparse models can also contributeto better understanding of the investigated phenomenons as they are easier tointerpret than full models.",http://arxiv.org/abs/1903.10978v1,,
435,Generative Adversarial Networks: recent developments,"In traditional generative modeling, good data representation is very often abase for a good machine learning model. It can be linked to goodrepresentations encoding more explanatory factors that are hidden in theoriginal data. With the invention of Generative Adversarial Networks (GANs), asubclass of generative models that are able to learn representations in anunsupervised and semi-supervised fashion, we are now able to adversariallylearn good mappings from a simple prior distribution to a target datadistribution. This paper presents an overview of recent developments in GANswith a focus on learning latent space representations.",http://arxiv.org/abs/1903.12266v1,,
436,Learning properties of Support Vector Machines,"We study the typical learning properties of the recently proposed SupportVectors Machines. The generalization error on linearly separable tasks, thecapacity, the typical number of Support Vectors, the margin, and the robustnessor noise tolerance of a class of Support Vector Machines are determined in theframework of Statistical Mechanics. The robustness is shown to be closelyrelated to the generalization properties of these machines.",http://arxiv.org/abs/cond-mat/9802179v1,,
437,"On the Consistency of the Bootstrap Approach for Support Vector Machines  and Related Kernel Based Methods","It is shown that bootstrap approximations of support vector machines (SVMs)based on a general convex and smooth loss function and on a general kernel areconsistent. This result is useful to approximate the unknown finite sampledistribution of SVMs by the bootstrap approach.",http://arxiv.org/abs/1301.6944v1,,
438,Bayesian Nonlinear Support Vector Machines for Big Data,"We propose a fast inference method for Bayesian nonlinear support vectormachines that leverages stochastic variational inference and inducing points.Our experiments show that the proposed method is faster than competing Bayesianapproaches and scales easily to millions of data points. It provides additionalfeatures over frequentist competitors such as accurate predictive uncertaintyestimates and automatic hyperparameter search.",http://arxiv.org/abs/1707.05532v1,,
439,Decreasing the size of the Restricted Boltzmann machine,"We propose a method to decrease the number of hidden units of the restrictedBoltzmann machine while avoiding decrease of the performance measured by theKullback-Leibler divergence. Then, we demonstrate our algorithm by usingnumerical simulations.",http://arxiv.org/abs/1807.02999v2,,
440,On the Limits of Learning Representations with Label-Based Supervision,"Advances in neural network based classifiers have transformed automaticfeature learning from a pipe dream of stronger AI to a routine and expectedproperty of practical systems. Since the emergence of AlexNet every winningsubmission of the ImageNet challenge has employed end-to-end representationlearning, and due to the utility of good representations for transfer learning,representation learning has become as an important and distinct task fromsupervised learning. At present, this distinction is inconsequential, assupervised methods are state-of-the-art in learning transferablerepresentations. But recent work has shown that generative models can also bepowerful agents of representation learning. Will the representations learnedfrom these generative methods ever rival the quality of those from theirsupervised competitors? In this work, we argue in the affirmative, that from aninformation theoretic perspective, generative models have greater potential forrepresentation learning. Based on several experimentally validated assumptions,we show that supervised learning is upper bounded in its capacity forrepresentation learning in ways that certain generative models, such asGenerative Adversarial Networks (GANs) are not. We hope that our analysis willprovide a rigorous motivation for further exploration of generativerepresentation learning.",http://arxiv.org/abs/1703.02156v1,,
441,"Realizing Continual Learning through Modeling a Learning System as a  Fiber Bundle","A human brain is capable of continual learning by nature; however the currentmainstream deep neural networks suffer from a phenomenon named catastrophicforgetting (i.e., learning a new set of patterns suddenly and completely wouldresult in fully forgetting what has already been learned). In this paper wepropose a generic learning model, which regards a learning system as a fiberbundle. By comparing the learning performance of our model with conventionalones whose neural networks are multilayer perceptrons through a variety ofmachine-learning experiments, we found our proposed model not only enjoys adistinguished capability of continual learning but also bears a highinformation capacity. In addition, we found in some learning scenarios thelearning performance can be further enhanced by making the learning time-awareto mimic the episodic memory in human brain. Last but not least, we found thatthe properties of forgetting in our model correspond well to those of humanmemory. This work may shed light on how a human brain learns.",http://arxiv.org/abs/1903.03511v1,,
442,Qualitative Robustness of Support Vector Machines,"Support vector machines have attracted much attention in theoretical and inapplied statistics. Main topics of recent interest are consistency, learningrates and robustness. In this article, it is shown that support vector machinesare qualitatively robust. Since support vector machines can be represented by afunctional on the set of all probability measures, qualitative robustness isproven by showing that this functional is continuous with respect to thetopology generated by weak convergence of probability measures. Combined withthe existence and uniqueness of support vector machines, our results show thatsupport vector machines are the solutions of a well-posed mathematical problemin Hadamard's sense.",http://arxiv.org/abs/0912.0874v2,,
443,Classifying Network Data with Deep Kernel Machines,"Inspired by a growing interest in analyzing network data, we study theproblem of node classification on graphs, focusing on approaches based onkernel machines. Conventionally, kernel machines are linear classifiers in theimplicit feature space. We argue that linear classification in the featurespace of kernels commonly used for graphs is often not enough to produce goodresults. When this is the case, one naturally considers nonlinear classifiersin the feature space. We show that repeating this process produces something wecall ""deep kernel machines."" We provide some examples where deep kernelmachines can make a big difference in classification performance, and point outsome connections to various recent literature on deep architectures inartificial intelligence and machine learning.",http://arxiv.org/abs/1001.4019v1,,
444,"Unsupervised Machine Learning for Networking: Techniques, Applications  and Research Challenges","While machine learning and artificial intelligence have long been applied innetworking research, the bulk of such works has focused on supervised learning.Recently there has been a rising trend of employing unsupervised machinelearning using unstructured raw network data to improve network performance andprovide services such as traffic engineering, anomaly detection, Internettraffic classification, and quality of service optimization. The interest inapplying unsupervised learning techniques in networking emerges from theirgreat success in other fields such as computer vision, natural languageprocessing, speech recognition, and optimal control (e.g., for developingautonomous self-driving cars). Unsupervised learning is interesting since itcan unconstrain us from the need of labeled data and manual handcrafted featureengineering thereby facilitating flexible, general, and automated methods ofmachine learning. The focus of this survey paper is to provide an overview ofthe applications of unsupervised learning in the domain of networking. Weprovide a comprehensive survey highlighting the recent advancements inunsupervised learning techniques and describe their applications for variouslearning tasks in the context of networking. We also provide a discussion onfuture directions and open research issues, while also identifying potentialpitfalls. While a few survey papers focusing on the applications of machinelearning in networking have previously been published, a survey of similarscope and breadth is missing in literature. Through this paper, we advance thestate of knowledge by carefully synthesizing the insights from these surveypapers while also providing contemporary coverage of recent advances.",http://arxiv.org/abs/1709.06599v1,,
445,Deep Reinforcement Learning: An Overview,"We give an overview of recent exciting achievements of deep reinforcementlearning (RL). We discuss six core elements, six important mechanisms, andtwelve applications. We start with background of machine learning, deeplearning and reinforcement learning. Next we discuss core RL elements,including value function, in particular, Deep Q-Network (DQN), policy, reward,model, planning, and exploration. After that, we discuss important mechanismsfor RL, including attention and memory, unsupervised learning, transferlearning, multi-agent RL, hierarchical RL, and learning to learn. Then wediscuss various applications of RL, including games, in particular, AlphaGo,robotics, natural language processing, including dialogue systems, machinetranslation, and text generation, computer vision, neural architecture design,business management, finance, healthcare, Industry 4.0, smart grid, intelligenttransportation systems, and computer systems. We mention topics not reviewedyet, and list a collection of RL resources. After presenting a brief summary,we close with discussions.  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significantupdate.",http://arxiv.org/abs/1701.07274v6,,
446,"A Review of Learning with Deep Generative Models from Perspective of  Graphical Modeling","This document aims to provide a review on learning with deep generativemodels (DGMs), which is an highly-active area in machine learning and moregenerally, artificial intelligence. This review is not meant to be a tutorial,but when necessary, we provide self-contained derivations for completeness.This review has two features. First, though there are different perspectives toclassify DGMs, we choose to organize this review from the perspective ofgraphical modeling, because the learning methods for directed DGMs andundirected DGMs are fundamentally different. Second, we differentiate modeldefinitions from model learning algorithms, since different learning algorithmscan be applied to solve the learning problem on the same model, and analgorithm can be applied to learn different models. We thus separate modeldefinition and model learning, with more emphasis on reviewing, differentiatingand connecting different learning algorithms. We also discuss promising futureresearch directions.",http://arxiv.org/abs/1808.01630v4,,
447,"Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep  Learning","Modern machine learning algorithms have been adopted in a range ofsignal-processing applications spanning computer vision, natural languageprocessing, and artificial intelligence. Many relevant problems involvesubspace-structured features, orthogonality constrained or low-rank constrainedobjective functions, or subspace distances. These mathematical characteristicsare expressed naturally using the Grassmann manifold. Unfortunately, this factis not yet explored in many traditional learning algorithms. In the last fewyears, there have been growing interests in studying Grassmann manifold totackle new learning problems. Such attempts have been reassured by substantialperformance improvements in both classic learning and learning using deepneural networks. We term the former as shallow and the latter deep Grassmannianlearning. The aim of this paper is to introduce the emerging area ofGrassmannian learning by surveying common mathematical problems and primarysolution approaches, and overviewing various applications. We hope to inspirepractitioners in different fields to adopt the powerful tool of Grassmannianlearning in their research.",http://arxiv.org/abs/1808.02229v2,,
448,Learning Invariances for Policy Generalization,"While recent progress has spawned very powerful machine learning systems,those agents remain extremely specialized and fail to transfer the knowledgethey gain to similar yet unseen tasks. In this paper, we study a simplereinforcement learning problem and focus on learning policies that encode theproper invariances for generalization to different settings. We evaluate threepotential methods for policy generalization: data augmentation, meta-learningand adversarial training. We find our data augmentation method to be effective,and study the potential of meta-learning and adversarial learning asalternative task-agnostic approaches.  Keywords: reinforcement learning, generalization, data augmentation,meta-learning, adversarial learning.",http://arxiv.org/abs/1809.02591v1,,
449,"Learning Feature Relevance Through Step Size Adaptation in  Temporal-Difference Learning","There is a long history of using meta learning as representation learning,specifically for determining the relevance of inputs. In this paper, we examinean instance of meta-learning in which feature relevance is learned by adaptingstep size parameters of stochastic gradient descent---building on a variety ofprior work in stochastic approximation, machine learning, and artificial neuralnetworks. In particular, we focus on stochastic meta-descent introduced in theIncremental Delta-Bar-Delta (IDBD) algorithm for setting individual step sizesfor each feature of a linear function approximator. Using IDBD, a feature withlarge or small step sizes will have a large or small impact on generalizationfrom training examples. As a main contribution of this work, we extend IDBD totemporal-difference (TD) learning---a form of learning which is effective insequential, non i.i.d. problems. We derive a variety of IDBD generalizationsfor TD learning, demonstrating that they are able to distinguish which featuresare relevant and which are not. We demonstrate that TD IDBD is effective atlearning feature relevance in both an idealized gridworld and a real-worldrobotic prediction task.",http://arxiv.org/abs/1903.03252v1,,
450,Experimental Realization of Quantum Artificial Intelligence,"Machines are possible to have some artificial intelligence like human beingsowing to particular algorithms or software. Such machines could learn knowledgefrom what people taught them and do works according to the knowledge. Inpractical learning cases, the data is often extremely complicated and large,thus classical learning machines often need huge computational resources.Quantum machine learning algorithm, on the other hand, could be exponentiallyfaster than classical machines using quantum parallelism. Here, we demonstratea quantum machine learning algorithm on a four-qubit NMR test bench to solve anoptical character recognition problem, also known as the handwritingrecognition. The quantum machine learns standard character fonts and thenrecognize handwritten characters from a set with two candidates. To our bestknowledge, this is the first artificial intelligence realized on a quantumprocessor. Due to the widespreading importance of artificial intelligence andits tremendous consuming of computational resources, quantum speedup would beextremely attractive against the challenges from the Big Data.",http://arxiv.org/abs/1410.1054v1,,
451,"Error-Resilient Machine Learning in Near Threshold Voltage via  Classifier Ensemble","In this paper, we present the design of error-resilient machine learningarchitectures by employing a distributed machine learning framework referred toas classifier ensemble (CE). CE combines several simple classifiers to obtain astrong one. In contrast, centralized machine learning employs a single complexblock. We compare the random forest (RF) and the support vector machine (SVM),which are representative techniques from the CE and centralized frameworks,respectively. Employing the dataset from UCI machine learning repository andarchitectural-level error models in a commercial 45 nm CMOS process, it isdemonstrated that RF-based architectures are significantly more robust than SVMarchitectures in presence of timing errors due to process variations innear-threshold voltage (NTV) regions (0.3 V - 0.7 V). In particular, the RFarchitecture exhibits a detection accuracy (P_{det}) that varies by 3.2% whilemaintaining a median P_{det} > 0.9 at a gate level delay variation of 28.9% .In comparison, SVM exhibits a P_{det} that varies by 16.8%. Additionally, wepropose an error weighted voting technique that incorporates the timing errorstatistics of the NTV circuit fabric to further enhance robustness. Simulationresults confirm that the error weighted voting achieves a P_{det} that variesby only 1.4%, which is 12X lower compared to SVM.",http://arxiv.org/abs/1607.07804v1,,
452,Machine learning for neural decoding,"Despite rapid advances in machine learning tools, the majority of neuraldecoding approaches still use traditional methods. Improving the performance ofneural decoding algorithms allows us to better understand the informationcontained in a neural population, and can help advance engineering applicationssuch as brain machine interfaces. Here, we apply modern machine learningtechniques, including neural networks and gradient boosting, to decode fromspiking activity in 1) motor cortex, 2) somatosensory cortex, and 3)hippocampus. We compare the predictive ability of these modern methods withtraditional decoding methods such as Wiener and Kalman filters. Modern methods,in particular neural networks and ensembles, significantly outperformed thetraditional approaches. For instance, for all of the three brain areas, an LSTMdecoder explained over 40% of the unexplained variance from a Wiener filter.These results suggest that modern machine learning techniques should become thestandard methodology for neural decoding. We provide a tutorial and code tofacilitate wider implementation of these methods.",http://arxiv.org/abs/1708.00909v2,,
453,On Ensuring that Intelligent Machines Are Well-Behaved,"Machine learning algorithms are everywhere, ranging from simple data analysisand pattern recognition tools used across the sciences to complex systems thatachieve super-human performance on various tasks. Ensuring that they arewell-behaved---that they do not, for example, cause harm to humans or act in aracist or sexist way---is therefore not a hypothetical problem to be dealt within the future, but a pressing one that we address here. We propose a newframework for designing machine learning algorithms that simplifies the problemof specifying and regulating undesirable behaviors. To show the viability ofthis new framework, we use it to create new machine learning algorithms thatpreclude the sexist and harmful behaviors exhibited by standard machinelearning algorithms in our experiments. Our framework for designing machinelearning algorithms simplifies the safe and responsible application of machinelearning.",http://arxiv.org/abs/1708.05448v1,,
454,"Evaluating Hospital Case Cost Prediction Models Using Azure Machine  Learning Studio","Ability for accurate hospital case cost modelling and prediction is criticalfor efficient health care financial management and budgetary planning. Avariety of regression machine learning algorithms are known to be effective forhealth care cost predictions. The purpose of this experiment was to build anAzure Machine Learning Studio tool for rapid assessment of multiple types ofregression models. The tool offers environment for comparing 14 types ofregression models in a unified experiment: linear regression, Bayesian linearregression, decision forest regression, boosted decision tree regression,neural network regression, Poisson regression, Gaussian processes forregression, gradient boosted machine, nonlinear least squares regression,projection pursuit regression, random forest regression, robust regression,robust regression with mm-type estimators, support vector regression. The toolpresents assessment results arranged by model accuracy in a single table usingfive performance metrics. Evaluation of regression machine learning models forperforming hospital case cost prediction demonstrated advantage of robustregression model, boosted decision tree regression and decision forestregression. The operational tool has been published to the web and openlyavailable for experiments and extensions.",http://arxiv.org/abs/1804.01825v2,,
455,Multimodal Machine Translation with Reinforcement Learning,"Multimodal machine translation is one of the applications that integratescomputer vision and language processing. It is a unique task given that in thefield of machine translation, many state-of-the-arts algorithms still onlyemploy textual information. In this work, we explore the effectiveness ofreinforcement learning in multimodal machine translation. We present a novelalgorithm based on the Advantage Actor-Critic (A2C) algorithm that specificallycater to the multimodal machine translation task of the EMNLP 2018 ThirdConference on Machine Translation (WMT18). We experiment our proposed algorithmon the Multi30K multilingual English-German image description dataset and theFlickr30K image entity dataset. Our model takes two channels of inputs, imageand text, uses translation evaluation metrics as training rewards, and achievesbetter results than supervised learning MLE baseline models. Furthermore, wediscuss the prospects and limitations of using reinforcement learning formachine translation. Our experiment results suggest a promising reinforcementlearning solution to the general task of multimodal sequence to sequencelearning.",http://arxiv.org/abs/1805.02356v1,,
456,"High Dimensional Model Representation as a Glass Box in Supervised  Machine Learning","Prediction and explanation are key objects in supervised machine learning,where predictive models are known as black boxes and explanatory models areknown as glass boxes. Explanation provides the necessary and sufficientinformation to interpret the model output in terms of the model input. Itincludes assessments of model output dependence on important input variablesand measures of input variable importance to model output. High dimensionalmodel representation (HDMR), also known as the generalized functional ANOVAexpansion, provides useful insight into the input-output behavior of supervisedmachine learning models. This article gives applications of HDMR in supervisedmachine learning. The first application is characterizing information leakagein ``big-data'' settings. The second application is reduced-orderrepresentation of elementary symmetric polynomials. The third application isanalysis of variance with correlated variables. The last application isestimation of HDMR from kernel machine and decision tree black boxrepresentations. These results suggest HDMR to have broad utility withinmachine learning as a glass box representation.",http://arxiv.org/abs/1807.10320v1,,
457,"An Amalgamation of Classical and Quantum Machine Learning For the  Classification of Adenocarcinoma and Squamous Cell Carcinoma Patients","The ability to accurately classify disease subtypes is of vital importance,especially in oncology where this capability could have a life saving impact.Here we report a classification between two subtypes of non-small cell lungcancer, namely Adeno- carcinoma vs Squamous cell carcinoma. The data consistsof approximately 20,000 gene expression values for each of 104 patients. Thedata was curated from [1] [2]. We used an amalgamation of classical and andquantum machine learning models to successfully classify these patients. Weutilized feature selection methods based on univariate statistics in additionto XGBoost [3]. A novel and proprietary data representation method developed byone of the authors called QCrush was also used as it was designed toincorporate a maximal amount of information under the size constraints of theD-Wave quantum annealing computer. The machine learning was performed by aQuantum Boltzmann Machine. This paper will report our results, the variousclassical methods, and the quantum machine learning approach we utilized.",http://arxiv.org/abs/1810.11959v1,,
458,Machine Translation : From Statistical to modern Deep-learning practices,"Machine translation (MT) is an area of study in Natural Language processingwhich deals with the automatic translation of human language, from one languageto another by the computer. Having a rich research history spanning nearlythree decades, Machine translation is one of the most sought after area ofresearch in the linguistics and computational community. In this paper, weinvestigate the models based on deep learning that have achieved substantialprogress in recent years and becoming the prominent method in MT. We shalldiscuss the two main deep-learning based Machine Translation methods, one atcomponent or domain level which leverages deep learning models to enhance theefficacy of Statistical Machine Translation (SMT) and end-to-end deep learningmodels in MT which uses neural networks to find correspondence between thesource and target languages using the encoder-decoder architecture. We concludethis paper by providing a time line of the major research problems solved bythe researchers and also provide a comprehensive overview of present areas ofresearch in Neural Machine Translation.",http://arxiv.org/abs/1812.04238v1,,
459,Ecological Data Analysis Based on Machine Learning Algorithms,"Classification is an important supervised machine learning method, which isnecessary and challenging issue for ecological research. It offers a way toclassify a dataset into subsets that share common patterns. Notably, there aremany classification algorithms to choose from, each making certain assumptionsabout the data and about how classification should be formed. In this paper, weapplied eight machine learning classification algorithms such as DecisionTrees, Random Forest, Artificial Neural Network, Support Vector Machine, LinearDiscriminant Analysis, k-nearest neighbors, Logistic Regression and Naive Bayeson ecological data. The goal of this study is to compare different machinelearning classification algorithms in ecological dataset. In this analysis wehave checked the accuracy test among the algorithms. In our study we concludethat Linear Discriminant Analysis and k-nearest neighbors are the best methodsamong all other methods",http://arxiv.org/abs/1812.09138v1,,
460,"Artificial Intelligence and Machine Learning to Predict and Improve  Efficiency in Manufacturing Industry","The overall equipment effectiveness (OEE) is a performance measurement metricwidely used. Its calculation provides to the managers the possibility toidentify the main losses that reduce the machine effectiveness and then takethe necessary decisions in order to improve the situation. However, thiscalculation is done a-posterior which is often too late. In the presentresearch, we implemented different Machine Learning algorithms namely; Supportvector machine, Optimized Support vector Machine (using Genetic Algorithm),Random Forest, XGBoost and Deep Learning to predict the estimate OEE value. Thedata used to train our models was provided by an automotive cable productionindustry. The results show that the Deep Learning and Random Forest are moreaccurate and present better performance for the prediction of the overallequipment effectiveness in our case study.",http://arxiv.org/abs/1901.02256v2,,
461,The Spatially-Conscious Machine Learning Model,"Successfully predicting gentrification could have many social and commercialapplications; however, real estate sales are difficult to predict because theybelong to a chaotic system comprised of intrinsic and extrinsiccharacteristics, perceived value, and market speculation. Using New York Cityreal estate as our subject, we combine modern techniques of data science andmachine learning with traditional spatial analysis to create robust real estateprediction models for both classification and regression tasks. We compareseveral cutting edge machine learning algorithms across spatial, semi-spatialand non-spatial feature engineering techniques, and we empirically show thatspatially-conscious machine learning models outperform non-spatial models whenmarried with advanced prediction techniques such as feed-forward artificialneural networks and gradient boosting machine models.",http://arxiv.org/abs/1902.00562v1,,
462,Stochastic Descent Analysis of Representation Learning Algorithms,"Although stochastic approximation learning methods have been widely used inthe machine learning literature for over 50 years, formal theoretical analysesof specific machine learning algorithms are less common because stochasticapproximation theorems typically possess assumptions which are difficult tocommunicate and verify. This paper presents a new stochastic approximationtheorem for state-dependent noise with easily verifiable assumptions applicableto the analysis and design of important deep learning algorithms including:adaptive learning, contrastive divergence learning, stochastic descentexpectation maximization, and active learning.",http://arxiv.org/abs/1412.5744v7,,
463,Recommending Learning Algorithms and Their Associated Hyperparameters,"The success of machine learning on a given task dependson, among otherthings, which learning algorithm is selected and its associatedhyperparameters. Selecting an appropriate learning algorithm and setting itshyperparameters for a given data set can be a challenging task, especially forusers who are not experts in machine learning. Previous work has examined usingmeta-features to predict which learning algorithm and hyperparameters should beused. However, choosing a set of meta-features that are predictive of algorithmperformance is difficult. Here, we propose to apply collaborative filteringtechniques to learning algorithm and hyperparameter selection, and find thatdoing so avoids determining which meta-features to use and outperformstraditional meta-learning approaches in many cases.",http://arxiv.org/abs/1407.1890v1,,
464,"A Comparative Study of Pairwise Learning Methods based on Kernel Ridge  Regression","Many machine learning problems can be formulated as predicting labels for apair of objects. Problems of that kind are often referred to as pairwiselearning, dyadic prediction or network inference problems. During the lastdecade kernel methods have played a dominant role in pairwise learning. Theystill obtain a state-of-the-art predictive performance, but a theoreticalanalysis of their behavior has been underexplored in the machine learningliterature.  In this work we review and unify existing kernel-based algorithms that arecommonly used in different pairwise learning settings, ranging from matrixfiltering to zero-shot learning. To this end, we focus on closed-form efficientinstantiations of Kronecker kernel ridge regression. We show that independenttask kernel ridge regression, two-step kernel ridge regression and a linearmatrix filter arise naturally as a special case of Kronecker kernel ridgeregression, implying that all these methods implicitly minimize a squared loss.In addition, we analyze universality, consistency and spectral filteringproperties. Our theoretical results provide valuable insights in assessing theadvantages and limitations of existing pairwise learning methods.",http://arxiv.org/abs/1803.01575v1,,
465,Agreement-based Learning,"Model selection is a problem that has occupied machine learning researchersfor a long time. Recently, its importance has become evident throughapplications in deep learning. We propose an agreement-based learning frameworkthat prevents many of the pitfalls associated with model selection. It relieson coupling the training of multiple models by encouraging them to agree ontheir predictions while training. In contrast with other model selection andcombination approaches used in machine learning, the proposed framework isinspired by human learning. We also propose a learning algorithm defined withinthis framework which manages to significantly outperform alternatives inpractice, and whose performance improves further with the availability ofunlabeled data. Finally, we describe a number of potential directions fordeveloping more flexible agreement-based learning algorithms.",http://arxiv.org/abs/1806.01258v1,,
466,Restricted Boltzmann Machines: Introduction and Review,"The restricted Boltzmann machine is a network of stochastic units withundirected interactions between pairs of visible and hidden units. This modelwas popularized as a building block of deep learning architectures and hascontinued to play an important role in applied and theoretical machinelearning. Restricted Boltzmann machines carry a rich structure, withconnections to geometry, applied algebra, probability, statistics, machinelearning, and other areas. The analysis of these models is attractive in itsown right and also as a platform to combine and generalize mathematical toolsfor graphical models with hidden variables. This article gives an introductionto the mathematical analysis of restricted Boltzmann machines, reviews recentresults on the geometry of the sets of probability distributions representableby these models, and suggests a few directions for further investigation.",http://arxiv.org/abs/1806.07066v1,,
467,Entanglement-Based Machine Learning on a Quantum Computer,"Machine learning, a branch of artificial intelligence, learns from previousexperience to optimize performance, which is ubiquitous in various fields suchas computer sciences, financial analysis, robotics, and bioinformatics. Achallenge is that machine learning with the rapidly growing ""big data"" couldbecome intractable for classical computers. Recently, quantum machine learningalgorithms [Lloyd, Mohseni, and Rebentrost, arXiv.1307.0411] were proposedwhich could offer an exponential speedup over classical algorithms. Here, wereport the first experimental entanglement-based classification of 2-, 4-, and8-dimensional vectors to different clusters using a small-scale photonicquantum computer, which are then used to implement supervised and unsupervisedmachine learning. The results demonstrate the working principle of usingquantum computers to manipulate and classify high-dimensional vectors, the coremathematical routine in machine learning. The method can in principle be scaledto larger number of qubits, and may provide a new route to accelerate machinelearning.",http://arxiv.org/abs/1409.7770v3,,
468,"Proceedings of the 5th Workshop on Machine Learning and Interpretation  in Neuroimaging (MLINI) at NIPS 2015","This volume is a collection of contributions from the 5th Workshop on MachineLearning and Interpretation in Neuroimaging (MLINI) at the Neural InformationProcessing Systems (NIPS 2015) conference. Modern multivariate statisticalmethods developed in the rapidly growing field of machine learning are beingincreasingly applied to various problems in neuroimaging, from cognitive statedetection to clinical diagnosis and prognosis. Multivariate pattern analysismethods are designed to examine complex relationships between high-dimensionalsignals, such as brain images, and outcomes of interest, such as the categoryof a stimulus, a type of a mental state of a subject, or a specific mentaldisorder. Such techniques are in contrast with the traditional mass-univariateapproaches that dominated neuroimaging in the past and treated each individualimaging measurement in isolation.  We believe that machine learning has a prominent role in shaping howquestions in neuroscience are framed, and that the machine-learning mind set isnow entering modern psychology and behavioral studies. It is also equallyimportant that practical applications in these fields motivate a rapidlyevolving line or research in the machine learning community. In parallel, thereis an intense interest in learning more about brain function in the context ofrich naturalistic environments and scenes. Efforts to go beyond highly specificparadigms that pinpoint a single function, towards schemes for measuring theinteraction with natural and more varied scene are made. The goal of theworkshop is to pinpoint the most pressing issues and common challenges acrossthe neuroscience, neuroimaging, psychology and machine learning fields, and tosketch future directions and open questions in the light of novel methodology.",http://arxiv.org/abs/1605.04435v1,,
469,Using Human Brain Activity to Guide Machine Learning,"Machine learning is a field of computer science that builds algorithms thatlearn. In many cases, machine learning algorithms are used to recreate a humanability like adding a caption to a photo, driving a car, or playing a game.While the human brain has long served as a source of inspiration for machinelearning, little effort has been made to directly use data collected fromworking brains as a guide for machine learning algorithms. Here we demonstratea new paradigm of ""neurally-weighted"" machine learning, which takes fMRImeasurements of human brain activity from subjects viewing images, and infusesthese data into the training process of an object recognition learningalgorithm to make it more consistent with the human brain. After training,these neurally-weighted classifiers are able to classify images withoutrequiring any additional neural data. We show that our neural-weightingapproach can lead to large performance gains when used with traditional machinevision features, as well as to significant improvements with alreadyhigh-performing convolutional neural network features. The effectiveness ofthis approach points to a path forward for a new class of hybrid machinelearning algorithms which take both inspiration and direct constraints fromneuronal data.",http://arxiv.org/abs/1703.05463v2,,
470,BEBP: An Poisoning Method Against Machine Learning Based IDSs,"In big data era, machine learning is one of fundamental techniques inintrusion detection systems (IDSs). However, practical IDSs generally updatetheir decision module by feeding new data then retraining learning models in aperiodical way. Hence, some attacks that comprise the data for training ortesting classifiers significantly challenge the detecting capability of machinelearning-based IDSs. Poisoning attack, which is one of the most recognizedsecurity threats towards machine learning-based IDSs, injects some adversarialsamples into the training phase, inducing data drifting of training data and asignificant performance decrease of target IDSs over testing data. In thispaper, we adopt the Edge Pattern Detection (EPD) algorithm to design a novelpoisoning method that attack against several machine learning algorithms usedin IDSs. Specifically, we propose a boundary pattern detection algorithm toefficiently generate the points that are near to abnormal data but consideredto be normal ones by current classifiers. Then, we introduce a Batch-EPDBoundary Pattern (BEBP) detection algorithm to overcome the limitation of thenumber of edge pattern points generated by EPD and to obtain more usefuladversarial samples. Based on BEBP, we further present a moderate but effectivepoisoning method called chronic poisoning attack. Extensive experiments onsynthetic and three real network data sets demonstrate the performance of theproposed poisoning method against several well-known machine learningalgorithms and a practical intrusion detection method named FMIFS-LSSVM-IDS.",http://arxiv.org/abs/1803.03965v1,,
471,"Entanglement-guided architectures of machine learning by quantum tensor  network","It is a fundamental, but still elusive question whether the schemes based onquantum mechanics, in particular on quantum entanglement, can be used forclassical information processing and machine learning. Even partial answer tothis question would bring important insights to both fields of machine learningand quantum mechanics. In this work, we implement simple numerical experiments,related to pattern/images classification, in which we represent the classifiersby many-qubit quantum states written in the matrix product states (MPS).Classical machine learning algorithm is applied to these quantum states tolearn the classical data. We explicitly show how quantum entanglement (i.e.,single-site and bipartite entanglement) can emerge in such represented images.Entanglement characterizes here the importance of data, and such informationare practically used to guide the architecture of MPS, and improve theefficiency. The number of needed qubits can be reduced to less than 1/10 of theoriginal number, which is within the access of the state-of-the-art quantumcomputers. We expect such numerical experiments could open new paths incharactering classical machine learning algorithms, and at the same time shedlights on the generic quantum simulations/computations of machine learningtasks.",http://arxiv.org/abs/1803.09111v3,,
472,Feasibility of Supervised Machine Learning for Cloud Security,"Cloud computing is gaining significant attention, however, security is thebiggest hurdle in its wide acceptance. Users of cloud services are underconstant fear of data loss, security threats and availability issues. Recently,learning-based methods for security applications are gaining popularity in theliterature with the advents in machine learning techniques. However, the majorchallenge in these methods is obtaining real-time and unbiased datasets. Manydatasets are internal and cannot be shared due to privacy issues or may lackcertain statistical characteristics. As a result of this, researchers prefer togenerate datasets for training and testing purpose in the simulated or closedexperimental environments which may lack comprehensiveness. Machine learningmodels trained with such a single dataset generally result in a semantic gapbetween results and their application. There is a dearth of research work whichdemonstrates the effectiveness of these models across multiple datasetsobtained in different environments. We argue that it is necessary to test therobustness of the machine learning models, especially in diversified operatingconditions, which are prevalent in cloud scenarios. In this work, we use theUNSW dataset to train the supervised machine learning models. We then testthese models with ISOT dataset. We present our results and argue that moreresearch in the field of machine learning is still required for itsapplicability to the cloud security.",http://arxiv.org/abs/1810.09878v1,,
473,Gradual Machine Learning for Entity Resolution,"Usually considered as a classification problem, entity resolution can be verychallenging on real data due to the prevalence of dirty values. Thestate-of-the-art solutions for ER were built on a variety of learning models(most notably deep neural networks), which require lots of accurately labeledtraining data. Unfortunately, high-quality labeled data usually requireexpensive manual work, and are therefore not readily available in many realscenarios. In this paper, we propose a novel learning paradigm for ER, calledgradual machine learning, which aims to enable effective machine learningwithout the requirement for manual labeling effort. It begins with some easyinstances in a task, which can be automatically labeled by the machine withhigh accuracy, and then gradually labels more challenging instances based oniterative factor graph inference. In gradual machine learning, the hardinstances in a task are gradually labeled in small stages based on theestimated evidential certainty provided by the labeled easier instances. Ourextensive experiments on real data have shown that the proposed approachperforms considerably better than its unsupervised alternatives, and it ishighly competitive with the state-of-the-art supervised techniques. Using ER asa test case, we demonstrate that gradual machine learning is a promisingparadigm potentially applicable to other challenging classification tasksrequiring extensive labeling effort.",http://arxiv.org/abs/1810.12125v2,,
474,A Machine Learning Perspective on Predictive Coding with PAQ,"PAQ8 is an open source lossless data compression algorithm that currentlyachieves the best compression rates on many benchmarks. This report presents adetailed description of PAQ8 from a statistical machine learning perspective.It shows that it is possible to understand some of the modules of PAQ8 and usethis understanding to improve the method. However, intuitive statisticalexplanations of the behavior of other modules remain elusive. We hope thedescription in this report will be a starting point for discussions that willincrease our understanding, lead to improvements to PAQ8, and facilitate atransfer of knowledge from PAQ8 to other machine learning methods, such arecurrent neural networks and stochastic memoizers. Finally, the reportpresents a broad range of new applications of PAQ to machine learning tasksincluding language modeling and adaptive text prediction, adaptive gameplaying, classification, and compression using features from the field of deeplearning.",http://arxiv.org/abs/1108.3298v1,,
475,MLlib: Machine Learning in Apache Spark,"Apache Spark is a popular open-source platform for large-scale dataprocessing that is well-suited for iterative machine learning tasks. In thispaper we present MLlib, Spark's open-source distributed machine learninglibrary. MLlib provides efficient functionality for a wide range of learningsettings and includes several underlying statistical, optimization, and linearalgebra primitives. Shipped with Spark, MLlib supports several languages andprovides a high-level API that leverages Spark's rich ecosystem to simplify thedevelopment of end-to-end machine learning pipelines. MLlib has experienced arapid growth due to its vibrant open-source community of over 140 contributors,and includes extensive documentation to support further growth and to let usersquickly get up to speed.",http://arxiv.org/abs/1505.06807v1,,
476,"Distributed Machine Learning in Materials that Couple Sensing,  Actuation, Computation and Communication","This paper reviews machine learning applications and approaches to detection,classification and control of intelligent materials and structures withembedded distributed computation elements. The purpose of this survey is toidentify desired tasks to be performed in each type of material or structure(e.g., damage detection in composites), identify and compare common approachesto learning such tasks, and investigate models and training paradigms used.Machine learning approaches and common temporal features used in the domains ofstructural health monitoring, morphable aircraft, wearable computing androbotic skins are explored. As the ultimate goal of this research is toincorporate the approaches described in this survey into a robotic materialparadigm, the potential for adapting the computational models used in theseapplications, and corresponding training algorithms, to an amorphous network ofcomputing nodes is considered. Distributed versions of support vector machines,graphical models and mixture models developed in the field of wireless sensornetworks are reviewed. Potential areas of investigation, including possiblearchitectures for incorporating machine learning into robotic nodes, trainingapproaches, and the possibility of using deep learning approaches for automaticfeature extraction, are discussed.",http://arxiv.org/abs/1606.03508v1,,
477,"A short note on extension theorems and their connection to universal  consistency in machine learning","Statistical machine learning plays an important role in modern statistics andcomputer science. One main goal of statistical machine learning is to provideuniversally consistent algorithms, i.e., the estimator converges in probabilityor in some stronger sense to the Bayes risk or to the Bayes decision function.Kernel methods based on minimizing the regularized risk over a reproducingkernel Hilbert space (RKHS) belong to these statistical machine learningmethods. It is in general unknown which kernel yields optimal results for aparticular data set or for the unknown probability measure. Hence variouskernel learning methods were proposed to choose the kernel and therefore alsoits RKHS in a data adaptive manner. Nevertheless, many practitioners often usethe classical Gaussian RBF kernel or certain Sobolev kernels with good success.The goal of this short note is to offer one possible theoretical explanationfor this empirical fact.",http://arxiv.org/abs/1604.04505v1,,
478,Learning molecular energies using localized graph kernels,"Recent machine learning methods make it possible to model potential energy ofatomic configurations with chemical-level accuracy (as calculated fromab-initio calculations) and at speeds suitable for molecular dynam- icssimulation. Best performance is achieved when the known physical constraintsare encoded in the machine learning models. For example, the atomic energy isinvariant under global translations and rotations, it is also invariant topermutations of same-species atoms. Although simple to state, these symmetriesare complicated to encode into machine learning algorithms. In this paper, wepresent a machine learning approach based on graph theory that naturallyincorporates translation, rotation, and permutation symmetries. Specifically,we use a random walk graph kernel to measure the similarity of two adjacencymatrices, each of which represents a local atomic environment. This GraphApproximated Energy (GRAPE) approach is flexible and admits many possibleextensions. We benchmark a simple version of GRAPE by predicting atomizationenergies on a standard dataset of organic molecules.",http://arxiv.org/abs/1612.00193v2,,
479,Clipper: A Low-Latency Online Prediction Serving System,"Machine learning is being deployed in a growing number of applications whichdemand real-time, accurate, and robust predictions under heavy query load.However, most machine learning frameworks and systems only address modeltraining and not deployment.  In this paper, we introduce Clipper, a general-purpose low-latency predictionserving system. Interposing between end-user applications and a wide range ofmachine learning frameworks, Clipper introduces a modular architecture tosimplify model deployment across frameworks and applications. Furthermore, byintroducing caching, batching, and adaptive model selection techniques, Clipperreduces prediction latency and improves prediction throughput, accuracy, androbustness without modifying the underlying machine learning frameworks. Weevaluate Clipper on four common machine learning benchmark datasets anddemonstrate its ability to meet the latency, accuracy, and throughput demandsof online serving applications. Finally, we compare Clipper to the TensorFlowServing system and demonstrate that we are able to achieve comparablethroughput and latency while enabling model composition and online learning toimprove accuracy and render more robust predictions.",http://arxiv.org/abs/1612.03079v2,,
480,Byzantine-Tolerant Machine Learning,"The growth of data, the need for scalability and the complexity of modelsused in modern machine learning calls for distributed implementations. Yet, asof today, distributed machine learning frameworks have largely ignored thepossibility of arbitrary (i.e., Byzantine) failures. In this paper, we studythe robustness to Byzantine failures at the fundamental level of stochasticgradient descent (SGD), the heart of most machine learning algorithms. Assuminga set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust canSGD be, without limiting the dimension, nor the size of the parameter space.  We first show that no gradient descent update rule based on a linearcombination of the vectors proposed by the workers (i.e, current approaches)tolerates a single Byzantine failure. We then formulate a resilience propertyof the update rule capturing the basic requirements to guarantee convergencedespite $f$ Byzantine workers. We finally propose Krum, an update rule thatsatisfies the resilience property aforementioned. For a $d$-dimensionallearning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$.",http://arxiv.org/abs/1703.02757v1,,
481,"Krylov Subspace Recycling for Fast Iterative Least-Squares in Machine  Learning","Solving symmetric positive definite linear problems is a fundamentalcomputational task in machine learning. The exact solution, famously, iscubicly expensive in the size of the matrix. To alleviate this problem, severallinear-time approximations, such as spectral and inducing-point methods, havebeen suggested and are now in wide use. These are low-rank approximations thatchoose the low-rank space a priori and do not refine it over time. While thisallows linear cost in the data-set size, it also causes a finite, uncorrectedapproximation error. Authors from numerical linear algebra have explored waysto iteratively refine such low-rank approximations, at a cost of a small numberof matrix-vector multiplications. This idea is particularly interesting in themany situations in machine learning where one has to solve a sequence ofrelated symmetric positive definite linear problems. From the machine learningperspective, such deflation methods can be interpreted as transfer learning ofa low-rank approximation across a time-series of numerical tasks. We study theuse of such methods for our field. Our empirical results show that, onregression and classification problems of intermediate size, this approach caninterpolate between low computational cost and numerical precision.",http://arxiv.org/abs/1706.00241v1,,
482,Gigamachine: incremental machine learning on desktop computers,"We present a concrete design for Solomonoff's incremental machine learningsystem suitable for desktop computers. We use R5RS Scheme and its standardlibrary with a few omissions as the reference machine. We introduce a LevinSearch variant based on a stochastic Context Free Grammar together with newupdate algorithms that use the same grammar as a guiding probabilitydistribution for incremental machine learning. The updates include adjustingproduction probabilities, re-using previous solutions, learning programmingidioms and discovery of frequent subprograms. The issues of extending the apriori probability distribution and bootstrapping are discussed. We haveimplemented a good portion of the proposed algorithms. Experiments with toyproblems show that the update algorithms work as expected.",http://arxiv.org/abs/1709.03413v1,,
483,"Introduction to Tensor Decompositions and their Applications in Machine  Learning","Tensors are multidimensional arrays of numerical values and thereforegeneralize matrices to multiple dimensions. While tensors first emerged in thepsychometrics community in the $20^{\text{th}}$ century, they have since thenspread to numerous other disciplines, including machine learning. Tensors andtheir decompositions are especially beneficial in unsupervised learningsettings, but are gaining popularity in other sub-disciplines like temporal andmulti-relational data analysis, too.  The scope of this paper is to give a broad overview of tensors, theirdecompositions, and how they are used in machine learning. As part of this, weare going to introduce basic tensor concepts, discuss why tensors can beconsidered more rigid than matrices with respect to the uniqueness of theirdecomposition, explain the most important factorization algorithms and theirproperties, provide concrete examples of tensor decomposition applications inmachine learning, conduct a case study on tensor-based estimation of mixturemodels, talk about the current state of research, and provide references toavailable software libraries.",http://arxiv.org/abs/1711.10781v1,,
484,"Detection of Adversarial Training Examples in Poisoning Attacks through  Anomaly Detection","Machine learning has become an important component for many systems andapplications including computer vision, spam filtering, malware and networkintrusion detection, among others. Despite the capabilities of machine learningalgorithms to extract valuable information from data and produce accuratepredictions, it has been shown that these algorithms are vulnerable to attacks.Data poisoning is one of the most relevant security threats against machinelearning systems, where attackers can subvert the learning process by injectingmalicious samples in the training data. Recent work in adversarial machinelearning has shown that the so-called optimal attack strategies cansuccessfully poison linear classifiers, degrading the performance of the systemdramatically after compromising a small fraction of the training dataset. Inthis paper we propose a defence mechanism to mitigate the effect of theseoptimal poisoning attacks based on outlier detection. We show empirically thatthe adversarial examples generated by these attack strategies are quitedifferent from genuine points, as no detectability constrains are considered tocraft the attack. Hence, they can be detected with an appropriate pre-filteringof the training dataset.",http://arxiv.org/abs/1802.03041v1,,
485,Global Model Interpretation via Recursive Partitioning,"In this work, we propose a simple but effective method to interpret black-boxmachine learning models globally. That is, we use a compact binary tree, theinterpretation tree, to explicitly represent the most important decision rulesthat are implicitly contained in the black-box machine learning models. Thistree is learned from the contribution matrix which consists of thecontributions of input variables to predicted scores for each singleprediction. To generate the interpretation tree, a unified process recursivelypartitions the input variable space by maximizing the difference in the averagecontribution of the split variable between the divided spaces. We demonstratethe effectiveness of our method in diagnosing machine learning models onmultiple tasks. Also, it is useful for new knowledge discovery as such insightsare not easily identifiable when only looking at single predictions. Ingeneral, our work makes it easier and more efficient for human beings tounderstand machine learning models.",http://arxiv.org/abs/1802.04253v2,,
486,Impact of Biases in Big Data,"The underlying paradigm of big data-driven machine learning reflects thedesire of deriving better conclusions from simply analyzing more data, withoutthe necessity of looking at theory and models. Is having simply more dataalways helpful? In 1936, The Literary Digest collected 2.3M filled inquestionnaires to predict the outcome of that year's US presidential election.The outcome of this big data prediction proved to be entirely wrong, whereasGeorge Gallup only needed 3K handpicked people to make an accurate prediction.Generally, biases occur in machine learning whenever the distributions oftraining set and test set are different. In this work, we provide a review ofdifferent sorts of biases in (big) data sets in machine learning. We providedefinitions and discussions of the most commonly appearing biases in machinelearning: class imbalance and covariate shift. We also show how these biasescan be quantified and corrected. This work is an introductory text for bothresearchers and practitioners to become more aware of this topic and thus toderive more reliable models for their learning problems.",http://arxiv.org/abs/1803.00897v1,,
487,"Classification of Building Information Model (BIM) Structures with Deep  Learning","In this work we study an application of machine learning to the constructionindustry and we use classical and modern machine learning methods to categorizeimages of building designs into three classes: Apartment building, Industrialbuilding or Other. No real images are used, but only images extracted fromBuilding Information Model (BIM) software, as these are used by theconstruction industry to store building designs. For this task, we comparedfour different methods: the first is based on classical machine learning, whereHistogram of Oriented Gradients (HOG) was used for feature extraction and aSupport Vector Machine (SVM) for classification; the other three methods arebased on deep learning, covering common pre-trained networks as well as onesdesigned from scratch. To validate the accuracy of the models, a database of240 images was used. The accuracy achieved is 57% for the HOG + SVM model, andabove 89% for the neural networks.",http://arxiv.org/abs/1808.00601v1,,
488,Hedging Algorithms and Repeated Matrix Games,"Playing repeated matrix games (RMG) while maximizing the cumulative returnsis a basic method to evaluate multi-agent learning (MAL) algorithms. Previouswork has shown that $UCB$, $M3$, $S$ or $Exp3$ algorithms have good behaviourson average in RMG. Besides, hedging algorithms have been shown to be effectiveon prediction problems. An hedging algorithm is made up with a top-levelalgorithm and a set of basic algorithms. To make its decision, an hedgingalgorithm uses its top-level algorithm to choose a basic algorithm, and thechosen algorithm makes the decision. This paper experimentally shows thatwell-selected hedging algorithms are better on average than all previous MALalgorithms on the task of playing RMG against various players. $S$ is a verygood top-level algorithm, and $UCB$ and $M3$ are very good basic algorithms.Furthermore, two-level hedging algorithms are more effective than one-levelhedging algorithms, and three levels are not better than two levels.",http://arxiv.org/abs/1810.06443v1,,
489,NSML: Meet the MLaaS platform with a real-world case study,"The boom of deep learning induced many industries and academies to introducemachine learning based approaches into their concern, competitively. However,existing machine learning frameworks are limited to sufficiently fulfill thecollaboration and management for both data and models. We proposed NSML, amachine learning as a service (MLaaS) platform, to meet these demands. NSMLhelps machine learning work be easily launched on a NSML cluster and provides acollaborative environment which can afford development at enterprise scale.Finally, NSML users can deploy their own commercial services with NSML cluster.In addition, NSML furnishes convenient visualization tools which assist theusers in analyzing their work. To verify the usefulness and accessibility ofNSML, we performed some experiments with common examples. Furthermore, weexamined the collaborative advantages of NSML through three competitions withreal-world use cases.",http://arxiv.org/abs/1810.09957v1,,
490,"Distilling Information from a Flood: A Possibility for the Use of  Meta-Analysis and Systematic Review in Machine Learning Research","The current flood of information in all areas of machine learning research,from computer vision to reinforcement learning, has made it difficult to makeaggregate scientific inferences. It can be challenging to distill a myriad ofsimilar papers into a set of useful principles, to determine which newmethodologies to use for a particular application, and to be confident that onehas compared against all relevant related work when developing new ideas.However, such a rapidly growing body of research literature is a problem thatother fields have already faced - in particular, medicine and epidemiology. Inthose fields, systematic reviews and meta-analyses have been used exactly fordealing with these issues and it is not uncommon for entire journals to bededicated to such analyses. Here, we suggest the field of machine learningmight similarly benefit from meta-analysis and systematic review, and weencourage further discussion and development along this direction.",http://arxiv.org/abs/1812.01074v1,,
491,"Steerable Wavelet Scattering for 3D Atomic Systems with Application to  Li-Si Energy Prediction","A general machine learning architecture is introduced that uses waveletscattering coefficients of an inputted three dimensional signal as features.Solid harmonic wavelet scattering transforms of three dimensional signals werepreviously introduced in a machine learning framework for the regression ofproperties of small organic molecules. Here this approach is extended forgeneral steerable wavelets which are equivariant to translations and rotations,resulting in a sparse model of the target function. The scattering coefficientsinherit from the wavelets invariance to translations and rotations. As anillustration of this approach a linear regression model is learned for theformation energy of amorphous lithium-silicon material states trained over adatabase generated using plane-wave Density Functional Theory methods.State-of-the-art results are produced as compared to other machine learningapproaches over similarly generated databases.",http://arxiv.org/abs/1812.02320v2,,
492,"Machine Learning for Anomaly Detection and Categorization in Multi-cloud  Environments","Recently, advances in machine learning techniques have attracted theattention of the research community to build intrusion detection systems (IDS)that can detect anomalies in the network traffic. Most of the research works,however, do not differentiate among different types of attacks. This is, infact, necessary for appropriate countermeasures and defense against attacks. Inthis paper, we investigate both detecting and categorizing anomalies ratherthan just detecting, which is a common trend in the contemporary researchworks. We have used a popular publicly available dataset to build and testlearning models for both detection and categorization of different attacks. Tobe precise, we have used two supervised machine learning techniques, namelylinear regression (LR) and random forest (RF). We show that even if detectionis perfect, categorization can be less accurate due to similarities betweenattacks. Our results demonstrate more than 99% detection accuracy andcategorization accuracy of 93.6%, with the inability to categorize someattacks. Further, we argue that such categorization can be applied tomulti-cloud environments using the same machine learning techniques.",http://arxiv.org/abs/1812.05443v1,,
493,"Stochastic Distributed Optimization for Machine Learning from  Decentralized Features","Distributed machine learning has been widely studied in the literature toscale up machine learning model training in the presence of an ever-increasingamount of data. We study distributed machine learning from another perspective,where the information about the training same samples are inherentlydecentralized and located on different parities. We propose an asynchronousstochastic gradient descent (SGD) algorithm for such a feature distributedmachine learning (FDML) problem, to jointly learn from decentralized features,with theoretical convergence guarantees under bounded asynchrony. Our algorithmdoes not require sharing the original feature data or even local modelparameters between parties, thus preserving a high level of dataconfidentiality. We implement our algorithm for FDML in a parameter serverarchitecture. We compare our system with fully centralized training (whichviolates data locality requirements) and training only based on local features,through extensive experiments performed on a large amount of data from areal-world application, involving 5 million samples and $8700$ features intotal. Experimental results have demonstrated the effectiveness and efficiencyof the proposed FDML system.",http://arxiv.org/abs/1812.06415v1,,
494,"The Adverse Effects of Code Duplication in Machine Learning Models of  Code","The field of big code relies on mining large corpora of code to perform somelearning task. A significant threat to this approach has been recentlyidentified by Lopes et al. (2017) who found a large amount of near-duplicatecode on GitHub. However, the impact of code duplication has not been noticed byresearchers devising machine learning models for source code. In this article,we examine the effect of code duplication on machine learning models showingthat reported metrics are sometimes inflated by up to 100% when testing onduplicated code corpora compared to the performance on de-duplicated corporawhich more accurately represent how machine learning models of code are used bysoftware engineers. We present an ""errata"" for widely used datasets, list bestpractices for collecting code corpora and evaluating machine learning models onthem, and release tools to help the community avoid this problem in futureresearch.",http://arxiv.org/abs/1812.06469v5,,
495,Elements of Sequential Monte Carlo,"A core problem in statistics and probabilistic machine learning is to computeprobability distributions and expectations. This is the fundamental problem ofBayesian statistics and machine learning, which frames all inference asexpectations with respect to the posterior distribution. The key challenge isto approximate these intractable expectations. In this tutorial, we reviewsequential Monte Carlo (SMC), a random-sampling-based class of methods forapproximate inference. First, we explain the basics of SMC, discuss practicalissues, and review theoretical results. We then examine two of the main userdesign choices: the proposal distributions and the so called intermediatetarget distributions. We review recent results on how variational inference andamortization can be used to learn efficient proposals and target distributions.Next, we discuss the SMC estimate of the normalizing constant, how this can beused for pseudo-marginal inference and inference evaluation. Throughout thetutorial we illustrate the use of SMC on various models commonly used inmachine learning, such as stochastic recurrent neural networks, probabilisticgraphical models, and probabilistic programs.",http://arxiv.org/abs/1903.04797v1,,
496,Online Deep Metric Learning,"Metric learning learns a metric function from training data to calculate thesimilarity or distance between samples. From the perspective of featurelearning, metric learning essentially learns a new feature space by featuretransformation (e.g., Mahalanobis distance metric). However, traditional metriclearning algorithms are shallow, which just learn one metric space (featuretransformation). Can we further learn a better metric space from the learntmetric space? In other words, can we learn metric progressively and nonlinearlylike deep learning by just using the existing metric learning algorithms? Tothis end, we present a hierarchical metric learning scheme and implement anonline deep metric learning framework, namely ODML. Specifically, we take oneonline metric learning algorithm as a metric layer, followed by a nonlinearlayer (i.e., ReLU), and then stack these layers modelled after the deeplearning. The proposed ODML enjoys some nice properties, indeed can learnmetric progressively and performs superiorly on some datasets. Variousexperiments with different settings have been conducted to verify theseproperties of the proposed ODML.",http://arxiv.org/abs/1805.05510v1,,
497,Learning Riemannian Metrics,"We propose a solution to the problem of estimating a Riemannian metricassociated with a given differentiable manifold. The metric learning problem isbased on minimizing the relative volume of a given set of points. We derive thedetails for a family of metrics on the multinomial simplex. The resultingmetric has applications in text classification and bears some similarity toTFIDF representation of text documents.",http://arxiv.org/abs/1212.2474v1,,
498,Normalized Online Learning,"We introduce online learning algorithms which are independent of featurescales, proving regret bounds dependent on the ratio of scales existent in thedata rather than the absolute scale. This has several useful effects: there isno need to pre-normalize data, the test-time and test-space complexity arereduced, and the algorithms are more robust.",http://arxiv.org/abs/1408.2065v1,,
499,Stretchy Polynomial Regression,"This article proposes a novel solution for stretchy polynomial regressionlearning. The solution comes in primal and dual closed-forms similar to that ofridge regression. Essentially, the proposed solution stretches the covariancecomputation via a power term thereby compresses or amplifies the estimation.Our experiments on both synthetic data and real-world data show effectivenessof the proposed method for compressive learning.",http://arxiv.org/abs/1408.5449v1,,
500,"The Sample Complexity of Learning Linear Predictors with the Squared  Loss","In this short note, we provide tight sample complexity bounds for learninglinear predictors with respect to the squared loss. Our focus is on an agnosticsetting, where no assumptions are made on the data distribution. This contrastswith standard results in the literature, which either make distributionalassumptions, refer to specific parameter settings, or use other performancemeasures.",http://arxiv.org/abs/1406.5143v2,,
501,Learning Markov Network Structure using Brownian Distance Covariance,"In this paper, we present a simple non-parametric method for learning thestructure of undirected graphs from data that drawn from an underlying unknowndistribution. We propose to use Brownian distance covariance to estimate theconditional independences between the random variables and encodes pairwiseMarkov graph. This framework can be applied in high-dimensional setting, wherethe number of parameters much be larger than the sample size.",http://arxiv.org/abs/1206.6361v1,,
502,The CTU Prague Relational Learning Repository,"The aim of the CTU Prague Relational Learning Repository is to supportmachine learning research with multi-relational data. The repository currentlycontains 50 SQL databases hosted on a public MySQL server located atrelational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., thenumber of tables in the database, the number of rows and columns in the tables,the number of foreign key constraints between tables).",http://arxiv.org/abs/1511.03086v1,,
503,Normalized Online Learning,"We introduce online learning algorithms which are independent of featurescales, proving regret bounds dependent on the ratio of scales existent in thedata rather than the absolute scale. This has several useful effects: there isno need to pre-normalize data, the test-time and test-space complexity arereduced, and the algorithms are more robust.",http://arxiv.org/abs/1305.6646v1,,
504,SelfieBoost: A Boosting Algorithm for Deep Learning,"We describe and analyze a new boosting algorithm for deep learning calledSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which constructensembles of classifiers, SelfieBoost boosts the accuracy of a single network.We prove a $\log(1/\epsilon)$ convergence rate for SelfieBoost under some ""SGDsuccess"" assumption which seems to hold in practice.",http://arxiv.org/abs/1411.3436v2,,
505,Muffled Semi-Supervised Learning,"We explore a novel approach to semi-supervised learning. This approach iscontrary to the common approach in that the unlabeled examples serve to""muffle,"" rather than enhance, the guidance provided by the labeled examples.We provide several variants of the basic algorithm and show experimentally thatthey can achieve significantly higher AUC than boosted trees, random forestsand logistic regression when unlabeled examples are available.",http://arxiv.org/abs/1605.08833v1,,
506,"End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural  Networks","We investigate sequence machine learning techniques on raw radio signaltime-series data. By applying deep recurrent neural networks we learn todiscriminate between several application layer traffic types on top of aconstant envelope modulation without using an expert demodulation algorithm. Weshow that complex protocol sequences can be learned and used for bothclassification and generation tasks using this approach.",http://arxiv.org/abs/1610.00564v1,,
507,Learning Product Automata,"In this paper we give an optimization for active learning algorithms,applicable to learning Moore machines where the output comprises severalobservables. These machines can be decomposed themselves by projecting on eachobservable, resulting in smaller components. These components can then belearnt with fewer queries. This is in particular interesting for learningsoftware, where compositional methods are important for guaranteeingscalability.",http://arxiv.org/abs/1705.02850v1,,
508,Global overview of Imitation Learning,"Imitation Learning is a sequential task where the learner tries to mimic anexpert's action in order to achieve the best performance. Several algorithmshave been proposed recently for this task. In this project, we aim at proposinga wide review of these algorithms, presenting their main features and comparingthem on their performance and their regret bounds.",http://arxiv.org/abs/1801.06503v1,,
509,Information Planning for Text Data,"Information planning enables faster learning with fewer training examples. Itis particularly applicable when training examples are costly to obtain. Thiswork examines the advantages of information planning for text data by focusingon three supervised models: Naive Bayes, supervised LDA and deep neuralnetworks. We show that planning based on entropy and mutual informationoutperforms random selection baseline and therefore accelerates learning.",http://arxiv.org/abs/1802.03360v3,,
510,"CytonRL: an Efficient Reinforcement Learning Open-source Toolkit  Implemented in C++","This paper presents an open-source enforcement learning toolkit named CytonRL(https://github.com/arthurxlw/cytonRL). The toolkit implements four recentadvanced deep Q-learning algorithms from scratch using C++ and NVIDIA'sGPU-accelerated libraries. The code is simple and elegant, owing to anopen-source general-purpose neural network library named CytonLib. Benchmarkshows that the toolkit achieves competitive performances on the popular Atarigame of Breakout.",http://arxiv.org/abs/1804.05834v1,,
511,"Deep Learning of Geometric Constellation Shaping including Fiber  Nonlinearities","A new geometric shaping method is proposed, leveraging unsupervised machinelearning to optimize the constellation design. The learned constellationmitigates nonlinear effects with gains up to 0.13 bit/4D when trained with asimplified fiber channel model.",http://arxiv.org/abs/1805.03785v1,,
512,"Efficient Stochastic Gradient Descent for Distributionally Robust  Learning","We consider a new stochastic gradient descent algorithm for efficientlysolving general min-max optimization problems that arise naturally indistributionally robust learning. By focusing on the entire dataset, currentapproaches do not scale well. We address this issue by initially focusing on asubset of the data and progressively increasing this support to statisticallycover the entire dataset.",http://arxiv.org/abs/1805.08728v1,,
513,"Reproduction Report on ""Learn to Pay Attention""","We have successfully implemented the ""Learn to Pay Attention"" model ofattention mechanism in convolutional neural networks, and have replicated theresults of the original paper in the categories of image classification andfine-grained recognition.",http://arxiv.org/abs/1812.04650v1,,
514,Poincar Wasserstein Autoencoder,"This work presents a reformulation of the recently proposed Wassersteinautoencoder framework on a non-Euclidean manifold, the Poincar\'e ball model ofthe hyperbolic space. By assuming the latent space to be hyperbolic, we can useits intrinsic hierarchy to impose structure on the learned latent spacerepresentations. We demonstrate the model in the visual domain to analyze someof its properties and show competitive results on a graph link prediction task.",http://arxiv.org/abs/1901.01427v1,,
515,Disentangling Video with Independent Prediction,"We propose an unsupervised variational model for disentangling video intoindependent factors, i.e. each factor's future can be predicted from its pastwithout considering the others. We show that our approach often learns factorswhich are interpretable as objects in a scene.",http://arxiv.org/abs/1901.05590v1,,
516,"Learning Linear-Quadratic Regulators Efficiently with only $\sqrt{T}$  Regret","We present the first computationally-efficient algorithm with $\widetildeO(\sqrt{T})$ regret for learning in Linear Quadratic Control systems withunknown dynamics. By that, we resolve an open question of Abbasi-Yadkori andSzepesv\'ari (2011) and Dean, Mania, Matni, Recht, and Tu (2018).",http://arxiv.org/abs/1902.06223v2,,
517,Efficient Private Algorithms for Learning Halfspaces,"We present new differentially private algorithms for learning a large-marginhalfspace. In contrast to previous algorithms, which are based on eitherdifferentially private simulations of the statistical query model or on privateconvex optimization, the sample complexity of our algorithms depends only onthe margin of the data, and not on the dimension.",http://arxiv.org/abs/1902.09009v1,,
518,Horn-ICE Learning for Synthesizing Invariants and Contracts,"We design learning algorithms for synthesizing invariants using Hornimplication counterexamples (Horn-ICE), extending the ICE-learning model. Inparticular, we describe a decision-tree learning algorithm that learns fromHorn-ICE samples, works in polynomial time, and uses statistical heuristics tolearn small trees that satisfy the samples. Since most verification proofs canbe modeled using Horn clauses, Horn-ICE learning is a more robust technique tolearn inductive annotations that prove programs correct. Our experiments showthat an implementation of our algorithm is able to learn adequate inductiveinvariants and contracts efficiently for a variety of sequential and concurrentprograms.",http://arxiv.org/abs/1712.09418v1,,
519,On Learning Finite-State Quantum Sources,"We examine the complexity of learning the distributions produced byfinite-state quantum sources. We show how prior techniques for learning hiddenMarkov models can be adapted to the quantum generator model to find that theanalogous state of affairs holds: information-theoretically, a polynomialnumber of samples suffice to approximately identify the distribution, butcomputationally, the problem is as hard as learning parities with noise, anotorious open question in computational learning theory.",http://arxiv.org/abs/0910.3713v1,,
520,Learning Approximate Stochastic Transition Models,"We examine the problem of learning mappings from state to state, suitable foruse in a model-based reinforcement-learning setting, that simultaneouslygeneralize to novel states and can capture stochastic transitions. We show thatcurrently popular generative adversarial networks struggle to learn thesestochastic transition models but a modification to their loss functions resultsin a powerful learning algorithm for this class of problems.",http://arxiv.org/abs/1710.09718v1,,
521,"Human-Algorithm Interaction Biases in the Big Data Cycle: A Markov Chain  Iterated Learning Framework","Early supervised machine learning algorithms have relied on reliable expertlabels to build predictive models. However, the gates of data generation haverecently been opened to a wider base of users who started participatingincreasingly with casual labeling, rating, annotating, etc. The increasedonline presence and participation of humans has led not only to ademocratization of unchecked inputs to algorithms, but also to a widedemocratization of the ""consumption"" of machine learning algorithms' outputs bygeneral users. Hence, these algorithms, many of which are becoming essentialbuilding blocks of recommender systems and other information filters, startedinteracting with users at unprecedented rates. The result is machine learningalgorithms that consume more and more data that is unchecked, or at the veryleast, not fitting conventional assumptions made by various machine learningalgorithms. These include biased samples, biased labels, diverging training andtesting sets, and cyclical interaction between algorithms, humans, informationconsumed by humans, and data consumed by algorithms. Yet, the continuousinteraction between humans and algorithms is rarely taken into account inmachine learning algorithm design and analysis. In this paper, we present apreliminary theoretical model and analysis of the mutual interaction betweenhumans and algorithms, based on an iterated learning framework that is inspiredfrom the study of human language evolution. We also define the concepts ofhuman and algorithm blind spots and outline machine learning approaches to menditerated bias through two novel notions: antidotes and reactive learning.",http://arxiv.org/abs/1608.07895v1,,
522,Online Learning: A Comprehensive Survey,"Online learning represents an important family of machine learningalgorithms, in which a learner attempts to resolve an online prediction (or anytype of decision-making) task by learning a model/hypothesis from a sequence ofdata instances one at a time. The goal of online learning is to ensure that theonline learner would make a sequence of accurate predictions (or correctdecisions) given the knowledge of correct answers to previous prediction orlearning tasks and possibly additional information. This is in contrast to manytraditional batch learning or offline machine learning algorithms that areoften designed to train a model in batch from a given collection of trainingdata instances. This survey aims to provide a comprehensive survey of theonline machine learning literatures through a systematic review of basic ideasand key principles and a proper categorization of different algorithms andtechniques. Generally speaking, according to the learning type and the forms offeedback information, the existing online learning works can be classified intothree major categories: (i) supervised online learning where full feedbackinformation is always available, (ii) online learning with limited feedback,and (iii) unsupervised online learning where there is no feedback available.Due to space limitation, the survey will be mainly focused on the firstcategory, but also briefly cover some basics of the other two categories.Finally, we also discuss some open issues and attempt to shed light onpotential future research directions in this field.",http://arxiv.org/abs/1802.02871v2,,
523,Tree Edit Distance Learning via Adaptive Symbol Embeddings,"Metric learning has the aim to improve classification accuracy by learning adistance measure which brings data points from the same class closer togetherand pushes data points from different classes further apart. Recent researchhas demonstrated that metric learning approaches can also be applied to trees,such as molecular structures, abstract syntax trees of computer programs, orsyntax trees of natural language, by learning the cost function of an editdistance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.However, learning such costs directly may yield an edit distance which violatesmetric axioms, is challenging to interpret, and may not generalize well. Inthis contribution, we propose a novel metric learning approach for trees whichwe call embedding edit distance learning (BEDL) and which learns an editdistance indirectly by embedding the tree nodes as vectors, such that theEuclidean distance between those vectors supports class discrimination. Welearn such embeddings by reducing the distance to prototypical trees from thesame class and increasing the distance to prototypical trees from differentclasses. In our experiments, we show that BEDL improves upon thestate-of-the-art in metric learning for trees on six benchmark data sets,ranging from computer science over biomedical data to a natural-languageprocessing data set containing over 300,000 nodes.",http://arxiv.org/abs/1806.05009v3,,
524,"Communication Topologies Between Learning Agents in Deep Reinforcement  Learning","A common technique to improve speed and robustness of learning in deepreinforcement learning (DRL) and many other machine learning algorithms is torun multiple learning agents in parallel. A neglected component in thedevelopment of these algorithms has been how best to arrange the learningagents involved to better facilitate distributed search. Here we draw uponresults from the networked optimization and collective intelligence literaturessuggesting that arranging learning agents in less than fully connectedtopologies (the implicit way agents are commonly arranged in) can improvelearning. We explore the relative performance of four popular families ofgraphs and observe that one such family (Erdos-Renyi random graphs) empiricallyoutperforms the standard fully-connected communication topology across severalDRL benchmark tasks. We observe that 1000 learning agents arranged in anErdos-Renyi graph can perform as well as 3000 agents arranged in the standardfully-connected topology, showing the large learning improvement possible whencarefully designing the topology over which agents communicate. We complementthese empirical results with a preliminary theoretical investigation of whyless than fully connected topologies can perform better. Overall, our worksuggests that distributed machine learning algorithms could be made moreefficient if the communication topology between learning agents was optimized.",http://arxiv.org/abs/1902.06740v1,,
525,New Optimisation Methods for Machine Learning,"A thesis submitted for the degree of Doctor of Philosophy of The AustralianNational University.  In this work we introduce several new optimisation methods for problems inmachine learning. Our algorithms broadly fall into two categories: optimisationof finite sums and of graph structured objectives. The finite sum problem issimply the minimisation of objective functions that are naturally expressed asa summation over a large number of terms, where each term has a similar oridentical weight. Such objectives most often appear in machine learning in theempirical risk minimisation framework in the non-online learning setting. Thesecond category, that of graph structured objectives, consists of objectivesthat result from applying maximum likelihood to Markov random field models.Unlike the finite sum case, all the non-linearity is contained within apartition function term, which does not readily decompose into a summation.  For the finite sum problem, we introduce the Finito and SAGA algorithms, aswell as variants of each.  For graph-structured problems, we take three complementary approaches. Welook at learning the parameters for a fixed structure, learning the structureindependently, and learning both simultaneously. Specifically, for the combinedapproach, we introduce a new method for encouraging graph structures with the""scale-free"" property. For the structure learning problem, we establishSHORTCUT, a O(n^{2.5}) expected time approximate structure learning method forGaussian graphical models. For problems where the structure is known but theparameters unknown, we introduce an approximate maximum likelihood learningalgorithm that is capable of learning a useful subclass of Gaussian graphicalmodels.",http://arxiv.org/abs/1510.02533v2,,
526,Metric and Kernel Learning using a Linear Transformation,"Metric and kernel learning are important in several machine learningapplications. However, most existing metric learning algorithms are limited tolearning metrics over low-dimensional data, while existing kernel learningalgorithms are often limited to the transductive setting and do not generalizeto new data points. In this paper, we study metric learning as a problem oflearning a linear transformation of the input data. We show that forhigh-dimensional data, a particular framework for learning a lineartransformation of the data based on the LogDet divergence can be efficientlykernelized to learn a metric (or equivalently, a kernel function) over anarbitrarily high dimensional space. We further demonstrate that a wide class ofconvex loss functions for learning linear transformations can similarly bekernelized, thereby considerably expanding the potential applications of metriclearning. We demonstrate our learning approach by applying it to large-scalereal world problems in computer vision and text mining.",http://arxiv.org/abs/0910.5932v1,,
527,"A Convex Formulation for Learning Task Relationships in Multi-Task  Learning","Multi-task learning is a learning paradigm which seeks to improve thegeneralization performance of a learning task with the help of some otherrelated tasks. In this paper, we propose a regularization formulation forlearning the relationships between tasks in multi-task learning. Thisformulation can be viewed as a novel generalization of the regularizationframework for single-task learning. Besides modeling positive task correlation,our method, called multi-task relationship learning (MTRL), can also describenegative task correlation and identify outlier tasks based on the sameunderlying principle. Under this regularization framework, the objectivefunction of MTRL is convex. For efficiency, we use an alternating method tolearn the optimal model parameters for each task as well as the relationshipsbetween tasks. We study MTRL in the symmetric multi-task learning setting andthen generalize it to the asymmetric setting as well. We also study therelationships between MTRL and some existing multi-task learning methods.Experiments conducted on a toy problem as well as several benchmark data setsdemonstrate the effectiveness of MTRL.",http://arxiv.org/abs/1203.3536v1,,
528,Deep Feature Learning for Graphs,"This paper presents a general graph representation learning framework calledDeepGL for learning deep node and edge representations from large (attributed)graphs. In particular, DeepGL begins by deriving a set of base features (e.g.,graphlet features) and automatically learns a multi-layered hierarchical graphrepresentation where each successive layer leverages the output from theprevious layer to learn features of a higher-order. Contrary to previous work,DeepGL learns relational functions (each representing a feature) thatgeneralize across-networks and therefore useful for graph-based transferlearning tasks. Moreover, DeepGL naturally supports attributed graphs, learnsinterpretable features, and is space-efficient (by learning sparse featurevectors). In addition, DeepGL is expressive, flexible with many interchangeablecomponents, efficient with a time complexity of $\mathcal{O}(|E|)$, andscalable for large networks via an efficient parallel implementation. Comparedwith the state-of-the-art method, DeepGL is (1) effective for across-networktransfer learning tasks and attributed graph representation learning, (2)space-efficient requiring up to 6x less memory, (3) fast with up to 182xspeedup in runtime performance, and (4) accurate with an average improvement of20% or more on many learning tasks.",http://arxiv.org/abs/1704.08829v2,,
529,Collaborative Deep Reinforcement Learning,"Besides independent learning, human learning process is highly improved bysummarizing what has been learned, communicating it with peers, andsubsequently fusing knowledge from different sources to assist the currentlearning goal. This collaborative learning procedure ensures that the knowledgeis shared, continuously refined, and concluded from different perspectives toconstruct a more profound understanding. The idea of knowledge transfer has ledto many advances in machine learning and data mining, but significantchallenges remain, especially when it comes to reinforcement learning,heterogeneous model structures, and different learning tasks. Motivated byhuman collaborative learning, in this paper we propose a collaborative deepreinforcement learning (CDRL) framework that performs adaptive knowledgetransfer among heterogeneous learning agents. Specifically, the proposed CDRLconducts a novel deep knowledge distillation method to address theheterogeneity among different learning tasks with a deep alignment network.Furthermore, we present an efficient collaborative Asynchronous AdvantageActor-Critic (cA3C) algorithm to incorporate deep knowledge distillation intothe online training of agents, and demonstrate the effectiveness of the CDRLframework using extensive empirical evaluation on OpenAI gym.",http://arxiv.org/abs/1702.05796v1,,
530,A Survey on Multi-Task Learning,"Multi-Task Learning (MTL) is a learning paradigm in machine learning and itsaim is to leverage useful information contained in multiple related tasks tohelp improve the generalization performance of all the tasks. In this paper, wegive a survey for MTL. First, we classify different MTL algorithms into severalcategories, including feature learning approach, low-rank approach, taskclustering approach, task relation learning approach, and decompositionapproach, and then discuss the characteristics of each approach. In order toimprove the performance of learning tasks further, MTL can be combined withother learning paradigms including semi-supervised learning, active learning,unsupervised learning, reinforcement learning, multi-view learning andgraphical models. When the number of tasks is large or the data dimensionalityis high, batch MTL models are difficult to handle this situation and online,parallel and distributed MTL models as well as dimensionality reduction andfeature hashing are reviewed to reveal their computational and storageadvantages. Many real-world applications use MTL to boost their performance andwe review representative works. Finally, we present theoretical analyses anddiscuss several future directions for MTL.",http://arxiv.org/abs/1707.08114v2,,
531,State Representation Learning for Control: An Overview,"Representation learning algorithms are designed to learn abstract featuresthat characterize data. State representation learning (SRL) focuses on aparticular kind of representation learning where learned features are in lowdimension, evolve through time, and are influenced by actions of an agent. Therepresentation is learned to capture the variation in the environment generatedby the agent's actions; this kind of representation is particularly suitablefor robotics and control scenarios. In particular, the low dimensioncharacteristic of the representation helps to overcome the curse ofdimensionality, provides easier interpretation and utilization by humans andcan help improve performance and speed in policy learning algorithms such asreinforcement learning.  This survey aims at covering the state-of-the-art on state representationlearning in the most recent years. It reviews different SRL methods thatinvolve interaction with the environment, their implementations and theirapplications in robotics control tasks (simulated or real). In particular, ithighlights how generic learning objectives are differently exploited in thereviewed algorithms. Finally, it discusses evaluation methods to assess therepresentation learned and summarizes current and future lines of research.",http://arxiv.org/abs/1802.04181v2,,
532,Sequence Transduction with Recurrent Neural Networks,"Many machine learning tasks can be expressed as the transformation---or\emph{transduction}---of input sequences into output sequences: speechrecognition, machine translation, protein secondary structure prediction andtext-to-speech to name but a few. One of the key challenges in sequencetransduction is learning to represent both the input and output sequences in away that is invariant to sequential distortions such as shrinking, stretchingand translating. Recurrent neural networks (RNNs) are a powerful sequencelearning architecture that has proven capable of learning such representations.However RNNs traditionally require a pre-defined alignment between the inputand output sequences to perform transduction. This is a severe limitation since\emph{finding} the alignment is the most difficult aspect of many sequencetransduction problems. Indeed, even determining the length of the outputsequence is often challenging. This paper introduces an end-to-end,probabilistic sequence transduction system, based entirely on RNNs, that is inprinciple able to transform any input sequence into any finite, discrete outputsequence. Experimental results for phoneme recognition are provided on theTIMIT speech corpus.",http://arxiv.org/abs/1211.3711v1,,
533,"Comparative Performance Analysis of Neural Networks Architectures on H2O  Platform for Various Activation Functions","Deep learning (deep structured learning, hierarchi- cal learning or deepmachine learning) is a branch of machine learning based on a set of algorithmsthat attempt to model high- level abstractions in data by using multipleprocessing layers with complex structures or otherwise composed of multiplenon-linear transformations. In this paper, we present the results of testingneural networks architectures on H2O platform for various activation functions,stopping metrics, and other parameters of machine learning algorithm. It wasdemonstrated for the use case of MNIST database of handwritten digits insingle-threaded mode that blind selection of these parameters can hugelyincrease (by 2-3 orders) the runtime without the significant increase ofprecision. This result can have crucial influence for opitmization of availableand new machine learning methods, especially for image recognition problems.",http://arxiv.org/abs/1707.04940v1,,
534,Deep Learning with Apache SystemML,"Enterprises operate large data lakes using Hadoop and Spark frameworks that(1) run a plethora of tools to automate powerful datapreparation/transformation pipelines, (2) run on shared, large clusters to (3)perform many different analytics tasks ranging from model preparation,building, evaluation, and tuning for both machine learning and deep learning.Developing machine/deep learning models on data in such shared environments ischallenging. Apache SystemML provides a unified framework for implementingmachine learning and deep learning algorithms in a variety of shared deploymentscenarios. SystemML's novel compilation approach automatically generatesruntime execution plans for machine/deep learning algorithms that are composedof single-node and distributed runtime operations depending on data and clustercharacteristics such as data size, data sparsity, cluster size, and memoryconfigurations, while still exploiting the capabilities of the underlying bigdata frameworks.",http://arxiv.org/abs/1802.04647v1,,
535,Learning Memory Access Patterns,"The explosion in workload complexity and the recent slow-down in Moore's lawscaling call for new approaches towards efficient computing. Researchers arenow beginning to use recent advances in machine learning in softwareoptimizations, augmenting or replacing traditional heuristics and datastructures. However, the space of machine learning for computer hardwarearchitecture is only lightly explored. In this paper, we demonstrate thepotential of deep learning to address the von Neumann bottleneck of memoryperformance. We focus on the critical problem of learning memory accesspatterns, with the goal of constructing accurate and efficient memoryprefetchers. We relate contemporary prefetching strategies to n-gram models innatural language processing, and show how recurrent neural networks can serveas a drop-in replacement. On a suite of challenging benchmark datasets, we findthat neural networks consistently demonstrate superior performance in terms ofprecision and recall. This work represents the first step towards practicalneural-network based prefetching, and opens a wide range of exciting directionsfor machine learning in computer architecture research.",http://arxiv.org/abs/1803.02329v1,,
536,"Experience, Imitation and Reflection; Confucius' Conjecture and Machine  Learning","Artificial intelligence recently had a great advancements caused by theemergence of new processing power and machine learning methods. Having saidthat, the learning capability of artificial intelligence is still at itsinfancy comparing to the learning capability of human and many animals. Many ofthe current artificial intelligence applications can only operate in a veryorchestrated, specific environments with an extensive training set that exactlydescribes the conditions that will occur during execution time. Having that inmind, and considering the several existing machine learning methods thisquestion rises that 'What are some of the best ways for a machine to learn?'Regarding the learning methods of human, Confucius' point of view is that theyare by experience, imitation and reflection. This paper tries to explore anddiscuss regarding these three ways of learning and their implementations inmachines by having a look at how they happen in minds.",http://arxiv.org/abs/1808.00222v1,,
537,OBOE: Collaborative Filtering for AutoML Initialization,"Algorithm selection and hyperparameter tuning remain two of the mostchallenging tasks in machine learning. The number of machine learningapplications is growing much faster than the number of machine learningexperts, hence we see an increasing demand for efficient automation of learningprocesses. Here, we introduce OBOE, an algorithm for time-constrained modelselection and hyperparameter tuning. Taking advantage of similarity betweendatasets, OBOE finds promising algorithm and hyperparameter configurationsthrough collaborative filtering. Our system explores these models under timeconstraints, so that rapid initializations can be provided to warm-start morefine-grained optimization methods. One novel aspect of our approach is a newheuristic for active learning in time-constrained matrix completion based onoptimal experiment design. Our experiments demonstrate that OBOE deliversstate-of-the-art performance faster than competing approaches on a test bed ofsupervised learning problems.",http://arxiv.org/abs/1808.03233v1,,
538,"On exponential convergence of SGD in non-convex over-parametrized  learning","Large over-parametrized models learned via stochastic gradient descent (SGD)methods have become a key element in modern machine learning. Although SGDmethods are very effective in practice, most theoretical analyses of SGDsuggest slower convergence than what is empirically observed. In our recentwork [8] we analyzed how interpolation, common in modern over-parametrizedlearning, results in exponential convergence of SGD with constant step size forconvex loss functions. In this note, we extend those results to a much broadernon-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. Anumber of important non-convex problems in machine learning, including someclasses of neural networks, have been recently shown to satisfy the PLcondition. We argue that the PL condition provides a relevant and attractivesetting for many machine learning problems, particularly in theover-parametrized regime.",http://arxiv.org/abs/1811.02564v1,,
539,"A Generalized Meta-loss function for regression and classification using  privileged information","Learning using privileged information (LUPI) is a powerful heterogenousfeature space machine learning framework that allows a machine learning modelto learn from highly informative or privileged features which are availableduring training only to generate test predictions using input space featureswhich are available both during training and testing. LUPI can significantlyimprove prediction performance in a variety of machine learning problems.However, existing large margin and neural network implementations of learningusing privileged information are mostly designed for classification tasks. Inthis work, we have proposed a simple yet effective formulation that allows usto perform regression using privileged information through a custom lossfunction. Apart from regression, our formulation allows general application ofLUPI to classification and other related problems as well. We have verified thecorrectness, applicability and effectiveness of our method on regression andclassification problems over different synthetic and real-world problems. Totest the usefulness of the proposed model in real-world problems, we haveevaluated our method on the problem of protein binding affinity prediction. Theproposed LUPI regression-based model has shown to outperform the currentstate-of-the-art predictor.",http://arxiv.org/abs/1811.06885v2,,
540,"Accelerated Learning in the Presence of Time Varying Features with  Applications to Machine Learning and Adaptive Control","Features in machine learning problems are often time varying and may berelated to outputs in an algebraic or dynamical manner. The dynamic nature ofthese machine learning problems renders current accelerated gradient descentmethods unstable or weakens their convergence guarantees. This paper proposesalgorithms for the case when time varying features are present, anddemonstrates provable performance guarantees. We develop a variationalperspective within a continuous time algorithm. This variational perspectiveincludes, among other things, higher-order learning concepts and normalization,both of which stem from adaptive control, and allows stability to beestablished for dynamical machine learning problems. These higher-orderalgorithms are also examined for achieving accelerated learning in adaptivecontrol. Simulations are provided to verify the theoretical results.",http://arxiv.org/abs/1903.04666v1,,
541,"Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using  Meta-Learning","Learning is an inherently continuous phenomenon. When humans learn a new taskthere is no explicit distinction between training and inference. As we learn atask, we keep learning about it while performing the task. What we learn andhow we learn it varies during different stages of learning. Learning how tolearn and adapt is a key property that enables us to generalize effortlessly tonew settings. This is in contrast with conventional settings in machinelearning where a trained model is frozen during inference. In this paper westudy the problem of learning to learn at both training and test time in thecontext of visual navigation. A fundamental challenge in navigation isgeneralization to unseen scenes. In this paper we propose a self-adaptivevisual navigation method (SAVN) which learns to adapt to new environmentswithout any explicit supervision. Our solution is a meta-reinforcement learningapproach where an agent learns a self-supervised interaction loss thatencourages effective navigation. Our experiments, performed in the AI2-THORframework, show major improvements in both success rate and SPL for visualnavigation in novel scenes. Our code and data are available at:https://github.com/allenai/savn .",http://arxiv.org/abs/1812.00971v2,,
542,Deep Super Learner: A Deep Ensemble for Classification Problems,"Deep learning has become very popular for tasks such as predictive modelingand pattern recognition in handling big data. Deep learning is a powerfulmachine learning method that extracts lower level features and feeds themforward for the next layer to identify higher level features that improveperformance. However, deep neural networks have drawbacks, which include manyhyper-parameters and infinite architectures, opaqueness into results, andrelatively slower convergence on smaller datasets. While traditional machinelearning algorithms can address these drawbacks, they are not typically capableof the performance levels achieved by deep neural networks. To improveperformance, ensemble methods are used to combine multiple base learners. Superlearning is an ensemble that finds the optimal combination of diverse learningalgorithms. This paper proposes deep super learning as an approach whichachieves log loss and accuracy results competitive to deep neural networkswhile employing traditional machine learning algorithms in a hierarchicalstructure. The deep super learner is flexible, adaptable, and easy to trainwith good performance across different tasks using identical hyper-parametervalues. Using traditional machine learning requires fewer hyper-parameters,allows transparency into results, and has relatively fast convergence onsmaller datasets. Experimental results show that the deep super learner hassuperior performance compared to the individual base learners, single-layerensembles, and in some cases deep neural networks. Performance of the deepsuper learner may further be improved with task-specific tuning.",http://arxiv.org/abs/1803.02323v1,,
543,Classifying Cue Phrases in Text and Speech Using Machine Learning,"Cue phrases may be used in a discourse sense to explicitly signal discoursestructure, but also in a sentential sense to convey semantic rather thanstructural information. This paper explores the use of machine learning forclassifying cue phrases as discourse or sentential. Two machine learningprograms (Cgrendel and C4.5) are used to induce classification rules from setsof pre-classified cue phrases and their features. Machine learning is shown tobe an effective technique for not only automating the generation ofclassification rules, but also for improving upon previous results.",http://arxiv.org/abs/cmp-lg/9405014v1,,
544,Journal of New Democratic Methods: An Introduction,"This paper describes a new breed of academic journals that use statisticalmachine learning techniques to make them more democratic. In particular, notonly can anyone submit an article, but anyone can also become a reviewer.Machine learning is used to decide which reviewers accurately represent theviews of the journal's readers and thus deserve to have their opinions carrymore weight. The paper concentrates on describing a specific experimentalprototype of a democratic journal called the Journal of New Democratic Methods(JNDM). The paper also mentions the wider implications that machine learningand the techniques used in the JNDM may have for representative democracy ingeneral.",http://arxiv.org/abs/cs/0408048v1,,
545,Restart Strategy Selection using Machine Learning Techniques,"Restart strategies are an important factor in the performance ofconflict-driven Davis Putnam style SAT solvers. Selecting a good restartstrategy for a problem instance can enhance the performance of a solver.Inspired by recent success applying machine learning techniques to predict theruntime of SAT solvers, we present a method which uses machine learning toboost solver performance through a smart selection of the restart strategy.Based on easy to compute features, we train both a satisfiability classifierand runtime models. We use these models to choose between restart strategies.We present experimental results comparing this technique with the most commonlyused restart strategies. Our results demonstrate that machine learning iseffective in improving solver performance.",http://arxiv.org/abs/0907.5032v1,,
546,MLPACK: A Scalable C++ Machine Learning Library,"MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learninglibrary released in late 2011 offering both a simple, consistent API accessibleto novice users and high performance and flexibility to expert users byleveraging modern features of C++. MLPACK provides cutting-edge algorithmswhose benchmarks exhibit far better performance than other leading machinelearning libraries. MLPACK version 1.0.3, licensed under the LGPL, is availableat http://www.mlpack.org.",http://arxiv.org/abs/1210.6293v1,,
547,A complexity analysis of statistical learning algorithms,"We apply information-based complexity analysis to support vector machine(SVM) algorithms, with the goal of a comprehensive continuous algorithmicanalysis of such algorithms. This involves complexity measures in which somehigher order operations (e.g., certain optimizations) are considered primitivefor the purposes of measuring complexity. We consider classes of informationoperators and algorithms made up of scaled families, and investigate theutility of scaling the complexities to minimize error. We look at the divisionof statistical learning into information and algorithmic components, at thecomplexities of each, and at applications to support vector machine (SVM) andmore general machine learning algorithms. We give applications to SVMalgorithms graded into linear and higher order components, and give an examplein biomedical informatics.",http://arxiv.org/abs/1212.4562v1,,
548,"Classifying Single-Trial EEG during Motor Imagery with a Small Training  Set","Before the operation of a motor imagery based brain-computer interface (BCI)adopting machine learning techniques, a cumbersome training procedure isunavoidable. The development of a practical BCI posed the challenge ofclassifying single-trial EEG with a small training set. In this letter, weaddressed this problem by employing a series of signal processing and machinelearning approaches to alleviate overfitting and obtained test accuracy similarto training accuracy on the datasets from BCI Competition III and our ownexperiments.",http://arxiv.org/abs/1306.3474v1,,
549,Quantum algorithms for supervised and unsupervised machine learning,"Machine-learning tasks frequently involve problems of manipulating andclassifying large numbers of vectors in high-dimensional spaces. Classicalalgorithms for solving such problems typically take time polynomial in thenumber of vectors and the dimension of the space. Quantum computers are good atmanipulating high-dimensional vectors in large tensor product spaces. Thispaper provides supervised and unsupervised quantum machine learning algorithmsfor cluster assignment and cluster finding. Quantum machine learning can taketime logarithmic in both the number of vectors and their dimension, anexponential speed-up over classical algorithms.",http://arxiv.org/abs/1307.0411v2,,
550,"Multi-Scale Local Shape Analysis and Feature Selection in Machine  Learning Applications","We introduce a method called multi-scale local shape analysis, or MLSA, forextracting features that describe the local structure of points within adataset. The method uses both geometric and topological features at multiplelevels of granularity to capture diverse types of local information forsubsequent machine learning algorithms operating on the dataset. Usingsynthetic and real dataset examples, we demonstrate significant performanceimprovement of classification algorithms constructed for these datasets withcorrespondingly augmented features.",http://arxiv.org/abs/1410.3169v1,,
551,"Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms  for Machine Learning","Frank-Wolfe algorithms for convex minimization have recently gainedconsiderable attention from the Optimization and Machine Learning communities,as their properties make them a suitable choice in a variety of applications.However, as each iteration requires to optimize a linear model, a cleverimplementation is crucial to make such algorithms viable on large-scaledatasets. For this purpose, approximation strategies based on a random samplinghave been proposed by several researchers. In this work, we perform anexperimental study on the effectiveness of these techniques, analyze possiblealternatives and provide some guidelines based on our results.",http://arxiv.org/abs/1410.4062v1,,
552,Using machine learning for medium frequency derivative portfolio trading,"We use machine learning for designing a medium frequency trading strategy fora portfolio of 5 year and 10 year US Treasury note futures. We formulate thisas a classification problem where we predict the weekly direction of movementof the portfolio using features extracted from a deep belief network trained ontechnical indicators of the portfolio constituents. The experimentation showsthat the resulting pipeline is effective in making a profitable trade.",http://arxiv.org/abs/1512.06228v1,,
553,Estimation of matrix trace using machine learning,"We present a new trace estimator of the matrix whose explicit form is notgiven but its matrix multiplication to a vector is available. The form of theestimator is similar to the Hutchison stochastic trace estimator, but insteadof the random noise vectors in Hutchison estimator, we use small number ofprobing vectors determined by machine learning. Evaluation of the quality ofestimates and bias correction are discussed. An unbiased estimator is proposedfor the calculation of the expectation value of a function of traces. In thenumerical experiments with random matrices, it is shown that the precision oftrace estimates with $\mathcal{O}(10)$ probing vectors determined by themachine learning is similar to that with $\mathcal{O}(10000)$ random noisevectors.",http://arxiv.org/abs/1606.05560v1,,
554,Machine Learning and the Future of Realism,"The preceding three decades have seen the emergence, rise, and proliferationof machine learning (ML). From half-recognised beginnings in perceptrons,neural nets, and decision trees, algorithms that extract correlations (that is,patterns) from a set of data points have broken free from their origin incomputational cognition to embrace all forms of problem solving, from voicerecognition to medical diagnosis to automated scientific research anddriverless cars, and it is now widely opined that the real industrialrevolution lies less in mobile phone and similar than in the maturation anduniversal application of ML. Among the consequences just might be the triumphof anti-realism over realism.",http://arxiv.org/abs/1704.04688v1,,
555,Optimization with First-Order Surrogate Functions,"In this paper, we study optimization methods consisting of iterativelyminimizing surrogates of an objective function. By proposing severalalgorithmic variants and simple convergence analyses, we make two maincontributions. First, we provide a unified viewpoint for several first-orderoptimization techniques such as accelerated proximal gradient, block coordinatedescent, or Frank-Wolfe algorithms. Second, we introduce a new incrementalscheme that experimentally matches or outperforms state-of-the-art solvers forlarge-scale optimization problems typically arising in machine learning.",http://arxiv.org/abs/1305.3120v1,,
556,"Feature Learning with Gaussian Restricted Boltzmann Machine for Robust  Speech Recognition","In this paper, we first present a new variant of Gaussian restrictedBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmannmachine (MGRBM), with its definition and learning algorithm. Then we proposeusing a learned GRBM or MGRBM to extract better features for robust speechrecognition. Our experiments on Aurora2 show that both GRBM-extracted andMGRBM-extracted feature performs much better than Mel-frequency cepstralcoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)acoustic model, and MGRBM-extracted feature is slightly better.",http://arxiv.org/abs/1309.6176v1,,
557,How to show a probabilistic model is better,"We present a simple theoretical framework, and corresponding practicalprocedures, for comparing probabilistic models on real data in a traditionalmachine learning setting. This framework is based on the theory of properscoring rules, but requires only basic algebra and probability theory tounderstand and verify. The theoretical concepts presented are well-studied,primarily in the statistics literature. The goal of this paper is to advocatetheir wider adoption for performance evaluation in empirical machine learning.",http://arxiv.org/abs/1502.03491v1,,
558,Communication Complexity of Distributed Convex Learning and Optimization,"We study the fundamental limits to communication-efficient distributedmethods for convex learning and optimization, under different assumptions onthe information available to individual machines, and the types of functionsconsidered. We identify cases where existing algorithms are already worst-caseoptimal, as well as cases where room for further improvement is still possible.Among other things, our results indicate that without similarity between thelocal objective functions (due to statistical data similarity or otherwise)many communication rounds may be required, even if the machines have unboundedcomputational power.",http://arxiv.org/abs/1506.01900v2,,
559,Using the Mean Absolute Percentage Error for Regression Models,"We study in this paper the consequences of using the Mean Absolute PercentageError (MAPE) as a measure of quality for regression models. We show thatfinding the best model under the MAPE is equivalent to doing weighted MeanAbsolute Error (MAE) regression. We show that universal consistency ofEmpirical Risk Minimization remains possible using the MAPE instead of the MAE.",http://arxiv.org/abs/1506.04176v1,,
560,"Search Strategies for Binary Feature Selection for a Naive Bayes  Classifier","We compare in this paper several feature selection methods for the NaiveBayes Classifier (NBC) when the data under study are described by a largenumber of redundant binary indicators. Wrapper approaches guided by the NBCestimation of the classification error probability out-perform filterapproaches while retaining a reasonable computational cost.",http://arxiv.org/abs/1506.04177v1,,
561,"Parameter Database : Data-centric Synchronization for Scalable Machine  Learning","We propose a new data-centric synchronization framework for carrying out ofmachine learning (ML) tasks in a distributed environment. Our frameworkexploits the iterative nature of ML algorithms and relaxes the applicationagnostic bulk synchronization parallel (BSP) paradigm that has previously beenused for distributed machine learning. Data-centric synchronization complementsfunction-centric synchronization based on using stale updates to increase thethroughput of distributed ML computations. Experiments to validate ourframework suggest that we can attain substantial improvement over BSP whileguaranteeing sequential correctness of ML tasks.",http://arxiv.org/abs/1508.00703v1,,
562,Concurrent Constraint Machine Improvisation: Models and Implementation,"Machine improvisation creates music either by explicit coding of rules or byapplying machine learning methods. We deal with the latter case. Animprovisation system capable of real-time must execute two processconcurrently: one to apply machine learning methods to musical sequences inorder to capture prominent musical features, and one to produce musicalsequences stylistically consistent with the learned material. As an example,the Concurrent Constraint Factor Oracle Model for Music Improvisation (ccfomi),based upon Non-deterministic Timed Concurrent Constraint (ntcc) calculus, usesthe Factor Oracle to store the learned sequences.",http://arxiv.org/abs/1510.02840v1,,
563,Directional Statistics in Machine Learning: a Brief Review,"The modern data analyst must cope with data encoded in various forms,vectors, matrices, strings, graphs, or more. Consequently, statistical andmachine learning models tailored to different data encodings are important. Wefocus on data encoded as normalized vectors, so that their ""direction"" is moreimportant than their magnitude. Specifically, we consider high-dimensionalvectors that lie either on the surface of the unit hypersphere or on the realprojective plane. For such data, we briefly review common mathematical modelsprevalent in machine learning, while also outlining some technical aspects,software, applications, and open mathematical challenges.",http://arxiv.org/abs/1605.00316v1,,
564,Efficient Distributed Learning with Sparsity,"We propose a novel, efficient approach for distributed sparse learning inhigh-dimensions, where observations are randomly partitioned across machines.Computationally, at each round our method only requires the master machine tosolve a shifted ell_1 regularized M-estimation problem, and other workers tocompute the gradient. In respect of communication, the proposed approachprovably matches the estimation error bound of centralized methods withinconstant rounds of communications (ignoring logarithmic factors). We conductextensive experiments on both simulated and real world datasets, anddemonstrate encouraging performances on high-dimensional regression andclassification tasks.",http://arxiv.org/abs/1605.07991v1,,
565,High Dimensional Human Guided Machine Learning,"Have you ever looked at a machine learning classification model and thought,I could have made that? Well, that is what we test in this project, comparingXGBoost trained on human engineered features to training directly on data. Thehuman engineered features do not outperform XGBoost trained di- rectly on thedata, but they are comparable. This project con- tributes a novel method forutilizing human created classifi- cation models on high dimensional datasets.",http://arxiv.org/abs/1609.00904v1,,
566,Interpretation of Prediction Models Using the Input Gradient,"State of the art machine learning algorithms are highly optimized to providethe optimal prediction possible, naturally resulting in complex models. Whilethese models often outperform simpler more interpretable models by order ofmagnitudes, in terms of understanding the way the model functions, we are oftenfacing a ""black box"".  In this paper we suggest a simple method to interpret the behavior of anypredictive model, both for regression and classification. Given a particularmodel, the information required to interpret it can be obtained by studying thepartial derivatives of the model with respect to the input. We exemplify thisinsight by interpreting convolutional and multi-layer neural networks in thefield of natural language processing.",http://arxiv.org/abs/1611.07634v1,,
567,Should I use TensorFlow,"Google's Machine Learning framework TensorFlow was open-sourced in November2015 [1] and has since built a growing community around it. TensorFlow issupposed to be flexible for research purposes while also allowing its models tobe deployed productively. This work is aimed towards people with experience inMachine Learning considering whether they should use TensorFlow in theirenvironment. Several aspects of the framework important for such a decision areexamined, such as the heterogenity, extensibility and its computation graph. Apure Python implementation of linear classification is compared with animplementation utilizing TensorFlow. I also contrast TensorFlow to otherpopular frameworks with respect to modeling capability, deployment andperformance and give a brief description of the current adaption of theframework.",http://arxiv.org/abs/1611.08903v1,,
568,"Machine Learning of Linear Differential Equations using Gaussian  Processes","This work leverages recent advances in probabilistic machine learning todiscover conservation laws expressed by parametric linear equations. Suchequations involve, but are not limited to, ordinary and partial differential,integro-differential, and fractional order operators. Here, Gaussian processpriors are modified according to the particular form of such operators and areemployed to infer parameters of the linear equations from scarce and possiblynoisy observations. Such observations may come from experiments or ""black-box""computer simulations.",http://arxiv.org/abs/1701.02440v1,,
569,"Towards Better Analysis of Machine Learning Models: A Visual Analytics  Perspective","Interactive model analysis, the process of understanding, diagnosing, andrefining a machine learning model with the help of interactive visualization,is very important for users to efficiently solve real-world artificialintelligence and data mining problems. Dramatic advances in big data analyticshas led to a wide variety of interactive model analysis tasks. In this paper,we present a comprehensive analysis and interpretation of this rapidlydeveloping area. Specifically, we classify the relevant work into threecategories: understanding, diagnosis, and refinement. Each category isexemplified by recent influential work. Possible future research opportunitiesare also explored and discussed.",http://arxiv.org/abs/1702.01226v1,,
570,"Neural Networks for Beginners. A fast implementation in Matlab, Torch,  TensorFlow","This report provides an introduction to some Machine Learning tools withinthe most common development environments. It mainly focuses on practicalproblems, skipping any theoretical introduction. It is oriented to bothstudents trying to approach Machine Learning and experts looking for newframeworks.",http://arxiv.org/abs/1703.05298v2,,
571,Practical Coreset Constructions for Machine Learning,"We investigate coresets - succinct, small summaries of large data sets - sothat solutions found on the summary are provably competitive with solutionfound on the full data set. We provide an overview over the state-of-the-art incoreset construction for machine learning. In Section 2, we present both theintuition behind and a theoretically sound framework to construct coresets forgeneral problems and apply it to $k$-means clustering. In Section 3 wesummarize existing coreset construction algorithms for a variety of machinelearning problems such as maximum likelihood estimation of mixture models,Bayesian non-parametric models, principal component analysis, regression andgeneral empirical risk minimization.",http://arxiv.org/abs/1703.06476v2,,
572,Persistence Diagrams with Linear Machine Learning Models,"Persistence diagrams have been widely recognized as a compact descriptor forcharacterizing multiscale topological features in data. When many datasets areavailable, statistical features embedded in those persistence diagrams can beextracted by applying machine learnings. In particular, the ability forexplicitly analyzing the inverse in the original data space from thosestatistical features of persistence diagrams is significantly important forpractical applications. In this paper, we propose a unified method for theinverse analysis by combining linear machine learning models with persistenceimages. The method is applied to point clouds and cubical sets, showing theability of the statistical inverse analysis and its advantages.",http://arxiv.org/abs/1706.10082v2,,
573,Machine learning application in the life time of materials,"Materials design and development typically takes several decades from theinitial discovery to commercialization with the traditional trial and errordevelopment approach. With the accumulation of data from both experimental andcomputational results, data based machine learning becomes an emerging field inmaterials discovery, design and property prediction. This manuscript reviewsthe history of materials science as a disciplinary the most common machinelearning method used in materials science, and specifically how they are usedin materials discovery, design, synthesis and even failure detection andanalysis after materials are deployed in real application. Finally, thelimitations of machine learning for application in materials science andchallenges in this emerging field is discussed.",http://arxiv.org/abs/1707.04826v1,,
574,Machine Learning Based Fast Power Integrity Classifier,"In this paper, we proposed a new machine learning based fast power integrityclassifier that quickly flags the EM/IR hotspots. We discussed the features toextract to describe the power grid, cell power density, routing impact andcontrolled collapse chip connection (C4) bumps, etc. The continuous anddiscontinuous cases are identified and treated using different machine learningmodels. Nearest neighbors, random forest and neural network models are comparedto select the best performance candidates. Experiments are run on open sourcebenchmark, and result is showing promising prediction accuracy.",http://arxiv.org/abs/1711.03406v1,,
575,"""I know it when I see it"". Visualization and Intuitive Interpretability","Most research on the interpretability of machine learning systems focuses onthe development of a more rigorous notion of interpretability. I suggest that abetter understanding of the deficiencies of the intuitive notion ofinterpretability is needed as well. I show that visualization enables but alsoimpedes intuitive interpretability, as it presupposes two levels of technicalpre-interpretation: dimensionality reduction and regularization. Furthermore, Iargue that the use of positive concepts to emulate the distributed semanticstructure of machine learning models introduces a significant human bias intothe model. As a consequence, I suggest that, if intuitive interpretability isneeded, singular representations of internal model states should be avoided.",http://arxiv.org/abs/1711.08042v2,,
576,"BioMM: Biologically-informed Multi-stage Machine learning for  identification of epigenetic fingerprints","The identification of reproducible biological patterns from high-dimensionaldata is a bottleneck for understanding the biology of complex illnesses such asschizophrenia. To address this, we developed a biologically informed,multi-stage machine learning (BioMM) framework. BioMM incorporates biologicalpathway information to stratify and aggregate high-dimensional biological data.We demonstrate the utility of this method using genome-wide DNA methylationdata and show that it substantially outperforms conventional machine learningapproaches. Therefore, the BioMM framework may be a fruitful machine learningstrategy in high-dimensional data and be the basis for future, integrativeanalysis approaches.",http://arxiv.org/abs/1712.00336v1,,
577,Persistence Codebooks for Topological Data Analysis,"Topological data analysis, such as persistent homology has shown beneficialproperties for machine learning in many tasks. Topological representations,such as the persistence diagram (PD), however, have a complex structure(multiset of intervals) which makes it difficult to combine with typicalmachine learning workflows. We present novel compact fixed-size vectorialrepresentations of PDs based on clustering and bag of words encodings that copewell with the inherent sparsity of PDs. Our novel representations outperformstate-of-the-art approaches from topological data analysis and arecomputationally more efficient.",http://arxiv.org/abs/1802.04852v2,,
578,"Reconsider HHL algorithm and its related quantum machine learning  algorithms","HHL quantum algorithm to solve linear systems is one of the most importantsubroutines in many quantum machine learning algorithms. In this work, wepresent and analyze several other caveats in HHL algorithm, which have beenignored in the past. Their influences on the efficiency, accuracy andpracticability of HHL algorithm and several related quantum machine learningalgorithms will be discussed. We also found that these caveats affect HHLalgorithm much deeper than the already noticed caveats. In order to obtain morepractical quantum machine learning algorithms with less assumptions based onHHL algorithm, we should pay more attention to these caveats.",http://arxiv.org/abs/1803.01486v1,,
579,"Machine Learning Harnesses Molecular Dynamics to Discover New $_$  Opioid Chemotypes","Computational chemists typically assay drug candidates by virtually screeningcompounds against crystal structures of a protein despite the fact that sometargets, like the $\mu$ Opioid Receptor and other members of the GPCR family,traverse many non-crystallographic states. We discover new conformationalstates of $\mu OR$ with molecular dynamics simulation and then machine learnligand-structure relationships to predict opioid ligand function. Theseartificial intelligence models identified a novel $\mu$ opioid chemotype.",http://arxiv.org/abs/1803.04479v1,,
580,Markov Chain Importance Sampling - a highly efficient estimator for MCMC,"Markov chain algorithms are ubiquitous in machine learning and statistics andmany other disciplines. In this work we present a novel estimator applicable toseveral classes of Markov chains, dubbed Markov chain importance sampling(MCIS). For a broad class of Metropolis-Hastings algorithms, MCIS efficientlymakes use of rejected proposals. For discretized Langevin diffusions, itprovides a novel way of correcting the discretization error. Our estimatorsatisfies a central limit theorem and improves on error per CPU cycle, often toa large extent. As a by-product it enables estimating the normalizing constant,an important quantity in Bayesian machine learning and statistics.",http://arxiv.org/abs/1805.07179v2,,
581,"Can machine learning identify interesting mathematics? An exploration  using empirically observed laws","We explore the possibility of using machine learning to identify interestingmathematical structures by using certain quantities that serve as fingerprints.In particular, we extract features from integer sequences using two empiricallaws: Benford's law and Taylor's law and experiment with various classifiers toidentify whether a sequence is, for example, nice, important, multiplicative,easy to compute or related to primes or palindromes.",http://arxiv.org/abs/1805.07431v3,,
582,Baselines and a datasheet for the Cerema AWP dataset,"This paper presents the recently published Cerema AWP (Adverse WeatherPedestrian) dataset for various machine learning tasks and its exports inmachine learning friendly format. We explain why this dataset can beinteresting (mainly because it is a greatly controlled and fully annotatedimage dataset) and present baseline results for various tasks. Moreover, wedecided to follow the very recent suggestions of datasheets for dataset, tryingto standardize all the available information of the dataset, with atransparency objective.",http://arxiv.org/abs/1806.04016v1,,
583,Generative Neural Machine Translation,"We introduce Generative Neural Machine Translation (GNMT), a latent variablearchitecture which is designed to model the semantics of the source and targetsentences. We modify an encoder-decoder translation model by adding a latentvariable as a language agnostic representation which is encouraged to learn themeaning of the sentence. GNMT achieves competitive BLEU scores on puretranslation tasks, and is superior when there are missing words in the sourcesentence. We augment the model to facilitate multilingual translation andsemi-supervised learning without adding parameters. This frameworksignificantly reduces overfitting when there is limited paired data available,and is effective for translating between pairs of languages not seen duringtraining.",http://arxiv.org/abs/1806.05138v1,,
584,"N-Gram Graph, A Novel Molecule Representation","Virtual high-throughput screening provides a strategy for prioritizingcompounds for physical screens. Machine learning methods offer an ancillarybenefit to make molecule predictions, yet the choice of representation has beenchallenging when selecting algorithms. We emphasize the effects of differentlevels of molecule representation. Then, we introduce N-gram graph, a novelrepresentation for a molecular graph. We demonstrate that N-gram graph is ableto attain most accurate prediction with several non-deep machine learningmethods on multiple tasks.",http://arxiv.org/abs/1806.09206v1,,
585,Optimal Bounds on the VC-dimension,"The VC-dimension of a set system is a way to capture its complexity and hasbeen a key parameter studied extensively in machine learning and geometrycommunities. In this paper, we resolve two longstanding open problems onbounding the VC-dimension of two fundamental set systems: $k$-foldunions/intersections of half-spaces, and the simplices set system. Among otherimplications, it settles an open question in machine learning that was firststudied in the 1989 foundational paper of Blumer, Ehrenfeucht, Haussler andWarmuth as well as by Eisenstat and Angluin and Johnson.",http://arxiv.org/abs/1807.07924v1,,
586,Mobile big data analysis with machine learning,"This paper investigates to identify the requirement and the development ofmachine learning-based mobile big data analysis through discussing the insightsof challenges in the mobile big data (MBD). Furthermore, it reviews thestate-of-the-art applications of data analysis in the area of MBD. Firstly, weintroduce the development of MBD. Secondly, the frequently adopted methods ofdata analysis are reviewed. Three typical applications of MBD analysis, namelywireless channel modeling, human online and offline behavior analysis, andspeech recognition in the internet of vehicles, are introduced respectively.Finally, we summarize the main challenges and future development directions ofmobile big data analysis.",http://arxiv.org/abs/1808.00803v1,,
587,Benchmarking Automatic Machine Learning Frameworks,"AutoML serves as the bridge between varying levels of expertise whendesigning machine learning systems and expedites the data science process. Awide range of techniques is taken to address this, however there does not existan objective comparison of these techniques. We present a benchmark of currentopen source AutoML solutions using open source datasets. We test auto-sklearn,TPOT, auto_ml, and H2O's AutoML solution against a compiled set of regressionand classification datasets sourced from OpenML and find that auto-sklearnperforms the best across classification datasets and TPOT performs the bestacross regression datasets.",http://arxiv.org/abs/1808.06492v1,,
588,Machine learning non-local correlations,"The ability to witness non-local correlations lies at the core offoundational aspects of quantum mechanics and its application in the processingof information. Commonly, this is achieved via the violation of Bellinequalities. Unfortunately, however, their systematic derivation quicklybecomes unfeasible as the scenario of interest grows in complexity. To copewith that, we propose here a machine learning approach for the detection andquantification of non-locality. It consists of an ensemble of multilayerperceptrons blended with genetic algorithms achieving a high performance in anumber of relevant Bell scenarios. Our results offer a novel method and aproof-of-principle for the relevance of machine learning for understandingnon-locality.",http://arxiv.org/abs/1808.07069v1,,
589,Co-Creative Level Design via Machine Learning,"Procedural Level Generation via Machine Learning (PLGML), the study ofgenerating game levels with machine learning, has received a large amount ofrecent academic attention. For certain measures these approaches have shownsuccess at replicating the quality of existing game levels. However, it isunclear the extent to which they might benefit human designers. In this paperwe present a framework for co-creative level design with a PLGML agent. Insupport of this framework we present results from a user study and results froma comparative study of PLGML approaches.",http://arxiv.org/abs/1809.09420v1,,
590,A Kernel for Multi-Parameter Persistent Homology,"Topological data analysis and its main method, persistent homology, provide atoolkit for computing topological information of high-dimensional and noisydata sets. Kernels for one-parameter persistent homology have been establishedto connect persistent homology with machine learning techniques. We contributea kernel construction for multi-parameter persistence by integrating aone-parameter kernel weighted along straight lines. We prove that our kernel isstable and efficiently computable, which establishes a theoretical connectionbetween topological data analysis and machine learning for multivariate dataanalysis.",http://arxiv.org/abs/1809.10231v1,,
591,On the Art and Science of Machine Learning Explanations,"This text discusses several explanatory methods that go beyond the errormeasurements and plots traditionally used to assess machine learning models.Some of the methods are tools of the trade while others are rigorously derivedand backed by long-standing theory. The methods, decision tree surrogatemodels, individual conditional expectation (ICE) plots, local interpretablemodel-agnostic explanations (LIME), partial dependence plots, and Shapleyexplanations, vary in terms of scope, fidelity, and suitable applicationdomain. Along with descriptions of these methods, this text presents real-worldusage recommendations supported by a use case and in-depth software examples.",http://arxiv.org/abs/1810.02909v1,,
592,Private Machine Learning in TensorFlow using Secure Computation,"We present a framework for experimenting with secure multi-party computationdirectly in TensorFlow. By doing so we benefit from several properties valuableto both researchers and practitioners, including tight integration withordinary machine learning processes, existing optimizations for distributedcomputation in TensorFlow, high-level abstractions for expressing complexalgorithms and protocols, and an expanded set of familiar tooling. We give anopen source implementation of a state-of-the-art protocol and report onconcrete benchmarks using typical models from private machine learning.",http://arxiv.org/abs/1810.08130v2,,
593,The Frontiers of Fairness in Machine Learning,"The last few years have seen an explosion of academic and popular interest inalgorithmic fairness. Despite this interest and the volume and velocity of workthat has been produced recently, the fundamental science of fairness in machinelearning is still in a nascent state. In March 2018, we convened a group ofexperts as part of a CCC visioning workshop to assess the state of the field,and distill the most promising research directions going forward. This reportsummarizes the findings of that workshop. Along the way, it surveys recenttheoretical work in the field and points towards promising directions forresearch.",http://arxiv.org/abs/1810.08810v1,,
594,"Short-Term Wind-Speed Forecasting Using Kernel Spectral Hidden Markov  Models","In machine learning, a nonparametric forecasting algorithm for time seriesdata has been proposed, called the kernel spectral hidden Markov model (KSHMM).In this paper, we propose a technique for short-term wind-speed predictionbased on KSHMM. We numerically compared the performance of our KSHMM-basedforecasting technique to other techniques with machine learning, usingwind-speed data offered by the National Renewable Energy Laboratory. Ourresults demonstrate that, compared to these methods, the proposed techniqueoffers comparable or better performance.",http://arxiv.org/abs/1811.06210v1,,
595,The Theory and Algorithm of Ergodic Inference,"Approximate inference algorithm is one of the fundamental research fields inmachine learning. The two dominant theoretical inference frameworks in machinelearning are variational inference (VI) and Markov chain Monte Carlo (MCMC).However, because of the fundamental limitation in the theory, it is verychallenging to improve existing VI and MCMC methods on both the computationalscalability and statistical efficiency. To overcome this obstacle, we propose anew theoretical inference framework called ergodic Inference based on thefundamental property of ergodic transformations. The key contribution of thiswork is to establish the theoretical foundation of ergodic inference for thedevelopment of practical algorithms in future work.",http://arxiv.org/abs/1811.07192v1,,
596,Please Stop Explaining Black Box Models for High Stakes Decisions,"Black box machine learning models are currently being used for high stakesdecision-making throughout society, causing problems throughout healthcare,criminal justice, and in other domains. People have hoped that creating methodsfor explaining these black box models will alleviate some of these problems,but trying to explain black box models, rather than creating models that areinterpretable in the first place, is likely to perpetuate bad practices and canpotentially cause catastrophic harm to society. There is a way forward - it isto design models that are inherently interpretable.",http://arxiv.org/abs/1811.10154v2,,
597,"A Deep Latent-Variable Model Application to Select Treatment Intensity  in Survival Analysis","In the following short article we adapt a new and popular machine learningmodel for inference on medical data sets. Our method is based on theVariational AutoEncoder (VAE) framework that we adapt to survival analysis onsmall data sets with missing values. In our model, the true health statusappears as a set of latent variables that affects the observed covariates andthe survival chances. We show that this flexible model allows insightfuldecision-making using a predicted distribution and outperforms a classicsurvival analysis model.",http://arxiv.org/abs/1811.12323v1,,
598,Expanding search in the space of empirical ML,"As researchers and practitioners of applied machine learning, we are given aset of requirements on the problem to be solved, the plausibly obtainable data,and the computational resources available. We aim to find (within those bounds)reliably useful combinations of problem, data, and algorithm. An emphasis onalgorithmic or technical novelty in ML conference publications leads toexploration of one dimension of this space. Data collection and ML deploymentat scale in industry settings offers an environment for exploring the others.Our conferences and reviewing criteria can better support empirical ML bysoliciting and incentivizing experimentation and synthesis independent ofalgorithmic innovation.",http://arxiv.org/abs/1812.01495v1,,
599,Persistence Bag-of-Words for Topological Data Analysis,"Persistent homology (PH) is a rigorous mathematical theory that provides arobust descriptor of data in the form of persistence diagrams (PDs). PDsexhibit, however, complex structure and are difficult to integrate in today'smachine learning workflows. This paper introduces persistence bag-of-words: anovel and stable vectorized representation of PDs that enables the seamlessintegration with machine learning. Comprehensive experiments show that the newrepresentation achieves state-of-the-art performance and beyond in much lesstime than alternative approaches.",http://arxiv.org/abs/1812.09245v2,,
600,Generic adaptation strategies for automated machine learning,"Automation of machine learning model development is increasingly becoming anestablished research area. While automated model selection and automated datapre-processing have been studied in depth, there is, however, a gap concerningautomated model adaptation strategies when multiple strategies are available.Manually developing an adaptation strategy, including estimation of relevantparameters can be time consuming and costly. In this paper we address thisissue by proposing generic adaptation strategies based on approaches fromearlier works. Experimental results after using the proposed strategies withthree adaptive algorithms on 36 datasets confirm their viability. Thesestrategies often achieve better or comparable performance with customadaptation strategies and naive methods such as repeatedly using only oneadaptive mechanism.",http://arxiv.org/abs/1812.10793v1,,
601,Machine-learning a virus assembly fitness landscape,"Realistic evolutionary fitness landscapes are notoriously difficult toconstruct. A recent cutting-edge model of virus assembly consists of adodecahedral capsid with $12$ corresponding packaging signals in three affinitybands. This whole genome/phenotype space consisting of $3^{12}$ genomes hasbeen explored via computationally expensive stochastic assembly models, givinga fitness landscape in terms of the assembly efficiency. Using latestmachine-learning techniques by establishing a neural network, we show that theintensive computation can be short-circuited in a matter of minutes toastounding accuracy.",http://arxiv.org/abs/1901.05051v1,,
602,"CodedPrivateML: A Fast and Privacy-Preserving Framework for Distributed  Machine Learning","How to train a machine learning model while keeping the data private andsecure? We present CodedPrivateML, a fast and scalable approach to thiscritical problem. CodedPrivateML keeps both the data and the modelinformation-theoretically private, while allowing efficient parallelization oftraining across distributed workers. We characterize CodedPrivateML's privacythreshold and prove its convergence for logistic (and linear) regression.Furthermore, via experiments over Amazon EC2, we demonstrate thatCodedPrivateML can provide an order of magnitude speedup (up to $\sim34\times$) over the state-of-the-art cryptographic approaches.",http://arxiv.org/abs/1902.00641v1,,
603,A tractable ellipsoidal approximation for voltage regulation problems,"We present a machine learning approach to the solution of chance constrainedoptimizations in the context of voltage regulation problems in power systemoperation. The novelty of our approach resides in approximating the feasibleregion of uncertainty with an ellipsoid. We formulate this problem using alearning model similar to Support Vector Machines (SVM) and propose a samplingalgorithm that efficiently trains the model. We demonstrate our approach on avoltage regulation problem using standard IEEE distribution test feeders.",http://arxiv.org/abs/1903.03763v1,,
604,"Inferring Personalized Bayesian Embeddings for Learning from  Heterogeneous Demonstration","For assistive robots and virtual agents to achieve ubiquity, machines willneed to anticipate the needs of their human counterparts. The field of Learningfrom Demonstration (LfD) has sought to enable machines to infer predictivemodels of human behavior for autonomous robot control. However, humans exhibitheterogeneity in decision-making, which traditional LfD approaches fail tocapture. To overcome this challenge, we propose a Bayesian LfD framework toinfer an integrated representation of all human task demonstrators by inferringhuman-specific embeddings, thereby distilling their unique characteristics. Wevalidate our approach is able to outperform state-of-the-art techniques on bothsynthetic and real-world data sets.",http://arxiv.org/abs/1903.06047v1,,
605,Time Series Imputation,"Multivariate time series is a very active topic in the research community andmany machine learning tasks are being used in order to extract information fromthis type of data. However, in real-world problems data has missing values,which may difficult the application of machine learning techniques to extractinformation. In this paper we focus on the task of imputation of time series.Many imputation methods for time series are based on regression methods.Unfortunately, these methods perform poorly when the variables are categorical.To address this case, we propose a new imputation method based on ExpectationMaximization over dynamic Bayesian networks. The approach is assessed withsynthetic and real data, and it outperforms several state-of-the art methods.",http://arxiv.org/abs/1903.09732v1,,
606,"Automated Monitoring Cropland Using Remote Sensing Data: Challenges and  Opportunities for Machine Learning","This paper provides an overview of how recent advances in machine learningand the availability of data from earth observing satellites can dramaticallyimprove our ability to automatically map croplands over long period and overlarge regions. It discusses three applications in the domain of crop monitoringwhere ML approaches are beginning to show great promise. For each application,it highlights machine learning challenges, proposed approaches, and recentresults. The paper concludes with discussion of major challenges that need tobe addressed before ML approaches will reach their full potential for thisproblem of great societal relevance.",http://arxiv.org/abs/1904.04329v1,,
607,Parallel Online Learning,"In this work we study parallelization of online learning, a core primitive inmachine learning. In a parallel environment all known approaches for parallelonline learning lead to delayed updates, where the model is updated usingout-of-date information. In the worst case, or when examples are temporallycorrelated, delay can have a very adverse effect on the learning algorithm.Here, we analyze and present preliminary empirical results on a set of learningarchitectures based on a feature sharding approach that present varioustradeoffs between delay, degree of parallelism, representation power andempirical performance.",http://arxiv.org/abs/1103.4204v1,,
608,Stochastic Low-Rank Kernel Learning for Regression,"We present a novel approach to learn a kernel-based regression function. Itis based on the useof conical combinations of data-based parameterized kernelsand on a new stochastic convex optimization procedure of which we establishconvergence guarantees. The overall learning procedure has the nice propertiesthat a) the learned conical combination is automatically designed to performthe regression task at hand and b) the updates implicated by the optimizationprocedure are quite inexpensive. In order to shed light on the appositeness ofour learning strategy, we present empirical results from experiments conductedon various benchmark datasets.",http://arxiv.org/abs/1201.2416v1,,
609,Para-active learning,"Training examples are not all equally informative. Active learning strategiesleverage this observation in order to massively reduce the number of examplesthat need to be labeled. We leverage the same observation to build a genericstrategy for parallelizing learning algorithms. This strategy is effectivebecause the search for informative examples is highly parallelizable andbecause we show that its performance does not deteriorate when the siftingprocess relies on a slightly outdated model. Parallel active learning isparticularly attractive to train nonlinear models with non-linearrepresentations because there are few practical parallel learning algorithmsfor such models. We report preliminary experiments using both kernel SVMs andSGD-trained neural networks.",http://arxiv.org/abs/1310.8243v1,,
610,A la Carte - Learning Fast Kernels,"Kernel methods have great promise for learning rich statisticalrepresentations of large modern datasets. However, compared to neural networks,kernel methods have been perceived as lacking in scalability and flexibility.We introduce a family of fast, flexible, lightly parametrized and generalpurpose kernel learning methods, derived from Fastfood basis functionexpansions. We provide mechanisms to learn the properties of groups of spectralfrequencies in these expansions, which require only O(mlogd) time and O(m)memory, for m basis functions and d input dimensions. We show that the proposedmethods can learn a wide class of kernels, outperforming the alternatives inaccuracy, speed, and memory consumption.",http://arxiv.org/abs/1412.6493v1,,
611,A Theory of Feature Learning,"Feature Learning aims to extract relevant information contained in data setsin an automated fashion. It is driving force behind the current deep learningtrend, a set of methods that have had widespread empirical success. What islacking is a theoretical understanding of different feature learning schemes.This work provides a theoretical framework for feature learning and thencharacterizes when features can be learnt in an unsupervised fashion. We alsoprovide means to judge the quality of features via rate-distortion theory andits generalizations.",http://arxiv.org/abs/1504.00083v1,,
612,On multi-view feature learning,"Sparse coding is a common approach to learning local features for objectrecognition. Recently, there has been an increasing interest in learningfeatures from spatio-temporal, binocular, or other multi-observation data,where the goal is to encode the relationship between images rather than thecontent of a single image. We provide an analysis of multi-view featurelearning, which shows that hidden variables encode transformations by detectingrotation angles in the eigenspaces shared among multiple image warps. Ouranalysis helps explain recent experimental results showing thattransformation-specific features emerge when training complex cell models onvideos. Our analysis also shows that transformation-invariant features canemerge as a by-product of learning representations of transformations.",http://arxiv.org/abs/1206.4609v1,,
613,"Computational Intractability of Dictionary Learning for Sparse  Representation","In this paper we consider the dictionary learning problem for sparserepresentation. We first show that this problem is NP-hard by polynomial timereduction of the densest cut problem. Then, using successive convexapproximation strategies, we propose efficient dictionary learning schemes tosolve several practical formulations of this problem to stationary points.Unlike many existing algorithms in the literature, such as K-SVD, our proposeddictionary learning scheme is theoretically guaranteed to converge to the setof stationary points under certain mild assumptions. For the image denoisingapplication, the performance and the efficiency of the proposed dictionarylearning scheme are comparable to that of K-SVD algorithm in simulation.",http://arxiv.org/abs/1511.01776v1,,
614,Localized Complexities for Transductive Learning,"We show two novel concentration inequalities for suprema of empiricalprocesses when sampling without replacement, which both take the variance ofthe functions into account. While these inequalities may potentially have broadapplications in learning theory in general, we exemplify their significance bystudying the transductive setting of learning theory. For which we provide thefirst excess risk bounds based on the localized complexity of the hypothesisclass, which can yield fast rates of convergence also in the transductivelearning setting. We give a preliminary analysis of the localized complexitiesfor the prominent case of kernel classes.",http://arxiv.org/abs/1411.7200v1,,
615,Onsager-corrected deep learning for sparse linear inverse problems,"Deep learning has gained great popularity due to its widespread success onmany inference problems. We consider the application of deep learning to thesparse linear inverse problem encountered in compressive sensing, where oneseeks to recover a sparse signal from a small number of noisy linearmeasurements. In this paper, we propose a novel neural-network architecturethat decouples prediction errors across layers in the same way that theapproximate message passing (AMP) algorithm decouples them across iterations:through Onsager correction. Numerical experiments suggest that our ""learnedAMP"" network significantly improves upon Gregor and LeCun's ""learned ISTA""network in both accuracy and complexity.",http://arxiv.org/abs/1607.05966v1,,
616,Learning a Driving Simulator,"Comma.ai's approach to Artificial Intelligence for self-driving cars is basedon an agent that learns to clone driver behaviors and plans maneuvers bysimulating future events in the road. This paper illustrates one of ourresearch approaches for driving simulation. One where we learn to simulate.Here we investigate variational autoencoders with classical and learned costfunctions using generative adversarial networks for embedding road frames.Afterwards, we learn a transition model in the embedded space using actionconditioned Recurrent Neural Networks. We show that our approach can keeppredicting realistic looking video for several frames despite the transitionmodel being optimized without a cost function in the pixel space.",http://arxiv.org/abs/1608.01230v1,,
617,A Threshold-based Scheme for Reinforcement Learning in Neural Networks,"A generic and scalable Reinforcement Learning scheme for Artificial NeuralNetworks is presented, providing a general purpose learning machine. Byreference to a node threshold three features are described 1) A mechanism forPrimary Reinforcement, capable of solving linearly inseparable problems 2) Thelearning scheme is extended to include a mechanism for ConditionedReinforcement, capable of forming long term strategy 3) The learning scheme ismodified to use a threshold-based deep learning algorithm, providing a robustand biologically inspired alternative to backpropagation. The model may be usedfor supervised as well as unsupervised training regimes.",http://arxiv.org/abs/1609.03348v4,,
618,"Learning to Play in a Day: Faster Deep Reinforcement Learning by  Optimality Tightening","We propose a novel training algorithm for reinforcement learning whichcombines the strength of deep Q-learning with a constrained optimizationapproach to tighten optimality and encourage faster reward propagation. Ournovel technique makes deep reinforcement learning more practical by drasticallyreducing the training time. We evaluate the performance of our approach on the49 games of the challenging Arcade Learning Environment, and report significantimprovements in both training time and accuracy.",http://arxiv.org/abs/1611.01606v1,,
619,"Learning Sparse, Distributed Representations using the Hebbian Principle","The ""fire together, wire together"" Hebbian model is a central principle forlearning in neuroscience, but surprisingly, it has found limited applicabilityin modern machine learning. In this paper, we take a first step towardsbridging this gap, by developing flavors of competitive Hebbian learning whichproduce sparse, distributed neural codes using online adaptation with minimaltuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning(AHL). We illustrate the distributed nature of the learned representations viaoutput entropy computations for synthetic data, and demonstrate superiorperformance, compared to standard alternatives such as autoencoders, intraining a deep convolutional net on standard image datasets.",http://arxiv.org/abs/1611.04228v1,,
620,A Survey of Quantum Learning Theory,"This paper surveys quantum learning theory: the theoretical aspects ofmachine learning using quantum computers. We describe the main results knownfor three models of learning: exact learning from membership queries, andProbably Approximately Correct (PAC) and agnostic learning from classical orquantum examples.",http://arxiv.org/abs/1701.06806v3,,
621,Learning Hierarchical Features from Generative Models,"Deep neural networks have been shown to be very successful at learningfeature hierarchies in supervised learning tasks. Generative models, on theother hand, have benefited less from hierarchical models with multiple layersof latent variables. In this paper, we prove that hierarchical latent variablemodels do not take advantage of the hierarchical structure when trained withexisting variational methods, and provide some limitations on the kind offeatures existing models can learn. Finally we propose an alternativearchitecture that do not suffer from these limitations. Our model is able tolearn highly interpretable and disentangled hierarchical features on severalnatural image datasets with no task specific regularization or prior knowledge.",http://arxiv.org/abs/1702.08396v2,,
622,Model-Based Multiple Instance Learning,"While Multiple Instance (MI) data are point patterns -- sets or multi-sets ofunordered points -- appropriate statistical point pattern models have not beenused in MI learning. This article proposes a framework for model-based MIlearning using point process theory. Likelihood functions for point patterndata derived from point process theory enable principled yet conceptuallytransparent extensions of learning tasks, such as classification, noveltydetection and clustering, to point pattern data. Furthermore, tractable pointpattern models as well as solutions for learning and decision making from pointpattern data are developed.",http://arxiv.org/abs/1703.02155v2,,
623,Learning Probabilistic Programs Using Backpropagation,"Probabilistic modeling enables combining domain knowledge with learning fromdata, thereby supporting learning from fewer training instances than purelydata-driven methods. However, learning probabilistic models is difficult andhas not achieved the level of performance of methods such as deep neuralnetworks on many tasks. In this paper, we attempt to address this issue bypresenting a method for learning the parameters of a probabilistic programusing backpropagation. Our approach opens the possibility to building deepprobabilistic programming models that are trained in a similar way to neuralnetworks.",http://arxiv.org/abs/1705.05396v1,,
624,"Advantages and Limitations of using Successor Features for Transfer in  Reinforcement Learning","One question central to Reinforcement Learning is how to learn a featurerepresentation that supports algorithm scaling and re-use of learnedinformation from different tasks. Successor Features approach this problem bylearning a feature representation that satisfies a temporal constraint. Wepresent an implementation of an approach that decouples the featurerepresentation from the reward function, making it suitable for transferringknowledge between domains. We then assess the advantages and limitations ofusing Successor Features for transfer.",http://arxiv.org/abs/1708.00102v1,,
625,Few-Shot Learning with Graph Neural Networks,"We propose to study the problem of few-shot learning with the prism ofinference on a partially observed graphical model, constructed from acollection of input images whose label can be either observed or not. Byassimilating generic message-passing inference algorithms with theirneural-network counterparts, we define a graph neural network architecture thatgeneralizes several of the recently proposed few-shot learning models. Besidesproviding improved numerical performance, our framework is easily extended tovariants of few-shot learning, such as semi-supervised or active learning,demonstrating the ability of graph-based models to operate well on 'relational'tasks.",http://arxiv.org/abs/1711.04043v3,,
626,"Block Neural Network Avoids Catastrophic Forgetting When Learning  Multiple Task","In the present work we propose a Deep Feed Forward network architecture whichcan be trained according to a sequential learning paradigm, where tasks ofincreasing difficulty are learned sequentially, yet avoiding catastrophicforgetting. The proposed architecture can re-use the features learned onprevious tasks in a new task when the old tasks and the new one are related.The architecture needs fewer computational resources (neurons and connections)and less data for learning the new task than a network trained from scratch",http://arxiv.org/abs/1711.10204v1,,
627,Deep Prior,"The recent literature on deep learning offers new tools to learn a richprobability distribution over high dimensional data such as images or sounds.In this work we investigate the possibility of learning the prior distributionover neural network parameters using such tools. Our resulting variationalBayes algorithm generalizes well to new tasks, even when very few trainingexamples are provided. Furthermore, this learned prior allows the model toextrapolate correctly far from a given task's training data on a meta-datasetof periodic signals.",http://arxiv.org/abs/1712.05016v2,,
628,Deep Learning: A Critical Appraisal,"Although deep learning has historical roots going back decades, neither theterm ""deep learning"" nor the approach was popular just over five years ago,when the field was reignited by papers such as Krizhevsky, Sutskever andHinton's now classic (2012) deep network model of Imagenet. What has the fielddiscovered in the five subsequent years? Against a background of considerableprogress in areas such as speech recognition, image recognition, and gameplaying, and considerable enthusiasm in the popular press, I present tenconcerns for deep learning, and suggest that deep learning must be supplementedby other techniques if we are to reach artificial general intelligence.",http://arxiv.org/abs/1801.00631v1,,
629,Learning Sparse Wavelet Representations,"In this work we propose a method for learning wavelet filters directly fromdata. We accomplish this by framing the discrete wavelet transform as amodified convolutional neural network. We introduce an autoencoder wavelettransform network that is trained using gradient descent. We show that themodel is capable of learning structured wavelet filters from synthetic and realdata. The learned wavelets are shown to be similar to traditional wavelets thatare derived using Fourier methods. Our method is simple to implement and easilyincorporated into neural network architectures. A major advantage to our modelis that we can learn from raw audio data.",http://arxiv.org/abs/1802.02961v1,,
630,Black-Box Reductions for Parameter-free Online Learning in Banach Spaces,"We introduce several new black-box reductions that significantly improve thedesign of adaptive and parameter-free online learning algorithms by simplifyinganalysis, improving regret guarantees, and sometimes even improving runtime. Wereduce parameter-free online learning to online exp-concave optimization, wereduce optimization in a Banach space to one-dimensional optimization, and wereduce optimization over a constrained domain to unconstrained optimization.All of our reductions run as fast as online gradient descent. We use our newtechniques to improve upon the previously best regret bounds for parameter-freelearning, and do so for arbitrary norms.",http://arxiv.org/abs/1802.06293v2,,
631,"Demystifying Deep Learning: A Geometric Approach to Iterative  Projections","Parametric approaches to Learning, such as deep learning (DL), are highlypopular in nonlinear regression, in spite of their extremely difficult trainingwith their increasing complexity (e.g. number of layers in DL). In this paper,we present an alternative semi-parametric framework which foregoes theordinarily required feedback, by introducing the novel idea of geometricregularization. We show that certain deep learning techniques such as residualnetwork (ResNet) architecture are closely related to our approach. Hence, ourtechnique can be used to analyze these types of deep learning. Moreover, wepresent preliminary results which confirm that our approach can be easilytrained to obtain complex structures.",http://arxiv.org/abs/1803.08416v1,,
632,Decoupling Dynamics and Reward for Transfer Learning,"Current reinforcement learning (RL) methods can successfully learn singletasks but often generalize poorly to modest perturbations in task domain ortraining procedure. In this work, we present a decoupled learning strategy forRL that creates a shared representation space where knowledge can be robustlytransferred. We separate learning the task representation, the forwarddynamics, the inverse dynamics and the reward function of the domain, and showthat this decoupling improves performance within the task, transfers well tochanges in dynamics and reward, and can be effectively used for onlineplanning. Empirical results show good performance in both continuous anddiscrete RL domains.",http://arxiv.org/abs/1804.10689v2,,
633,SaaS: Speed as a Supervisor for Semi-supervised Learning,"We introduce the SaaS Algorithm for semi-supervised learning, which useslearning speed during stochastic gradient descent in a deep neural network tomeasure the quality of an iterative estimate of the posterior probability ofunknown labels. Training speed in supervised learning correlates strongly withthe percentage of correct labels, so we use it as an inference criterion forthe unknown labels, without attempting to infer the model parameters at first.Despite its simplicity, SaaS achieves state-of-the-art results insemi-supervised learning benchmarks.",http://arxiv.org/abs/1805.00980v1,,
634,Bayesian active learning for choice models with deep Gaussian processes,"In this paper, we propose an active learning algorithm and models which cangradually learn individual's preference through pairwise comparisons. Theactive learning scheme aims at finding individual's most preferred choice withminimized number of pairwise comparisons. The pairwise comparisons are encodedinto probabilistic models based on assumptions of choice models and deepGaussian processes. The next-to-compare decision is determined by a novelacquisition function. We benchmark the proposed algorithm and models usingfunctions with multiple local optima and one public airline itinerary dataset.The experiments indicate the effectiveness of our active learning algorithm andmodels.",http://arxiv.org/abs/1805.01867v1,,
635,"Positive and Unlabeled Learning through Negative Selection and  Imbalance-aware Classification","Motivated by applications in protein function prediction, we consider achallenging supervised classification setting in which positive labels arescarce and there are no explicit negative labels. The learning algorithm mustthus select which unlabeled examples to use as negative training points,possibly ending up with an unbalanced learning problem. We address these issuesby proposing an algorithm that combines active learning (for selecting negativeexamples) with imbalance-aware learning (for mitigating the label imbalance).In our experiments we observe that these two techniques operatesynergistically, outperforming state-of-the-art methods on standard proteinfunction prediction benchmarks.",http://arxiv.org/abs/1805.07331v2,,
636,"Improved Learning of One-hidden-layer Convolutional Neural Networks with  Overlaps","We propose a new algorithm to learn a one-hidden-layer convolutional neuralnetwork where both the convolutional weights and the outputs weights areparameters to be learned. Our algorithm works for a general class of(potentially overlapping) patches, including commonly used structures forcomputer vision tasks. Our algorithm draws ideas from (1) isotonic regressionfor learning neural networks and (2) landscape analysis of non-convex matrixfactorization problems. We believe these findings may inspire furtherdevelopment in designing provable algorithms for learning neural networks andother complex models.",http://arxiv.org/abs/1805.07798v2,,
637,Scalable Coordinated Exploration in Concurrent Reinforcement Learning,"We consider a team of reinforcement learning agents that concurrently operatein a common environment, and we develop an approach to efficient coordinatedexploration that is suitable for problems of practical scale. Our approachbuilds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized valuefunction learning (Osband et al., 2016). We demonstrate that, for simpletabular contexts, the approach is competitive with previously proposed tabularmodel learning methods (Dimakopoulou and Van Roy, 2018). With ahigher-dimensional problem and a neural network value function representation,the approach learns quickly with far fewer agents than alternative explorationschemes.",http://arxiv.org/abs/1805.08948v2,,
638,Metalearning with Hebbian Fast Weights,"We unify recent neural approaches to one-shot learning with older ideas ofassociative memory in a model for metalearning. Our model learns jointly torepresent data and to bind class labels to representations in a single shot. Itbuilds representations via slow weights, learned across tasks through SGD,while fast weights constructed by a Hebbian learning rule implement one-shotbinding for each new task. On the Omniglot, Mini-ImageNet, and Penn Treebankone-shot learning benchmarks, our model achieves state-of-the-art results.",http://arxiv.org/abs/1807.05076v1,,
639,Multi-Agent Generative Adversarial Imitation Learning,"Imitation learning algorithms can be used to learn a policy from expertdemonstrations without access to a reward signal. However, most existingapproaches are not applicable in multi-agent settings due to the existence ofmultiple (Nash) equilibria and non-stationary environments. We propose a newframework for multi-agent imitation learning for general Markov games, where webuild upon a generalized notion of inverse reinforcement learning. We furtherintroduce a practical multi-agent actor-critic algorithm with good empiricalperformance. Our method can be used to imitate complex behaviors inhigh-dimensional environments with multiple cooperative or competing agents.",http://arxiv.org/abs/1807.09936v1,,
640,Adversarial Sampling for Active Learning,"This paper describes ASAL a new active learning strategy that usesuncertainty sampling, adversarial sample generation and sample matching.Compared to traditional pool-based uncertainty sampling strategies, ASALsynthesizes uncertain samples instead of performing an exhaustive search ineach active learning cycle. Then, the sample matching efficiently selectssimilar samples from the pool. We present a comprehensive set of experiments onMNIST and CIFAR-10 and show that ASAL outperforms similar methods and clearlyexceeds passive learning. To the best of our knowledge this is the firstpool-based adversarial active learning technique and the first that is appliedfor multi-label classification using deep convolutional classifiers.",http://arxiv.org/abs/1808.06671v1,,
641,"Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement  Learning","In hierarchical reinforcement learning a major challenge is determiningappropriate low-level policies. We propose an unsupervised learning scheme,based on asymmetric self-play from Sukhbaatar et al. (2018), that automaticallylearns a good representation of sub-goals in the environment and a low-levelpolicy that can execute them. A high-level policy can then direct the lower oneby generating a sequence of continuous sub-goal vectors. We evaluate our modelusing Mazebase and Mujoco environments, including the challenging AntGathertask. Visualizations of the sub-goal embeddings reveal a logical decompositionof tasks within the environment. Quantitatively, our approach obtainscompelling performance gains over non-hierarchical approaches.",http://arxiv.org/abs/1811.09083v1,,
642,Generative Adversarial Self-Imitation Learning,"This paper explores a simple regularizer for reinforcement learning byproposing Generative Adversarial Self-Imitation Learning (GASIL), whichencourages the agent to imitate past good trajectories via generativeadversarial imitation learning framework. Instead of directly maximizingrewards, GASIL focuses on reproducing past good trajectories, which canpotentially make long-term credit assignment easier when rewards are sparse anddelayed. GASIL can be easily combined with any policy gradient objective byusing GASIL as a learned shaped reward function. Our experimental results showthat GASIL improves the performance of proximal policy optimization on 2D PointMass and MuJoCo environments with delayed reward and stochastic dynamics.",http://arxiv.org/abs/1812.00950v1,,
643,No Peek: A Survey of private distributed deep learning,"We survey distributed deep learning models for training or inference withoutaccessing raw data from clients. These methods aim to protect confidentialpatterns in data while still allowing servers to train models. The distributeddeep learning methods of federated learning, split learning and large batchstochastic gradient descent are compared in addition to private and secureapproaches of differential privacy, homomorphic encryption, oblivious transferand garbled circuits in the context of neural networks. We study theirbenefits, limitations and trade-offs with regards to computational resources,data leakage and communication efficiency and also share our anticipated futuretrends.",http://arxiv.org/abs/1812.03288v1,,
644,Off-the-grid model based deep learning (O-MODL),"We introduce a model based off-the-grid image reconstruction algorithm usingdeep learned priors. The main difference of the proposed scheme with currentdeep learning strategies is the learning of non-linear annihilation relationsin Fourier space. We rely on a model based framework, which allows us to use asignificantly smaller deep network, compared to direct approaches that alsolearn how to invert the forward model. Preliminary comparisons against imagedomain MoDL approach demonstrates the potential of the off-the-gridformulation. The main benefit of the proposed scheme compared to structuredlow-rank methods is the quite significant reduction in computationalcomplexity.",http://arxiv.org/abs/1812.10747v1,,
645,"Monte-Carlo Sampling applied to Multiple Instance Learning for  Histological Image Classification","We propose a patch sampling strategy based on a sequential Monte-Carlo methodfor high resolution image classification in the context of Multiple InstanceLearning. When compared with grid sampling and uniform sampling techniques, itachieves higher generalization performance. We validate the strategy on twoartificial datasets and two histological datasets for breast cancer and sunexposure classification.",http://arxiv.org/abs/1812.11560v1,,
646,"On the Global Convergence of Imitation Learning: A Case for Linear  Quadratic Regulator","We study the global convergence of generative adversarial imitation learningfor linear quadratic regulators, which is posed as minimax optimization. Toaddress the challenges arising from non-convex-concave geometry, we analyze thealternating gradient algorithm and establish its Q-linear rate of convergenceto a unique saddle point, which simultaneously recovers the globally optimalpolicy and reward function. We hope our results may serve as a small steptowards understanding and taming the instability in imitation learning as wellas in more general non-convex-concave alternating minimax optimization thatarises from reinforcement learning and generative adversarial learning.",http://arxiv.org/abs/1901.03674v1,,
647,Mixed Formal Learning: A Path to Transparent Machine Learning,"This paper presents Mixed Formal Learning, a new architecture that learnsmodels based on formal mathematical representations of the domain of interestand exposes latent variables. The second element in the architecture learns aparticular skill, typically by using traditional prediction or classificationmechanisms. Our key findings include that this architecture: (1) Facilitatestransparency by exposing key latent variables based on a learned mathematicalmodel; (2) Enables Low Shot and Zero Shot training of machine learning withoutsacrificing accuracy or recall.",http://arxiv.org/abs/1901.06622v1,,
648,Learning Independently-Obtainable Reward Functions,"We present a novel method for learning a set of disentangled reward functionsthat sum to the original environment reward and are constrained to beindependently obtainable. We define independent obtainability in terms of valuefunctions with respect to obtaining one learned reward while pursuing anotherlearned reward. Empirically, we illustrate that our method can learn meaningfulreward decompositions in a variety of domains and that these decompositionsexhibit some form of generalization performance when the environment's rewardis modified. Theoretically, we derive results about the effect of maximizingour method's objective on the resulting reward functions and theircorresponding optimal policies.",http://arxiv.org/abs/1901.08649v3,,
649,"EILearn: Learning Incrementally Using Previous Knowledge Obtained From  an Ensemble of Classifiers","We propose an algorithm for incremental learning of classifiers. The proposedmethod enables an ensemble of classifiers to learn incrementally byaccommodating new training data. We use an effective mechanism to overcome thestability-plasticity dilemma. In incremental learning, the general conventionis to use only the knowledge acquired in the previous phase but not thepreviously seen data. We follow this convention by retaining the previouslyacquired knowledge which is relevant and using it along with the current data.The performance of each classifier is monitored to eliminate the poorlyperforming classifiers in the subsequent phases. Experimental results show thatthe proposed approach outperforms the existing incremental learning approaches.",http://arxiv.org/abs/1902.02948v1,,
650,Analysis Dictionary Learning: An Efficient and Discriminative Solution,"Discriminative Dictionary Learning (DL) methods have been widely advocatedfor image classification problems. To further sharpen their discriminativecapabilities, most state-of-the-art DL methods have additional constraintsincluded in the learning stages. These various constraints, however, lead toadditional computational complexity. We hence propose an efficientDiscriminative Convolutional Analysis Dictionary Learning (DCADL) method, as alower cost Discriminative DL framework, to both characterize the imagestructures and refine the interclass structure representations. The proposedDCADL jointly learns a convolutional analysis dictionary and a universalclassifier, while greatly reducing the time complexity in both training andtesting phases, and achieving a competitive accuracy, thus demonstrating greatperformance in many experiments with standard databases.",http://arxiv.org/abs/1903.03058v1,,
651,"Gradient Descent based Optimization Algorithms for Deep Learning Models  Training","In this paper, we aim at providing an introduction to the gradient descentbased optimization algorithms for learning deep neural network models. Deeplearning models involving multiple nonlinear projection layers are verychallenging to train. Nowadays, most of the deep learning model training stillrelies on the back propagation algorithm actually. In back propagation, themodel variables will be updated iteratively until convergence with gradientdescent based optimization algorithms. Besides the conventional vanillagradient descent algorithm, many gradient descent variants have also beenproposed in recent years to improve the learning performance, includingMomentum, Adagrad, Adam, Gadam, etc., which will all be introduced in thispaper respectively.",http://arxiv.org/abs/1903.03614v1,,
652,"Temporal Logic Guided Safe Reinforcement Learning Using Control Barrier  Functions","Using reinforcement learning to learn control policies is a challenge whenthe task is complex with potentially long horizons. Ensuring adequate but safeexploration is also crucial for controlling physical systems. In this paper, weuse temporal logic to facilitate specification and learning of complex tasks.We combine temporal logic with control Lyapunov functions to improveexploration. We incorporate control barrier functions to safeguard theexploration and deployment process. We develop a flexible and learnable systemthat allows users to specify task objectives and constraints in different formsand at various levels. The framework is also able to take advantage of knownsystem dynamics and handle unknown environmental dynamics by integratingmodel-free learning with model-based planning.",http://arxiv.org/abs/1903.09885v1,,
653,Learning Good Representation via Continuous Attention,"In this paper we present our scientific discovery that good representationcan be learned via continuous attention during the interaction betweenUnsupervised Learning(UL) and Reinforcement Learning(RL) modules driven byintrinsic motivation. Specifically, we designed intrinsic rewards generatedfrom UL modules for driving the RL agent to focus on objects for a period oftime and to learn good representations of objects for later object recognitiontask. We evaluate our proposed algorithm in both with and without extrinsicreward settings. Experiments with end-to-end training in simulated environmentswith applications to few-shot object recognition demonstrated the effectivenessof the proposed algorithm.",http://arxiv.org/abs/1903.12344v2,,
654,Multitask Soft Option Learning,"We present Multitask Soft Option Learning (MSOL), a hierarchical multitaskframework based on Planning as Inference. MSOL extends the concept of options,using separate variational posteriors for each task, regularized by a sharedprior. This allows fine-tuning of options for new tasks without forgettingtheir learned policies, leading to faster training without reducing theexpressiveness of the hierarchical policy. Additionally, MSOL avoids severalinstabilities during training in a multitask setting and provides a natural wayto not only learn intra-option policies, but also their terminations. Wedemonstrate empirically that MSOL significantly outperforms both hierarchicaland flat transfer-learning baselines in challenging multi-task environments.",http://arxiv.org/abs/1904.01033v1,,
655,Generative predecessor models for sample-efficient imitation learning,"We propose Generative Predecessor Models for Imitation Learning (GPRIL), anovel imitation learning algorithm that matches the state-action distributionto the distribution observed in expert demonstrations, using generative modelsto reason probabilistically about alternative histories of demonstrated states.We show that this approach allows an agent to learn robust policies using onlya small number of expert demonstrations and self-supervised interactions withthe environment. We derive this approach from first principles and compare itempirically to a state-of-the-art imitation learning method, showing that itoutperforms or matches its performance on two simulated robot manipulationtasks and demonstrate significantly higher sample efficiency by applying thealgorithm on a real robot.",http://arxiv.org/abs/1904.01139v1,,
656,"Efficient Structured Prediction with Latent Variables for General  Graphical Models","In this paper we propose a unified framework for structured prediction withlatent variables which includes hidden conditional random fields and latentstructured support vector machines as special cases. We describe a localentropy approximation for this general formulation using duality, and derive anefficient message passing algorithm that is guaranteed to converge. Wedemonstrate its effectiveness in the tasks of image segmentation as well as 3Dindoor scene understanding from single images, showing that our approach issuperior to latent structured support vector machines and hidden conditionalrandom fields.",http://arxiv.org/abs/1206.6436v1,,
657,Local Support Vector Machines:Formulation and Analysis,"We provide a formulation for Local Support Vector Machines (LSVMs) thatgeneralizes previous formulations, and brings out the explicit connections tolocal polynomial learning used in nonparametric estimation literature. Weinvestigate the simplest type of LSVMs called Local Linear Support VectorMachines (LLSVMs). For the first time we establish conditions under whichLLSVMs make Bayes consistent predictions at each test point $x_0$. We alsoestablish rates at which the local risk of LLSVMs converges to the minimumvalue of expected local risk at each point $x_0$. Using stability arguments weestablish generalization error bounds for LLSVMs.",http://arxiv.org/abs/1309.3699v1,,
658,Empirical Study on Deep Learning Models for Question Answering,"In this paper we explore deep learning models with memory component orattention mechanism for question answering task. We combine and compare threemodels, Neural Machine Translation, Neural Turing Machine, and Memory Networksfor a simulated QA data set. This paper is the first one that uses NeuralMachine Translation and Neural Turing Machines for solving QA tasks. Ourresults suggest that the combination of attention and memory have potential tosolve certain QA problem.",http://arxiv.org/abs/1510.07526v3,,
659,Machine Comprehension Based on Learning to Rank,"Machine comprehension plays an essential role in NLP and has been widelyexplored with dataset like MCTest. However, this dataset is too simple and toosmall for learning true reasoning abilities. \cite{hermann2015teaching}therefore release a large scale news article dataset and propose a deep LSTMreader system for machine comprehension. However, the training process isexpensive. We therefore try feature-engineered approach with semantics on thenew dataset to see how traditional machine learning technique and semantics canhelp with machine comprehension. Meanwhile, our proposed L2R reader systemachieves good performance with efficiency and less training data.",http://arxiv.org/abs/1605.03284v2,,
660,"Universal Consistency and Robustness of Localized Support Vector  Machines","The massive amount of available data potentially used to discover patters inmachine learning is a challenge for kernel based algorithms with respect toruntime and storage capacities. Local approaches might help to relieve theseissues. From a statistical point of view local approaches allow additionally todeal with different structures in the data in different ways. This paperanalyses properties of localized kernel based, non-parametric statisticalmachine learning methods, in particular of support vector machines (SVMs) andmethods close to them. We will show there that locally learnt kernel methodsare universal consistent. Furthermore, we give an upper bound for the maxbiasin order to show statistical robustness of the proposed method.",http://arxiv.org/abs/1703.06528v1,,
661,Membership Inference Attacks on Sequence-to-Sequence Models,"Data privacy is an important issue for ""machine learning as a service""providers. We focus on the problem of membership inference attacks: given adata sample and black-box access to a model's API, determine whether the sampleexisted in the model's training data. Our contribution is an investigation ofthis problem in the context of sequence-to-sequence models, which are importantin applications such as machine translation and video captioning. We define themembership inference problem for sequence generation, provide an open datasetbased on state-of-the-art machine translation models, and report initialresults on whether these models leak private information against several kindsof membership inference attacks.",http://arxiv.org/abs/1904.05506v1,,
662,Support Spinor Machine,"We generalize a support vector machine to a support spinor machine by usingthe mathematical structure of wedge product over vector machine in order toextend field from vector field to spinor field. The separated hyperplane isextended to Kolmogorov space in time series data which allow us to extend astructure of support vector machine to a support tensor machine and a supporttensor machine moduli space. Our performance test on support spinor machine isdone over one class classification of end point in physiology state of timeseries data after empirical mode analysis and compared with support vectormachine test. We implement algorithm of support spinor machine by usingHolo-Hilbert amplitude modulation for fully nonlinear and nonstationary timeseries data analysis.",http://arxiv.org/abs/1709.03943v1,,
663,"A novel improved fuzzy support vector machine based stock price trend  forecast model","Application of fuzzy support vector machine in stock price forecast. Supportvector machine is a new type of machine learning method proposed in 1990s. Itcan deal with classification and regression problems very successfully. Due tothe excellent learning performance of support vector machine, the technologyhas become a hot research topic in the field of machine learning, and it hasbeen successfully applied in many fields. However, as a new technology, thereare many limitations to support vector machines. There is a large amount offuzzy information in the objective world. If the training of support vectormachine contains noise and fuzzy information, the performance of the supportvector machine will become very weak and powerless. As the complexity of manyfactors influence the stock price prediction, the prediction results oftraditional support vector machine cannot meet people with precision, thisstudy improved the traditional support vector machine fuzzy predictionalgorithm is proposed to improve the new model precision. NASDAQ Stock Market,Standard & Poor's (S&P) Stock market are considered. Novel advanced- fuzzysupport vector machine (NA-FSVM) is the proposed methodology.",http://arxiv.org/abs/1801.00681v1,,
664,Types of Cost in Inductive Concept Learning,"Inductive concept learning is the task of learning to assign cases to adiscrete set of classes. In real-world applications of concept learning, thereare many different types of cost involved. The majority of the machine learningliterature ignores all types of cost (unless accuracy is interpreted as a typeof cost measure). A few papers have investigated the cost of misclassificationerrors. Very few papers have examined the many other types of cost. In thispaper, we attempt to create a taxonomy of the different types of cost that areinvolved in inductive concept learning. This taxonomy may help to organize theliterature on cost-sensitive learning. We hope that it will inspire researchersto investigate all types of cost in inductive concept learning in more depth.",http://arxiv.org/abs/cs/0212034v1,,
665,Surrogate Losses in Passive and Active Learning,"Active learning is a type of sequential design for supervised machinelearning, in which the learning algorithm sequentially requests the labels ofselected instances from a large pool of unlabeled data points. The objective isto produce a classifier of relatively low risk, as measured under the 0-1 loss,ideally using fewer label requests than the number of random labeled datapoints sufficient to achieve the same. This work investigates the potentialuses of surrogate loss functions in the context of active learning.Specifically, it presents an active learning algorithm based on an arbitraryclassification-calibrated surrogate loss function, along with an analysis ofthe number of label requests sufficient for the classifier returned by thealgorithm to achieve a given risk under the 0-1 loss. Interestingly, theseresults cannot be obtained by simply optimizing the surrogate risk via activelearning to an extent sufficient to provide a guarantee on the 0-1 loss, as iscommon practice in the analysis of surrogate losses for passive learning. Someof the results have additional implications for the use of surrogate losses inpassive learning.",http://arxiv.org/abs/1207.3772v3,,
666,"A Metric-learning based framework for Support Vector Machines and  Multiple Kernel Learning","Most metric learning algorithms, as well as Fisher's Discriminant Analysis(FDA), optimize some cost function of different measures of within-andbetween-class distances. On the other hand, Support Vector Machines(SVMs) andseveral Multiple Kernel Learning (MKL) algorithms are based on the SVM largemargin theory. Recently, SVMs have been analyzed from SVM and metric learning,and to develop new algorithms that build on the strengths of each. Inspired bythe metric learning interpretation of SVM, we develop here a newmetric-learning based SVM framework in which we incorporate metric learningconcepts within SVM. We extend the optimization problem of SVM to include somemeasure of the within-class distance and along the way we develop a newwithin-class distance measure which is appropriate for SVM. In addition, weadopt the same approach for MKL and show that it can be also formulated as aMahalanobis metric learning problem. Our end result is a number of SVM/MKLalgorithms that incorporate metric learning concepts. We experiment with themon a set of benchmark datasets and observe important predictive performanceimprovements.",http://arxiv.org/abs/1309.3877v1,,
667,Deep Transductive Semi-supervised Maximum Margin Clustering,"Semi-supervised clustering is an very important topic in machine learning andcomputer vision. The key challenge of this problem is how to learn a metric,such that the instances sharing the same label are more likely close to eachother on the embedded space. However, little attention has been paid to learnbetter representations when the data lie on non-linear manifold. Fortunately,deep learning has led to great success on feature learning recently. Inspiredby the advances of deep learning, we propose a deep transductivesemi-supervised maximum margin clustering approach. More specifically, givenpairwise constraints, we exploit both labeled and unlabeled data to learn anon-linear mapping under maximum margin framework for clustering analysis.Thus, our model unifies transductive learning, feature learning and maximummargin techniques in the semi-supervised clustering framework. We pretrain thedeep network structure with restricted Boltzmann machines (RBMs) layer by layergreedily, and optimize our objective function with gradient descent. Bychecking the most violated constraints, our approach updates the modelparameters through error backpropagation, in which deep features are learnedautomatically. The experimental results shows that our model is significantlybetter than the state of the art on semi-supervised clustering.",http://arxiv.org/abs/1501.06237v1,,
668,Iterated Support Vector Machines for Distance Metric Learning,"Distance metric learning aims to learn from the given training data a validdistance metric, with which the similarity between data samples can be moreeffectively evaluated for classification. Metric learning is often formulatedas a convex or nonconvex optimization problem, while many existing metriclearning algorithms become inefficient for large scale problems. In this paper,we formulate metric learning as a kernel classification problem, and solve itby iterated training of support vector machines (SVM). The new formulation iseasy to implement, efficient in training, and tractable for large-scaleproblems. Two novel metric learning models, namely Positive-semidefiniteConstrained Metric Learning (PCML) and Nonnegative-coefficient ConstrainedMetric Learning (NCML), are developed. Both PCML and NCML can guarantee theglobal optimality of their solutions. Experimental results on UCI datasetclassification, handwritten digit recognition, face verification and personre-identification demonstrate that the proposed metric learning methods achievehigher classification accuracy than state-of-the-art methods and they aresignificantly more efficient in training.",http://arxiv.org/abs/1502.00363v1,,
669,Lifelong Generative Modeling,"Lifelong learning is the problem of learning multiple consecutive tasks in asequential manner where knowledge gained from previous tasks is retained andused for future learning. It is essential towards the development ofintelligent machines that can adapt to their surroundings. In this work wefocus on a lifelong learning approach to generative modeling where wecontinuously incorporate newly observed distributions into our learnt model. Wedo so through a student-teacher Variational Autoencoder architecture whichallows us to learn and preserve all the distributions seen so far without theneed to retain the past data nor the past models. Through the introduction of anovel cross-model regularizer, inspired by a Bayesian update rule, the studentmodel leverages the information learnt by the teacher, which acts as a summaryof everything seen till now. The regularizer has the additional benefit ofreducing the effect of catastrophic interference that appears when we learnover sequences of distributions. We demonstrate its efficacy in learningsequentially observed distributions as well as its ability to learn a commonlatent representation across a complex transfer learning scenario.",http://arxiv.org/abs/1705.09847v4,,
670,Actively Learning what makes a Discrete Sequence Valid,"Deep learning techniques have been hugely successful for traditionalsupervised and unsupervised machine learning problems. In large part, thesetechniques solve continuous optimization problems. Recently however, discretegenerative deep learning models have been successfully used to efficientlysearch high-dimensional discrete spaces. These methods work by representingdiscrete objects as sequences, for which powerful sequence-based deep modelscan be employed. Unfortunately, these techniques are significantly hindered bythe fact that these generative models often produce invalid sequences. As astep towards solving this problem, we propose to learn a deep recurrentvalidator model. Given a partial sequence, our model learns the probability ofthat sequence occurring as the beginning of a full valid sequence. Thus thisidentifies valid versus invalid sequences and crucially it also providesinsight about how individual sequence elements influence the validity ofdiscrete objects. To learn this model we propose an approach inspired byseminal work in Bayesian active learning. On a synthetic dataset, wedemonstrate the ability of our model to distinguish valid and invalidsequences. We believe this is a key step toward learning generative models thatfaithfully produce valid discrete objects.",http://arxiv.org/abs/1708.04465v1,,
671,"Using Task Descriptions in Lifelong Machine Learning for Improved  Performance and Zero-Shot Transfer","Knowledge transfer between tasks can improve the performance of learnedmodels, but requires an accurate estimate of the inter-task relationships toidentify the relevant knowledge to transfer. These inter-task relationships aretypically estimated based on training data for each task, which is inefficientin lifelong learning settings where the goal is to learn each consecutive taskrapidly from as little data as possible. To reduce this burden, we develop alifelong learning method based on coupled dictionary learning that utilizeshigh-level task descriptions to model the inter-task relationships. We showthat using task descriptors improves the performance of the learned taskpolicies, providing both theoretical justification for the benefit andempirical demonstration of the improvement across a variety of learningproblems. Given only the descriptor for a new task, the lifelong learner isalso able to accurately predict a model for the new task through zero-shotlearning using the coupled dictionary, eliminating the need to gather trainingdata before addressing the task.",http://arxiv.org/abs/1710.03850v1,,
672,A Self-paced Regularization Framework for Partial-Label Learning,"Partial label learning (PLL) aims to solve the problem where each traininginstance is associated with a set of candidate labels, one of which is thecorrect label. Most PLL algorithms try to disambiguate the candidate label set,by either simply treating each candidate label equally or iterativelyidentifying the true label. Nonetheless, existing algorithms usually treat alllabels and instances equally, and the complexities of both labels and instancesare not taken into consideration during the learning stage. Inspired by thesuccessful application of self-paced learning strategy in machine learningfield, we integrate the self-paced regime into the partial label learningframework and propose a novel Self-Paced Partial-Label Learning (SP-PLL)algorithm, which could control the learning process to alleviate the problem byranking the priorities of the training examples together with their candidatelabels during each learning iteration. Extensive experiments and comparisonswith other baseline methods demonstrate the effectiveness and robustness of theproposed method.",http://arxiv.org/abs/1804.07759v2,,
673,Understanding Self-Paced Learning under Concave Conjugacy Theory,"By simulating the easy-to-hard learning manners of humans/animals, thelearning regimes called curriculum learning~(CL) and self-paced learning~(SPL)have been recently investigated and invoked broad interests. However, theintrinsic mechanism for analyzing why such learning regimes can work has notbeen comprehensively investigated. To this issue, this paper proposes a concaveconjugacy theory for looking into the insight of CL/SPL. Specifically, by usingthis theory, we prove the equivalence of the SPL regime and a latent concaveobjective, which is closely related to the known non-convex regularized penaltywidely used in statistics and machine learning. Beyond the previous theory forexplaining CL/SPL insights, this new theoretical framework on one handfacilitates two direct approaches for designing new SPL models for certaintasks, and on the other hand can help conduct the latent objective ofself-paced curriculum learning, which is the advanced version of both CL/SPLand possess advantages of both learning regimes to a certain extent. Thisfurther facilitates a theoretical understanding for SPCL, instead of onlyCL/SPL as conventional. Under this theory, we attempt to attain intrinsiclatent objectives of two curriculum forms, the partial order and groupcurriculums, which easily follow the theoretical understanding of thecorresponding SPCL regimes.",http://arxiv.org/abs/1805.08096v1,,
674,Learning Dynamics of Linear Denoising Autoencoders,"Denoising autoencoders (DAEs) have proven useful for unsupervisedrepresentation learning, but a thorough theoretical understanding is stilllacking of how the input noise influences learning. Here we develop theory forhow noise influences learning in DAEs. By focusing on linear DAEs, we are ableto derive analytic expressions that exactly describe their learning dynamics.We verify our theoretical predictions with simulations as well as experimentson MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noiseallows DAEs to ignore low variance directions in the inputs while learning toreconstruct them. Furthermore, in a comparison of the learning dynamics of DAEsto standard regularised autoencoders, we show that noise has a similarregularisation effect to weight decay, but with faster training dynamics. Wealso show that our theoretical predictions approximate learning dynamics onreal-world data and qualitatively match observed dynamics in nonlinear DAEs.",http://arxiv.org/abs/1806.05413v2,,
675,Incremental Few-Shot Learning with Attention Attractor Networks,"Machine learning classifiers are often trained to recognize a set ofpre-defined classes. However, in many real applications, it is often desirableto have the flexibility of learning additional concepts, without re-training onthe full training set. This paper addresses this problem, incremental few-shotlearning, where a regular classification network has already been trained torecognize a set of base classes; and several extra novel classes are beingconsidered, each with only a few labeled examples. After learning the novelclasses, the model is then evaluated on the overall performance of both baseand novel classes. To this end, we propose a meta-learning model, the AttentionAttractor Network, which regularizes the learning of novel classes. In eachepisode, we train a set of new weights to recognize novel classes until theyconverge, and we show that the technique of recurrent back-propagation canback-propagate through the optimization process and facilitate the learning ofthe attractor network regularizer. We demonstrate that the learned attractornetwork can recognize novel classes while remembering old classes without theneed to review the original training set, outperforming baselines that do notrely on an iterative optimization process.",http://arxiv.org/abs/1810.07218v1,,
676,Self-Adversarially Learned Bayesian Sampling,"Scalable Bayesian sampling is playing an important role in modern machinelearning, especially in the fast-developed unsupervised-(deep)-learning models.While tremendous progresses have been achieved via scalable Bayesian samplingsuch as stochastic gradient MCMC (SG-MCMC) and Stein variational gradientdescent (SVGD), the generated samples are typically highly correlated.Moreover, their sample-generation processes are often criticized to beinefficient. In this paper, we propose a novel self-adversarial learningframework that automatically learns a conditional generator to mimic thebehavior of a Markov kernel (transition kernel). High-quality samples can beefficiently generated by direct forward passes though a learned generator. Mostimportantly, the learning process adopts a self-learning paradigm, requiring noinformation on existing Markov kernels, e.g., knowledge of how to draw samplesfrom them. Specifically, our framework learns to use current samples, eitherfrom the generator or pre-provided training data, to update the generator suchthat the generated samples progressively approach a target distribution, thusit is called self-learning. Experiments on both synthetic and real datasetsverify advantages of our framework, outperforming related methods in terms ofboth sampling efficiency and sample quality.",http://arxiv.org/abs/1811.08929v1,,
677,"A Tutorial on Distance Metric Learning: Mathematical Foundations,  Algorithms and Software","This paper describes the discipline of distance metric learning, a branch ofmachine learning that aims to learn distances from the data. Distance metriclearning can be useful to improve similarity learning algorithms, and also hasapplications in dimensionality reduction. We describe the distance metriclearning problem and analyze its main mathematical foundations. We discuss someof the most popular distance metric learning techniques used in classification,showing their goals and the required information to understand and use them.Furthermore, we present a Python package that collects a set of 17 distancemetric learning techniques explained in this paper, with some experiments toevaluate the performance of the different algorithms. Finally, we discussseveral possibilities of future work in this topic.",http://arxiv.org/abs/1812.05944v1,,
678,ALiPy: Active Learning in Python,"Supervised machine learning methods usually require a large set of labeledexamples for model training. However, in many real applications, there areplentiful unlabeled data but limited labeled data; and the acquisition oflabels is costly. Active learning (AL) reduces the labeling cost by iterativelyselecting the most valuable data to query their labels from the annotator. Thisarticle introduces a Python toobox ALiPy for active learning. ALiPy provides amodule based implementation of active learning framework, which allows users toconveniently evaluate, compare and analyze the performance of active learningmethods. In the toolbox, multiple options are available for each component ofthe learning framework, including data process, active selection, label query,results visualization, etc. In addition to the implementations of more than 20state-of-the-art active learning algorithms, ALiPy also supports users toeasily configure and implement their own approaches under different activelearning settings, such as AL for multi-label data, AL with noisy annotators,AL with different costs and so on. The toolbox is well-documented andopen-source on Github, and can be easily installed through PyPI.",http://arxiv.org/abs/1901.03802v1,,
679,Deep Mean Functions for Meta-Learning in Gaussian Processes,"Fitting machine learning models in the low-data limit is challenging. Themain challenge is to obtain suitable prior knowledge and encode it into themodel, for instance in the form of a Gaussian process prior. Recent advances inmeta-learning offer powerful methods for extracting such prior knowledge fromdata acquired in related tasks. When it comes to meta-learning in Gaussianprocess models, approaches in this setting have mostly focused on learning thekernel function of the prior, but not on learning its mean function. In thiswork, we propose to parameterize the mean function of a Gaussian process with adeep neural network and train it with a meta-learning procedure. We presentanalytical and empirical evidence that mean function learning can be superiorto kernel learning alone, particularly if data is scarce.",http://arxiv.org/abs/1901.08098v1,,
680,Improved Reinforcement Learning with Curriculum,"Humans tend to learn complex abstract concepts faster if examples arepresented in a structured manner. For instance, when learning how to play aboard game, usually one of the first concepts learned is how the game ends,i.e. the actions that lead to a terminal state (win, lose or draw). Theadvantage of learning end-games first is that once the actions which lead to aterminal state are understood, it becomes possible to incrementally learn theconsequences of actions that are further away from a terminal state - we callthis an end-game-first curriculum. Currently the state-of-the-art machinelearning player for general board games, AlphaZero by Google DeepMind, does notemploy a structured training curriculum; instead learning from the entire gameat all times. By employing an end-game-first training curriculum to train anAlphaZero inspired player, we empirically show that the rate of learning of anartificial player can be improved during the early stages of training whencompared to a player not using a training curriculum.",http://arxiv.org/abs/1903.12328v1,,
681,Deep Meta-Learning: Learning to Learn in the Concept Space,"Few-shot learning remains challenging for meta-learning that learns alearning algorithm (meta-learner) from many related tasks. In this work, weargue that this is due to the lack of a good representation for meta-learning,and propose deep meta-learning to integrate the representation power of deeplearning into meta-learning. The framework is composed of three modules, aconcept generator, a meta-learner, and a concept discriminator, which arelearned jointly. The concept generator, e.g. a deep residual net, extracts arepresentation for each instance that captures its high-level concept, on whichthe meta-learner performs few-shot learning, and the concept discriminatorrecognizes the concepts. By learning to learn in the concept space rather thanin the complicated instance space, deep meta-learning can substantially improvevanilla meta-learning, which is demonstrated on various few-shot imagerecognition problems. For example, on 5-way-1-shot image recognition onCIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%,and improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%,respectively.",http://arxiv.org/abs/1802.03596v1,,
682,"Proof of a Combinatorial Conjecture Coming from the PAC-Bayesian Machine  Learning Theory","We give a proof of a conjecture of A. Lacasse in his doctoral thesis whichhas applications in machine learning algorithms. The proof relies on someinteresting binomial sums identities introduced by Abel (1839), and on theirgeneralization to the multinomial case by Hurwitz (1902).",http://arxiv.org/abs/1209.0824v1,,
683,Bio-inspired data mining: Treating malware signatures as biosequences,"The application of machine learning to bioinformatics problems is wellestablished. Less well understood is the application of bioinformaticstechniques to machine learning and, in particular, the representation ofnon-biological data as biosequences. The aim of this paper is to explore theeffects of giving amino acid representation to problematic machine learningdata and to evaluate the benefits of supplementing traditional machine learningwith bioinformatics tools and techniques. The signatures of 60 computer virusesand 60 computer worms were converted into amino acid representations and firstmultiply aligned separately to identify conserved regions across differentfamilies within each class (virus and worm). This was followed by a secondalignment of all 120 aligned signatures together so that non-conserved regionswere identified prior to input to a number of machine learning techniques.Differences in length between virus and worm signatures after the firstalignment were resolved by the second alignment. Our first set of experimentsindicates that representing computer malware signatures as amino acid sequencesfollowed by alignment leads to greater classification and prediction accuracy.Our second set of experiments indicates that checking the results of datamining from artificial virus and worm data against known proteins can lead togeneralizations being made from the domain of naturally occurring proteins tomalware signatures. However, further work is needed to determine the advantagesand disadvantages of different representations and sequence alignment methodsfor handling problematic machine learning data.",http://arxiv.org/abs/1302.3668v1,,
684,Cross-conformal predictors,"This note introduces the method of cross-conformal prediction, which is ahybrid of the methods of inductive conformal prediction and cross-validation,and studies its validity and predictive efficiency empirically.",http://arxiv.org/abs/1208.0806v1,,
685,Dirichlet draws are sparse with high probability,"This note provides an elementary proof of the folklore fact that draws from aDirichlet distribution (with parameters less than 1) are typically sparse (mostcoordinates are small).",http://arxiv.org/abs/1301.4917v1,,
686,Stochastic Optimization of PCA with Capped MSG,"We study PCA as a stochastic optimization problem and propose a novelstochastic approximation algorithm which we refer to as ""Matrix StochasticGradient"" (MSG), as well as a practical variant, Capped MSG. We study themethod both theoretically and empirically.",http://arxiv.org/abs/1307.1674v1,,
687,Norm-Based Capacity Control in Neural Networks,"We investigate the capacity, convexity and characterization of a generalfamily of norm-constrained feed-forward networks.",http://arxiv.org/abs/1503.00036v2,,
688,A note on adjusting $R^2$ for using with cross-validation,"We show how to adjust the coefficient of determination ($R^2$) when used formeasuring predictive accuracy via leave-one-out cross-validation.",http://arxiv.org/abs/1605.01703v1,,
689,Induction and physical theory formation by Machine Learning,"Machine learning presents a general, systematic framework for the generationof formal theoretical models for physical description and prediction.Tentatively standard linear modeling techniques are reviewed; followed by abrief discussion of generalizations to deep forward networks for approximatingnonlinear phenomena.",http://arxiv.org/abs/1609.03862v1,,
690,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision  Sciences, and Data Products","Machine learning algorithms increasingly influence our decisions and interactwith us in all parts of our daily lives. Therefore, just as we consider thesafety of power plants, highways, and a variety of other engineeredsocio-technical systems, we must also take into account the safety of systemsinvolving machine learning. Heretofore, the definition of safety has not beenformalized in a machine learning context. In this paper, we do so by definingmachine learning safety in terms of risk, epistemic uncertainty, and the harmincurred by unwanted outcomes. We then use this definition to examine safety inall sorts of applications in cyber-physical systems, decision sciences, anddata products. We find that the foundational principle of modern statisticalmachine learning, empirical risk minimization, is not always a sufficientobjective. Finally, we discuss how four different categories of strategies forachieving safety in engineering, including inherently safe design, safetyreserves, safe fail, and procedural safeguards can be mapped to a machinelearning context. We then discuss example techniques that can be adopted ineach category, such as considering interpretability and causality of predictivemodels, objective functions beyond expected prediction accuracy, humaninvolvement for labeling difficult or rare examples, and user experience designof software and open data.",http://arxiv.org/abs/1610.01256v2,,
691,A note on the triangle inequality for the Jaccard distance,"Two simple proofs of the triangle inequality for the Jaccard distance interms of nonnegative, monotone, submodular functions are given and discussed.",http://arxiv.org/abs/1612.02696v1,,
692,Breeding electric zebras in the fields of Medicine,"A few notes on the use of machine learning in medicine and the relatedunintended consequences.",http://arxiv.org/abs/1701.04077v3,,
693,Human-Level Intelligence or Animal-Like Abilities?,"The vision systems of the eagle and the snake outperform everything that wecan make in the laboratory, but snakes and eagles cannot build an eyeglass or atelescope or a microscope. (Judea Pearl)",http://arxiv.org/abs/1707.04327v1,,
694,Recency-weighted Markovian inference,"We describe a Markov latent state space (MLSS) model, where the latent statedistribution is a decaying mixture over multiple past states. We present asimple sampling algorithm that allows to approximate such high-order MLSS withfixed time and memory costs.",http://arxiv.org/abs/1711.03038v1,,
695,Paranom: A Parallel Anomaly Dataset Generator,"In this paper, we present Paranom, a parallel anomaly dataset generator. Wediscuss its design and provide brief experimental results demonstrating itsusefulness in improving the classification correctness of LSTM-AD, astate-of-the-art anomaly detection model.",http://arxiv.org/abs/1801.03164v1,,
696,SparCML: High-Performance Sparse Communication for Machine Learning,"One of the main drivers behind the rapid recent advances in machine learninghas been the availability of efficient system support. Despite existingprogress, scaling compute-intensive machine learning workloads to a largenumber of compute nodes is still a challenging task. In this paper, we addressthis challenge, by proposing SparCML, a general, scalable communication layerfor machine learning applications. SparCML is built on the observation thatmany distributed machine learning algorithms either have naturally sparsecommunication patterns, or have updates which can be sparsified in a structuredway for improved performance, without loss of convergence or accuracy. Toexploit this insight, we analyze, design, and implement a set ofcommunication-efficient protocols for sparse input data, in conjunction withefficient machine learning algorithms which can leverage these primitives. Ourcommunication protocols generalize standard collective operations, by allowingprocesses to contribute sparse input data vectors, of heterogeneous sizes. Ourgeneric communication layer is enriched with additional features, such assupport for non-blocking (asynchronous) operations and support forlow-precision data representations. We validate our algorithmic resultsexperimentally on a range of large-scale machine learning applications andtarget architectures, showing that we can leverage sparsity fororder-of-magnitude runtime savings, compared to existing methods andframeworks.",http://arxiv.org/abs/1802.08021v2,,
697,Random perturbation and matrix sparsification and completion,"We discuss general perturbation inequalities when the perturbation is random.As applications, we obtain several new results concerning two importantproblems: matrix sparsification and matrix completion.",http://arxiv.org/abs/1803.00679v1,,
698,"Hybrid Forecasting of Chaotic Processes: Using Machine Learning in  Conjunction with a Knowledge-Based Model","A model-based approach to forecasting chaotic dynamical systems utilizesknowledge of the physical processes governing the dynamics to build anapproximate mathematical model of the system. In contrast, machine learningtechniques have demonstrated promising results for forecasting chaotic systemspurely from past time series measurements of system state variables (trainingdata), without prior knowledge of the system dynamics. The motivation for thispaper is the potential of machine learning for filling in the gaps in ourunderlying mechanistic knowledge that cause widely-used knowledge-based modelsto be inaccurate. Thus we here propose a general method that leverages theadvantages of these two approaches by combining a knowledge-based model and amachine learning technique to build a hybrid forecasting scheme. Potentialapplications for such an approach are numerous (e.g., improving weatherforecasting). We demonstrate and test the utility of this approach using aparticular illustrative version of a machine learning known as reservoircomputing, and we apply the resulting hybrid forecaster to a low-dimensionalchaotic system, as well as to a high-dimensional spatiotemporal chaotic system.These tests yield extremely promising results in that our hybrid technique isable to accurately predict for a much longer period of time than either itsmachine-learning component or its model-based component alone.",http://arxiv.org/abs/1803.04779v1,,
699,Machine Learning for Exam Triage,"In this project, we extend the state-of-the-art CheXNet (Rajpurkar et al.[2017]) by making use of the additional non-image features in the dataset. Ourmodel produced better AUROC scores than the original CheXNet.",http://arxiv.org/abs/1805.00503v1,,
700,"Deploying Customized Data Representation and Approximate Computing in  Machine Learning Applications","Major advancements in building general-purpose and customized hardware havebeen one of the key enablers of versatility and pervasiveness of machinelearning models such as deep neural networks. To sustain this ubiquitousdeployment of machine learning models and cope with their computational andstorage complexity, several solutions such as low-precision representation ofmodel parameters using fixed-point representation and deploying approximatearithmetic operations have been employed. Studying the potency of suchsolutions in different applications requires integrating them into existingmachine learning frameworks for high-level simulations as well as implementingthem in hardware to analyze their effects on power/energy dissipation,throughput, and chip area. Lop is a library for design space exploration thatbridges the gap between machine learning and efficient hardware realization. Itcomprises a Python module, which can be integrated with some of the existingmachine learning frameworks and implements various customizable datarepresentations including fixed-point and floating-point as well as approximatearithmetic operations.Furthermore, it includes a highly-parameterized Scalamodule, which allows synthesizing hardware based on the said datarepresentations and arithmetic operations. Lop allows researchers and designersto quickly compare quality of their models using various data representationsand arithmetic operations in Python and contrast the hardware cost of viablerepresentations by synthesizing them on their target platforms (e.g., FPGA orASIC). To the best of our knowledge, Lop is the first library that allows bothsoftware simulation and hardware realization using customized datarepresentations and approximate computing techniques.",http://arxiv.org/abs/1806.00875v1,,
701,"Attack and defence in cellular decision-making: lessons from machine  learning","Machine learning algorithms are sensitive to meaningless (or ""adversarial"")perturbations. This is reminiscent of cellular decision-making where ligands(called ""antagonists"") prevent correct signalling, like in early immunerecognition. We draw a formal analogy between neural networks used in machinelearning and models of cellular decision-making (adaptive proofreading). Weapply attacks from machine learning to simple decision-making models, and showexplicitly the correspondence to antagonism by weakly bound ligands. Suchantagonism is absent in more nonlinear models, which inspired us to implement abiomimetic defence in neural networks filtering out adversarial perturbations.We then apply a gradient-descent approach from machine learning to differentcellular decision-making models, and we reveal the existence of two regimescharacterized by the presence or absence of a critical point. The criticalpoint causes the strongest antagonists to lie close to the threshold. This isvalidated in the loss landscapes of robust neural networks and cellulardecision-making models, and observed experimentally for immune cells. For bothregimes, we explain how associated defence mechanisms shape the geometry of theloss landscape, and why different adversarial attacks are effective indifferent regimes. Our work connects evolved cellular decision-making tomachine learning, and motivates the design of a general theory of adversarialperturbations, both for in vivo and in silico systems.",http://arxiv.org/abs/1807.04270v2,,
702,"OCLEP+: One-class Anomaly and Intrusion Detection Using Minimal Length  of Emerging Patterns","This paper presents a method called One-class Classification using Lengthstatistics of Emerging Patterns Plus (OCLEP+).",http://arxiv.org/abs/1811.09842v1,,
703,"Modeling Heterogeneity in Mode-Switching Behavior Under a  Mobility-on-Demand Transit System: An Interpretable Machine Learning Approach","Recent years have witnessed an increased focus on interpretability and theuse of machine learning to inform policy analysis and decision making. Thispaper applies machine learning to examine travel behavior and, in particular,on modeling changes in travel modes when individuals are presented with a novel(on-demand) mobility option. It addresses the following question: Can machinelearning be applied to model individual taste heterogeneity (preferenceheterogeneity for travel modes and response heterogeneity to travel attributes)in travel mode choice? This paper first develops a high-accuracy classifier topredict mode-switching behavior under a hypothetical Mobility-on-Demand Transitsystem (i.e., stated-preference data), which represents the case studyunderlying this research. We show that this classifier naturally capturesindividual heterogeneity available in the data. Moreover, the paper derivesinsights on heterogeneous switching behaviors through the generation ofmarginal effects and elasticities by current travel mode, partial dependenceplots, and individual conditional expectation plots. The paper also proposestwo new model-agnostic interpretation tools for machine learning, i.e.,conditional partial dependence plots and conditional individual partialdependence plots, specifically designed to examine response heterogeneity. Theresults on the case study show that the machine-learning classifier, togetherwith model-agnostic interpretation tools, provides valuable insights on travelmode switching behavior for different individuals and population segments. Forexample, the existing drivers are more sensitive to additional pickups thanpeople using other travel modes, and current transit users are generallywilling to share rides but reluctant to take any additional transfers.",http://arxiv.org/abs/1902.02904v1,,
704,"Zeno++: robust asynchronous SGD with arbitrary number of Byzantine  workers","We propose Zeno++, a new robust asynchronous synchronous Stochastic GradientDescent~(SGD) under a general Byzantine failure model with unbounded number ofByzantine workers.",http://arxiv.org/abs/1903.07020v1,,
705,Deep Mixtures of Factor Analysers,"An efficient way to learn deep density models that have many layers of latentvariables is to learn one layer at a time using a model that has only one layerof latent variables. After learning each layer, samples from the posteriordistributions for that layer are used as training data for learning the nextlayer. This approach is commonly used with Restricted Boltzmann Machines, whichare undirected graphical models with a single hidden layer, but it can also beused with Mixtures of Factor Analysers (MFAs) which are directed graphicalmodels. In this paper, we present a greedy layer-wise learning algorithm forDeep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be convertedto an equivalent shallow MFA by multiplying together the factor loadingmatrices at different levels, learning and inference are much more efficient ina DMFA and the sharing of each lower-level factor loading matrix by manydifferent higher level MFAs prevents overfitting. We demonstrate empiricallythat DMFAs learn better density models than both MFAs and two types ofRestricted Boltzmann Machine on a wide variety of datasets.",http://arxiv.org/abs/1206.4635v1,,
706,Big Learning with Bayesian Methods,"Explosive growth in data and availability of cheap computing resources havesparked increasing interest in Big learning, an emerging subfield that studiesscalable machine learning algorithms, systems, and applications with Big Data.Bayesian methods represent one important class of statistic methods for machinelearning, with substantial recent developments on adaptive, flexible andscalable Bayesian learning. This article provides a survey of the recentadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,including nonparametric Bayesian methods for adaptively inferring modelcomplexity, regularized Bayesian inference for improving the flexibility viaposterior regularization, and scalable algorithms and systems based onstochastic subsampling and distributed computing for dealing with large-scaleapplications.",http://arxiv.org/abs/1411.6370v2,,
707,"Variational Information Maximisation for Intrinsically Motivated  Reinforcement Learning","The mutual information is a core statistical quantity that has applicationsin all areas of machine learning, whether this is in training of density modelsover multiple data modalities, in maximising the efficiency of noisytransmission channels, or when learning behaviour policies for exploration byartificial agents. Most learning algorithms that involve optimisation of themutual information rely on the Blahut-Arimoto algorithm --- an enumerativealgorithm with exponential complexity that is not suitable for modern machinelearning applications. This paper provides a new approach for scalableoptimisation of the mutual information by merging techniques from variationalinference and deep learning. We develop our approach by focusing on the problemof intrinsically-motivated learning, where the mutual information forms thedefinition of a well-known internal drive known as empowerment. Using avariational lower bound on the mutual information, combined with convolutionalnetworks for handling visual input streams, we develop a stochasticoptimisation algorithm that allows for scalable information maximisation andempowerment-based reasoning directly from pixels to actions.",http://arxiv.org/abs/1509.08731v1,,
708,SOL: A Library for Scalable Online Learning Algorithms,"SOL is an open-source library for scalable online learning algorithms, and isparticularly suitable for learning with high-dimensional data. The libraryprovides a family of regular and sparse online learning algorithms forlarge-scale binary and multi-class classification tasks with high efficiency,scalability, portability, and extensibility. SOL was implemented in C++, andprovided with a collection of easy-to-use command-line tools, python wrappersand library calls for users and developers, as well as comprehensive documentsfor both beginners and advanced users. SOL is not only a practical machinelearning toolbox, but also a comprehensive experimental platform for onlinelearning research. Experiments demonstrate that SOL is highly efficient andscalable for large-scale machine learning with high-dimensional data.",http://arxiv.org/abs/1610.09083v1,,
709,Learning Features of Music from Scratch,"This paper introduces a new large-scale music dataset, MusicNet, to serve asa source of supervision and evaluation of machine learning methods for musicresearch. MusicNet consists of hundreds of freely-licensed classical musicrecordings by 10 composers, written for 11 instruments, together withinstrument/note annotations resulting in over 1 million temporal labels on 34hours of chamber music performances under various studio and microphoneconditions.  The paper defines a multi-label classification task to predict notes inmusical recordings, along with an evaluation protocol, and benchmarks severalmachine learning architectures for this task: i) learning from spectrogramfeatures; ii) end-to-end learning with a neural net; iii) end-to-end learningwith a convolutional neural net. These experiments show that end-to-end modelstrained for note prediction learn frequency selective filters as a low-levelrepresentation of audio.",http://arxiv.org/abs/1611.09827v2,,
710,Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets,"Deep learning models (aka Deep Neural Networks) have revolutionized manyfields including computer vision, natural language processing, speechrecognition, and is being increasingly used in clinical healthcareapplications. However, few works exist which have benchmarked the performanceof the deep learning models with respect to the state-of-the-art machinelearning models and prognostic scoring systems on publicly available healthcaredatasets. In this paper, we present the benchmarking results for severalclinical prediction tasks such as mortality prediction, length of stayprediction, and ICD-9 code group prediction using Deep Learning models,ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFAscores. We used the Medical Information Mart for Intensive Care III (MIMIC-III)(v1.4) publicly available dataset, which includes all patients admitted to anICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for thebenchmarking tasks. Our results show that deep learning models consistentlyoutperform all the other approaches especially when the `raw' clinical timeseries data is used as input features to the models.",http://arxiv.org/abs/1710.08531v1,,
711,"Incremental and Iterative Learning of Answer Set Programs from Mutually  Distinct Examples","Over the years the Artificial Intelligence (AI) community has producedseveral datasets which have given the machine learning algorithms theopportunity to learn various skills across various domains. However, a subclassof these machine learning algorithms that aimed at learning logic programs,namely the Inductive Logic Programming algorithms, have often failed at thetask due to the vastness of these datasets. This has impacted the usability ofknowledge representation and reasoning techniques in the development of AIsystems. In this research, we try to address this scalability issue for thealgorithms that learn answer set programs. We present a sound and completealgorithm which takes the input in a slightly different manner and performs anefficient and more user controlled search for a solution. We show viaexperiments that our algorithm can learn from two popular datasets from machinelearning community, namely bAbl (a question answering dataset) and MNIST (adataset for handwritten digit recognition), which to the best of our knowledgewas not previously possible. The system is publicly available athttps://goo.gl/KdWAcV. This paper is under consideration for acceptance inTPLP.",http://arxiv.org/abs/1802.07966v2,,
712,"Proximal SCOPE for Distributed Sparse Learning: Better Data Partition  Implies Faster Convergence Rate","Distributed sparse learning with a cluster of multiple machines has attractedmuch attention in machine learning, especially for large-scale applicationswith high-dimensional data. One popular way to implement sparse learning is touse $L_1$ regularization. In this paper, we propose a novel method, calledproximal \mbox{SCOPE}~(\mbox{pSCOPE}), for distributed sparse learning with$L_1$ regularization. pSCOPE is based on a \underline{c}ooperative\underline{a}utonomous \underline{l}ocal \underline{l}earning~(\mbox{CALL})framework. In the \mbox{CALL} framework of \mbox{pSCOPE}, we find that the datapartition affects the convergence of the learning procedure, and subsequentlywe define a metric to measure the goodness of a data partition. Based on thedefined metric, we theoretically prove that pSCOPE is convergent with a linearconvergence rate if the data partition is good enough. We also prove thatbetter data partition implies faster convergence rate. Furthermore, pSCOPE isalso communication efficient. Experimental results on real data sets show thatpSCOPE can outperform other state-of-the-art distributed methods for sparselearning.",http://arxiv.org/abs/1803.05621v2,,
713,Deep Learning in the Wild,"Deep learning with neural networks is applied by an increasing number ofpeople outside of classic research environments, due to the vast success of themethodology on a wide range of machine perception tasks. While this interest isfueled by beautiful success stories, practical work in deep learning on noveltasks without existing baselines remains challenging. This paper explores thespecific challenges arising in the realm of real world tasks, based on casestudies from research \& development in conjunction with industry, and extractslessons learned from them. It thus fills a gap between the publication oflatest algorithmic and methodical developments, and the usually omittednitty-gritty of how to make them work. Specifically, we give insight into deeplearning projects on face matching, print media monitoring, industrial qualitycontrol, music scanning, strategy game playing, and automated machine learning,thereby providing best practices for deep learning in practice.",http://arxiv.org/abs/1807.04950v1,,
714,Knowledge-based Transfer Learning Explanation,"Machine learning explanation can significantly boost machine learning'sapplication in decision making, but the usability of current methods is limitedin human-centric explanation, especially for transfer learning, an importantmachine learning branch that aims at utilizing knowledge from one learningdomain (i.e., a pair of dataset and prediction task) to enhance predictionmodel training in another learning domain. In this paper, we propose anontology-based approach for human-centric explanation of transfer learning.Three kinds of knowledge-based explanatory evidence, with differentgranularities, including general factors, particular narrators and corecontexts are first proposed and then inferred with both local ontologies andexternal knowledge bases. The evaluation with US flight data and DBpedia haspresented their confidence and availability in explaining the transferabilityof feature representation in flight departure delay forecasting.",http://arxiv.org/abs/1807.08372v1,,
715,The implicit fairness criterion of unconstrained learning,"We clarify what fairness guarantees we can and cannot expect to follow fromunconstrained machine learning. Specifically, we characterize whenunconstrained learning on its own implies group calibration, that is, theoutcome variable is conditionally independent of group membership given thescore. We show that under reasonable conditions, the deviation from satisfyinggroup calibration is upper bounded by the excess risk of the learned scorerelative to the Bayes optimal score function. A lower bound confirms theoptimality of our upper bound. Moreover, we prove that as the excess risk ofthe learned score decreases, it strongly violates separation and independence,two other standard fairness criteria.  Our results show that group calibration is the fairness criterion thatunconstrained learning implicitly favors. On the one hand, this means thatcalibration is often satisfied on its own without the need for activeintervention, albeit at the cost of violating other criteria that are at oddswith calibration. On the other hand, it suggests that we should be satisfiedwith calibration as a fairness criterion only if we are at ease with the use ofunconstrained machine learning in a given application.",http://arxiv.org/abs/1808.10013v2,,
716,Deep learning for in vitro prediction of pharmaceutical formulations,"Current pharmaceutical formulation development still strongly relies on thetraditional trial-and-error approach by individual experiences ofpharmaceutical scientists, which is laborious, time-consuming and costly.Recently, deep learning has been widely applied in many challenging domainsbecause of its important capability of automatic feature extraction. The aim ofthis research is to use deep learning to predict pharmaceutical formulations.In this paper, two different types of dosage forms were chosen as modelsystems. Evaluation criteria suitable for pharmaceutics were applied toassessing the performance of the models. Moreover, an automatic datasetselection algorithm was developed for selecting the representative data asvalidation and test datasets. Six machine learning methods were compared withdeep learning. The result shows the accuracies of both two deep neural networkswere above 80% and higher than other machine learning models, which showed goodprediction in pharmaceutical formulations. In summary, deep learning with theautomatic data splitting algorithm and the evaluation criteria suitable forpharmaceutical formulation data was firstly developed for the prediction ofpharmaceutical formulations. The cross-disciplinary integration ofpharmaceutics and artificial intelligence may shift the paradigm ofpharmaceutical researches from experience-dependent studies to data-drivenmethodologies.",http://arxiv.org/abs/1809.02069v1,,
717,Biconvex Landscape In SDP-Related Learning,"Many machine learning problems can be reduced to learning a low-rank positivesemidefinite matrix (denoted as $Z$), which encounters semidefinite program(SDP). Existing SDP solvers are often expensive for large-scale learning. Toavoid directly solving SDP, some works convert SDP into a nonconvex program byfactorizing $Z$ as $XX^\top$. However, this would bring higher-ordernonlinearity, resulting in scarcity of structure in subsequent optimization. Inthis paper, we propose a novel surrogate for SDP-related learning, in which thestructure of subproblem is exploited. More specifically, we surrogateunconstrained SDP by a biconvex problem, through factorizing $Z$ as $XY^\top$and using a Courant penalty to penalize the difference of $X$ and $Y$, in whichthe resultant subproblems are convex. Furthermore, we provide a theoreticalbound for the associated penalty parameter under the assumption that theobjective function is Lipschitz-smooth, such that the proposed surrogate willsolve the original SDP when the penalty parameter is larger than this bound.Experiments on two SDP-related machine learning applications demonstrate thatthe proposed algorithm is as accurate as the state-of-the-art, but is faster onlarge-scale learning.",http://arxiv.org/abs/1811.01198v1,,
718,"Integrating Reinforcement Learning to Self Training for Pulmonary Nodule  Segmentation in Chest X-rays","Machine learning applications in medical imaging are frequently limited bythe lack of quality labeled data. In this paper, we explore the self trainingmethod, a form of semi-supervised learning, to address the labeling burden. Byintegrating reinforcement learning, we were able to expand the application ofself training to complex segmentation networks without any further humanannotation. The proposed approach, reinforced self training (ReST), fine tunesa semantic segmentation networks by introducing a policy network that learns togenerate pseudolabels. We incorporate an expert demonstration network, based oninverse reinforcement learning, to enhance clinical validity and convergence ofthe policy network. The model was tested on a pulmonary nodule segmentationtask in chest X-rays and achieved the performance of a standard U-Net whileusing only 50% of the labeled data, by exploiting unlabeled data. When the samenumber of labeled data was used, a moderate to significant cross validationaccuracy improvement was achieved depending on the absolute number of labelsused.",http://arxiv.org/abs/1811.08840v1,,
719,A Hybrid Instance-based Transfer Learning Method,"In recent years, supervised machine learning models have demonstratedtremendous success in a variety of application domains. Despite the promisingresults, these successful models are data hungry and their performance reliesheavily on the size of training data. However, in many healthcare applicationsit is difficult to collect sufficiently large training datasets. Transferlearning can help overcome this issue by transferring the knowledge fromreadily available datasets (source) to a new dataset (target). In this work, wepropose a hybrid instance-based transfer learning method that outperforms a setof baselines including state-of-the-art instance-based transfer learningapproaches. Our method uses a probabilistic weighting strategy to fuseinformation from the source domain to the model learned in the target domain.Our method is generic, applicable to multiple source domains, and robust withrespect to negative transfer. We demonstrate the effectiveness of our approachthrough extensive experiments for two different applications.",http://arxiv.org/abs/1812.01063v1,,
720,Machine Learning for Molecular Dynamics on Long Timescales,"Molecular Dynamics (MD) simulation is widely used to analyze the propertiesof molecules and materials. Most practical applications, such as comparisonwith experimental measurements, designing drug molecules, or optimizingmaterials, rely on statistical quantities, which may be prohibitively expensiveto compute from direct long-time MD simulations. Classical Machine Learning(ML) techniques have already had a profound impact on the field, especially forlearning low-dimensional models of the long-time dynamics and for devising moreefficient sampling schemes for computing long-time statistics. Novel ML methodshave the potential to revolutionize long-timescale MD and to obtaininterpretable models. ML concepts such as statistical estimator theory,end-to-end learning, representation learning and active learning are highlyinteresting for the MD researcher and will help to develop new solutions tohard MD problems. With the aim of better connecting the MD and ML researchareas and spawning new research on this interface, we define the learningproblems in long-timescale MD, present successful approaches and outline someof the unsolved ML problems in this application field.",http://arxiv.org/abs/1812.07669v1,,
721,"From Language to Goals: Inverse Reinforcement Learning for Vision-Based  Instruction Following","Reinforcement learning is a promising framework for solving control problems,but its use in practical situations is hampered by the fact that rewardfunctions are often difficult to engineer. Specifying goals and tasks forautonomous machines, such as robots, is a significant challenge:conventionally, reward functions and goal states have been used to communicateobjectives. But people can communicate objectives to each other simply bydescribing or demonstrating them. How can we build learning algorithms thatwill allow us to tell machines what we want them to do? In this work, weinvestigate the problem of grounding language commands as reward functionsusing inverse reinforcement learning, and argue that language-conditionedrewards are more transferable than language-conditioned policies to newenvironments. We propose language-conditioned reward learning (LC-RL), whichgrounds language commands as a reward function represented by a deep neuralnetwork. We demonstrate that our model learns rewards that transfer to noveltasks and environments on realistic, high-dimensional visual environments withnatural language commands, whereas directly learning a language-conditionedpolicy leads to poor performance.",http://arxiv.org/abs/1902.07742v1,,
722,Crowd-Machine Collaboration for Item Screening,"In this paper we describe how crowd and machine classifier can be efficientlycombined to screen items that satisfy a set of predicates. We show that this isa recurring problem in many domains, present machine-human (hybrid) algorithmsthat screen items efficiently and estimate the gain over human-only ormachine-only screening in terms of performance and cost.",http://arxiv.org/abs/1803.07947v1,,
723,End-to-End Deep Reinforcement Learning for Lane Keeping Assist,"Reinforcement learning is considered to be a strong AI paradigm which can beused to teach machines through interaction with the environment and learningfrom their mistakes, but it has not yet been successfully used for automotiveapplications. There has recently been a revival of interest in the topic,however, driven by the ability of deep learning algorithms to learn goodrepresentations of the environment. Motivated by Google DeepMind's successfuldemonstrations of learning for games from Breakout to Go, we will proposedifferent methods for autonomous driving using deep reinforcement learning.This is of particular interest as it is difficult to pose autonomous driving asa supervised learning problem as it has a strong interaction with theenvironment including other vehicles, pedestrians and roadworks. As this is arelatively new area of research for autonomous driving, we will formulate twomain categories of algorithms: 1) Discrete actions category, and 2) Continuousactions category. For the discrete actions category, we will deal with DeepQ-Network Algorithm (DQN) while for the continuous actions category, we willdeal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition tothat, We will also discover the performance of these two categories on an opensource car simulator for Racing called (TORCS) which stands for The Open Racingcar Simulator. Our simulation results demonstrate learning of autonomousmaneuvering in a scenario of complex road curvatures and simple interactionwith other vehicles. Finally, we explain the effect of some restrictedconditions, put on the car during the learning phase, on the convergence timefor finishing its learning phase.",http://arxiv.org/abs/1612.04340v1,,
724,Computational Theories of Curiosity-Driven Learning,"What are the functions of curiosity? What are the mechanisms ofcuriosity-driven learning? We approach these questions about the living usingconcepts and tools from machine learning and developmental robotics. We arguethat curiosity-driven learning enables organisms to make discoveries to solvecomplex problems with rare or deceptive rewards. By fostering exploration anddiscovery of a diversity of behavioural skills, and ignoring these rewards,curiosity can be efficient to bootstrap learning when there is no information,or deceptive information, about local improvement towards these problems. Wealso explain the key role of curiosity for efficient learning of world models.We review both normative and heuristic computational frameworks used tounderstand the mechanisms of curiosity in humans, conceptualizing the child asa sense-making organism. These frameworks enable us to discuss thebi-directional causal links between curiosity and learning, and to provide newhypotheses about the fundamental role of curiosity in self-organizingdevelopmental structures through curriculum learning. We present variousdevelopmental robotics experiments that study these mechanisms in action, bothsupporting these hypotheses to understand better curiosity in humans andopening new research avenues in machine learning and artificial intelligence.Finally, we discuss challenges for the design of experimental paradigms forstudying curiosity in psychology and cognitive neuroscience.  Keywords: Curiosity, intrinsic motivation, lifelong learning, predictions,world model, rewards, free-energy principle, learning progress, machinelearning, AI, developmental robotics, development, curriculum learning,self-organization.",http://arxiv.org/abs/1802.10546v2,,
725,Generalization Analysis for Game-Theoretic Machine Learning,"For Internet applications like sponsored search, cautions need to be takenwhen using machine learning to optimize their mechanisms (e.g., auction) sinceself-interested agents in these applications may change their behaviors (andthus the data distribution) in response to the mechanisms. To tackle thisproblem, a framework called game-theoretic machine learning (GTML) was recentlyproposed, which first learns a Markov behavior model to characterize agents'behaviors, and then learns the optimal mechanism by simulating agents' behaviorchanges in response to the mechanism. While GTML has demonstrated practicalsuccess, its generalization analysis is challenging because the behavior dataare non-i.i.d. and dependent on the mechanism. To address this challenge,first, we decompose the generalization error for GTML into the behaviorlearning error and the mechanism learning error; second, for the behaviorlearning error, we obtain novel non-asymptotic error bounds for both parametricand non-parametric behavior learning methods; third, for the mechanism learningerror, we derive a uniform convergence bound based on a new concept callednested covering number of the mechanism space and the generalization analysistechniques developed for mixing sequences. To the best of our knowledge, thisis the first work on the generalization analysis of GTML, and we believe it hasgeneral implications to the theoretical analysis of other complicated machinelearning problems.",http://arxiv.org/abs/1410.3341v1,,
726,"Learning a Code: Machine Learning for Approximate Non-Linear Coded  Computation","Machine learning algorithms are typically run on large scale, distributedcompute infrastructure that routinely face a number of unavailabilities such asfailures and temporary slowdowns. Adding redundant computations usingcoding-theoretic tools called ""codes"" is an emerging technique to alleviate theadverse effects of such unavailabilities. A code consists of an encodingfunction that proactively introduces redundant computation and a decodingfunction that reconstructs unavailable outputs using the available ones. Pastwork focuses on using codes to provide resilience for linear computations andspecific iterative optimization algorithms. However, computations performed fora variety of applications including inference on state-of-the-art machinelearning algorithms, such as neural networks, typically fall outside thisrealm. In this paper, we propose taking a learning-based approach to designingcodes that can handle non-linear computations. We present carefully designedneural network architectures and a training methodology for learning encodingand decoding functions that produce approximate reconstructions of unavailablecomputation results. We present extensive experimental results demonstratingthe effectiveness of the proposed approach: we show that the our learned codescan accurately reconstruct $64 - 98\%$ of the unavailable predictions fromneural-network based image classifiers on the MNIST, Fashion-MNIST, andCIFAR-10 datasets. To the best of our knowledge, this work proposes the firstlearning-based approach for designing codes, and also presents the firstcoding-theoretic solution that can provide resilience for any non-linear(differentiable) computation. Our results show that learning can be aneffective technique for designing codes, and that learned codes are a highlypromising approach for bringing the benefits of coding to non-linearcomputations.",http://arxiv.org/abs/1806.01259v1,,
727,Dynamic Difficulty Awareness Training for Continuous Emotion Prediction,"Time-continuous emotion prediction has become an increasingly compelling taskin machine learning. Considerable efforts have been made to advance theperformance of these systems. Nonetheless, the main focus has been thedevelopment of more sophisticated models and the incorporation of differentexpressive modalities (e. g., speech, face, and physiology). In this paper,motivated by the benefit of difficulty awareness in a human learning procedure,we propose a novel machine learning framework, namely, Dynamic DifficultyAwareness Training (DDAT), which sheds fresh light on the research -- directlyexploiting the difficulties in learning to boost the machine learning process.The DDAT framework consists of two stages: information retrieval andinformation exploitation. In the first stage, we make use of the reconstructionerror of input features or the annotation uncertainty to estimate thedifficulty of learning specific information. The obtained difficulty level isthen used in tandem with original features to update the model input in asecond learning stage with the expectation that the model can learn to focus onhigh difficulty regions of the learning process. We perform extensiveexperiments on a benchmark database (RECOLA) to evaluate the effectiveness ofthe proposed framework. The experimental results show that our approachoutperforms related baselines as well as other well-established time-continuousemotion prediction systems, which suggests that dynamically integrating thedifficulty information for neural networks can help enhance the learningprocess.",http://arxiv.org/abs/1810.05507v1,,
728,Structured Sparsity and Generalization,"We present a data dependent generalization bound for a large class ofregularized algorithms which implement structured sparsity constraints. Thebound can be applied to standard squared-norm regularization, the Lasso, thegroup Lasso, some versions of the group Lasso with overlapping groups, multiplekernel learning and other regularization schemes. In all these casescompetitive results are obtained. A novel feature of our bound is that it canbe applied in an infinite dimensional setting such as the Lasso in a separableHilbert space or multiple kernel learning with a countable number of kernels.",http://arxiv.org/abs/1108.3476v2,,
729,Infinite Mixtures of Multivariate Gaussian Processes,"This paper presents a new model called infinite mixtures of multivariateGaussian processes, which can be used to learn vector-valued functions andapplied to multitask learning. As an extension of the single multivariateGaussian process, the mixture model has the advantages of modeling multimodaldata and alleviating the computationally cubic complexity of the multivariateGaussian process. A Dirichlet process prior is adopted to allow the (possiblyinfinite) number of mixture components to be automatically inferred fromtraining data, and Markov chain Monte Carlo sampling techniques are used forparameter and latent variable inference. Preliminary experimental results onmultivariate regression show the feasibility of the proposed model.",http://arxiv.org/abs/1307.7028v1,,
730,"Key point selection and clustering of swimmer coordination through  Sparse Fisher-EM","To answer the existence of optimal swimmer learning/teaching strategies, thiswork introduces a two-level clustering in order to analyze temporal dynamics ofmotor learning in breaststroke swimming. Each level have been performed throughSparse Fisher-EM, a unsupervised framework which can be applied efficiently onlarge and correlated datasets. The induced sparsity selects key points of thecoordination phase without any prior knowledge.",http://arxiv.org/abs/1401.1489v1,,
731,Support vector comparison machines,"In ranking problems, the goal is to learn a ranking function from labeledpairs of input points. In this paper, we consider the related comparisonproblem, where the label indicates which element of the pair is better, or ifthere is no significant difference. We cast the learning problem as a marginmaximization, and show that it can be solved by converting it to a standardSVM. We use simulated nonlinear patterns, a real learning to rank sushi dataset, and a chess data set to show that our proposed SVMcompare algorithmoutperforms SVMrank when there are equality pairs.",http://arxiv.org/abs/1401.8008v2,,
732,A Tutorial on Independent Component Analysis,"Independent component analysis (ICA) has become a standard data analysistechnique applied to an array of problems in signal processing and machinelearning. This tutorial provides an introduction to ICA based on linear algebraformulating an intuition for ICA from first principles. The goal of thistutorial is to provide a solid foundation on this advanced topic so that onemight learn the motivation behind ICA, learn why and when to apply thistechnique and in the process gain an introduction to this exciting field ofactive research.",http://arxiv.org/abs/1404.2986v1,,
733,Learning with incremental iterative regularization,"Within a statistical learning setting, we propose and study an iterativeregularization algorithm for least squares defined by an incremental gradientmethod. In particular, we show that, if all other parameters are fixed apriori, the number of passes over the data (epochs) acts as a regularizationparameter, and prove strong universal consistency, i.e. almost sure convergenceof the risk, as well as sharp finite sample bounds for the iterates. Ourresults are a step towards understanding the effect of multiple epochs instochastic gradient techniques in machine learning and rely on integratingstatistical and optimization results.",http://arxiv.org/abs/1405.0042v2,,
734,Machine Learning Techniques in Cognitive Radio Networks,"Cognitive radio is an intelligent radio that can be programmed and configureddynamically to fully use the frequency resources that are not used by licensedusers. It defines the radio devices that are capable of learning and adaptingto their transmission to the external radio environment, which means it hassome kind of intelligence for monitoring the radio environment, learning theenvironment and make smart decisions. In this paper, we are reviewing someexamples of the usage of machine learning techniques in cognitive radionetworks for implementing the intelligent radio.",http://arxiv.org/abs/1410.3145v1,,
735,Machine Learning of Coq Proof Guidance: First Experiments,"We report the results of the first experiments with learning proofdependencies from the formalizations done with the Coq system. We explain theprocess of obtaining the dependencies from the Coq proofs, the characterizationof formulas that is used for the learning, and the evaluation method. Variousmachine learning methods are compared on a dataset of 5021 toplevel Coq proofscoming from the CoRN repository. The best resulting method covers on average75% of the needed proof dependencies among the first 100 predictions, which isa comparable performance of such initial experiments on other large-theorycorpora.",http://arxiv.org/abs/1410.5467v1,,
736,"Vector-valued Reproducing Kernel Banach Spaces with Applications to  Multi-task Learning","Motivated by multi-task machine learning with Banach spaces, we propose thenotion of vector-valued reproducing kernel Banach spaces (RKBS). Basicproperties of the spaces and the associated reproducing kernels areinvestigated. We also present feature map constructions and several concreteexamples of vector-valued RKBS. The theory is then applied to multi-taskmachine learning. Especially, the representer theorem and characterizationequations for the minimizer of regularized learning schemes in vector-valuedRKBS are established.",http://arxiv.org/abs/1111.1037v2,,
737,Predicting online user behaviour using deep learning algorithms,"We propose a robust classifier to predict buying intentions based on userbehaviour within a large e-commerce website. In this work we comparetraditional machine learning techniques with the most advanced deep learningapproaches. We show that both Deep Belief Networks and Stacked Denoisingauto-Encoders achieved a substantial improvement by extracting features fromhigh dimensional data during the pre-train phase. They prove also to be moreconvenient to deal with severe class imbalance.",http://arxiv.org/abs/1511.06247v3,,
738,'Almost Sure' Chaotic Properties of Machine Learning Methods,"It has been demonstrated earlier that universal computation is 'almostsurely' chaotic. Machine learning is a form of computational fixed pointiteration, iterating over the computable function space. We showcase someproperties of this iteration, and establish in general that the iteration is'almost surely' of chaotic nature. This theory explains the observation in thecounter intuitive properties of deep learning methods. This paper demonstratesthat these properties are going to be universal to any learning method.",http://arxiv.org/abs/1407.7417v1,,
739,IllinoisSL: A JAVA Library for Structured Prediction,"IllinoisSL is a Java library for learning structured prediction models. Itsupports structured Support Vector Machines and structured Perceptron. Thelibrary consists of a core learning module and several applications, which canbe executed from command-lines. Documentation is provided to guide users. InComparison to other structured learning libraries, IllinoisSL is efficient,general, and easy to use.",http://arxiv.org/abs/1509.07179v1,,
740,Can Boosting with SVM as Week Learners Help?,"Object recognition in images involves identifying objects with partialocclusions, viewpoint changes, varying illumination, cluttered backgrounds.Recent work in object recognition uses machine learning techniques SVM-KNN,Local Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, wewant to utilize SVM as week learners in AdaBoost. Experiments are done withclassifiers like near- est neighbor, k-nearest neighbor, Support vectormachines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariantdescriptors and Pyramid his- togram of gradient descriptors. AdaBoost istrained with set of week classifier as SVMs, each with kernel distance functionon different descriptors. Results shows AdaBoost with SVM outperform othermethods for Object Categorization dataset.",http://arxiv.org/abs/1604.05242v2,,
741,Online Machine Learning Techniques for Predicting Operator Performance,"This thesis explores a number of online machine learning algorithms. From atheoret- ical perspective, it assesses their employability for a particularfunction approximation problem where the analytical models fall short.Furthermore, it discusses the applica- tion of theoretically suitable learningalgorithms to the function approximation problem at hand through an efficientimplementation that exploits various computational and mathematical shortcuts.Finally, this thesis work evaluates the implemented learning algorithmsaccording to various evaluation criteria through rigorous testing.",http://arxiv.org/abs/1605.01029v1,,
742,Machine Learning Techniques for Stackelberg Security Games: a Survey,"The present survey aims at presenting the current machine learning techniquesemployed in security games domains. Specifically, we focused on papers andworks developed by the Teamcore of University of Southern California, whichdeepened different directions in this field. After a brief introduction onStackelberg Security Games (SSGs) and the poaching setting, the rest of thework presents how to model a boundedly rational attacker taking into accounther human behavior, then describes how to face the problem of having attacker'spayoffs not defined and how to estimate them and, finally, presents how onlinelearning techniques have been exploited to learn a model of the attacker.",http://arxiv.org/abs/1609.09341v1,,
743,Spatial contrasting for deep unsupervised learning,"Convolutional networks have marked their place over the last few years as thebest performing model for various visual tasks. They are, however, most suitedfor supervised learning from large amounts of labeled data. Previous attemptshave been made to use unlabeled data to improve model performance by applyingunsupervised techniques. These attempts require different architectures andtraining methods. In this work we present a novel approach for unsupervisedtraining of Convolutional networks that is based on contrasting between spatialregions within images. This criterion can be employed within conventionalneural networks and trained using standard techniques such as SGD andback-propagation, thus complementing supervised methods.",http://arxiv.org/abs/1611.06996v1,,
744,"Manifold Alignment Determination: finding correspondences across  different data views","We present Manifold Alignment Determination (MAD), an algorithm for learningalignments between data points from multiple views or modalities. The approachis capable of learning correspondences between views as well as correspondencesbetween individual data-points. The proposed method requires only a few alignedexamples from which it is capable to recover a global alignment through aprobabilistic model. The strong, yet flexible regularization provided by thegenerative model is sufficient to align the views. We provide experiments onboth synthetic and real data to highlight the benefit of the proposed approach.",http://arxiv.org/abs/1701.03449v1,,
745,Learning Rates for Kernel-Based Expectile Regression,"Conditional expectiles are becoming an increasingly important tool in financeas well as in other areas of applications. We analyse a support vector machinetype approach for estimating conditional expectiles and establish learningrates that are minimax optimal modulo a logarithmic factor if Gaussian RBFkernels are used and the desired expectile is smooth in a Besov sense. As aspecial case, our learning rates improve the best known rates for kernel-basedleast squares regression in this scenario. Key ingredients of our statisticalanalysis are a general calibration inequality for the asymmetric least squaresloss, a corresponding variance bound as well as an improved entropy numberbound for Gaussian RBF kernels.",http://arxiv.org/abs/1702.07552v2,,
746,Changing Model Behavior at Test-Time Using Reinforcement Learning,"Machine learning models are often used at test-time subject to constraintsand trade-offs not present at training-time. For example, a computer visionmodel operating on an embedded device may need to perform real-time inference,or a translation model operating on a cell phone may wish to bound its averagecompute time in order to be power-efficient. In this work we describe amixture-of-experts model and show how to change its test-time resource-usage ona per-input basis using reinforcement learning. We test our method on a smallMNIST-based example.",http://arxiv.org/abs/1702.07780v1,,
747,Ambiguity set and learning via Bregman and Wasserstein,"Construction of ambiguity set in robust optimization relies on the choice ofdivergences between probability distributions. In distribution learning,choosing appropriate probability distributions based on observed data iscritical for approximating the true distribution. To improve the performance ofmachine learning models, there has recently been interest in designingobjective functions based on Lp-Wasserstein distance rather than the classicalKullback-Leibler (KL) divergence. In this paper, we derive concentration andasymptotic results using Bregman divergence. We propose a novel asymmetricstatistical divergence called Wasserstein-Bregman divergence as ageneralization of L2-Wasserstein distance. We discuss how these results can beapplied to the construction of ambiguity set in robust optimization.",http://arxiv.org/abs/1705.08056v1,,
748,Bayesian LSTMs in medicine,"The medical field stands to see significant benefits from the recent advancesin deep learning. Knowing the uncertainty in the decision made by any machinelearning algorithm is of utmost importance for medical practitioners. Thisstudy demonstrates the utility of using Bayesian LSTMs for classification ofmedical time series. Four medical time series datasets are used to show theaccuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, weshow cherry-picked examples of confident and uncertain classifications of themedical time series. With simple modifications of the common practice for deeplearning, significant improvements can be made for the medical practitioner andpatient.",http://arxiv.org/abs/1706.01242v1,,
749,"A Statistical Approach to Increase Classification Accuracy in Supervised  Learning Algorithms","Probabilistic mixture models have been widely used for different machinelearning and pattern recognition tasks such as clustering, dimensionalityreduction, and classification. In this paper, we focus on trying to solve themost common challenges related to supervised learning algorithms by usingmixture probability distribution functions. With this modeling strategy, weidentify sub-labels and generate synthetic data in order to reach betterclassification accuracy. It means we focus on increasing the training datasynthetically to increase the classification accuracy.",http://arxiv.org/abs/1709.01439v1,,
750,Power Plant Performance Modeling with Concept Drift,"Power plant is a complex and nonstationary system for which the traditionalmachine learning modeling approaches fall short of expectations. Theensemble-based online learning methods provide an effective way to continuouslylearn from the dynamic environment and autonomously update models to respond toenvironmental changes. This paper proposes such an online ensemble regressionapproach to model power plant performance, which is critically important foroperation optimization. The experimental results on both simulated and realdata show that the proposed method can achieve performance with less than 1%mean average percentage error, which meets the general expectations in fieldoperations.",http://arxiv.org/abs/1710.07314v1,,
751,"Unsupervised learning of dynamical and molecular similarity using  variance minimization","In this report, we present an unsupervised machine learning method fordetermining groups of molecular systems according to similarity in theirdynamics or structures using Ward's minimum variance objective function. Wefirst apply the minimum variance clustering to a set of simulated tripeptidesusing the information theoretic Jensen-Shannon divergence between Markoviantransition matrices in order to gain insight into how point mutations affectprotein dynamics. Then, we extend the method to partition two chemoinformaticdatasets according to structural similarity to motivate a train/validation/testsplit for supervised learning that avoids overfitting.",http://arxiv.org/abs/1712.07704v1,,
752,Predicting Movie Genres Based on Plot Summaries,"This project explores several Machine Learning methods to predict moviegenres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and RecurrentNeural Networks are used for text classification, while K-binarytransformation, rank method and probabilistic classification with learnedprobability threshold are employed for the multi-label problem involved in thegenre tagging task.Experiments with more than 250,000 movies show thatemploying the Gated Recurrent Units (GRU) neural networks for the probabilisticclassification with learned probability threshold approach achieves the bestresult on the test set. The model attains a Jaccard Index of 50.0%, a F-scoreof 0.56, and a hit rate of 80.5%.",http://arxiv.org/abs/1801.04813v1,,
753,VBALD - Variational Bayesian Approximation of Log Determinants,"Evaluating the log determinant of a positive definite matrix is ubiquitous inmachine learning. Applications thereof range from Gaussian processes,minimum-volume ellipsoids, metric learning, kernel learning, Bayesian neuralnetworks, Determinental Point Processes, Markov random fields to partitionfunctions of discrete graphical models. In order to avoid the canonical, yetprohibitive, Cholesky $\mathcal{O}(n^{3})$ computational cost, we propose anovel approach, with complexity $\mathcal{O}(n^{2})$, based on a constrainedvariational Bayes algorithm. We compare our method to Taylor, Chebyshev andLanczos approaches and show state of the art performance on both synthetic andreal-world datasets.",http://arxiv.org/abs/1802.08054v1,,
754,Learning Device Models with Recurrent Neural Networks,"Recurrent neural networks (RNNs) are powerful constructs capable of modelingcomplex systems, up to and including Turing Machines. However, learning suchcomplex models from finite training sets can be difficult. In this paper weempirically show that RNNs can learn models of computer peripheral devicesthrough input and output state observation. This enables automated developmentof functional software-only models of hardware devices. Such models areapplicable to any number of tasks, including device validation, driverdevelopment, code de-obfuscation, and reverse engineering. We show that thesame RNN structure successfully models six different devices from simple testcircuits up to a 16550 UART serial port, and verify that these models arecapable of producing equivalent output to real hardware.",http://arxiv.org/abs/1805.07869v1,,
755,Metric-Optimized Example Weights,"Real-world machine learning applications often have complex test metrics, andmay have training and test data that follow different distributions. We proposeaddressing these issues by using a weighted loss function with a standardconvex loss, but with weights on the training examples that are learned tooptimize the test metric of interest on the validation set. Thesemetric-optimized example weights can be learned for any test metric, includingblack box losses and customized metrics for specific applications. Weillustrate the performance of our proposal with public benchmark datasets andreal-world applications with domain shift and custom loss functions thatbalance multiple objectives, impose fairness policies, and are non-convex andnon-decomposable.",http://arxiv.org/abs/1805.10582v1,,
756,On Machine Learning and Structure for Mobile Robots,"Due to recent advances - compute, data, models - the role of learning inautonomous systems has expanded significantly, rendering new applicationspossible for the first time. While some of the most significant benefits areobtained in the perception modules of the software stack, other aspectscontinue to rely on known manual procedures based on prior knowledge ongeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance inthese modules when data collection and curation become easier than manual ruledesign. Building on this coarse and broad survey of current research, the finalsections aim to provide insights into future potentials and challenges as wellas the necessity of structure in current practical applications.",http://arxiv.org/abs/1806.06003v1,,
757,How Could Polyhedral Theory Harness Deep Learning?,"The holy grail of deep learning is to come up with an automatic method todesign optimal architectures for different applications. In other words, howcan we effectively dimension and organize neurons along the network layersbased on the computational resources, input size, and amount of training data?We outline promising research directions based on polyhedral theory andmixed-integer representability that may offer an analytical approach to thisquestion, in contrast to the empirical techniques often employed.",http://arxiv.org/abs/1806.06365v1,,
758,Learning Representations of Missing Data for Predicting Patient Outcomes,"Extracting actionable insight from Electronic Health Records (EHRs) posesseveral challenges for traditional machine learning approaches. Patients areoften missing data relative to each other; the data comes in a variety ofmodalities, such as multivariate time series, free text, and categoricaldemographic information; important relationships among patients can bedifficult to detect; and many others. In this work, we propose a novel approachto address these first three challenges using a representation learning schemebased on message passing. We show that our proposed approach is competitivewith or outperforms the state of the art for predicting in-hospital mortality(binary classification), the length of hospital visits (regression) and thedischarge destination (multiclass classification).",http://arxiv.org/abs/1811.04752v1,,
759,Model-Based Reinforcement Learning for Sepsis Treatment,"Sepsis is a dangerous condition that is a leading cause of patient mortality.Treating sepsis is highly challenging, because individual patients respond verydifferently to medical interventions and there is no universally agreed-upontreatment for sepsis. In this work, we explore the use of continuousstate-space model-based reinforcement learning (RL) to discover high-qualitytreatment policies for sepsis patients. Our quantitative evaluation revealsthat by blending the treatment strategy discovered with RL with what cliniciansfollow, we can obtain improved policies, potentially allowing for bettermedical treatment for sepsis.",http://arxiv.org/abs/1811.09602v1,,
760,Cluster-Based Learning from Weakly Labeled Bags in Digital Pathology,"To alleviate the burden of gathering detailed expert annotations whentraining deep neural networks, we propose a weakly supervised learning approachto recognize metastases in microscopic images of breast lymph nodes. Wedescribe an alternative training loss which clusters weakly labeled bags inlatent space to inform relevance of patch-instances during training of aconvolutional neural network. We evaluate our method on the Camelyon datasetwhich contains high-resolution digital slides of breast lymph nodes, wherelabels are provided at the image-level and only subsets of patches are madeavailable during training.",http://arxiv.org/abs/1812.00884v1,,
761,"Adversarial Attacks, Regression, and Numerical Stability Regularization","Adversarial attacks against neural networks in a regression setting are acritical yet understudied problem. In this work, we advance the state of theart by investigating adversarial attacks against regression networks and byformulating a more effective defense against these attacks. In particular, wetake the perspective that adversarial attacks are likely caused by numericalinstability in learned functions. We introduce a stability inducing,regularization based defense against adversarial attacks in the regressionsetting. Our new and easy to implement defense is shown to outperform priorapproaches and to improve the numerical stability of learned functions.",http://arxiv.org/abs/1812.02885v1,,
762,SMART: An Open Source Data Labeling Platform for Supervised Learning,"SMART is an open source web application designed to help data scientists andresearch teams efficiently build labeled training data sets for supervisedmachine learning tasks. SMART provides users with an intuitive interface forcreating labeled data sets, supports active learning to help reduce therequired amount of labeled data, and incorporates inter-rater reliabilitystatistics to provide insight into label quality. SMART is designed to beplatform agnostic and easily deployable to meet the needs of as many differentresearch teams as possible. The project website contains links to the coderepository and extensive user documentation.",http://arxiv.org/abs/1812.06591v1,,
763,Improved Accounting for Differentially Private Learning,"We consider the problem of differential privacy accounting, i.e. estimationof privacy loss bounds, in machine learning in a broad sense. We propose twoversions of a generic privacy accountant suitable for a wide range of learningalgorithms. Both versions are derived in a simple and principled way usingwell-known tools from probability theory, such as concentration inequalities.We demonstrate that our privacy accountant is able to achieve state-of-the-artestimates of DP guarantees and can be applied to new areas like variationalinference. Moreover, we show that the latter enjoys differential privacy atminor cost.",http://arxiv.org/abs/1901.09697v1,,
764,Towards Aggregating Weighted Feature Attributions,"Current approaches for explaining machine learning models fall into twodistinct classes: antecedent event influence and value attribution. The formerleverages training instances to describe how much influence a training pointexerts on a test point, while the latter attempts to attribute value to thefeatures most pertinent to a given prediction. In this work, we discuss analgorithm, AVA: Aggregate Valuation of Antecedents, that fuses these twoexplanation classes to form a new approach to feature attribution that not onlyretrieves local explanations but also captures global patterns learned by amodel. Our experimentation convincingly favors weighting and aggregatingfeature attributions via AVA.",http://arxiv.org/abs/1901.10040v1,,
765,"Probability Functional Descent: A Unifying Perspective on GANs,  Variational Inference, and Reinforcement Learning","The goal of this paper is to provide a unifying view of a wide range ofproblems of interest in machine learning by framing them as the minimization offunctionals defined on the space of probability measures. In particular, weshow that generative adversarial networks, variational inference, andactor-critic methods in reinforcement learning can all be seen through the lensof our framework. We then discuss a generic optimization algorithm for ourformulation, called probability functional descent (PFD), and show how thisalgorithm recovers existing methods developed independently in the settingsmentioned earlier.",http://arxiv.org/abs/1901.10691v1,,
766,Improving Deep Image Clustering With Spatial Transformer Layers,"Image clustering is an important but challenging task in machine learning. Asin most image processing areas, the latest improvements came from models basedon the deep learning approach. However, classical deep learning methods haveproblems to deal with spatial image transformations like scale and rotation. Inthis paper, we propose the use of visual attention techniques to reduce thisproblem in image clustering methods. We evaluate the combination of a deepimage clustering model called Deep Adaptive Clustering (DAC) with the VisualSpatial Transformer Networks (STN). The proposed model is evaluated in thedatasets MNIST and FashionMNIST and outperformed the baseline model inexperiments.",http://arxiv.org/abs/1902.05401v1,,
767,On Learning from Ghost Imaging without Imaging,"Computational ghost imaging is an imaging technique with which an object isimaged from light collected using a single-pixel detector with no spatialresolution. Recently, ghost cytometry has been proposed for an ultrafastcell-classification method that involves ghost imaging and machine learning inflow cytometry. Ghost cytometry skipped the reconstruction of cell images fromsignals and directly used signals for cell-classification because thisreconstruction is the bottleneck in a high-speed analysis. In this paper, weprovide a theoretical analysis for learning from ghost imaging without imaging.",http://arxiv.org/abs/1903.06009v2,,
768,Generative Models For Deep Learning with Very Scarce Data,"The goal of this paper is to deal with a data scarcity scenario where deeplearning techniques use to fail. We compare the use of two well establishedtechniques, Restricted Boltzmann Machines and Variational Auto-encoders, asgenerative models in order to increase the training set in a classificationframework. Essentially, we rely on Markov Chain Monte Carlo (MCMC) algorithmsfor generating new samples. We show that generalization can be improvedcomparing this methodology to other state-of-the-art techniques, e.g.semi-supervised learning with ladder networks. Furthermore, we show that RBM isbetter than VAE generating new samples for training a classifier with goodgeneralization capabilities.",http://arxiv.org/abs/1903.09030v1,,
769,"Connections Between Adaptive Control and Optimization in Machine  Learning","This paper demonstrates many immediate connections between adaptive controland optimization methods commonly employed in machine learning. Starting fromcommon output error formulations, similarities in update law modifications areexamined. Concepts in stability, performance, and learning, common to bothfields are then discussed. Building on the similarities in update laws andcommon concepts, new intersections and opportunities for improved algorithmanalysis are provided. In particular, a specific problem related to higherorder learning is solved through insights obtained from these intersections.",http://arxiv.org/abs/1904.05856v1,,
770,Self-configuration from a Machine-Learning Perspective,"The goal of machine learning is to provide solutions which are trained bydata or by experience coming from the environment. Many training algorithmsexist and some brilliant successes were achieved. But even in structuredenvironments for machine learning (e.g. data mining or board games), mostapplications beyond the level of toy problems need careful hand-tuning or humaningenuity (i.e. detection of interesting patterns) or both. We discuss severalaspects how self-configuration can help to alleviate these problems. One aspectis the self-configuration by tuning of algorithms, where recent advances havebeen made in the area of SPO (Sequen- tial Parameter Optimization). Anotheraspect is the self-configuration by pattern detection or feature construction.Forming multiple features (e.g. random boolean functions) and using algorithms(e.g. random forests) which easily digest many fea- tures can largely increaselearning speed. However, a full-fledged theory of feature construction is notyet available and forms a current barrier in machine learning. We discussseveral ideas for systematic inclusion of feature construction. This may leadto partly self-configuring machine learning solutions which show robustness,flexibility, and fast learning in potentially changing environments.",http://arxiv.org/abs/1105.1951v2,,
771,The interplay between system identification and machine learning,"Learning from examples is one of the key problems in science and engineering.It deals with function reconstruction from a finite set of direct and noisysamples. Regularization in reproducing kernel Hilbert spaces (RKHSs) is widelyused to solve this task and includes powerful estimators such as regularizationnetworks. Recent achievements include the proof of the statistical consistencyof these kernel- based approaches. Parallel to this, many different systemidentification techniques have been developed but the interaction with machinelearning does not appear so strong yet. One reason is that the RKHSs usuallyemployed in machine learning do not embed the information available on dynamicsystems, e.g. BIBO stability. In addition, in system identification theindependent data assumptions routinely adopted in machine learning are neversatisfied in practice. This paper provides new results which strengthen theconnection between system identification and machine learning. Our startingpoint is the introduction of RKHSs of dynamic systems. They contain functionalsover spaces defined by system inputs and allow to interpret systemidentification as learning from examples. In both linear and nonlinearsettings, it is shown that this perspective permits to derive in a relativelysimple way conditions on RKHS stability (i.e. the property of containing onlyBIBO stable systems or predictors), also facilitating the design of new kernelsfor system identification. Furthermore, we prove the convergence of theregularized estimator to the optimal predictor under conditions typical ofdynamic systems.",http://arxiv.org/abs/1612.09158v1,,
772,MoleculeNet: A Benchmark for Molecular Machine Learning,"Molecular machine learning has been maturing rapidly over the last few years.Improved methods and the presence of larger datasets have enabled machinelearning algorithms to make increasingly accurate predictions about molecularproperties. However, algorithmic progress has been limited due to the lack of astandard benchmark to compare the efficacy of proposed methods; most newalgorithms are benchmarked on different datasets making it challenging to gaugethe quality of proposed methods. This work introduces MoleculeNet, a largescale benchmark for molecular machine learning. MoleculeNet curates multiplepublic datasets, establishes metrics for evaluation, and offers high qualityopen-source implementations of multiple previously proposed molecularfeaturization and learning algorithms (released as part of the DeepChem opensource library). MoleculeNet benchmarks demonstrate that learnablerepresentations are powerful tools for molecular machine learning and broadlyoffer the best performance. However, this result comes with caveats. Learnablerepresentations still struggle to deal with complex tasks under data scarcityand highly imbalanced classification. For quantum mechanical and biophysicaldatasets, the use of physics-aware featurizations can be more important thanchoice of particular learning algorithm.",http://arxiv.org/abs/1703.00564v3,,
773,"Machine Learning for Wireless Communications in the Internet of Things:  A Comprehensive Survey","The Internet of Things (IoT) is expected to require more effective andefficient wireless communications than ever before. For this reason, techniquessuch as spectrum sharing and dynamic spectrum access will soon become essentialcomponents of the IoT wireless communication process. In this vision, IoTdevices must be able to not only learn to autonomously extract spectrumknowledge on-the-fly from the network but also leverage such knowledge todynamically change appropriate wireless parameters (e.g., frequency band,symbol modulation, coding rate, route selection etc) to reach the network'soptimal operating point. To address the above challenges, much research hasbeen devoted to exploring the use of machine learning to address problems inthe IoT wireless communications domain. The reason behind machine learning'spopularity is that it provides a general framework to solve very complexproblems where a model of the phenomenon being learned is too complex to deriveor too dynamic to be summarized in mathematical terms. This work provides acomprehensive survey of the state of the art in the application of machinelearning techniques to address key problems in IoT wireless communications withan emphasis on its ad hoc networking aspect. First, we present extensivebackground notions on machine learning techniques. Then, by adopting abottom-up approach, we examine existing work on machine learning for the IoT atthe physical, data-link and network layer of the protocol stack. Thereafter, wediscuss directions taken by the community towards hardware implementation toensure the feasibility of these techniques. Finally, we provide a series ofresearch challenges associated with the applications of machine learningtechniques for IoT wireless communications.",http://arxiv.org/abs/1901.07947v1,,
774,Hyperbox based machine learning algorithms: A comprehensive survey,"With the rapid development of digital information, the data volume generatedby humans and machines is growing exponentially. Along with this trend, machinelearning algorithms have been formed and evolved continuously to discover newinformation and knowledge from different data sources. Learning algorithmsusing hyperboxes as fundamental representational and building blocks are abranch of machine learning methods. These algorithms have enormous potentialfor high scalability and online adaptation of predictors built using hyperboxdata representations to the dynamically changing environments and streamingdata. This paper aims to give a comprehensive survey of literature onhyperbox-based machine learning models. In general, according to thearchitecture and characteristic features of the resulting models, the existinghyperbox-based learning algorithms may be grouped into three major categories:fuzzy min-max neural networks, hyperbox-based hybrid models, and otheralgorithms based on hyperbox representations. Within each of these groups, thispaper shows a brief description of the structure of models, associated learningalgorithms, and an analysis of their advantages and drawbacks. Mainapplications of these hyperbox-based models to the real-world problems are alsodescribed in this paper. Finally, we discuss some open problems and identifypotential future research directions in this field.",http://arxiv.org/abs/1901.11303v3,,
775,"Learning Whenever Learning is Possible: Universal Learning under General  Stochastic Processes","This work initiates a general study of learning and generalization withoutthe i.i.d. assumption, starting from first principles. While the standardapproach to statistical learning theory is based on assumptions chosen largelyfor their convenience (e.g., i.i.d. or stationary ergodic), in this work we areinterested in developing a theory of learning based only on the mostfundamental and natural assumptions implicit in the requirements of thelearning problem itself. We specifically study universally consistent functionlearning, where the objective is to obtain low long-run average loss for anytarget function, when the data follow a given stochastic process. We are theninterested in the question of whether there exist learning rules guaranteed tobe universally consistent given only the assumption that universally consistentlearning is possible for the given data process. The reasoning that motivatesthis criterion emanates from a kind of optimist's decision theory, and so werefer to such learning rules as being optimistically universal. We study thisquestion in three natural learning settings: inductive, self-adaptive, andonline. Remarkably, as our strongest positive result, we find thatoptimistically universal learning rules do indeed exist in the self-adaptivelearning setting. Establishing this fact requires us to develop new approachesto the design of learning algorithms. Along the way, we also identify concisecharacterizations of the family of processes under which universally consistentlearning is possible in the inductive and self-adaptive settings. Weadditionally pose a number of enticing open problems, particularly for theonline learning setting.",http://arxiv.org/abs/1706.01418v1,,
776,Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning,"Bayesian network structure learning algorithms with limited data are beingused in domains such as systems biology and neuroscience to gain insight intothe underlying processes that produce observed data. Learning reliable networksfrom limited data is difficult, therefore transfer learning can improve therobustness of learned networks by leveraging data from related tasks. Existingtransfer learning algorithms for Bayesian network structure learning give asingle maximum a posteriori estimate of network models. Yet, many other modelsmay be equally likely, and so a more informative result is provided by Bayesianstructure discovery. Bayesian structure discovery algorithms estimate posteriorprobabilities of structural features, such as edges. We present transferlearning for Bayesian structure discovery which allows us to explore the sharedand unique structural features among related tasks. Efficient computationrequires that our transfer learning objective factors into local calculations,which we prove is given by a broad class of transfer biases. Theoretically, weshow the efficiency of our approach. Empirically, we show that compared tosingle task learning, transfer learning is better able to positively identifytrue edges. We apply the method to whole-brain neuroimaging data.",http://arxiv.org/abs/1307.2312v1,,
777,"Theoretical Comparisons of Positive-Unlabeled Learning against  Positive-Negative Learning","In PU learning, a binary classifier is trained from positive (P) andunlabeled (U) data without negative (N) data. Although N data is missing, itsometimes outperforms PN learning (i.e., ordinary supervised learning).Hitherto, neither theoretical nor experimental analysis has been given toexplain this phenomenon. In this paper, we theoretically compare PU (and NU)learning against PN learning based on the upper bounds on estimation errors. Wefind simple conditions when PU and NU learning are likely to outperform PNlearning, and we prove that, in terms of the upper bounds, either PU or NUlearning (depending on the class-prior probability and the sizes of P and Ndata) given infinite U data will improve on PN learning. Our theoreticalfindings well agree with the experimental results on artificial and benchmarkdata even when the experimental setup does not match the theoreticalassumptions exactly.",http://arxiv.org/abs/1603.03130v3,,
778,"An Overview on Data Representation Learning: From Traditional Feature  Learning to Recent Deep Learning","Since about 100 years ago, to learn the intrinsic structure of data, manyrepresentation learning approaches have been proposed, including both linearones and nonlinear ones, supervised ones and unsupervised ones. Particularly,deep architectures are widely applied for representation learning in recentyears, and have delivered top results in many tasks, such as imageclassification, object detection and speech recognition. In this paper, wereview the development of data representation learning methods. Specifically,we investigate both traditional feature learning algorithms andstate-of-the-art deep learning models. The history of data representationlearning is introduced, while available resources (e.g. online course, tutorialand book information) and toolboxes are provided. Finally, we conclude thispaper with remarks and some interesting research directions on datarepresentation learning.",http://arxiv.org/abs/1611.08331v1,,
779,Analysis of dropout learning regarded as ensemble learning,"Deep learning is the state-of-the-art in fields such as visual objectrecognition and speech recognition. This learning uses a large number oflayers, huge number of units, and connections. Therefore, overfitting is aserious problem. To avoid this problem, dropout learning is proposed. Dropoutlearning neglects some inputs and hidden units in the learning process with aprobability, p, and then, the neglected inputs and hidden units are combinedwith the learned network to express the final output. We find that the processof combining the neglected hidden units with the learned network can beregarded as ensemble learning, so we analyze dropout learning from this pointof view.",http://arxiv.org/abs/1706.06859v1,,
780,A Brief Survey of Deep Reinforcement Learning,"Deep reinforcement learning is poised to revolutionise the field of AI andrepresents a step towards building autonomous systems with a higher levelunderstanding of the visual world. Currently, deep learning is enablingreinforcement learning to scale to problems that were previously intractable,such as learning to play video games directly from pixels. Deep reinforcementlearning algorithms are also applied to robotics, allowing control policies forrobots to be learned directly from camera inputs in the real world. In thissurvey, we begin with an introduction to the general field of reinforcementlearning, then progress to the main streams of value-based and policy-basedmethods. Our survey will cover central algorithms in deep reinforcementlearning, including the deep $Q$-network, trust region policy optimisation, andasynchronous advantage actor-critic. In parallel, we highlight the uniqueadvantages of deep neural networks, focusing on visual understanding viareinforcement learning. To conclude, we describe several current areas ofresearch within the field.",http://arxiv.org/abs/1708.05866v2,,
781,What Really is Deep Learning Doing?,"Deep learning has achieved a great success in many areas, from computervision to natural language processing, to game playing, and much more. Yet,what deep learning is really doing is still an open question. There are a lotof works in this direction. For example, [5] tried to explain deep learning bygroup renormalization, and [6] tried to explain deep learning from the view offunctional approximation. In order to address this very crucial question, herewe see deep learning from perspective of mechanical learning and learningmachine (see [1], [2]). From this particular angle, we can see deep learningmuch better and answer with confidence: What deep learning is really doing? whyit works well, how it works, and how much data is necessary for learning. Wealso will discuss advantages and disadvantages of deep learning at the end ofthis work.",http://arxiv.org/abs/1711.03577v1,,
782,"Competitive Learning Enriches Learning Representation and Accelerates  the Fine-tuning of CNNs","In this study, we propose the integration of competitive learning intoconvolutional neural networks (CNNs) to improve the representation learning andefficiency of fine-tuning. Conventional CNNs use back propagation learning, andit enables powerful representation learning by a discrimination task. However,it requires huge amount of labeled data, and acquisition of labeled data ismuch harder than that of unlabeled data. Thus, efficient use of unlabeled datais getting crucial for DNNs. To address the problem, we introduce unsupervisedcompetitive learning into the convolutional layer, and utilize unlabeled datafor effective representation learning. The results of validation experimentsusing a toy model demonstrated that strong representation learning effectivelyextracted bases of images into convolutional filters using unlabeled data, andaccelerated the speed of the fine-tuning of subsequent supervised backpropagation learning. The leverage was more apparent when the number of filterswas sufficiently large, and, in such a case, the error rate steeply decreasedin the initial phase of fine-tuning. Thus, the proposed method enlarged thenumber of filters in CNNs, and enabled a more detailed and generalizedrepresentation. It could provide a possibility of not only deep but broadneural networks.",http://arxiv.org/abs/1804.09859v1,,
783,"Holarchic Structures for Decentralized Deep Learning - A Performance  Analysis","Structure plays a key role in learning performance. In centralizedcomputational systems, hyperparameter optimization and regularizationtechniques such as dropout are computational means to enhance learningperformance by adjusting the deep hierarchical structure. However, indecentralized deep learning by the Internet of Things, the structure is anactual network of autonomous interconnected devices such as smart phones thatinteract via complex network protocols. Self-adaptation of the learningstructure is a challenge. Uncertainties such as network latency, node and linkfailures or even bottlenecks by limited processing capacity and energyavailability can signif- icantly downgrade learning performance. Networkself-organization and self-management is complex, while it requires additionalcomputational and network resources that hinder the feasibility ofdecentralized deep learning. In contrast, this paper introduces a self-adaptivelearning approach based on holarchic learning structures for exploring,mitigating and boosting learning performance in distributed environments withuncertainties. A large-scale performance analysis with 864000 experiments fedwith synthetic and real-world data from smart grid and smart city pilotprojects confirm the cost-effectiveness of holarchic structures fordecentralized deep learning.",http://arxiv.org/abs/1805.02686v2,,
784,Adversarial Imitation via Variational Inverse Reinforcement Learning,"We consider a problem of learning the reward and policy from expert examplesunder unknown dynamics. Our proposed method builds on the framework ofgenerative adversarial networks and introduces the empowerment-regularizedmaximum-entropy inverse reinforcement learning to learn near-optimal rewardsand policies. Empowerment-based regularization prevents the policy fromoverfitting to expert demonstrations, which advantageously leads to moregeneralized behaviors that result in learning near-optimal rewards. Our methodsimultaneously learns empowerment through variational information maximizationalong with the reward and policy under the adversarial learning formulation. Weevaluate our approach on various high-dimensional complex control tasks. Wealso test our learned rewards in challenging transfer learning problems wheretraining and testing environments are made to be different from each other interms of dynamics or structure. The results show that our proposed method notonly learns near-optimal rewards and policies that are matching expert behaviorbut also performs significantly better than state-of-the-art inversereinforcement learning algorithms.",http://arxiv.org/abs/1809.06404v3,,
785,"Pre-training with Non-expert Human Demonstration for Deep Reinforcement  Learning","Deep reinforcement learning (deep RL) has achieved superior performance incomplex sequential tasks by using deep neural networks as functionapproximators to learn directly from raw input images. However, learningdirectly from raw images is data inefficient. The agent must learn featurerepresentation of complex states in addition to learning a policy. As a result,deep RL typically suffers from slow learning speeds and often requires aprohibitively large amount of training time and data to reach reasonableperformance, making it inapplicable to real-world settings where data isexpensive. In this work, we improve data efficiency in deep RL by addressingone of the two learning goals, feature learning. We leverage supervisedlearning to pre-train on a small set of non-expert human demonstrations andempirically evaluate our approach using the asynchronous advantage actor-criticalgorithms (A3C) in the Atari domain. Our results show significant improvementsin learning speed, even when the provided demonstration is noisy and of lowquality.",http://arxiv.org/abs/1812.08904v1,,
786,"A 128 channel Extreme Learning Machine based Neural Decoder for Brain  Machine Interfaces","Currently, state-of-the-art motor intention decoding algorithms inbrain-machine interfaces are mostly implemented on a PC and consume significantamount of power. A machine learning co-processor in 0.35um CMOS for motorintention decoding in brain-machine interfaces is presented in this paper.Using Extreme Learning Machine algorithm and low-power analog processing, itachieves an energy efficiency of 290 GMACs/W at a classification rate of 50 Hz.The learning in second stage and corresponding digitally stored coefficientsare used to increase robustness of the core analog processor. The chip isverified with neural data recorded in monkey finger movements experiment,achieving a decoding accuracy of 99.3% for movement type. The same co-processoris also used to decode time of movement from asynchronous neural spikes. Withtime-delayed feature dimension enhancement, the classification accuracy can beincreased by 5% with limited number of input channels. Further, a sparsitypromoting training scheme enables reduction of number of programmable weightsby ~2X.",http://arxiv.org/abs/1509.07450v2,,
787,"Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing  System Failure","As machine learning systems move from computer-science laboratories into theopen world, their accountability becomes a high priority problem.Accountability requires deep understanding of system behavior and its failures.Current evaluation methods such as single-score error metrics and confusionmatrices provide aggregate views of system performance that hide importantshortcomings. Understanding details about failures is important for identifyingpathways for refinement, communicating the reliability of systems in differentsettings, and for specifying appropriate human oversight and engagement.Characterization of failures and shortcomings is particularly complex forsystems composed of multiple machine learned components. For such systems,existing evaluation methods have limited expressiveness in describing andexplaining the relationship among input content, the internal states of systemcomponents, and final output quality. We present Pandora, a set of hybridhuman-machine methods and tools for describing and explaining system failures.Pandora leverages both human and system-generated observations to summarizeconditions of system malfunction with respect to the input content and systemarchitecture. We share results of a case study with a machine learning pipelinefor image captioning that show how detailed performance views can be beneficialfor analysis and debugging.",http://arxiv.org/abs/1809.07424v1,,
788,Extreme Learning Machine for land cover classification,"This paper explores the potential of extreme learning machine basedsupervised classification algorithm for land cover classification. Incomparison to a backpropagation neural network, which requires setting ofseveral user-defined parameters and may produce local minima, extreme learningmachine require setting of one parameter and produce a unique solution. ETM+multispectral data set (England) was used to judge the suitability of extremelearning machine for remote sensing classifications. A back propagation neuralnetwork was used to compare its performance in term of classification accuracyand computational cost. Results suggest that the extreme learning machineperform equally well to back propagation neural network in term ofclassification accuracy with this data set. The computational cost usingextreme learning machine is very small in comparison to back propagation neuralnetwork.",http://arxiv.org/abs/0802.1412v1,,
789,On Combining Machine Learning with Decision Making,"We present a new application and covering number bound for the framework of""Machine Learning with Operational Costs (MLOC),"" which is an exploratory formof decision theory. The MLOC framework incorporates knowledge about how apredictive model will be used for a subsequent task, thus combining machinelearning with the decision that is made afterwards. In this work, we use theMLOC framework to study a problem that has implications for power gridreliability and maintenance, called the Machine Learning and TravelingRepairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a""repair crew,"" which repairs nodes on a graph. The repair crew aims to minimizethe cost of failures at the nodes, but as in many real situations, the failureprobabilities are not known and must be estimated. The MLOC framework allows usto understand how this uncertainty influences the repair route. We also presentnew covering number generalization bounds for the MLOC framework.",http://arxiv.org/abs/1104.5061v2,,
790,Bioclimating Modelling: A Machine Learning Perspective,"Many machine learning (ML) approaches are widely used to generate bioclimaticmodels for prediction of geographic range of organism as a function of climate.Applications such as prediction of range shift in organism, range of invasivespecies influenced by climate change are important parameters in understandingthe impact of climate change. However, success of machine learning-basedapproaches depends on a number of factors. While it can be safely said that noparticular ML technique can be effective in all applications and success of atechnique is predominantly dependent on the application or the type of theproblem, it is useful to understand their behaviour to ensure informed choiceof techniques. This paper presents a comprehensive review of machinelearning-based bioclimatic model generation and analyses the factorsinfluencing success of such models. Considering the wide use of statisticaltechniques, in our discussion we also include conventional statisticaltechniques used in bioclimatic modelling.",http://arxiv.org/abs/1306.4152v1,,
791,Scaling Datalog for Machine Learning on Big Data,"In this paper, we present the case for a declarative foundation fordata-intensive machine learning systems. Instead of creating a new system foreach specific flavor of machine learning task, or hardcoding new optimizations,we argue for the use of recursive queries to program a variety of machinelearning systems. By taking this approach, database query optimizationtechniques can be utilized to identify effective execution plans, and theresulting runtime plans can be executed on a single unified data-parallel queryprocessing engine. As a proof of concept, we consider two programmingmodels--Pregel and Iterative Map-Reduce-Update---from the machine learningdomain, and show how they can be captured in Datalog, tuned for a specifictask, and then compiled into an optimized physical plan. Experiments performedon a large computing cluster with real data demonstrate that this declarativeapproach can provide very good performance while offering both increasedgenerality and programming ease.",http://arxiv.org/abs/1203.0160v2,,
792,Automatic Differentiation of Algorithms for Machine Learning,"Automatic differentiation---the mechanical transformation of numeric computerprograms to calculate derivatives efficiently and accurately---dates to theorigin of the computer age. Reverse mode automatic differentiation bothantedates and generalizes the method of backwards propagation of errors used inmachine learning. Despite this, practitioners in a variety of fields, includingmachine learning, have been little influenced by automatic differentiation, andmake scant use of available tools. Here we review the technique of automaticdifferentiation, describe its two main modes, and explain how it can benefitmachine learning practitioners. To reach the widest possible audience ourtreatment assumes only elementary differential calculus, and does not assumeany knowledge of linear algebra.",http://arxiv.org/abs/1404.7456v1,,
793,"Machine Learning in Wireless Sensor Networks: Algorithms, Strategies,  and Applications","Wireless sensor networks monitor dynamic environments that change rapidlyover time. This dynamic behavior is either caused by external factors orinitiated by the system designers themselves. To adapt to such conditions,sensor networks often adopt machine learning techniques to eliminate the needfor unnecessary redesign. Machine learning also inspires many practicalsolutions that maximize resource utilization and prolong the lifespan of thenetwork. In this paper, we present an extensive literature review over theperiod 2002-2013 of machine learning methods that were used to address commonissues in wireless sensor networks (WSNs). The advantages and disadvantages ofeach proposed algorithm are evaluated against the corresponding problem. Wealso provide a comparative guide to aid WSN designers in developing suitablemachine learning solutions for their specific application challenges.",http://arxiv.org/abs/1405.4463v2,,
794,Learning Machines Implemented on Non-Deterministic Hardware,"This paper highlights new opportunities for designing large-scale machinelearning systems as a consequence of blurring traditional boundaries that haveallowed algorithm designers and application-level practitioners to stay -- forthe most part -- oblivious to the details of the underlying hardware-levelimplementations. The hardware/software co-design methodology advocated herehinges on the deployment of compute-intensive machine learning kernels ontocompute platforms that trade-off determinism in the computation for improvementin speed and/or energy efficiency. To achieve this, we revisit digitalstochastic circuits for approximating matrix computations that are ubiquitousin machine learning algorithms. Theoretical and empirical evaluation isundertaken to assess the impact of the hardware-induced computational noise onalgorithm performance. As a proof-of-concept, a stochastic hardware simulatoris employed for training deep neural networks for image recognition problems.",http://arxiv.org/abs/1409.2620v1,,
795,Freeze-Thaw Bayesian Optimization,"In this paper we develop a dynamic form of Bayesian optimization for machinelearning models with the goal of rapidly finding good hyperparameter settings.Our method uses the partial information gained during the training of a machinelearning model in order to decide whether to pause training and start a newmodel, or resume the training of a previously-considered model. We specificallytailor our method to machine learning problems by developing a novelpositive-definite covariance kernel to capture a variety of training curves.Furthermore, we develop a Gaussian process prior that scales gracefully withadditional temporal observations. Finally, we provide an information-theoreticframework to automate the decision process. Experiments on several commonmachine learning models show that our approach is extremely effective inpractice.",http://arxiv.org/abs/1406.3896v1,,
796,Second-Order Stochastic Optimization for Machine Learning in Linear Time,"First-order stochastic methods are the state-of-the-art in large-scalemachine learning optimization owing to efficient per-iteration complexity.Second-order methods, while able to provide faster convergence, have been muchless explored due to the high cost of computing the second-order information.In this paper we develop second-order stochastic methods for optimizationproblems in machine learning that match the per-iteration cost of gradientbased methods, and in certain settings improve upon the overall running timeover popular first-order methods. Furthermore, our algorithm has the desirableproperty of being implementable in time linear in the sparsity of the inputdata.",http://arxiv.org/abs/1602.03943v5,,
797,Interpretable Machine Learning Models for the Digital Clock Drawing Test,"The Clock Drawing Test (CDT) is a rapid, inexpensive, and popularneuropsychological screening tool for cognitive conditions. The Digital ClockDrawing Test (dCDT) uses novel software to analyze data from a digitizingballpoint pen that reports its position with considerable spatial and temporalprecision, making possible the analysis of both the drawing process and finalproduct. We developed methodology to analyze pen stroke data from thesedrawings, and computed a large collection of features which were then analyzedwith a variety of machine learning techniques. The resulting scoring systemswere designed to be more accurate than the systems currently used byclinicians, but just as interpretable and easy to use. The systems also allowus to quantify the tradeoff between accuracy and interpretability. We createdautomated versions of the CDT scoring systems currently used by clinicians,allowing us to benchmark our models, which indicated that our machine learningmodels substantially outperformed the existing scoring systems.",http://arxiv.org/abs/1606.07163v1,,
798,Machine Learning Markets,"Prediction markets show considerable promise for developing flexiblemechanisms for machine learning. Here, machine learning markets formultivariate systems are defined, and a utility-based framework is establishedfor their analysis. This differs from the usual approach of defining staticbetting functions. It is shown that such markets can implement modelcombination methods used in machine learning, such as product of expert andmixture of expert approaches as equilibrium pricing models, by varying agentutility functions. They can also implement models composed of local potentials,and message passing methods. Prediction markets also allow for more flexiblecombinations, by combining multiple different utility functions. Conversely,the market mechanisms implement inference in the relevant probabilistic models.This means that market mechanism can be utilized for implementing parallelizedmodel building and inference for probabilistic modelling.",http://arxiv.org/abs/1106.4509v1,,
799,Isoelastic Agents and Wealth Updates in Machine Learning Markets,"Recently, prediction markets have shown considerable promise for developingflexible mechanisms for machine learning. In this paper, agents with isoelasticutilities are considered. It is shown that the costs associated withhomogeneous markets of agents with isoelastic utilities produce equilibriumprices corresponding to alpha-mixtures, with a particular form of mixingcomponent relating to each agent's wealth. We also demonstrate that wealthaccumulation for logarithmic and other isoelastic agents (through payoffs onprediction of training targets) can implement both Bayesian model updates andmixture weight updates by imposing different market payoff structures. Aniterative algorithm is given for market equilibrium computation. We demonstratethat inhomogeneous markets of agents with isoelastic utilities outperform stateof the art aggregate classifiers such as random forests, as well as singleclassifiers (neural networks, decision trees) on a number of machine learningbenchmarks, and show that isoelastic combination methods are generally betterthan their logarithmic counterparts.",http://arxiv.org/abs/1206.6443v2,,
800,"Multi-period Trading Prediction Markets with Connections to Machine  Learning","We present a new model for prediction markets, in which we use risk measuresto model agents and introduce a market maker to describe the trading process.This specific choice on modelling tools brings us mathematical convenience. Theanalysis shows that the whole market effectively approaches a global objective,despite that the market is designed such that each agent only cares about itsown goal. Additionally, the market dynamics provides a sensible algorithm foroptimising the global objective. An intimate connection between machinelearning and our markets is thus established, such that we could 1) analyse amarket by applying machine learning methods to the global objective, and 2)solve machine learning problems by setting up and running certain markets.",http://arxiv.org/abs/1403.0648v1,,
801,"The Use of Machine Learning Algorithms in Recommender Systems: A  Systematic Review","Recommender systems use algorithms to provide users with product or servicerecommendations. Recently, these systems have been using machine learningalgorithms from the field of artificial intelligence. However, choosing asuitable machine learning algorithm for a recommender system is difficultbecause of the number of algorithms described in the literature. Researchersand practitioners developing recommender systems are left with littleinformation about the current approaches in algorithm usage. Moreover, thedevelopment of a recommender system using a machine learning algorithm oftenhas problems and open questions that must be evaluated, so software engineersknow where to focus research efforts. This paper presents a systematic reviewof the literature that analyzes the use of machine learning algorithms inrecommender systems and identifies research opportunities for softwareengineering research. The study concludes that Bayesian and decision treealgorithms are widely used in recommender systems because of their relativesimplicity, and that requirement and design phases of recommender systemdevelopment appear to offer opportunities for further research.",http://arxiv.org/abs/1511.05263v4,,
802,"Evaluation of a Tree-based Pipeline Optimization Tool for Automating  Data Science","As the field of data science continues to grow, there will be anever-increasing demand for tools that make machine learning accessible tonon-experts. In this paper, we introduce the concept of tree-based pipelineoptimization for automating one of the most tedious parts of machinelearning---pipeline design. We implement an open source Tree-based PipelineOptimization Tool (TPOT) in Python and demonstrate its effectiveness on aseries of simulated and real-world benchmark data sets. In particular, we showthat TPOT can design machine learning pipelines that provide a significantimprovement over a basic machine learning analysis while requiring little to noinput nor prior knowledge from the user. We also address the tendency for TPOTto design overly complex pipelines by integrating Pareto optimization, whichproduces compact pipelines without sacrificing classification accuracy. Assuch, this work represents an important step toward fully automating machinelearning pipeline design.",http://arxiv.org/abs/1603.06212v1,,
803,Machine Learning at Scale,"It takes skill to build a meaningful predictive model even with the abundanceof implementations of modern machine learning algorithms and readily availablecomputing resources. Building a model becomes challenging if hundreds ofterabytes of data need to be processed to produce the training data set. In adigital advertising technology setting, we are faced with the need to buildthousands of such models that predict user behavior and power advertisingcampaigns in a 24/7 chaotic real-time production environment. As datascientists, we also have to convince other internal departments critical toimplementation success, our management, and our customers that our machinelearning system works. In this paper, we present the details of the design andimplementation of an automated, robust machine learning platform that impactsbillions of advertising impressions monthly. This platform enables us tocontinuously optimize thousands of campaigns over hundreds of millions ofusers, on multiple continents, against varying performance objectives.",http://arxiv.org/abs/1402.6076v1,,
804,Crafting Adversarial Input Sequences for Recurrent Neural Networks,"Machine learning models are frequently used to solve complex securityproblems, as well as to make decisions in sensitive situations like guidingautonomous vehicles or predicting financial market behaviors. Previous effortshave shown that numerous machine learning models were vulnerable to adversarialmanipulations of their inputs taking the form of adversarial samples. Suchinputs are crafted by adding carefully selected perturbations to legitimateinputs so as to force the machine learning model to misbehave, for instance byoutputting a wrong class if the machine learning task of interest isclassification. In fact, to the best of our knowledge, all previous work onadversarial samples crafting for neural network considered models used to solveclassification tasks, most frequently in computer vision applications. In thispaper, we contribute to the field of adversarial machine learning byinvestigating adversarial input sequences for recurrent neural networksprocessing sequential data. We show that the classes of algorithms introducedpreviously to craft adversarial samples misclassified by feed-forward neuralnetworks can be adapted to recurrent neural networks. In a experiment, we showthat adversaries can craft adversarial sequences misleading both categoricaland sequential recurrent neural networks.",http://arxiv.org/abs/1604.08275v1,,
805,Machine Learning for Antimicrobial Resistance,"Biological datasets amenable to applied machine learning are more availabletoday than ever before, yet they lack adequate representation in theData-for-Good community. Here we present a work in progress case studyperforming analysis on antimicrobial resistance (AMR) using standard ensemblemachine learning techniques and note the successes and pitfalls such workentails. Broadly, applied machine learning (AML) techniques are well suited toAMR, with classification accuracies ranging from mid-90% to low- 80% dependingon sample size. Additionally, these techniques prove successful at identifyinggene regions known to be associated with the AMR phenotype. We believe that theextensive amount of biological data available, the plethora of problemspresented, and the global impact of such work merits the consideration of theData- for-Good community.",http://arxiv.org/abs/1607.01224v1,,
806,Membership Inference Attacks against Machine Learning Models,"We quantitatively investigate how machine learning models leak informationabout the individual data records on which they were trained. We focus on thebasic membership inference attack: given a data record and black-box access toa model, determine if the record was in the model's training dataset. Toperform membership inference against a target model, we make adversarial use ofmachine learning and train our own inference model to recognize differences inthe target model's predictions on the inputs that it trained on versus theinputs that it did not train on.  We empirically evaluate our inference techniques on classification modelstrained by commercial ""machine learning as a service"" providers such as Googleand Amazon. Using realistic datasets and classification tasks, including ahospital discharge dataset whose membership is sensitive from the privacyperspective, we show that these models can be vulnerable to membershipinference attacks. We then investigate the factors that influence this leakageand evaluate mitigation strategies.",http://arxiv.org/abs/1610.05820v2,,
807,"Source localization in an ocean waveguide using supervised machine  learning","Source localization in ocean acoustics is posed as a machine learning problemin which data-driven methods learn source ranges directly from observedacoustic data. The pressure received by a vertical linear array is preprocessedby constructing a normalized sample covariance matrix (SCM) and used as theinput. Three machine learning methods (feed-forward neural networks (FNN),support vector machines (SVM) and random forests (RF)) are investigated in thispaper, with focus on the FNN. The range estimation problem is solved both as aclassification problem and as a regression problem by these three machinelearning algorithms. The results of range estimation for the Noise09 experimentare compared for FNN, SVM, RF and conventional matched-field processing anddemonstrate the potential of machine learning for underwater sourcelocalization..",http://arxiv.org/abs/1701.08431v4,,
808,"Intercomparison of Machine Learning Methods for Statistical Downscaling:  The Case of Daily and Extreme Precipitation","Statistical downscaling of global climate models (GCMs) allows researchers tostudy local climate change effects decades into the future. A wide range ofstatistical models have been applied to downscaling GCMs but recent advances inmachine learning have not been explored. In this paper, we compare fourfundamental statistical methods, Bias Correction Spatial Disaggregation (BCSD),Ordinary Least Squares, Elastic-Net, and Support Vector Machine, with threemore advanced machine learning methods, Multi-task Sparse Structure Learning(MSSL), BCSD coupled with MSSL, and Convolutional Neural Networks to downscaledaily precipitation in the Northeast United States. Metrics to evaluate of eachmethod's ability to capture daily anomalies, large scale climate shifts, andextremes are analyzed. We find that linear methods, led by BCSD, consistentlyoutperform non-linear approaches. The direct application of state-of-the-artmachine learning methods to statistical downscaling does not provideimprovements over simpler, longstanding approaches.",http://arxiv.org/abs/1702.04018v1,,
809,"HolStep: A Machine Learning Dataset for Higher-order Logic Theorem  Proving","Large computer-understandable proofs consist of millions of intermediatelogical steps. The vast majority of such steps originate from manually selectedand manually guided heuristics applied to intermediate goals. So far, machinelearning has generally not been used to filter or generate these steps. In thispaper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, forthe purpose of developing new machine learning-based theorem-provingstrategies. We make this dataset publicly available under the BSD license. Wepropose various machine learning tasks that can be performed on this dataset,and discuss their significance for theorem proving. We also benchmark a set ofsimple baseline machine learning models suited for the tasks (includinglogistic regression, convolutional neural networks and recurrent neuralnetworks). The results of our baseline models show the promise of applyingmachine learning to HOL theorem proving.",http://arxiv.org/abs/1703.00426v1,,
810,"PMLB: A Large Benchmark Suite for Machine Learning Evaluation and  Comparison","The selection, development, or comparison of machine learning methods in datamining can be a difficult task based on the target problem and goals of aparticular study. Numerous publicly available real-world and simulatedbenchmark datasets have emerged from different sources, but their organizationand adoption as standards have been inconsistent. As such, selecting andcurating specific benchmarks remains an unnecessary burden on machine learningpractitioners and data scientists. The present study introduces an accessible,curated, and developing public benchmark resource to facilitate identificationof the strengths and weaknesses of different machine learning methodologies. Wecompare meta-features among the current set of benchmark datasets in thisresource to characterize the diversity of available data. Finally, we apply anumber of established machine learning methods to the entire benchmark suiteand analyze how datasets and algorithms cluster in terms of performance. Thiswork is an important first step towards understanding the limitations ofpopular benchmarking suites and developing a resource that connects existingbenchmarking standards to more diverse and efficient standards in the future.",http://arxiv.org/abs/1703.00512v1,,
811,"On the Use of Default Parameter Settings in the Empirical Evaluation of  Classification Algorithms","We demonstrate that, for a range of state-of-the-art machine learningalgorithms, the differences in generalisation performance obtained usingdefault parameter settings and using parameters tuned via cross-validation canbe similar in magnitude to the differences in performance observed betweenstate-of-the-art and uncompetitive learning systems. This means that fair andrigorous evaluation of new learning algorithms requires performance comparisonagainst benchmark methods with best-practice model selection procedures, ratherthan using default parameter settings. We investigate the sensitivity of threekey machine learning algorithms (support vector machine, random forest androtation forest) to their default parameter settings, and provide guidance ondetermining sensible default parameter values for implementations of thesealgorithms. We also conduct an experimental comparison of these threealgorithms on 121 classification problems and find that, perhaps surprisingly,rotation forest is significantly more accurate on average than both randomforest and a support vector machine.",http://arxiv.org/abs/1703.06777v1,,
812,Perspective: Energy Landscapes for Machine Learning,"Machine learning techniques are being increasingly used as flexiblenon-linear fitting and prediction tools in the physical sciences. Fittingfunctions that exhibit multiple solutions as local minima can be analysed interms of the corresponding machine learning landscape. Methods to explore andvisualise molecular potential energy landscapes can be applied to these machinelearning landscapes to gain new insight into the solution space involved intraining and the nature of the corresponding predictions. In particular, we candefine quantities analogous to molecular structure, thermodynamics, andkinetics, and relate these emergent properties to the structure of theunderlying landscape. This Perspective aims to describe these analogies withexamples from recent applications, and suggest avenues for newinterdisciplinary research.",http://arxiv.org/abs/1703.07915v1,,
813,"Machine Learning for Networking: Workflow, Advances and Opportunities","Recently, machine learning has been used in every possible field to leverageits amazing power. For a long time, the net-working and distributed computingsystem is the key infrastructure to provide efficient computational resourcefor machine learning. Networking itself can also benefit from this promisingtechnology. This article focuses on the application of Machine Learningtechniques for Networking (MLN), which can not only help solve the intractableold network questions but also stimulate new network applications. In thisarticle, we summarize the basic workflow to explain how to apply the machinelearning technology in the networking domain. Then we provide a selectivesurvey of the latest representative advances with explanations on their designprinciples and benefits. These advances are divided into several network designobjectives and the detailed information of how they perform in each step of MLNworkflow is presented. Finally, we shed light on the new opportunities onnetworking design and community building of this new inter-discipline. Our goalis to provide a broad research guideline on networking with machine learning tohelp and motivate researchers to develop innovative algorithms, standards andframeworks.",http://arxiv.org/abs/1709.08339v2,,
814,"Rough extreme learning machine: a new classification method based on  uncertainty measure","Extreme learning machine (ELM) is a new single hidden layer feedback neuralnetwork. The weights of the input layer and the biases of neurons in hiddenlayer are randomly generated, the weights of the output layer can beanalytically determined. ELM has been achieved good results for a large numberof classification tasks. In this paper, a new extreme learning machine calledrough extreme learning machine (RELM) was proposed. RELM uses rough set todivide data into upper approximation set and lower approximation set, and thetwo approximation sets are utilized to train upper approximation neurons andlower approximation neurons. In addition, an attribute reduction is executed inthis algorithm to remove redundant attributes. The experimental results showed,comparing with the comparison algorithms, RELM can get a better accuracy andrepeatability in most cases, RELM can not only maintain the advantages of fastspeed, but also effectively cope with the classification task forhigh-dimensional data.",http://arxiv.org/abs/1710.10824v2,,
815,"Short-term Mortality Prediction for Elderly Patients Using Medicare  Claims Data","Risk prediction is central to both clinical medicine and public health. Whilemany machine learning models have been developed to predict mortality, they arerarely applied in the clinical literature, where classification tasks typicallyrely on logistic regression. One reason for this is that existing machinelearning models often seek to optimize predictions by incorporating featuresthat are not present in the databases readily available to providers and policymakers, limiting generalizability and implementation. Here we tested a numberof machine learning classifiers for prediction of six-month mortality in apopulation of elderly Medicare beneficiaries, using an administrative claimsdatabase of the kind available to the majority of health care payers andproviders. We show that machine learning classifiers substantially outperformcurrent widely-used methods of risk prediction but only when used with animproved feature set incorporating insights from clinical medicine, developedfor this study. Our work has applications to supporting patient and providerdecision making at the end of life, as well as population health-orientedefforts to identify patients at high risk of poor outcomes.",http://arxiv.org/abs/1712.00644v1,,
816,Machine Learning Cosmic Expansion History,"We use the machine learning techniques, for the first time, to study thebackground evolution of the universe in light of 30 cosmic chronometers. From 7machine learning algorithms, using the principle of mean squared errorminimization on testing set, we find that Bayesian ridge regression is theoptimal method to extract the information from cosmic chronometers. By use of apower-law polynomial expansion, we obtain the first Hubble constant estimation$H_0=65.95^{+6.98}_{-6.36}$ km s$^{-1}$ Mpc$^{-1}$ from machine learning. Fromthe view of machine learning, we may rule out a large number of cosmologicalmodels, the number of physical parameters of which containing $H_0$ is largerthan 3. Very importantly and interestingly, we find that the parameter spacesof 3 specific cosmological models can all be clearly compressed by consideringboth their explanation and generalization abilities.",http://arxiv.org/abs/1712.09208v1,,
817,"Machine Learning for Building Energy and Indoor Environment: A  Perspective","Machine learning is a promising technique for many practical applications. Inthis perspective, we illustrate the development and application for machinelearning. It is indicated that the theories and applications of machinelearning method in the field of energy conservation and indoor environment arenot mature, due to the difficulty of the determination for model structure withbetter prediction. In order to significantly contribute to the problems, weutilize the ANN model to predict the indoor culturable fungi concentration,which achieves the better accuracy and convenience. The proposal of hybridmethod is further expand the application fields of machine learning method.Further, ANN model based on HTS was successfully applied for the optimizationof building energy system. We hope that this novel method could capture moreattention from investigators via our introduction and perspective, due to itspotential development with accuracy and reliability. However, its feasibilityin other fields needs to be promoted further.",http://arxiv.org/abs/1801.00779v1,,
818,"Attack Strength vs. Detectability Dilemma in Adversarial Machine  Learning","As the prevalence and everyday use of machine learning algorithms, along withour reliance on these algorithms grow dramatically, so do the efforts to attackand undermine these algorithms with malicious intent, resulting in a growinginterest in adversarial machine learning. A number of approaches have beendeveloped that can render a machine learning algorithm ineffective throughpoisoning or other types of attacks. Most attack algorithms typically usesophisticated optimization approaches, whose objective function is designed tocause maximum damage with respect to accuracy and performance of the algorithmwith respect to some task. In this effort, we show that while such an objectivefunction is indeed brutally effective in causing maximum damage on an embeddedfeature selection task, it often results in an attack mechanism that can beeasily detected with an embarrassingly simple novelty or outlier detectionalgorithm. We then propose an equally simple yet elegant solution by adding aregularization term to the attacker's objective function that penalizesoutlying attack points.",http://arxiv.org/abs/1802.07295v1,,
819,"The State of the Art in Integrating Machine Learning into Visual  Analytics","Visual analytics systems combine machine learning or other analytictechniques with interactive data visualization to promote sensemaking andanalytical reasoning. It is through such techniques that people can make senseof large, complex data. While progress has been made, the tactful combinationof machine learning and data visualization is still under-explored. Thisstate-of-the-art report presents a summary of the progress that has been madeby highlighting and synthesizing select research advances. Further, it presentsopportunities and challenges to enhance the synergy between machine learningand visual analytics for impactful future research directions.",http://arxiv.org/abs/1802.07954v1,,
820,"Trustless Machine Learning Contracts; Evaluating and Exchanging Machine  Learning Models on the Ethereum Blockchain","Using blockchain technology, it is possible to create contracts that offer areward in exchange for a trained machine learning model for a particular dataset. This would allow users to train machine learning models for a reward in atrustless manner. The smart contract will use the blockchain to automaticallyvalidate the solution, so there would be no debate about whether the solutionwas correct or not. Users who submit the solutions won't have counterparty riskthat they won't get paid for their work. Contracts can be created easily byanyone with a dataset, even programmatically by software agents. This creates amarket where parties who are good at solving machine learning problems candirectly monetize their skillset, and where any organization or software agentthat has a problem to solve with AI can solicit solutions from all over theworld. This will incentivize the creation of better machine learning models,and make AI more accessible to companies and software agents.",http://arxiv.org/abs/1802.10185v1,,
821,"Two Use Cases of Machine Learning for SDN-Enabled IP/Optical Networks:  Traffic Matrix Prediction and Optical Path Performance Prediction","We describe two applications of machine learning in the context of IP/Opticalnetworks. The first one allows agile management of resources at a coreIP/Optical network by using machine learning for short-term and long-termprediction of traffic flows and joint global optimization of IP and opticallayers using colorless/directionless (CD) flexible ROADMs. Multilayercoordination allows for significant cost savings, flexible new services to meetdynamic capacity needs, and improved robustness by being able to proactivelyadapt to new traffic patterns and network conditions. The second application isimportant as we migrate our metro networks to Open ROADM networks, to allowphysical routing without the need for detailed knowledge of optical parameters.We discuss a proof-of-concept study, where detailed performance data forwavelengths on a current flexible ROADM network is used for machine learning topredict the optical performance of each wavelength. Both applications can beefficiently implemented by using a SDN (Software Defined Network) controller.",http://arxiv.org/abs/1804.07433v2,,
822,"Modelling tourism demand to Spain with machine learning techniques. The  impact of forecast horizon on model selection","This study assesses the influence of the forecast horizon on the forecastingperformance of several machine learning techniques. We compare the fo recastaccuracy of Support Vector Regression (SVR) to Neural Network (NN) models,using a linear model as a benchmark. We focus on international tourism demandto all seventeen regions of Spain. The SVR with a Gaussian radial basisfunction kernel outperforms the rest of the models for the longest forecasthorizons. We also find that machine learning methods improve their forecastingaccuracy with respect to linear models as forecast horizons increase. Thisresult shows the suitability of SVR for medium and long term forecasting.",http://arxiv.org/abs/1805.00878v1,,
823,Ariadne: Analysis for Machine Learning Program,"Machine learning has transformed domains like vision and translation, and isnow increasingly used in science, where the correctness of such code is vital.Python is popular for machine learning, in part because of its wealth ofmachine learning libraries, and is felt to make development faster; however,this dynamic language has less support for error detection at code creationtime than tools like Eclipse. This is especially problematic for machinelearning: given its statistical nature, code with subtle errors may run andproduce results that look plausible but are meaningless. This can vitiatescientific results. We report on Ariadne: applying a static framework, WALA, tomachine learning code that uses TensorFlow. We have created static analysis forPython, a type system for tracking tensors---Tensorflow's core datastructures---and a data flow analysis to track their usage. We report on how itwas built and present some early results.",http://arxiv.org/abs/1805.04058v1,,
824,TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service,"Machine learning methods are widely used for a variety of predictionproblems. \emph{Prediction as a service} is a paradigm in which serviceproviders with technological expertise and computational resources may performpredictions for clients. However, data privacy severely restricts theapplicability of such services, unless measures to keep client data private(even from the service provider) are designed. Equally important is to minimizethe amount of computation and communication required between client and server.Fully homomorphic encryption offers a possible way out, whereby clients mayencrypt their data, and on which the server may perform arithmeticcomputations. The main drawback of using fully homomorphic encryption is theamount of time required to evaluate large machine learning models on encrypteddata. We combine ideas from the machine learning literature, particularly workon binarization and sparsification of neural networks, together withalgorithmic tools to speed-up and parallelize computation using encrypted data.",http://arxiv.org/abs/1806.03461v1,,
825,"chemmodlab: A Cheminformatics Modeling Laboratory for Fitting and  Assessing Machine Learning Models","The goal of chemmodlab is to streamline the fitting and assessment pipelinefor many machine learning models in R, making it easy for researchers tocompare the utility of new models. While focused on implementing methods formodel fitting and assessment that have been accepted by experts in thecheminformatics field, all of the methods in chemmodlab have broad utility forthe machine learning community. chemmodlab contains several assessmentutilities including a plotting function that constructs accumulation curves anda function that computes many performance measures. The most novel feature ofchemmodlab is the ease with which statistically significant performancedifferences for many machine learning models is presented by means of themultiple comparisons similarity plot. Differences are assessed using repeatedk-fold cross validation where blocking increases precision and multiplicityadjustments are applied.",http://arxiv.org/abs/1807.00243v3,,
826,"Machine Learning Classifiers Do Not Improve the Prediction of Academic  Risk: Evidence from Australia","Machine learning methods tend to outperform traditional statistical models atprediction. In the prediction of academic achievement, ML models have not shownsubstantial improvement over linear and logistic regression. So far, theseresults have almost entirely focused on college achievement, due to theavailability of administrative datasets, and have contained relatively smallsample sizes by ML standards. In this article we apply popular machine learningmodels to a large dataset ($n=2.2$ million) containing primary and middleschool performance on NAPLAN, a test given annually to all Australian studentsin grades 3, 5, 7, and 9. We show that machine learning models do notoutperform logistic regression for detecting students who will perform in the`below standard' band of achievement upon sitting their next test.",http://arxiv.org/abs/1807.07215v3,,
827,"Using Randomness to Improve Robustness of Machine-Learning Models  Against Evasion Attacks","Machine learning models have been widely used in security applications suchas intrusion detection, spam filtering, and virus or malware detection.However, it is well-known that adversaries are always trying to adapt theirattacks to evade detection. For example, an email spammer may guess whatfeatures spam detection models use and modify or remove those features to avoiddetection. There has been some work on making machine learning models morerobust to such attacks. However, one simple but promising approach called {\emrandomization} is underexplored. This paper proposes a novelrandomization-based approach to improve robustness of machine learning modelsagainst evasion attacks. The proposed approach incorporates randomization intoboth model training time and model application time (meaning when the model isused to detect attacks). We also apply this approach to random forest, anexisting ML method which already has some degree of randomness. Experiments onintrusion detection and spam filtering data show that our approach furtherimproves robustness of random-forest method. We also discuss how this approachcan be applied to other ML models.",http://arxiv.org/abs/1808.03601v1,,
828,"Learning of Tree-Structured Gaussian Graphical Models on Distributed  Data under Communication Constraints","In this paper, learning of tree-structured Gaussian graphical models fromdistributed data is addressed. In our model, samples are stored in a set ofdistributed machines where each machine has access to only a subset offeatures. A central machine is then responsible for learning the structurebased on received messages from the other nodes. We present a set ofcommunication efficient strategies, which are theoretically proved to conveysufficient information for reliable learning of the structure. In particular,our analyses show that even if each machine sends only the signs of its localdata samples to the central node, the tree structure can still be recoveredwith high accuracy. Our simulation results on both synthetic and real-worlddatasets show that our strategies achieve a desired accuracy in inferring theunderlying structure, while spending a small budget on communication.",http://arxiv.org/abs/1809.08067v1,,
829,Explainable Black-Box Attacks Against Model-based Authentication,"Establishing unique identities for both humans and end systems has been anactive research problem in the security community, giving rise to innovativemachine learning-based authentication techniques. Although such techniquesoffer an automated method to establish identity, they have not been vettedagainst sophisticated attacks that target their core machine learningtechnique. This paper demonstrates that mimicking the unique signaturesgenerated by host fingerprinting and biometric authentication systems ispossible. We expose the ineffectiveness of underlying machine learningclassification models by constructing a blind attack based around the querysynthesis framework and utilizing Explainable-AI (XAI) techniques. We launch anattack in under 130 queries on a state-of-the-art face authentication system,and under 100 queries on a host authentication system. We examine how theseattacks can be defended against and explore their limitations. XAI provides aneffective means for adversaries to infer decision boundaries and provides a newway forward in constructing attacks against systems using machine learningmodels for authentication.",http://arxiv.org/abs/1810.00024v1,,
830,Topographic Representation for Quantum Machine Learning,"This paper proposes a brain-inspired approach to quantum machine learningwith the goal of circumventing many of the complications of other approaches.The fact that quantum processes are unitary presents both opportunities andchallenges. A principal opportunity is that a large number of computations canbe carried out in parallel in linear superposition, that is, quantumparallelism. The challenge is that the process is linear, and most approachesto machine learning depend significantly on nonlinear processes. Fortunately,the situation is not hopeless, for we know that nonlinear processes can beembedded in unitary processes, as is familiar from the circuit model of quantumcomputation. This paper explores an approach to the quantum implementation ofmachine learning involving nonlinear functions operating on informationrepresented topographically (by computational maps), as common in neuralcortex.",http://arxiv.org/abs/1810.06992v2,,
831,Law and Adversarial Machine Learning,"When machine learning systems fail because of adversarial manipulation, howshould society expect the law to respond? Through scenarios grounded inadversarial ML literature, we explore how some aspects of computer crime,copyright, and tort law interface with perturbation, poisoning, model stealingand model inversion attacks to show how some attacks are more likely to resultin liability than others. We end with a call for action to ML researchers toinvest in transparent benchmarks of attacks and defenses; architect ML systemswith forensics in mind and finally, think more about adversarial machinelearning in the context of civil liberties. The paper is targeted towards MLresearchers who have no legal background.",http://arxiv.org/abs/1810.10731v3,,
832,Symmetry constrained machine learning,"Symmetry, a central concept in understanding the laws of nature, has beenused for centuries in physics, mathematics, and chemistry, to help makemathematical models tractable. Yet, despite its power, symmetry has not beenused extensively in machine learning, until rather recently. In this article weshow a general way to incorporate symmetries into machine learning models. Wedemonstrate this with a detailed analysis on a rather simple real world machinelearning system - a neural network for classifying handwritten digits, lackingbias terms for every neuron. We demonstrate that ignoring symmetries can havedire over-fitting consequences, and that incorporating symmetry into the modelreduces over-fitting, while at the same time reducing complexity, ultimatelyrequiring less training data, and taking less time and resources to train.",http://arxiv.org/abs/1811.07051v1,,
833,Correspondence Analysis of Government Expenditure Patterns,"We analyze expenditure patterns of discretionary funds by Brazilian congressmembers. This analysis is based on a large dataset containing over $7$ millionexpenses made publicly available by the Brazilian government. This dataset has,up to now, remained widely untouched by machine learning methods. Our maincontributions are two-fold: (i) we provide a novel dataset benchmark formachine learning-based efforts for government transparency to the broaderresearch community, and (ii) introduce a neural network-based approach foranalyzing and visualizing outlying expense patterns. Our hope is that theapproach presented here can inspire new machine learning methodologies forgovernment transparency applicable to other developing nations.",http://arxiv.org/abs/1812.01105v1,,
834,"A New Strategy in Applying the Learning Machine to Study Phase  Transitions","In this Letter, we present a new strategy for applying the learning machineto study phase transitions. We train the learning machine with samples onlyobtained at a non-critical parameter point, aiming to establish intrinsiccorrelations between the learning machine and the target system. Then, we findthat the accuracy of the learning machine, which is the most importantperformance index in conventional learning machines, is no longer a key goal ofthe training in our approach. Instead, relatively low accuracy of identifyingunlabeled data category can help to determine the critical point with greaterprecision, manifesting the singularity around the critical point. It thusprovides a robust tool to study the phase transition. The classicalferromagnetic and percolation phase transitions are employed as illustrativeexamples.",http://arxiv.org/abs/1901.00774v1,,
835,"DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine  Learning","As an emerging field, Automated Machine Learning (AutoML) aims to reduce oreliminate manual operations that require expertise in machine learning. In thispaper, a graph-based architecture is employed to represent flexiblecombinations of ML models, which provides a large searching space compared totree-based and stacking-based architectures. Based on this, an evolutionaryalgorithm is proposed to search for the best architecture, where the mutationand heredity operators are the key for architecture evolution. With Bayesianhyper-parameter optimization, the proposed approach can automate the workflowof machine learning. On the PMLB dataset, the proposed approach shows thestate-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn.Some of the optimized models are with complex structures which are difficult toobtain in manual design.",http://arxiv.org/abs/1901.08013v1,,
836,"A Framework for Understanding Unintended Consequences of Machine  Learning","As machine learning increasingly affects people and society, it is importantthat we strive for a comprehensive and unified understanding of how and whyunwanted consequences arise. For instance, downstream harms to particulargroups are often blamed on ""biased data,"" but this concept encompass too manyissues to be useful in developing solutions. In this paper, we provide aframework that partitions sources of downstream harm in machine learning intofive distinct categories spanning the data generation and machine learningpipeline. We describe how these issues arise, how they are relevant toparticular applications, and how they motivate different solutions. In doingso, we aim to facilitate the development of solutions that stem from anunderstanding of application-specific populations and data generationprocesses, rather than relying on general claims about what may or may not be""fair.""",http://arxiv.org/abs/1901.10002v1,,
837,Assessing the Local Interpretability of Machine Learning Models,"The increasing adoption of machine learning tools has led to calls foraccountability via model interpretability. But what does it mean for a machinelearning model to be interpretable by humans, and how can this be assessed? Wefocus on two definitions of interpretability that have been introduced in themachine learning literature: simulatability (a user's ability to run a model ona given input) and ""what if"" local explainability (a user's ability tocorrectly indicate the outcome to a model under local changes to the input).Through a user study with 1000 participants, we test whether humans performwell on tasks that mimic the definitions of simulatability and ""what if"" localexplainability on models that are typically considered locally interpretable.We find evidence consistent with the common intuition that decision trees andlogistic regression models are interpretable and are more interpretable thanneural networks. We propose a metric - the runtime operation count on thesimulatability task - to indicate the relative interpretability of models andshow that as the number of operations increases the users' accuracy on thelocal interpretability tasks decreases.",http://arxiv.org/abs/1902.03501v1,,
838,"ATMSeer: Increasing Transparency and Controllability in Automated  Machine Learning","To relieve the pain of manually selecting machine learning algorithms andtuning hyperparameters, automated machine learning (AutoML) methods have beendeveloped to automatically search for good models. Due to the huge model searchspace, it is impossible to try all models. Users tend to distrust automaticresults and increase the search budget as much as they can, thereby underminingthe efficiency of AutoML. To address these issues, we design and implementATMSeer, an interactive visualization tool that supports users in refining thesearch space of AutoML and analyzing the results. To guide the design ofATMSeer, we derive a workflow of using AutoML based on interviews with machinelearning experts. A multi-granularity visualization is proposed to enable usersto monitor the AutoML process, analyze the searched models, and refine thesearch space in real time. We demonstrate the utility and usability of ATMSeerthrough two case studies, expert interviews, and a user study with 13 endusers.",http://arxiv.org/abs/1902.05009v1,,
839,"Continuous Integration of Machine Learning Models with ease.ml/ci:  Towards a Rigorous Yet Practical Treatment","Continuous integration is an indispensable step of modern softwareengineering practices to systematically manage the life cycles of systemdevelopment. Developing a machine learning model is no difference - it is anengineering process with a life cycle, including design, implementation,tuning, testing, and deployment. However, most, if not all, existing continuousintegration engines do not support machine learning as first-class citizens.  In this paper, we present ease.ml/ci, to our best knowledge, the firstcontinuous integration system for machine learning. The challenge of buildingease.ml/ci is to provide rigorous guarantees, e.g., single accuracy point errortolerance with 0.999 reliability, with a practical amount of labeling effort,e.g., 2K labels per test. We design a domain specific language that allowsusers to specify integration conditions with reliability constraints, anddevelop simple novel optimizations that can lower the number of labels requiredby up to two orders of magnitude for test conditions popularly used in realproduction systems.",http://arxiv.org/abs/1903.00278v1,,
840,Materials development by interpretable machine learning,"Machine learning technologies are expected to be great tools for scientificdiscoveries. In particular, materials development (which has brought a lot ofinnovation by finding new and better functional materials) is one of the mostattractive scientific fields. To apply machine learning to actual materialsdevelopment, collaboration between scientists and machine learning is becominginevitable. However, such collaboration has been restricted so far due to blackbox machine learning, in which it is difficult for scientists to interpret thedata-driven model from the viewpoint of material science and physics. Here, weshow a material development success story that was achieved by goodcollaboration between scientists and one type of interpretable (explainable)machine learning called factorized asymptotic Bayesian inference hierarchicalmixture of experts (FAB/HMEs). Based on material science and physics, weinterpreted the data-driven model constructed by the FAB/HMEs, so that wediscovered surprising correlation and knowledge about thermoelectric material.Guided by this, we carried out actual material synthesis that led toidentification of a novel spin-driven thermoelectric material with the largestthermopower to date.",http://arxiv.org/abs/1903.02175v1,,
841,Few-shot machine learning in the three-dimensional Ising model,"We investigate theoretically the phase transition in three dimensional cubicIsing model utilizing state-of-the-art machine learning algorithms. Supervisedmachine learning models show high accuracies (~99\%) in phase classificationand very small relative errors ($< 10^{-4}$) of the energies in different spinconfigurations. Unsupervised machine learning models are introduced to studythe spin configuration reconstructions and reductions, and the phases ofreconstructed spin configurations can be accurately classified by a linearlogistic algorithm. Based on the comparison between various machine learningmodels, we develop a few-shot strategy to predict phase transitions in largerlattices from trained sample in smaller lattices. The few-shot machine learningstrategy for three dimensional(3D) Ising model enable us to study 3D isingmodel efficiently and provides a new integrated and highly accurate approach toother spin models.",http://arxiv.org/abs/1903.08061v2,,
842,Machine learning and the physical sciences,"Machine learning encompasses a broad range of algorithms and modeling toolsused for a vast array of data processing tasks, which has entered mostscientific disciplines in recent years. We review in a selective way the recentresearch on the interface between machine learning and physical sciences.Thisincludes conceptual developments in machine learning (ML) motivated by physicalinsights, applications of machine learning techniques to several domains inphysics, and cross-fertilization between the two fields. After giving basicnotion of machine learning methods and principles, we describe examples of howstatistical physics is used to understand methods in ML. We then move todescribe applications of ML methods in particle physics and cosmology, quantummany body physics, quantum computing, and chemical and material physics. Wealso highlight research and development into novel computing architecturesaimed at accelerating ML. In each of the sections we describe recent successesas well as domain-specific methodology and challenges.",http://arxiv.org/abs/1903.10563v1,,
843,"On the Functional Equivalence of TSK Fuzzy Systems to Neural Networks,  Mixture of Experts, CART, and Stacking Ensemble Regression","Fuzzy systems have achieved great success in numerous applications. However,there are still many challenges in designing an optimal fuzzy system, e.g., howto efficiently train its parameters, how to improve its performance withoutadding too many parameters, how to balance the trade-off between cooperationsand competitions among the rules, how to overcome the curse of dimensionality,etc. Literature has shown that by making appropriate connections between fuzzysystems and other machine learning approaches, good practices from otherdomains may be used to improve the fuzzy systems, and vice versa. This papergives an overview on the functional equivalence between Takagi-Sugeno-Kangfuzzy systems and four classic machine learning approaches -- neural networks,mixture of experts, classification and regression trees, and stacking ensembleregression -- for regression problems. We also point out some promising newresearch directions, inspired by the functional equivalence, that could lead tosolutions to the aforementioned problems. To our knowledge, this is so far themost comprehensive overview on the connections between fuzzy systems and otherpopular machine learning approaches, and hopefully will stimulate morehybridization between different machine learning algorithms.",http://arxiv.org/abs/1903.10572v1,,
844,"Interoperability and machine-to-machine translation model with mappings  to machine learning tasks","Modern large-scale automation systems integrate thousands to hundreds ofthousands of physical sensors and actuators. Demands for more flexiblereconfiguration of production systems and optimization across differentinformation models, standards and legacy systems challenge current systeminteroperability concepts. Automatic semantic translation across informationmodels and standards is an increasingly important problem that needs to beaddressed to fulfill these demands in a cost-efficient manner under constraintsof human capacity and resources in relation to timing requirements and systemcomplexity. Here we define a translator-based operational interoperabilitymodel for interacting cyber-physical systems in mathematical terms, whichincludes system identification and ontology-based translation as special cases.We present alternative mathematical definitions of the translator learning taskand mappings to similar machine learning tasks and solutions based on recentdevelopments in machine learning. Possibilities to learn translators betweenartefacts without a common physical context, for example in simulations ofdigital twins and across layers of the automation pyramid are brieflydiscussed.",http://arxiv.org/abs/1903.10735v1,,
845,"SCADA System Testbed for Cybersecurity Research Using Machine Learning  Approach","This paper presents the development of a Supervisory Control and DataAcquisition (SCADA) system testbed used for cybersecurity research. The testbedconsists of a water storage tank's control system, which is a stage in theprocess of water treatment and distribution. Sophisticated cyber-attacks wereconducted against the testbed. During the attacks, the network traffic wascaptured, and features were extracted from the traffic to build a dataset fortraining and testing different machine learning algorithms. Five traditionalmachine learning algorithms were trained to detect the attacks: Random Forest,Decision Tree, Logistic Regression, Naive Bayes and KNN. Then, the trainedmachine learning models were built and deployed in the network, where new testswere made using online network traffic. The performance obtained during thetraining and testing of the machine learning models was compared to theperformance obtained during the online deployment of these models in thenetwork. The results show the efficiency of the machine learning models indetecting the attacks in real time. The testbed provides a good understandingof the effects and consequences of attacks on real SCADA environments",http://arxiv.org/abs/1904.00753v1,,
846,Adaptive Sequential Machine Learning,"A framework previously introduced in [3] for solving a sequence of stochasticoptimization problems with bounded changes in the minimizers is extended andapplied to machine learning problems such as regression and classification. Thestochastic optimization problems arising in these machine learning problems issolved using algorithms such as stochastic gradient descent (SGD). A methodbased on estimates of the change in the minimizers and properties of theoptimization algorithm is introduced for adaptively selecting the number ofsamples at each time step to ensure that the excess risk, i.e., the expectedgap between the loss achieved by the approximate minimizer produced by theoptimization algorithm and the exact minimizer, does not exceed a target level.A bound is developed to show that the estimate of the change in the minimizersis non-trivial provided that the excess risk is small enough. Extensionsrelevant to the machine learning setting are considered, including a cost-basedapproach to select the number of samples with a cost budget over a fixedhorizon, and an approach to applying cross-validation for model selection.Finally, experiments with synthetic and real data are used to validate thealgorithms.",http://arxiv.org/abs/1904.02773v1,,
847,Every Local Minimum is a Global Minimum of an Induced Model,"For non-convex optimization in machine learning, this paper proves that everylocal minimum achieves the global optimality of the perturbable gradient basismodel at any differentiable point. As a result, non-convex machine learning istheoretically as supported as convex machine learning with a hand-crafted basisin terms of the loss at differentiable local minima, except in the case when apreference is given to the hand-crafted basis over the perturbable gradientbasis. The proofs of these results are derived under mild assumptions.Accordingly, the proven results are directly applicable to many machinelearning models, including practical deep neural networks, without anymodification of practical methods. Furthermore, as special cases of our generalresults, this paper improves or complements several state-of-the-arttheoretical results in the literature with a simple and unified prooftechnique.",http://arxiv.org/abs/1904.03673v1,,
848,Rademacher Complexity of the Restricted Boltzmann Machine,"Boltzmann machine, as a fundamental construction block of deep belief networkand deep Boltzmann machines, is widely used in deep learning community andgreat success has been achieved. However, theoretical understanding of manyaspects of it is still far from clear. In this paper, we studied the Rademachercomplexity of both the asymptotic restricted Boltzmann machine and thepractical implementation with single-step contrastive divergence (CD-1)procedure. Our results disclose the fact that practical implementation trainingprocedure indeed increased the Rademacher complexity of restricted Boltzmannmachines. A further research direction might be the investigation of the VCdimension of a compositional function used in the CD-1 procedure.",http://arxiv.org/abs/1512.01914v1,,
849,Boltzmann machines and energy-based models,"We review Boltzmann machines and energy-based models. A Boltzmann machinedefines a probability distribution over binary-valued patterns. One can learnparameters of a Boltzmann machine via gradient based approaches in a way thatlog likelihood of data is increased. The gradient and Hessian of a Boltzmannmachine admit beautiful mathematical representations, although computing themis in general intractable. This intractability motivates approximate methods,including Gibbs sampler and contrastive divergence, and tractable alternatives,namely energy-based models.",http://arxiv.org/abs/1708.06008v2,,
850,"Human-Machine Inference Networks For Smart Decision Making:  Opportunities and Challenges","The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combinescomplementary cognitive strengths of humans and machines in an intelligentmanner to tackle various inference tasks and achieves higher performance thaneither humans or machines by themselves. While inference performanceoptimization techniques for human-only or sensor-only networks are quitemature, HuMaINs require novel signal processing and machine learning solutions.In this paper, we present an overview of the HuMaINs architecture with a focuson three main issues that include architecture design, inference algorithmsincluding security/privacy challenges, and application areas/use cases.",http://arxiv.org/abs/1801.09626v1,,
851,"Knowledge is at the Edge! How to Search in Distributed Machine Learning  Models","With the advent of the Internet of Things and Industry 4.0 an enormous amountof data is produced at the edge of the network. Due to a lack of computingpower, this data is currently send to the cloud where centralized machinelearning models are trained to derive higher level knowledge. With the recentdevelopment of specialized machine learning hardware for mobile devices, a newera of distributed learning is about to begin that raises a new researchquestion: How can we search in distributed machine learning models? Machinelearning at the edge of the network has many benefits, such as low-latencyinference and increased privacy. Such distributed machine learning models canalso learn personalized for a human user, a specific context, or applicationscenario. As training data stays on the devices, control over possiblysensitive data is preserved as it is not shared with a third party. This newform of distributed learning leads to the partitioning of knowledge betweenmany devices which makes access difficult. In this paper we tackle the problemof finding specific knowledge by forwarding a search request (query) to adevice that can answer it best. To that end, we use a entropy based qualitymetric that takes the context of a query and the learning quality of a deviceinto account. We show that our forwarding strategy can achieve over 95%accuracy in a urban mobility scenario where we use data from 30 000 peoplecommuting in the city of Trento, Italy.",http://arxiv.org/abs/1710.04890v1,,
852,Generative Adversarial Imitation Learning,"Consider learning a policy from example expert behavior, without interactionwith the expert or access to reinforcement signal. One approach is to recoverthe expert's cost function with inverse reinforcement learning, then extract apolicy from that cost function with reinforcement learning. This approach isindirect and can be slow. We propose a new general framework for directlyextracting a policy from data, as if it were obtained by reinforcement learningfollowing inverse reinforcement learning. We show that a certain instantiationof our framework draws an analogy between imitation learning and generativeadversarial networks, from which we derive a model-free imitation learningalgorithm that obtains significant performance gains over existing model-freemethods in imitating complex behaviors in large, high-dimensional environments.",http://arxiv.org/abs/1606.03476v1,,
853,Learning a Policy for Opportunistic Active Learning,"Active learning identifies data points to label that are expected to be themost useful in improving a supervised model. Opportunistic active learningincorporates active learning into interactive tasks that constrain possiblequeries during interactions. Prior work has shown that opportunistic activelearning can be used to improve grounding of natural language descriptions inan interactive object retrieval task. In this work, we use reinforcementlearning for such an object retrieval task, to learn a policy that effectivelytrades off task completion with model improvement that would benefit futuretasks.",http://arxiv.org/abs/1808.10009v1,,
854,Learning Thermodynamics with Boltzmann Machines,"A Boltzmann machine is a stochastic neural network that has been extensivelyused in the layers of deep architectures for modern machine learningapplications. In this paper, we develop a Boltzmann machine that is capable ofmodelling thermodynamic observables for physical systems in thermalequilibrium. Through unsupervised learning, we train the Boltzmann machine ondata sets constructed with spin configurations importance-sampled from thepartition function of an Ising Hamiltonian at different temperatures usingMonte Carlo (MC) methods. The trained Boltzmann machine is then used togenerate spin states, for which we compare thermodynamic observables to thosecomputed by direct MC sampling. We demonstrate that the Boltzmann machine canfaithfully reproduce the observables of the physical system. Further, weobserve that the number of neurons required to obtain accurate resultsincreases as the system is brought close to criticality.",http://arxiv.org/abs/1606.02718v1,,
855,"ICE: Enabling Non-Experts to Build Models Interactively for Large-Scale  Lopsided Problems","Quick interaction between a human teacher and a learning machine presentsnumerous benefits and challenges when working with web-scale data. The humanteacher guides the machine towards accomplishing the task of interest. Thelearning machine leverages big data to find examples that maximize the trainingvalue of its interaction with the teacher. When the teacher is restricted tolabeling examples selected by the machine, this problem is an instance ofactive learning. When the teacher can provide additional information to themachine (e.g., suggestions on what examples or predictive features should beused) as the learning task progresses, then the problem becomes one ofinteractive learning.  To accommodate the two-way communication channel needed for efficientinteractive learning, the teacher and the machine need an environment thatsupports an interaction language. The machine can access, process, andsummarize more examples than the teacher can see in a lifetime. Based on themachine's output, the teacher can revise the definition of the task or make itmore precise. Both the teacher and the machine continuously learn and benefitfrom the interaction.  We have built a platform to (1) produce valuable and deployable models and(2) support research on both the machine learning and user interface challengesof the interactive learning problem. The platform relies on a dedicated,low-latency, distributed, in-memory architecture that allows us to constructweb-scale learning machines with quick interaction speed. The purpose of thispaper is to describe this architecture and demonstrate how it supports ourresearch efforts. Preliminary results are presented as illustrations of thearchitecture but are not the primary focus of the paper.",http://arxiv.org/abs/1409.4814v1,,
856,"An Adaptive Strategy for the Classification of G-Protein Coupled  Receptors","One of the major problems in computational biology is the inability ofexisting classification models to incorporate expanding and new domainknowledge. This problem of static classification models is addressed in thispaper by the introduction of incremental learning for problems inbioinformatics. Many machine learning tools have been applied to this problemusing static machine learning structures such as neural networks or supportvector machines that are unable to accommodate new information into theirexisting models. We utilize the fuzzy ARTMAP as an alternate machine learningsystem that has the ability of incrementally learning new data as it becomesavailable. The fuzzy ARTMAP is found to be comparable to many of the widespreadmachine learning systems. The use of an evolutionary strategy in the selectionand combination of individual classifiers into an ensemble system, coupled withthe incremental learning ability of the fuzzy ARTMAP is proven to be suitableas a pattern classifier. The algorithm presented is tested using data from theG-Coupled Protein Receptors Database and shows good accuracy of 83%. The systempresented is also generally applicable, and can be used in problems in genomicsand proteomics.",http://arxiv.org/abs/0704.3453v1,,
857,Relevance As a Metric for Evaluating Machine Learning Algorithms,"In machine learning, the choice of a learning algorithm that is suitable forthe application domain is critical. The performance metric used to comparedifferent algorithms must also reflect the concerns of users in the applicationdomain under consideration. In this work, we propose a novel probability-basedperformance metric called Relevance Score for evaluating supervised learningalgorithms. We evaluate the proposed metric through empirical analysis on adataset gathered from an intelligent lighting pilot installation. In comparisonto the commonly used Classification Accuracy metric, the Relevance Score provesto be more appropriate for a certain class of applications.",http://arxiv.org/abs/1303.7093v3,,
858,"Reinforcement learning based sensing policy optimization for energy  efficient cognitive radio networks","This paper introduces a machine learning based collaborative multi-bandspectrum sensing policy for cognitive radios. The proposed sensing policyguides secondary users to focus the search of unused radio spectrum to thosefrequencies that persistently provide them high data rate. The proposed policyis based on machine learning, which makes it adaptive with the temporally andspatially varying radio spectrum. Furthermore, there is no need for dynamicmodeling of the primary activity since it is implicitly learned over time.Energy efficiency is achieved by minimizing the number of assigned sensors pereach subband under a constraint on miss detection probability. It is importantto control the missed detections because they cause collisions with primarytransmissions and lead to retransmissions at both the primary and secondaryuser. Simulations show that the proposed machine learning based sensing policyimproves the overall throughput of the secondary network and improves theenergy efficiency while controlling the miss detection probability.",http://arxiv.org/abs/1106.1770v3,,
859,Encrypted statistical machine learning: new privacy preserving methods,"We present two new statistical machine learning methods designed to learn onfully homomorphic encrypted (FHE) data. The introduction of FHE schemesfollowing Gentry (2009) opens up the prospect of privacy preserving statisticalmachine learning analysis and modelling of encrypted data without compromisingsecurity constraints. We propose tailored algorithms for applying extremelyrandom forests, involving a new cryptographic stochastic fraction estimator,and na\""{i}ve Bayes, involving a semi-parametric model for the class decisionboundary, and show how they can be used to learn and predict from encrypteddata. We demonstrate that these techniques perform competitively on a varietyof classification data sets and provide detailed information about thecomputational practicalities of these and other FHE methods.",http://arxiv.org/abs/1508.06845v1,,
860,"Semi-Supervised Prediction of Gene Regulatory Networks Using Machine  Learning Algorithms","Use of computational methods to predict gene regulatory networks (GRNs) fromgene expression data is a challenging task. Many studies have been conductedusing unsupervised methods to fulfill the task; however, such methods usuallyyield low prediction accuracies due to the lack of training data. In thisarticle, we propose semi-supervised methods for GRN prediction by utilizing twomachine learning algorithms, namely support vector machines (SVM) and randomforests (RF). The semi-supervised methods make use of unlabeled data fortraining. We investigate inductive and transductive learning approaches, bothof which adopt an iterative procedure to obtain reliable negative training datafrom the unlabeled data. We then apply our semi-supervised methods to geneexpression data of Escherichia coli and Saccharomyces cerevisiae, and evaluatethe performance of our methods using the expression data. Our analysisindicated that the transductive learning approach outperformed the inductivelearning approach for both organisms. However, there was no conclusivedifference identified in the performance of SVM and RF. Experimental resultsalso showed that the proposed semi-supervised methods performed better thanexisting supervised methods for both organisms.",http://arxiv.org/abs/1608.03530v1,,
861,By-passing the Kohn-Sham equations with machine learning,"Last year, at least 30,000 scientific papers used the Kohn-Sham scheme ofdensity functional theory to solve electronic structure problems in a widevariety of scientific fields, ranging from materials science to biochemistry toastrophysics. Machine learning holds the promise of learning the kinetic energyfunctional via examples, by-passing the need to solve the Kohn-Sham equations.This should yield substantial savings in computer time, allowing either largersystems or longer time-scales to be tackled, but attempts to machine-learn thisfunctional have been limited by the need to find its derivative. The presentwork overcomes this difficulty by directly learning the density-potential andenergy-density maps for test systems and various molecules. Both improvedaccuracy and lower computational cost with this method are demonstrated byreproducing DFT energies for a range of molecular geometries generated duringmolecular dynamics simulations. Moreover, the methodology could be applieddirectly to quantum chemical calculations, allowing construction of densityfunctionals of quantum-chemical accuracy.",http://arxiv.org/abs/1609.02815v3,,
862,Private Learning on Networks,"Continual data collection and widespread deployment of machine learningalgorithms, particularly the distributed variants, have raised new privacychallenges. In a distributed machine learning scenario, the dataset is storedamong several machines and they solve a distributed optimization problem tocollectively learn the underlying model. We present a secure multi-partycomputation inspired privacy preserving distributed algorithm for optimizing aconvex function consisting of several possibly non-convex functions. Eachindividual objective function is privately stored with an agent while theagents communicate model parameters with neighbor machines connected in anetwork. We show that our algorithm can correctly optimize the overallobjective function and learn the underlying model accurately. We further provethat under a vertex connectivity condition on the topology, our algorithmpreserves privacy of individual objective functions. We establish limits on thewhat a coalition of adversaries can learn by observing the messages and statesshared over a network.",http://arxiv.org/abs/1612.05236v1,,
863,"Optimization Methods for Supervised Machine Learning: From Linear Models  to Deep Learning","The goal of this tutorial is to introduce key models, algorithms, and openquestions related to the use of optimization methods for solving problemsarising in machine learning. It is written with an INFORMS audience in mind,specifically those readers who are familiar with the basics of optimizationalgorithms, but less familiar with machine learning. We begin by deriving aformulation of a supervised learning problem and show how it leads to variousoptimization problems, depending on the context and underlying assumptions. Wethen discuss some of the distinctive features of these optimization problems,focusing on the examples of logistic regression and the training of deep neuralnetworks. The latter half of the tutorial focuses on optimization algorithms,first for convex logistic regression, for which we discuss the use offirst-order methods, the stochastic gradient method, variance reducingstochastic methods, and second-order methods. Finally, we discuss how theseapproaches can be employed to the training of deep neural networks, emphasizingthe difficulties that arise from the complex, nonconvex structure of thesemodels.",http://arxiv.org/abs/1706.10207v1,,
864,Decoupled classifiers for fair and efficient machine learning,"When it is ethical and legal to use a sensitive attribute (such as gender orrace) in machine learning systems, the question remains how to do so. We showthat the naive application of machine learning algorithms using sensitivefeatures leads to an inherent tradeoff in accuracy between groups. We provide asimple and efficient decoupling technique, that can be added on top of anyblack-box machine learning algorithm, to learn different classifiers fordifferent groups. Transfer learning is used to mitigate the problem of havingtoo little data on any one group.  The method can apply to a range of fairness criteria. In particular, werequire the application designer to specify as joint loss function that makesexplicit the trade-off between fairness and accuracy. Our reduction is shown toefficiently find the minimum loss as long as the objective has a certainnatural monotonicity property which may be of independent interest in the studyof fairness in algorithms.",http://arxiv.org/abs/1707.06613v1,,
865,Machine learning of molecular properties: locality and active learning,"In recent years the machine learning techniques have shown a great potentialin various problems from a multitude of disciplines, including materials designand drug discovery. The high computational speed on the one hand and theaccuracy comparable to that of DFT on another hand make machine learningalgorithms efficient for high-throughput screening through chemical andconfigurational space. However, the machine learning algorithms available inthe literature require large training datasets to reach the chemical accuracyand also show large errors for the so-called outliers - the out-of-samplemolecules, not well-represented in the training set. In the present paper wepropose a new machine learning algorithm for predicting molecular propertiesthat addresses these two issues: it is based on a local model of interatomicinteractions providing high accuracy when trained on relatively small trainingsets and an active learning algorithm of optimally choosing the training setthat significantly reduces the errors for the outliers. We compare our model tothe other state-of-the-art algorithms from the literature on the widely usedbenchmark tests.",http://arxiv.org/abs/1709.07082v2,,
866,"Enabling End-To-End Machine Learning Replicability: A Case Study in  Educational Data Mining","The use of machine learning techniques has expanded in education research,driven by the rich data from digital learning environments and institutionaldata warehouses. However, replication of machine learned models in the domainof the learning sciences is particularly challenging due to a confluence ofexperimental, methodological, and data barriers. We discuss the challenges ofend-to-end machine learning replication in this context, and present anopen-source software toolkit, the MOOC Replication Framework (MORF), to addressthem. We demonstrate the use of MORF by conducting a replication at scale, andprovide a complete executable container, with unique DOIs documenting theconfigurations of each individual trial, for replication or future extension athttps://github.com/educational-technology-collective/fy2015-replication. Thiswork demonstrates an approach to end-to-end machine learning replication whichis relevant to any domain with large, complex or multi-format,privacy-protected data with a consistent schema.",http://arxiv.org/abs/1806.05208v2,,
867,"The Helmholtz Method: Using Perceptual Compression to Reduce Machine  Learning Complexity","This paper proposes a fundamental answer to a frequently asked question inmultimedia computing and machine learning: Do artifacts from perceptualcompression contribute to error in the machine learning process and if so, howmuch? Our approach to the problem is a reinterpretation of the Helmholtz FreeEnergy formula from physics to explain the relationship between content andnoise when using sensors (such as cameras or microphones) to capture multimediadata. The reinterpretation allows a bit-measurement of the noise contained inimages, audio, and video by combining a classifier with perceptual compression,such as JPEG or MP3. Our experiments on CIFAR-10 as well as Fraunhofer'sIDMT-SMT-Audio-Effects dataset indicate that, at the right quality level,perceptual compression is actually not harmful but contributes to a significantreduction of complexity of the machine learning process. That is, our noisequantification method can be used to speed up the training of deep learningclassifiers significantly while maintaining, or sometimes even improving,overall classification accuracy. Moreover, our results provide insights intothe reasons for the success of deep learning.",http://arxiv.org/abs/1807.10569v1,,
868,"Reinforcement Learning for Autonomous Defence in Software-Defined  Networking","Despite the successful application of machine learning (ML) in a wide rangeof domains, adaptability---the very property that makes machine learningdesirable---can be exploited by adversaries to contaminate training and evadeclassification. In this paper, we investigate the feasibility of applying aspecific class of machine learning algorithms, namely, reinforcement learning(RL) algorithms, for autonomous cyber defence in software-defined networking(SDN). In particular, we focus on how an RL agent reacts towards differentforms of causative attacks that poison its training process, includingindiscriminate and targeted, white-box and black-box attacks. In addition, wealso study the impact of the attack timing, and explore potentialcountermeasures such as adversarial training.",http://arxiv.org/abs/1808.05770v1,,
869,Parameter Transfer Extreme Learning Machine based on Projective Model,"Recent years, transfer learning has attracted much attention in the communityof machine learning. In this paper, we mainly focus on the tasks of parametertransfer under the framework of extreme learning machine (ELM). Unlike theexisting parameter transfer approaches, which incorporate the source modelinformation into the target by regularizing the di erence between the sourceand target domain parameters, an intuitively appealing projective-model isproposed to bridge the source and target model parameters. Specifically, weformulate the parameter transfer in the ELM networks by the means of parameterprojection, and train the model by optimizing the projection matrix andclassifier parameters jointly. Further more, the `L2,1-norm structured sparsitypenalty is imposed on the source domain parameters, which encourages the jointfeature selection and parameter transfer. To evaluate the e ectiveness of theproposed method, comprehensive experiments on several commonly used domainadaptation datasets are presented. The results show that the proposed methodsignificantly outperforms the non-transfer ELM networks and other classicaltransfer learning methods.",http://arxiv.org/abs/1809.01018v2,,
870,"Comparison of Deep Learning and the Classical Machine Learning Algorithm  for the Malware Detection","Recently, Deep Learning has been showing promising results in variousArtificial Intelligence applications like image recognition, natural languageprocessing, language modeling, neural machine translation, etc. Although, ingeneral, it is computationally more expensive as compared to classical machinelearning techniques, their results are found to be more effective in somecases. Therefore, in this paper, we investigated and compared one of the DeepLearning Architecture called Deep Neural Network (DNN) with the classicalRandom Forest (RF) machine learning algorithm for the malware classification.We studied the performance of the classical RF and DNN with 2, 4 & 7 layersarchitectures with the four different feature sets, and found that irrespectiveof the features inputs, the classical RF accuracy outperforms the DNN.",http://arxiv.org/abs/1809.05889v1,,
871,"Machine Learning Algorithms for Classification of Microcirculation  Images from Septic and Non-Septic Patients","Sepsis is a life-threatening disease and one of the major causes of death inhospitals. Imaging of microcirculatory dysfunction is a promising approach forautomated diagnosis of sepsis. We report a machine learning classifier capableof distinguishing non-septic and septic images from dark field microcirculationvideos of patients. The classifier achieves an accuracy of 89.45%. The areaunder the receiver operating characteristics of the classifier was 0.92, theprecision was 0.92 and the recall was 0.84. Codes representing the learnedfeature space of trained classifier were visualized using t-SNE embedding andwere separable and distinguished between images from critically ill andnon-septic patients. Using an unsupervised convolutional autoencoder,independent of the clinical diagnosis, we also report clustering of learnedfeatures from a compressed representation associated with healthy images andthose with microcirculatory dysfunction. The feature space used by our trainedclassifier to distinguish between images from septic and non-septic patientshas potential diagnostic application.",http://arxiv.org/abs/1811.02659v2,,
872,"Patient Clustering Improves Efficiency of Federated Machine Learning to  predict mortality and hospital stay time using distributed Electronic Medical  Records","Electronic medical records (EMRs) supports the development of machinelearning algorithms for predicting disease incidence, patient response totreatment, and other healthcare events. But insofar most algorithms have beencentralized, taking little account of the decentralized, non-identicallyindependently distributed (non-IID), and privacy-sensitive characteristics ofEMRs that can complicate data collection, sharing and learning. To address thischallenge, we introduced a community-based federated machine learning (CBFL)algorithm and evaluated it on non-IID ICU EMRs. Our algorithm clustered thedistributed data into clinically meaningful communities that captured similardiagnoses and geological locations, and learnt one model for each community.Throughout the learning process, the data was kept local on hospitals, whilelocally-computed results were aggregated on a server. Evaluation results showthat CBFL outperformed the baseline FL algorithm in terms of Area Under theReceiver Operating Characteristic Curve (ROC AUC), Area Under thePrecision-Recall Curve (PR AUC), and communication cost between hospitals andthe server. Furthermore, communities' performance difference could be explainedby how dissimilar one community was to others.",http://arxiv.org/abs/1903.09296v1,,
873,"Bayesian Optimization for Policy Search via Online-Offline  Experimentation","Online field experiments are the gold-standard way of evaluating changes toreal-world interactive machine learning systems. Yet our ability to explorecomplex, multi-dimensional policy spaces - such as those found inrecommendation and ranking problems - is often constrained by the limitednumber of experiments that can be run simultaneously. To alleviate theseconstraints, we augment online experiments with an offline simulator and applymulti-task Bayesian optimization to tune live machine learning systems. Wedescribe practical issues that arise in these types of applications, includingbiases that arise from using a simulator and assumptions for the multi-taskkernel. We measure empirical learning curves which show substantial gains fromincluding data from biased offline experiments, and show how these learningcurves are consistent with theoretical results for multi-task Gaussian processgeneralization. We find that improved kernel inference is a significant driverof multi-task generalization. Finally, we show several examples of Bayesianoptimization efficiently tuning a live machine learning system by combiningoffline and online experiments.",http://arxiv.org/abs/1904.01049v1,,
874,Online Learning of a Memory for Learning Rates,"The promise of learning to learn for robotics rests on the hope that byextracting some information about the learning process itself we can speed upsubsequent similar learning tasks. Here, we introduce a computationallyefficient online meta-learning algorithm that builds and optimizes a memorymodel of the optimal learning rate landscape from previously observed gradientbehaviors. While performing task specific optimization, this memory of learningrates predicts how to scale currently observed gradients. After applying thegradient scaling our meta-learner updates its internal memory based on theobserved effect its prediction had. Our meta-learner can be combined with anygradient-based optimizer, learns on the fly and can be transferred to newoptimization tasks. In our evaluations we show that our meta-learning algorithmspeeds up learning of MNIST classification and a variety of learning controltasks, either in batch or online learning settings.",http://arxiv.org/abs/1709.06709v2,,
875,Limit Learning Equivalence Structures,"While most research in Gold-style learning focuses on learning formallanguages, we consider the identification of computable structures,specifically equivalence structures. In our core model the learner gets moreand more information about which pairs of elements of a structure are relatedand which are not. The aim of the learner is to find (an effective descriptionof) the isomorphism type of the structure presented in the limit. In accordancewith language learning we call this learning criterion InfEx-learning(explanatory learning from informant).  Our main contribution is a complete characterization of which families ofequivalence structures are InfEx-learnable. This characterization allows us toderive a bound of $\mathbf{0''}$ on the computational complexity required tolearn uniformly enumerable families of equivalence structures. We alsoinvestigate variants of InfEx-learning, including learning from text (where theonly information provided is which elements are related, and not which elementsare not related) and finite learning (where the first actual conjecture of thelearner has to be correct). Finally, we show how learning families ofstructures relates to learning classes of languages by mapping learning tasksfor structures to equivalent learning tasks for languages.",http://arxiv.org/abs/1902.08006v1,,
876,"How to Organize your Deep Reinforcement Learning Agents: The Importance  of Communication Topology","In this empirical paper, we investigate how learning agents can be arrangedin more efficient communication topologies for improved learning. This is animportant problem because a common technique to improve speed and robustness oflearning in deep reinforcement learning and many other machine learningalgorithms is to run multiple learning agents in parallel. The standardcommunication architecture typically involves all agents intermittentlycommunicating with each other (fully connected topology) or with a centralizedserver (star topology). Unfortunately, optimizing the topology of communicationover the space of all possible graphs is a hard problem, so we borrow resultsfrom the networked optimization and collective intelligence literatures whichsuggest that certain families of network topologies can lead to strongimprovements over fully-connected networks. We start by introducing alternativenetwork topologies to DRL benchmark tasks under the Evolution Strategiesparadigm which we call Network Evolution Strategies. We explore the relativeperformance of the four main graph families and observe that one such family(Erdos-Renyi random graphs) empirically outperforms all other families,including the de facto fully-connected communication topologies. Additionally,the use of alternative network topologies has a multiplicative performanceeffect: we observe that when 1000 learning agents are arranged in a carefullydesigned communication topology, they can compete with 3000 agents arranged inthe de facto fully-connected topology. Overall, our work suggests thatdistributed machine learning algorithms would learn more efficiently if thecommunication topology between learning agents was optimized.",http://arxiv.org/abs/1811.12556v2,,
877,"A Greedy Approach to Adapting the Trace Parameter for Temporal  Difference Learning","One of the main obstacles to broad application of reinforcement learningmethods is the parameter sensitivity of our core learning algorithms. In manylarge-scale applications, online computation and function approximationrepresent key strategies in scaling up reinforcement learning algorithms. Inthis setting, we have effective and reasonably well understood algorithms foradapting the learning-rate parameter, online during learning. Suchmeta-learning approaches can improve robustness of learning and enablespecialization to current task, improving learning speed. Fortemporal-difference learning algorithms which we study here, there is yetanother parameter, $\lambda$, that similarly impacts learning speed andstability in practice. Unfortunately, unlike the learning-rate parameter,$\lambda$ parametrizes the objective function that temporal-difference methodsoptimize. Different choices of $\lambda$ produce different fixed-pointsolutions, and thus adapting $\lambda$ online and characterizing theoptimization is substantially more complex than adapting the learning-rateparameter. There are no meta-learning method for $\lambda$ that can achieve (1)incremental updating, (2) compatibility with function approximation, and (3)maintain stability of learning under both on and off-policy sampling. In thispaper we contribute a novel objective function for optimizing $\lambda$ as afunction of state rather than time. We derive a new incremental, linearcomplexity $\lambda$-adaption algorithm that does not require offline batchupdating or access to a model of the world, and present a suite of experimentsillustrating the practicality of our new algorithm in three different settings.Taken together, our contributions represent a concrete step towards black-boxapplication of temporal-difference learning methods in real world problems.",http://arxiv.org/abs/1607.00446v2,,
878,Learning to Transfer,"Transfer learning borrows knowledge from a source domain to facilitatelearning in a target domain. Two primary issues to be addressed in transferlearning are what and how to transfer. For a pair of domains, adoptingdifferent transfer learning algorithms results in different knowledgetransferred between them. To discover the optimal transfer learning algorithmthat maximally improves the learning performance in the target domain,researchers have to exhaustively explore all existing transfer learningalgorithms, which is computationally intractable. As a trade-off, a sub-optimalalgorithm is selected, which requires considerable expertise in an ad-hoc way.Meanwhile, it is widely accepted in educational psychology that human beingsimprove transfer learning skills of deciding what to transfer throughmeta-cognitive reflection on inductive transfer learning practices. Motivatedby this, we propose a novel transfer learning framework known as Learning toTransfer (L2T) to automatically determine what and how to transfer are the bestby leveraging previous transfer learning experiences. We establish the L2Tframework in two stages: 1) we first learn a reflection function encryptingtransfer learning skills from experiences; and 2) we infer what and how totransfer for a newly arrived pair of domains by optimizing the reflectionfunction. Extensive experiments demonstrate the L2T's superiority over severalstate-of-the-art transfer learning algorithms and its effectiveness ondiscovering more transferable knowledge.",http://arxiv.org/abs/1708.05629v1,,
879,Sparse coding for multitask and transfer learning,"We investigate the use of sparse coding and dictionary learning in thecontext of multitask and transfer learning. The central assumption of ourlearning method is that the tasks parameters are well approximated by sparselinear combinations of the atoms of a dictionary on a high or infinitedimensional space. This assumption, together with the large quantity ofavailable data in the multitask and transfer learning settings, allows aprincipled choice of the dictionary. We provide bounds on the generalizationerror of this approach, for both settings. Numerical experiments on onesynthetic and two real datasets show the advantage of our method over singletask learning, a previous method based on orthogonal and dense representationof the tasks and a related method learning task grouping.",http://arxiv.org/abs/1209.0738v3,,
880,Guaranteed Classification via Regularized Similarity Learning,"Learning an appropriate (dis)similarity function from the available data is acentral problem in machine learning, since the success of many machine learningalgorithms critically depends on the choice of a similarity function to compareexamples. Despite many approaches for similarity metric learning have beenproposed, there is little theoretical study on the links between similaritymet- ric learning and the classification performance of the result classifier.In this paper, we propose a regularized similarity learning formulationassociated with general matrix-norms, and establish their generalizationbounds. We show that the generalization error of the resulting linear separatorcan be bounded by the derived generalization bound of similarity learning. Thisshows that a good gen- eralization of the learnt similarity function guaranteesa good classification of the resulting linear classifier. Our results extendand improve those obtained by Bellet at al. [3]. Due to the techniquesdependent on the notion of uniform stability [6], the bound obtained thereholds true only for the Frobenius matrix- norm regularization. Our techniquesusing the Rademacher complexity [5] and its related Khinchin-type inequalityenable us to establish bounds for regularized similarity learning formulationsassociated with general matrix-norms including sparse L 1 -norm and mixed(2,1)-norm.",http://arxiv.org/abs/1306.3108v2,,
881,Parameter-Free Spectral Kernel Learning,"Due to the growing ubiquity of unlabeled data, learning with unlabeled datais attracting increasing attention in machine learning. In this paper, wepropose a novel semi-supervised kernel learning method which can seamlesslycombine manifold structure of unlabeled data and Regularized Least-Squares(RLS) to learn a new kernel. Interestingly, the new kernel matrix can beobtained analytically with the use of spectral decomposition of graph Laplacianmatrix. Hence, the proposed algorithm does not require any numericaloptimization solvers. Moreover, by maximizing kernel target alignment onlabeled data, we can also learn model parameters automatically with aclosed-form solution. For a given graph Laplacian matrix, our proposed methoddoes not need to tune any model parameter including the tradeoff parameter inRLS and the balance parameter for unlabeled data. Extensive experiments on tenbenchmark datasets show that our proposed two-stage parameter-free spectralkernel learning algorithm can obtain comparable performance with fine-tunedmanifold regularization methods in transductive setting, and outperformmultiple kernel learning in supervised setting.",http://arxiv.org/abs/1203.3495v1,,
882,Learning Granger Causality for Hawkes Processes,"Learning Granger causality for general point processes is a very challengingtask. In this paper, we propose an effective method, learning Grangercausality, for a special but significant type of point processes --- Hawkesprocess. We reveal the relationship between Hawkes process's impact functionand its Granger causality graph. Specifically, our model represents impactfunctions using a series of basis functions and recovers the Granger causalitygraph via group sparsity of the impact functions' coefficients. We propose aneffective learning algorithm combining a maximum likelihood estimator (MLE)with a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility ofour model allows to incorporate the clustering structure event types intolearning framework. We analyze our learning algorithm and propose an adaptiveprocedure to select basis functions. Experiments on both synthetic andreal-world data show that our method can learn the Granger causality graph andthe triggering patterns of the Hawkes processes simultaneously.",http://arxiv.org/abs/1602.04511v2,,
883,Matching Networks for One Shot Learning,"Learning from a few examples remains a key challenge in machine learning.Despite recent advances in important domains such as vision and language, thestandard supervised deep learning paradigm does not offer a satisfactorysolution for learning new concepts rapidly from little data. In this work, weemploy ideas from metric learning based on deep neural features and from recentadvances that augment neural networks with external memories. Our frameworklearns a network that maps a small labelled support set and an unlabelledexample to its label, obviating the need for fine-tuning to adapt to new classtypes. We then define one-shot learning problems on vision (using Omniglot,ImageNet) and language tasks. Our algorithm improves one-shot accuracy onImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared tocompeting approaches. We also demonstrate the usefulness of the same model onlanguage modeling by introducing a one-shot task on the Penn Treebank.",http://arxiv.org/abs/1606.04080v2,,
884,A Kernel Classification Framework for Metric Learning,"Learning a distance metric from the given training samples plays a crucialrole in many machine learning tasks, and various models and optimizationalgorithms have been proposed in the past decade. In this paper, we generalizeseveral state-of-the-art metric learning methods, such as large margin nearestneighbor (LMNN) and information theoretic metric learning (ITML), into a kernelclassification framework. First, doublets and triplets are constructed from thetraining samples, and a family of degree-2 polynomial kernel functions areproposed for pairs of doublets or triplets. Then, a kernel classificationframework is established, which can not only generalize many popular metriclearning methods such as LMNN and ITML, but also suggest new metric learningmethods, which can be efficiently implemented, interestingly, by using thestandard support vector machine (SVM) solvers. Two novel metric learningmethods, namely doublet-SVM and triplet-SVM, are then developed under theproposed framework. Experimental results show that doublet-SVM and triplet-SVMachieve competitive classification accuracies with state-of-the-art metriclearning methods such as ITML and LMNN but with significantly less trainingtime.",http://arxiv.org/abs/1309.5823v1,,
885,"Learning rates of $l^q$ coefficient regularization learning with  Gaussian kernel","Regularization is a well recognized powerful strategy to improve theperformance of a learning machine and $l^q$ regularization schemes with$0<q<\infty$ are central in use. It is known that different $q$ leads todifferent properties of the deduced estimators, say, $l^2$ regularization leadsto smooth estimators while $l^1$ regularization leads to sparse estimators.Then, how does the generalization capabilities of $l^q$ regularization learningvary with $q$? In this paper, we study this problem in the framework ofstatistical learning theory and show that implementing $l^q$ coefficientregularization schemes in the sample dependent hypothesis space associated withGaussian kernel can attain the same almost optimal learning rates for all$0<q<\infty$. That is, the upper and lower bounds of learning rates for $l^q$regularization learning are asymptotically identical for all $0<q<\infty$. Ourfinding tentatively reveals that, in some modeling contexts, the choice of $q$might not have a strong impact with respect to the generalization capability.From this perspective, $q$ can be arbitrarily specified, or specified merely byother no generalization criteria like smoothness, computational complexity,sparsity, etc..",http://arxiv.org/abs/1312.5465v3,,
886,Fast Cross-Validation for Incremental Learning,"Cross-validation (CV) is one of the main tools for performance estimation andparameter tuning in machine learning. The general recipe for computing CVestimate is to run a learning algorithm separately for each CV fold, acomputationally expensive process. In this paper, we propose a new approach toreduce the computational burden of CV-based performance estimation. As opposedto all previous attempts, which are specific to a particular learning model orproblem domain, we propose a general method applicable to a large class ofincremental learning algorithms, which are uniquely fitted to big dataproblems. In particular, our method applies to a wide range of supervised andunsupervised learning tasks with different performance criteria, as long as thebase learning algorithm is incremental. We show that the running time of thealgorithm scales logarithmically, rather than linearly, in the number of CVfolds. Furthermore, the algorithm has favorable properties for parallel anddistributed implementation. Experiments with state-of-the-art incrementallearning algorithms confirm the practicality of the proposed method.",http://arxiv.org/abs/1507.00066v1,,
887,"Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced  Datasets in Machine Learning","Imbalanced-learn is an open-source python toolbox aiming at providing a widerange of methods to cope with the problem of imbalanced dataset frequentlyencountered in machine learning and pattern recognition. The implementedstate-of-the-art methods can be categorized into 4 groups: (i) under-sampling,(ii) over-sampling, (iii) combination of over- and under-sampling, and (iv)ensemble learning methods. The proposed toolbox only depends on numpy, scipy,and scikit-learn and is distributed under MIT license. Furthermore, it is fullycompatible with scikit-learn and is part of the scikit-learn-contrib supportedproject. Documentation, unit tests as well as integration tests are provided toease usage and contribution. The toolbox is publicly available in GitHub:https://github.com/scikit-learn-contrib/imbalanced-learn.",http://arxiv.org/abs/1609.06570v1,,
888,Coordinated Multi-Agent Imitation Learning,"We study the problem of imitation learning from demonstrations of multiplecoordinating agents. One key challenge in this setting is that learning a goodmodel of coordination can be difficult, since coordination is often implicit inthe demonstrations and must be inferred as a latent variable. We propose ajoint approach that simultaneously learns a latent coordination model alongwith the individual policies. In particular, our method integrates unsupervisedstructure learning with conventional imitation learning. We illustrate thepower of our approach on a difficult problem of learning multiple policies forfine-grained behavior modeling in team sports, where different players occupydifferent roles in the coordinated team strategy. We show that having acoordination model to infer the roles of players yields substantially improvedimitation loss compared to conventional baselines.",http://arxiv.org/abs/1703.03121v2,,
889,"Learning Multimodal Transition Dynamics for Model-Based Reinforcement  Learning","In this paper we study how to learn stochastic, multimodal transitiondynamics in reinforcement learning (RL) tasks. We focus on evaluatingtransition function estimation, while we defer planning over this model tofuture work. Stochasticity is a fundamental property of many task environments.However, discriminative function approximators have difficulty estimatingmultimodal stochasticity. In contrast, deep generative models do capturecomplex high-dimensional outcome distributions. First we discuss why, amongstsuch models, conditional variational inference (VI) is theoretically mostappealing for model-based RL. Subsequently, we compare different VI models ontheir ability to learn complex stochasticity on simulated functions, as well ason a typical RL gridworld with multimodal dynamics. Results show VIsuccessfully predicts multimodal outcomes, but also robustly ignores these fordeterministic parts of the transition dynamics. In summary, we show a robustmethod to learn multimodal transitions using function approximation, which is akey preliminary for model-based RL in stochastic domains.",http://arxiv.org/abs/1705.00470v2,,
890,"Differentiable plasticity: training plastic neural networks with  backpropagation","How can we build agents that keep learning from experience, quickly andefficiently, after their initial training? Here we take inspiration from themain mechanism of learning in biological brains: synaptic plasticity, carefullytuned by evolution to produce efficient lifelong learning. We show thatplasticity, just like connection weights, can be optimized by gradient descentin large (millions of parameters) recurrent networks with Hebbian plasticconnections. First, recurrent plastic networks with more than two millionparameters can be trained to memorize and reconstruct sets of novel,high-dimensional 1000+ pixels natural images not seen during training.Crucially, traditional non-plastic recurrent networks fail to solve this task.Furthermore, trained plastic networks can also solve generic meta-learningtasks such as the Omniglot task, with competitive results and little parameteroverhead. Finally, in reinforcement learning settings, plastic networksoutperform a non-plastic equivalent in a maze exploration task. We concludethat differentiable plasticity may provide a powerful novel approach to thelearning-to-learn problem.",http://arxiv.org/abs/1804.02464v3,,
891,Deep Reinforcement Learning: An Overview,"In recent years, a specific machine learning method called deep learning hasgained huge attraction, as it has obtained astonishing results in broadapplications such as pattern recognition, speech recognition, computer vision,and natural language processing. Recent research has also been shown that deeplearning techniques can be combined with reinforcement learning methods tolearn useful representations for the problems with high dimensional raw datainput. This chapter reviews the recent advances in deep reinforcement learningwith a focus on the most used deep architectures such as autoencoders,convolutional neural networks and recurrent neural networks which havesuccessfully been come together with the reinforcement learning framework.",http://arxiv.org/abs/1806.08894v1,,
892,"Adaptive Learning Method of Recurrent Temporal Deep Belief Network to  Analyze Time Series Data","Deep Learning has the hierarchical network architecture to represent thecomplicated features of input patterns. Such architecture is well known torepresent higher learning capability compared with some conventional models ifthe best set of parameters in the optimal network structure is found. We havebeen developing the adaptive learning method that can discover the optimalnetwork structure in Deep Belief Network (DBN). The learning method canconstruct the network structure with the optimal number of hidden neurons ineach Restricted Boltzmann Machine and with the optimal number of layers in theDBN during learning phase. The network structure of the learning method can beself-organized according to given input patterns of big data set. In thispaper, we embed the adaptive learning method into the recurrent temporal RBMand the self-generated layer into DBN. In order to verify the effectiveness ofour proposed method, the experimental results are higher classificationcapability than the conventional methods in this paper.",http://arxiv.org/abs/1807.03953v1,,
893,"Joint Domain Alignment and Discriminative Feature Learning for  Unsupervised Deep Domain Adaptation","Recently, considerable effort has been devoted to deep domain adaptation incomputer vision and machine learning communities. However, most of existingwork only concentrates on learning shared feature representation by minimizingthe distribution discrepancy across different domains. Due to the fact that allthe domain alignment approaches can only reduce, but not remove the domainshift. Target domain samples distributed near the edge of the clusters, or farfrom their corresponding class centers are easily to be misclassified by thehyperplane learned from the source domain. To alleviate this issue, we proposeto joint domain alignment and discriminative feature learning, which couldbenefit both domain alignment and final classification. Specifically, aninstance-based discriminative feature learning method and a center-baseddiscriminative feature learning method are proposed, both of which guaranteethe domain invariant features with better intra-class compactness andinter-class separability. Extensive experiments show that learning thediscriminative features in the shared feature space can significantly boost theperformance of deep domain adaptation methods.",http://arxiv.org/abs/1808.09347v2,,
894,Deep learning systems as complex networks,"Thanks to the availability of large scale digital datasets and massiveamounts of computational power, deep learning algorithms can learnrepresentations of data by exploiting multiple levels of abstraction. Thesemachine learning methods have greatly improved the state-of-the-art in manychallenging cognitive tasks, such as visual object recognition, speechprocessing, natural language understanding and automatic translation. Inparticular, one class of deep learning models, known as deep belief networks,can discover intricate statistical structure in large data sets in a completelyunsupervised fashion, by learning a generative model of the data usingHebbian-like learning mechanisms. Although these self-organizing systems can beconveniently formalized within the framework of statistical mechanics, theirinternal functioning remains opaque, because their emergent dynamics cannot besolved analytically. In this article we propose to study deep belief networksusing techniques commonly employed in the study of complex networks, in orderto gain some insights into the structural and functional properties of thecomputational graph resulting from the learning process.",http://arxiv.org/abs/1809.10941v1,,
895,Multi-Source Neural Variational Inference,"Learning from multiple sources of information is an important problem inmachine-learning research. The key challenges are learning representations andformulating inference methods that take into account the complementarity andredundancy of various information sources. In this paper we formulate avariational autoencoder based multi-source learning framework in which eachencoder is conditioned on a different information source. This allows us torelate the sources via the shared latent variables by computing divergencemeasures between individual source's posterior approximations. We explore avariety of options to learn these encoders and to integrate the beliefs theycompute into a consistent posterior approximation. We visualise learned beliefson a toy dataset and evaluate our methods for learning shared representationsand structured output prediction, showing trade-offs of learning separateencoders for each information source. Furthermore, we demonstrate how conflictdetection and redundancy can increase robustness of inference in a multi-sourcesetting.",http://arxiv.org/abs/1811.04451v2,,
896,Iterative Deep Learning Based Unbiased Stereology With Human-in-the-Loop,"Lack of enough labeled data is a major problem in building machine learningbased models when the manual annotation (labeling) is error-prone, expensive,tedious, and time-consuming. In this paper, we introduce an iterative deeplearning based method to improve segmentation and counting of cells based onunbiased stereology applied to regions of interest of extended depth of field(EDF) images. This method uses an existing machine learning algorithm calledthe adaptive segmentation algorithm (ASA) to generate masks (verified by auser) for EDF images to train deep learning models. Then an iterative deeplearning approach is used to feed newly predicted and accepted deep learningmasks/images (verified by a user) to the training set of the deep learningmodel. The error rate in unbiased stereology count of cells on an unseen testset reduced from about 3 % to less than 1 % after 5 iterations of the iterativedeep learning based unbiased stereology process.",http://arxiv.org/abs/1901.04355v1,,
897,A Unifying Bayesian View of Continual Learning,"Some machine learning applications require continual learning - where datacomes in a sequence of datasets, each is used for training and then permanentlydiscarded. From a Bayesian perspective, continual learning seemsstraightforward: Given the model posterior one would simply use this as theprior for the next task. However, exact posterior evaluation is intractablewith many models, especially with Bayesian neural networks (BNNs). Instead,posterior approximations are often sought. Unfortunately, when posteriorapproximations are used, prior-focused approaches do not succeed in evaluationsdesigned to capture properties of realistic continual learning use cases. As analternative to prior-focused methods, we introduce a new approximate Bayesianderivation of the continual learning loss. Our loss does not rely on theposterior from earlier tasks, and instead adapts the model itself by changingthe likelihood term. We call these approaches likelihood-focused. We thencombine prior- and likelihood-focused methods into one objective, tying the twoviews together under a single unifying framework of approximate Bayesiancontinual learning.",http://arxiv.org/abs/1902.06494v1,,
898,Implicit Kernel Learning,"Kernels are powerful and versatile tools in machine learning and statistics.Although the notion of universal kernels and characteristic kernels has beenstudied, kernel selection still greatly influences the empirical performance.While learning the kernel in a data driven way has been investigated, in thispaper we explore learning the spectral distribution of kernel via implicitgenerative models parametrized by deep neural networks. We called our methodImplicit Kernel Learning (IKL). The proposed framework is simple to train andinference is performed via sampling random Fourier features. We investigate twoapplications of the proposed IKL as examples, including generative adversarialnetworks with MMD (MMD GAN) and standard supervised learning. Empirically, MMDGAN with IKL outperforms vanilla predefined kernels on both image and textgeneration benchmarks; using IKL with Random Kitchen Sinks also leads tosubstantial improvement over existing state-of-the-art kernel learningalgorithms on popular supervised learning benchmarks. Theory and conditions forusing IKL in both applications are also studied as well as connections toprevious state-of-the-art methods.",http://arxiv.org/abs/1902.10214v1,,
899,Online Convex Dictionary Learning,"Dictionary learning is a dimensionality reduction technique widely used indata mining, machine learning and signal processing alike. Nevertheless, manydictionary learning algorithms such as variants of Matrix Factorization (MF) donot adequately scale with the size of available datasets. Furthermore, scalabledictionary learning methods lack interpretability of the derived dictionarymatrix. To mitigate these two issues, we propose a novel low-complexity, batchonline convex dictionary learning algorithm. The algorithm sequentiallyprocesses small batches of data maintained in a fixed amount of storage space,and produces meaningful dictionaries that satisfy convexity constraints. Ouranalytical results are two-fold. First, we establish convergence guarantees forthe proposed online learning scheme. Second, we show that a subsequence of thegenerated dictionaries converges to a stationary point of theapproximation-error function. Experimental results on synthetic and real worlddatasets demonstrate both the computational savings of the proposed onlinemethod with respect to convex non-negative MF, and performance guaranteescomparable to those of online non-convex learning.",http://arxiv.org/abs/1904.02580v1,,
900,Building Machines That Learn and Think Like People,"Recent progress in artificial intelligence (AI) has renewed interest inbuilding systems that learn and think like people. Many advances have come fromusing deep neural networks trained end-to-end in tasks such as objectrecognition, video games, and board games, achieving performance that equals oreven beats humans in some respects. Despite their biological inspiration andperformance achievements, these systems differ from human intelligence incrucial ways. We review progress in cognitive science suggesting that trulyhuman-like learning and thinking machines will have to reach beyond currentengineering trends in both what they learn, and how they learn it.Specifically, we argue that these machines should (a) build causal models ofthe world that support explanation and understanding, rather than merelysolving pattern recognition problems; (b) ground learning in intuitive theoriesof physics and psychology, to support and enrich the knowledge that is learned;and (c) harness compositionality and learning-to-learn to rapidly acquire andgeneralize knowledge to new tasks and situations. We suggest concretechallenges and promising routes towards these goals that can combine thestrengths of recent neural network advances with more structured cognitivemodels.",http://arxiv.org/abs/1604.00289v3,,
901,Distributed Multi-Task Relationship Learning,"Multi-task learning aims to learn multiple tasks jointly by exploiting theirrelatedness to improve the generalization performance for each task.Traditionally, to perform multi-task learning, one needs to centralize datafrom all the tasks to a single machine. However, in many real-worldapplications, data of different tasks may be geo-distributed over differentlocal machines. Due to heavy communication caused by transmitting the data andthe issue of data privacy and security, it is impossible to send data ofdifferent task to a master machine to perform multi-task learning. Therefore,in this paper, we propose a distributed multi-task learning framework thatsimultaneously learns predictive models for each task as well as taskrelationships between tasks alternatingly in the parameter server paradigm. Inour framework, we first offer a general dual form for a family of regularizedmulti-task relationship learning methods. Subsequently, we propose acommunication-efficient primal-dual distributed optimization algorithm to solvethe dual problem by carefully designing local subproblems to make the dualproblem decomposable. Moreover, we provide a theoretical convergence analysisfor the proposed algorithm, which is specific for distributed multi-taskrelationship learning. We conduct extensive experiments on both synthetic andreal-world datasets to evaluate our proposed framework in terms ofeffectiveness and convergence.",http://arxiv.org/abs/1612.04022v3,,
902,"An Improvement of Data Classification Using Random Multimodel Deep  Learning (RMDL)","The exponential growth in the number of complex datasets every year requiresmore enhancement in machine learning methods to provide robust and accuratedata classification. Lately, deep learning approaches have achieved surpassingresults in comparison to previous machine learning algorithms. However, findingthe suitable structure for these models has been a challenge for researchers.This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,deep learning approach for classification. RMDL solves the problem of findingthe best deep learning structure and architecture while simultaneouslyimproving robustness and accuracy through ensembles of deep learningarchitectures. In short, RMDL trains multiple randomly generated models of DeepNeural Network (DNN), Convolutional Neural Network (CNN) and Recurrent NeuralNetwork (RNN) in parallel and combines their results to produce better resultof any of those models individually. In this paper, we describe RMDL model andcompare the results for image and text classification as well as facerecognition. We used MNIST and CIFAR-10 datasets as ground truth datasets forimage classification and WOS, Reuters, IMDB, and 20newsgroup datasets for textclassification. Lastly, we used ORL dataset to compare the model performance onface recognition task.",http://arxiv.org/abs/1808.08121v1,,
903,"Post-Proceedings of the First International Workshop on Learning and  Nonmonotonic Reasoning","Knowledge Representation and Reasoning and Machine Learning are two importantfields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming(ASP) provide formal languages for representing and reasoning with commonsenseknowledge and realize declarative problem solving in AI. On the other side,Inductive Logic Programming (ILP) realizes Machine Learning in logicprogramming, which provides a formal background to inductive learning and thetechniques have been applied to the fields of relational learning and datamining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning whilelack the ability of learning. By contrast, ILP realizes inductive learningwhile most techniques have been developed under the classical monotonic logic.With this background, some researchers attempt to combine techniques in thecontext of nonmonotonic ILP. Such combination will introduce a learningmechanism to programs and would exploit new applications on the NMLP side,while on the ILP side it will extend the representation language and enable usto use existing solvers. Cross-fertilization between learning and nonmonotonicreasoning can also occur in such as the use of answer set solvers for ILP,speed-up learning while running answer set solvers, learning action theories,learning transition rules in dynamical systems, abductive learning, learningbiological networks with inhibition, and applications involving default andnegation. This workshop is the first attempt to provide an open forum for theidentification of problems and discussion of possible collaborations amongresearchers with complementary expertise. The workshop was held on September15th of 2013 in Corunna, Spain. This post-proceedings contains five technicalpapers (out of six accepted papers) and the abstract of the invited talk by LucDe Raedt.",http://arxiv.org/abs/1311.4639v1,,
904,"Large Scale Local Online Similarity/Distance Learning Framework based on  Passive/Aggressive","Similarity/Distance measures play a key role in many machine learning,pattern recognition, and data mining algorithms, which leads to the emergenceof metric learning field. Many metric learning algorithms learn a globaldistance function from data that satisfy the constraints of the problem.However, in many real-world datasets that the discrimination power of featuresvaries in the different regions of input space, a global metric is often unableto capture the complexity of the task. To address this challenge, local metriclearning methods are proposed that learn multiple metrics across the differentregions of input space. Some advantages of these methods are high flexibilityand the ability to learn a nonlinear mapping but typically achieves at theexpense of higher time requirement and overfitting problem. To overcome thesechallenges, this research presents an online multiple metric learningframework. Each metric in the proposed framework is composed of a global and alocal component learned simultaneously. Adding a global component to a localmetric efficiently reduce the problem of overfitting. The proposed framework isalso scalable with both sample size and the dimension of input data. To thebest of our knowledge, this is the first local online similarity/distancelearning framework based on PA (Passive/Aggressive). In addition, forscalability with the dimension of input data, DRP (Dual Random Projection) isextended for local online learning in the present work. It enables our methodsto be run efficiently on high-dimensional datasets, while maintains theirpredictive performance. The proposed framework provides a straightforward localextension to any global online similarity/distance learning algorithm based onPA.",http://arxiv.org/abs/1804.01900v1,,
905,"Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning  Workloads","We present ease.ml, a declarative machine learning service platform we builtto support more than ten research groups outside the computer sciencedepartments at ETH Zurich for their machine learning needs. With ease.ml, auser defines the high-level schema of a machine learning application andsubmits the task via a Web interface. The system automatically deals with therest, such as model selection and data movement. In this paper, we describe theease.ml architecture and focus on a novel technical problem introduced byease.ml regarding resource allocation. We ask, as a ""service provider"" thatmanages a shared cluster of machines among all our users running machinelearning workloads, what is the resource allocation strategy that maximizes theglobal satisfaction of all our users?  Resource allocation is a critical yet subtle issue in this multi-tenantscenario, as we have to balance between efficiency and fairness. We firstformalize the problem that we call multi-tenant model selection, aiming forminimizing the total regret of all users running automatic model selectiontasks. We then develop a novel algorithm that combines multi-armed bandits withBayesian optimization and prove a regret bound under the multi-tenant setting.Finally, we report our evaluation of ease.ml on synthetic data and on oneservice we are providing to our users, namely, image classification with deepneural networks. Our experimental evaluation results show that our proposedsolution can be up to 9.8x faster in achieving the same global quality for allusers as the two popular heuristics used by our users before ease.ml.",http://arxiv.org/abs/1708.07308v1,,
906,Calibrated Boosting-Forest,"Excellent ranking power along with well calibrated probability estimates areneeded in many classification tasks. In this paper, we introduce a technique,Calibrated Boosting-Forest that captures both. This novel technique is anensemble of gradient boosting machines that can support both continuous andbinary labels. While offering superior ranking power over any individualregression or classification model, Calibrated Boosting-Forest is able topreserve well calibrated posterior probabilities. Along with these benefits, weprovide an alternative to the tedious step of tuning gradient boostingmachines. We demonstrate that tuning Calibrated Boosting-Forest can be reducedto a simple hyper-parameter selection. We further establish that increasingthis hyper-parameter improves the ranking performance under a diminishingreturn. We examine the effectiveness of Calibrated Boosting-Forest onligand-based virtual screening where both continuous and binary labels areavailable and compare the performance of Calibrated Boosting-Forest withlogistic regression, gradient boosting machine and deep learning. CalibratedBoosting-Forest achieved an approximately 48% improvement compared to astate-of-art deep learning model. Moreover, it achieved around 95% improvementon probability quality measurement compared to the best individual gradientboosting machine. Calibrated Boosting-Forest offers a benchmark demonstrationthat in the field of ligand-based virtual screening, deep learning is not theuniversally dominant machine learning model and good calibrated probabilitiescan better facilitate virtual screening process.",http://arxiv.org/abs/1710.05476v3,,
907,"A Deep Belief Network Based Machine Learning System for Risky Host  Detection","To assure cyber security of an enterprise, typically SIEM (SecurityInformation and Event Management) system is in place to normalize securityevent from different preventive technologies and flag alerts. Analysts in thesecurity operation center (SOC) investigate the alerts to decide if it is trulymalicious or not. However, generally the number of alerts is overwhelming withmajority of them being false positive and exceeding the SOC's capacity tohandle all alerts. There is a great need to reduce the false positive rate asmuch as possible. While most previous research focused on network intrusiondetection, we focus on risk detection and propose an intelligent Deep BeliefNetwork machine learning system. The system leverages alert information,various security logs and analysts' investigation results in a real enterpriseenvironment to flag hosts that have high likelihood of being compromised. Textmining and graph based method are used to generate targets and create featuresfor machine learning. In the experiment, Deep Belief Network is compared withother machine learning algorithms, including multi-layer neural network, randomforest, support vector machine and logistic regression. Results on realenterprise data indicate that the deep belief network machine learning systemperforms better than other algorithms for our problem and is six times moreeffective than current rule-based system. We also implement the whole systemfrom data collection, label creation, feature engineering to host scoregeneration in a real enterprise production environment.",http://arxiv.org/abs/1801.00025v1,,
908,Computational Power and the Social Impact of Artificial Intelligence,"Machine learning is a computational process. To that end, it is inextricablytied to computational power - the tangible material of chips and semiconductorsthat the algorithms of machine intelligence operate on. Most obviously,computational power and computing architectures shape the speed of training andinference in machine learning, and therefore influence the rate of progress inthe technology. But, these relationships are more nuanced than that: hardwareshapes the methods used by researchers and engineers in the design anddevelopment of machine learning models. Characteristics such as the powerconsumption of chips also define where and how machine learning can be used inthe real world.  Despite this, many analyses of the social impact of the current wave ofprogress in AI have not substantively brought the dimension of hardware intotheir accounts. While a common trope in both the popular press and scholarlyliterature is to highlight the massive increase in computational power that hasenabled the recent breakthroughs in machine learning, the analysis frequentlygoes no further than this observation around magnitude. This paper aims to digmore deeply into the relationship between computational power and thedevelopment of machine learning. Specifically, it examines how changes incomputing architectures, machine learning methodologies, and supply chainsmight influence the future of AI. In doing so, it seeks to trace a set ofspecific relationships between this underlying hardware layer and the broadersocial impacts and risks around AI.",http://arxiv.org/abs/1803.08971v1,,
909,"Machine Learning Distinguishes Neurosurgical Skill Levels in a Virtual  Reality Tumor Resection Task","Background: Virtual reality simulators and machine learning have thepotential to augment understanding, assessment and training of psychomotorperformance in neurosurgery residents. Objective: This study outlines the firstapplication of machine learning to distinguish ""skilled"" and ""novice""psychomotor performance during a virtual reality neurosurgical task. Methods:Twenty-three neurosurgeons and senior neurosurgery residents comprising the""skilled"" group and 92 junior neurosurgery residents and medical students the""novice"" group. The task involved removing a series of virtual brain tumorswithout causing injury to surrounding tissue. Over 100 features were extractedand 68 selected using t-test analysis. These features were provided to 4classifiers: K-Nearest Neighbors, Parzen Window, Support Vector Machine, andFuzzy K-Nearest Neighbors. Equal Error Rate was used to assess classifierperformance. Results: Ratios of train set size to test set size from 10% to 90%and 5 to 30 features, chosen by the forward feature selection algorithm, wereemployed. A working point of 50% train to test set size ratio and 15 featuresresulted in an equal error rates as low as 8.3% using the Fuzzy K-NearestNeighbors classifier. Conclusion: Machine learning may be one component helpingrealign the traditional apprenticeship educational paradigm to a more objectivemodel based on proven performance standards.  Keywords: Artificial intelligence, Classifiers, Machine learning,Neurosurgery skill assessment, Surgical education, Tumor resection, Virtualreality simulation",http://arxiv.org/abs/1811.08159v1,,
910,"Shapley regressions: A framework for statistical inference on machine  learning models","Machine learning models often excel in the accuracy of their predictions butare opaque due to their non-linear and non-parametric structure. This makesstatistical inference challenging and disqualifies them from many applicationswhere model interpretability is crucial. This paper proposes the Shapleyregression framework as an approach for statistical inference on non-linear ornon-parametric models. Inference is performed based on the Shapley valuedecomposition of a model, a pay-off concept from cooperative game theory. Ishow that universal approximators from machine learning are estimationconsistent and introduce hypothesis tests for individual variablecontributions, model bias and parametric functional forms. The inferenceproperties of state-of-the-art machine learning models - like artificial neuralnetworks, support vector machines and random forests - are investigated usingnumerical simulations and real-world data. The proposed framework is unique inthe sense that it is identical to the conventional case of statisticalinference on a linear model if the model is linear in parameters. This makes ita well-motivated extension to more general models and strengthens the case forthe use of machine learning to inform decisions.",http://arxiv.org/abs/1903.04209v1,,
911,Learning to Teach with Dynamic Loss Functions,"Teaching is critical to human society: it is with teaching that prospectivestudents are educated and human civilization can be inherited and advanced. Agood teacher not only provides his/her students with qualified teachingmaterials (e.g., textbooks), but also sets up appropriate learning objectives(e.g., course projects and exams) considering different situations of astudent. When it comes to artificial intelligence, treating machine learningmodels as students, the loss functions that are optimized act as perfectcounterparts of the learning objective set by the teacher. In this work, weexplore the possibility of imitating human teaching behaviors by dynamicallyand automatically outputting appropriate loss functions to train machinelearning models. Different from typical learning settings in which the lossfunction of a machine learning model is predefined and fixed, in our framework,the loss function of a machine learning model (we call it student) is definedby another machine learning model (we call it teacher). The ultimate goal ofteacher model is cultivating the student to have better performance measured ondevelopment dataset. Towards that end, similar to human teaching, the teacher,a parametric model, dynamically outputs different loss functions that will beused and optimized by its student model at different training stages. Wedevelop an efficient learning method for the teacher model that makes gradientbased optimization possible, exempt of the ineffective solutions such as policyoptimization. We name our method as ""learning to teach with dynamic lossfunctions"" (L2T-DLF for short). Extensive experiments on real world tasksincluding image classification and neural machine translation demonstrate thatour method significantly improves the quality of various student models.",http://arxiv.org/abs/1810.12081v1,,
912,Online Learning for Matrix Factorization and Sparse Coding,"Sparse coding--that is, modelling data vectors as sparse linear combinationsof basis elements--is widely used in machine learning, neuroscience, signalprocessing, and statistics. This paper focuses on the large-scale matrixfactorization problem that consists of learning the basis set, adapting it tospecific data. Variations of this problem include dictionary learning in signalprocessing, non-negative matrix factorization and sparse principal componentanalysis. In this paper, we propose to address these tasks with a new onlineoptimization algorithm, based on stochastic approximations, which scales upgracefully to large datasets with millions of training samples, and extendsnaturally to various matrix factorization formulations, making it suitable fora wide range of learning problems. A proof of convergence is presented, alongwith experiments with natural images and genomic data demonstrating that itleads to state-of-the-art performance in terms of speed and optimization forboth small and large datasets.",http://arxiv.org/abs/0908.0050v2,,
913,On the Universality of Online Mirror Descent,"We show that for a general class of convex online learning problems, MirrorDescent can always achieve a (nearly) optimal regret guarantee.",http://arxiv.org/abs/1107.4080v1,,
914,Deep Learning using Linear Support Vector Machines,"Recently, fully-connected and convolutional neural networks have been trainedto achieve state-of-the-art performance on a wide variety of tasks such asspeech recognition, image classification, natural language processing, andbioinformatics. For classification tasks, most of these ""deep learning"" modelsemploy the softmax activation function for prediction and minimizecross-entropy loss. In this paper, we demonstrate a small but consistentadvantage of replacing the softmax layer with a linear support vector machine.Learning minimizes a margin-based loss instead of the cross-entropy loss. Whilethere have been various combinations of neural nets and SVMs in prior art, ourresults using L2-SVMs show that by simply replacing softmax with linear SVMsgives significant gains on popular deep learning datasets MNIST, CIFAR-10, andthe ICML 2013 Representation Learning Workshop's face expression recognitionchallenge.",http://arxiv.org/abs/1306.0239v4,,
915,Ground Metric Learning,"Transportation distances have been used for more than a decade now in machinelearning to compare histograms of features. They have one parameter: the groundmetric, which can be any metric between the features themselves. As is the casefor all parameterized distances, transportation distances can only prove usefulin practice when this parameter is carefully chosen. To date, the only optionavailable to practitioners to set the ground metric parameter was to rely on apriori knowledge of the features, which limited considerably the scope ofapplication of transportation distances. We propose to lift this limitation andconsider instead algorithms that can learn the ground metric using only atraining set of labeled histograms. We call this approach ground metriclearning. We formulate the problem of learning the ground metric as theminimization of the difference of two polyhedral convex functions over a convexset of distance matrices. We follow the presentation of our algorithms withpromising experimental results on binary classification tasks using GISTdescriptors of images taken in the Caltech-256 set.",http://arxiv.org/abs/1110.2306v1,,
916,Distance Metric Learning for Kernel Machines,"Recent work in metric learning has significantly improved thestate-of-the-art in k-nearest neighbor classification. Support vector machines(SVM), particularly with RBF kernels, are amongst the most popularclassification algorithms that uses distance metrics to compare examples. Thispaper provides an empirical analysis of the efficacy of three of the mostpopular Mahalanobis metric learning algorithms as pre-processing for SVMtraining. We show that none of these algorithms generate metrics that lead toparticularly satisfying improvements for SVM-RBF classification. As a remedy weintroduce support vector metric learning (SVML), a novel algorithm thatseamlessly combines the learning of a Mahalanobis metric with the training ofthe RBF-SVM parameters. We demonstrate the capabilities of SVML on ninebenchmark data sets of varying sizes and difficulties. In our study, SVMLoutperforms all alternative state-of-the-art metric learning algorithms interms of accuracy and establishes itself as a serious alternative to thestandard Euclidean metric with model selection by cross validation.",http://arxiv.org/abs/1208.3422v2,,
917,"Challenges in Representation Learning: A report on three machine  learning contests","The ICML 2013 Workshop on Challenges in Representation Learning focused onthree challenges: the black box learning challenge, the facial expressionrecognition challenge, and the multimodal learning challenge. We describe thedatasets created for these challenges and summarize the results of thecompetitions. We provide suggestions for organizers of future challenges andsome comments on what kind of knowledge can be gained from machine learningcompetitions.",http://arxiv.org/abs/1307.0414v1,,
918,Actively Learning to Attract Followers on Twitter,"Twitter, a popular social network, presents great opportunities for on-linemachine learning research. However, previous research has focused almostentirely on learning from passively collected data. We study the problem oflearning to acquire followers through normative user behavior, as opposed tothe mass following policies applied by many bots. We formalize the problem as acontextual bandit problem, in which we consider retweeting content to be theaction chosen and each tweet (content) is accompanied by context. We designreward signals based on the change in followers. The result of our month longexperiment with 60 agents suggests that (1) aggregating experience acrossagents can adversely impact prediction accuracy and (2) the Twitter community'sresponse to different actions is non-stationary. Our findings suggest thatactively learning on-line can provide deeper insights about how to attractfollowers than machine learning over passively collected data alone.",http://arxiv.org/abs/1504.04114v1,,
919,Monotonic Calibrated Interpolated Look-Up Tables,"Real-world machine learning applications may require functions that arefast-to-evaluate and interpretable. In particular, guaranteed monotonicity ofthe learned function can be critical to user trust. We propose meeting thesegoals for low-dimensional machine learning problems by learning flexible,monotonic functions using calibrated interpolated look-up tables. We extend thestructural risk minimization framework of lattice regression to train monotoniclook-up tables by solving a convex problem with appropriate linear inequalityconstraints. In addition, we propose jointly learning interpretablecalibrations of each feature to normalize continuous features and handlecategorical or missing data, at the cost of making the objective non-convex. Weaddress large-scale learning through parallelization, mini-batching, andpropose random sampling of additive regularizer terms. Case studies withreal-world problems with five to sixteen features and thousands to millions oftraining samples demonstrate the proposed monotonic functions can achievestate-of-the-art accuracy on practical problems while providing greatertransparency to users.",http://arxiv.org/abs/1505.06378v3,,
920,Large-Scale Feature Learning With Spike-and-Slab Sparse Coding,"We consider the problem of object recognition with a large number of classes.In order to overcome the low amount of labeled examples available in thissetting, we introduce a new feature learning and extraction procedure based ona factor model we call spike-and-slab sparse coding (S3C). Prior work on S3Chas not prioritized the ability to exploit parallel architectures and scale S3Cto the enormous problem sizes needed for object recognition. We present a novelinference procedure for appropriate for use with GPUs which allows us todramatically increase both the training set size and the amount of latentfactors that S3C may be trained with. We demonstrate that this approachimproves upon the supervised learning capabilities of both sparse coding andthe spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10dataset. We use the CIFAR-100 dataset to demonstrate that our method scales tolarge numbers of classes better than previous methods. Finally, we use ourmethod to win the NIPS 2011 Workshop on Challenges In Learning HierarchicalModels? Transfer Learning Challenge.",http://arxiv.org/abs/1206.6407v1,,
921,Similarity Learning for Provably Accurate Sparse Linear Classification,"In recent years, the crucial importance of metrics in machine learningalgorithms has led to an increasing interest for optimizing distance andsimilarity functions. Most of the state of the art focus on learningMahalanobis distances (requiring to fulfill a constraint of positivesemi-definiteness) for use in a local k-NN algorithm. However, no theoreticallink is established between the learned metrics and their performance inclassification. In this paper, we make use of the formal framework of goodsimilarities introduced by Balcan et al. to design an algorithm for learning anon PSD linear similarity optimized in a nonlinear feature space, which is thenused to build a global linear classifier. We show that our approach has uniformstability and derive a generalization bound on the classification error.Experiments performed on various datasets confirm the effectiveness of ourapproach compared to state-of-the-art methods and provide evidence that (i) itis fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.",http://arxiv.org/abs/1206.6476v1,,
922,Joint Stochastic Approximation learning of Helmholtz Machines,"Though with progress, model learning and performing posterior inference stillremains a common challenge for using deep generative models, especially forhandling discrete hidden variables. This paper is mainly concerned withalgorithms for learning Helmholz machines, which is characterized by pairingthe generative model with an auxiliary inference model. A common drawback ofprevious learning algorithms is that they indirectly optimize some bounds ofthe targeted marginal log-likelihood. In contrast, we successfully develop anew class of algorithms, based on stochastic approximation (SA) theory of theRobbins-Monro type, to directly optimize the marginal log-likelihood andsimultaneously minimize the inclusive KL-divergence. The resulting learningalgorithm is thus called joint SA (JSA). Moreover, we construct an effectiveMCMC operator for JSA. Our results on the MNIST datasets demonstrate that theJSA's performance is consistently superior to that of competing algorithms likeRWS, for learning a range of difficult models.",http://arxiv.org/abs/1603.06170v2,,
923,Operator-valued Kernels for Learning from Functional Response Data,"In this paper we consider the problems of supervised classification andregression in the case where attributes and labels are functions: a data isrepresented by a set of functions, and the label is also a function. We focuson the use of reproducing kernel Hilbert space theory to learn from suchfunctional data. Basic concepts and properties of kernel-based learning areextended to include the estimation of function-valued functions. In thissetting, the representer theorem is restated, a set of rigorously definedinfinite-dimensional operator-valued kernels that can be valuably applied whenthe data are functions is described, and a learning algorithm for nonlinearfunctional data analysis is introduced. The methodology is illustrated throughspeech and audio signal processing experiments.",http://arxiv.org/abs/1510.08231v3,,
924,Relational Similarity Machines,"This paper proposes Relational Similarity Machines (RSM): a fast, accurate,and flexible relational learning framework for supervised and semi-supervisedlearning tasks. Despite the importance of relational learning, most existingmethods are hard to adapt to different settings, due to issues with efficiency,scalability, accuracy, and flexibility for handling a wide variety ofclassification problems, data, constraints, and tasks. For instance, manyexisting methods perform poorly for multi-class classification problems, graphsthat are sparsely labeled or network data with low relational autocorrelation.In contrast, the proposed relational learning framework is designed to be (i)fast for learning and inference at real-time interactive rates, and (ii)flexible for a variety of learning settings (multi-class problems), constraints(few labeled instances), and application domains. The experiments demonstratethe effectiveness of RSM for a variety of tasks and data.",http://arxiv.org/abs/1608.00876v1,,
925,Applying Deep Learning to Basketball Trajectories,"One of the emerging trends for sports analytics is the growing use of playerand ball tracking data. A parallel development is deep learning predictiveapproaches that use vast quantities of data with less reliance on featureengineering. This paper applies recurrent neural networks in the form ofsequence modeling to predict whether a three-point shot is successful. Themodels are capable of learning the trajectory of a basketball without anyknowledge of physics. For comparison, a baseline static machine learning modelwith a full set of features, such as angle and velocity, in addition to thepositional data is also tested. Using a dataset of over 20,000 three pointersfrom NBA SportVu data, the models based simply on sequential positional dataoutperform a static feature rich machine learning model in predicting whether athree-point shot is successful. This suggests deep learning models may offer animprovement to traditional feature based machine learning methods for trackingdata.",http://arxiv.org/abs/1608.03793v2,,
926,Towards Interrogating Discriminative Machine Learning Models,"It is oftentimes impossible to understand how machine learning models reach adecision. While recent research has proposed various technical approaches toprovide some clues as to how a learning model makes individual decisions, theycannot provide users with ability to inspect a learning model as a completeentity. In this work, we propose a new technical approach that augments aBayesian regression mixture model with multiple elastic nets. Using theenhanced mixture model, we extract explanations for a target model throughglobal approximation. To demonstrate the utility of our approach, we evaluateit on different learning models covering the tasks of text mining and imagerecognition. Our results indicate that the proposed approach not onlyoutperforms the state-of-the-art technique in explaining individual decisionsbut also provides users with an ability to discover the vulnerabilities of alearning model.",http://arxiv.org/abs/1705.08564v1,,
927,Latent Intention Dialogue Models,"Developing a dialogue agent that is capable of making autonomous decisionsand communicating by natural language is one of the long-term goals of machinelearning research. Traditional approaches either rely on hand-crafting a smallstate-action set for applying reinforcement learning that is not scalable orconstructing deterministic models for learning dialogue sentences that fail tocapture natural conversational variability. In this paper, we propose a LatentIntention Dialogue Model (LIDM) that employs a discrete latent variable tolearn underlying dialogue intentions in the framework of neural variationalinference. In a goal-oriented dialogue scenario, these latent intentions can beinterpreted as actions guiding the generation of machine responses, which canbe further refined autonomously by reinforcement learning. The experimentalevaluation of LIDM shows that the model out-performs published benchmarks forboth corpus-based and human evaluation, demonstrating the effectiveness ofdiscrete latent variable models for learning goal-oriented dialogues.",http://arxiv.org/abs/1705.10229v1,,
928,"Counterfactual Learning from Bandit Feedback under Deterministic  Logging: A Case Study in Statistical Machine Translation","The goal of counterfactual learning for statistical machine translation (SMT)is to optimize a target SMT system from logged data that consist of userfeedback to translations that were predicted by another, historic SMT system. Achallenge arises by the fact that risk-averse commercial SMT systemsdeterministically log the most probable translation. The lack of sufficientexploration of the SMT output space seemingly contradicts the theoreticalrequirements for counterfactual learning. We show that counterfactual learningfrom deterministic bandit logs is possible nevertheless by smoothing outdeterministic components in learning. This can be achieved by additive andmultiplicative control variates that avoid degenerate behavior in empiricalrisk minimization. Our simulation experiments show improvements of up to 2 BLEUpoints by counterfactual learning from deterministic bandit feedback.",http://arxiv.org/abs/1707.09118v3,,
929,Representation Learning on Graphs: Methods and Applications,"Machine learning on graphs is an important and ubiquitous task withapplications ranging from drug design to friendship recommendation in socialnetworks. The primary challenge in this domain is finding a way to represent,or encode, graph structure so that it can be easily exploited by machinelearning models. Traditionally, machine learning approaches relied onuser-defined heuristics to extract features encoding structural informationabout a graph (e.g., degree statistics or kernel functions). However, recentyears have seen a surge in approaches that automatically learn to encode graphstructure into low-dimensional embeddings, using techniques based on deeplearning and nonlinear dimensionality reduction. Here we provide a conceptualreview of key advancements in this area of representation learning on graphs,including matrix factorization-based methods, random-walk based algorithms, andgraph neural networks. We review methods to embed individual nodes as well asapproaches to embed entire (sub)graphs. In doing so, we develop a unifiedframework to describe these recent approaches, and we highlight a number ofimportant applications and directions for future work.",http://arxiv.org/abs/1709.05584v3,,
930,Markov chain Hebbian learning algorithm with ternary synaptic units,"In spite of remarkable progress in machine learning techniques, thestate-of-the-art machine learning algorithms often keep machines from real-timelearning (online learning) due in part to computational complexity in parameteroptimization. As an alternative, a learning algorithm to train a memory in realtime is proposed, which is named as the Markov chain Hebbian learningalgorithm. The algorithm pursues efficient memory use during training in that(i) the weight matrix has ternary elements (-1, 0, 1) and (ii) each updatefollows a Markov chain--the upcoming update does not need past weight memory.The algorithm was verified by two proof-of-concept tasks (handwritten digitrecognition and multiplication table memorization) in which numbers were takenas symbols. Particularly, the latter bases multiplication arithmetic on memory,which may be analogous to humans' mental arithmetic. The memory-basedmultiplication arithmetic feasibly offers the basis of factorization,supporting novel insight into the arithmetic.",http://arxiv.org/abs/1711.08679v1,,
931,"Deep Learning with Permutation-invariant Operator for Multi-instance  Histopathology Classification","The computer-aided analysis of medical scans is a longstanding goal in themedical imaging field. Currently, deep learning has became a dominantmethodology for supporting pathologists and radiologist. Deep learningalgorithms have been successfully applied to digital pathology and radiology,nevertheless, there are still practical issues that prevent these tools to bewidely used in practice. The main obstacles are low number of available casesand large size of images (a.k.a. the small n, large p problem in machinelearning), and a very limited access to annotation at a pixel level that canlead to severe overfitting and large computational requirements. We propose tohandle these issues by introducing a framework that processes a medical imageas a collection of small patches using a single, shared neural network. Thefinal diagnosis is provided by combining scores of individual patches using apermutation-invariant operator (combination). In machine learning communitysuch approach is called a multi-instance learning (MIL).",http://arxiv.org/abs/1712.00310v2,,
932,Neural Conditional Gradients,"The move from hand-designed to learned optimizers in machine learning hasbeen quite successful for gradient-based and -free optimizers. When facing aconstrained problem, however, maintaining feasibility typically requires aprojection step, which might be computationally expensive and notdifferentiable. We show how the design of projection-free convex optimizationalgorithms can be cast as a learning problem based on Frank-Wolfe Networks:recurrent networks implementing the Frank-Wolfe algorithm aka. conditionalgradients. This allows them to learn to exploit structure when, e.g.,optimizing over rank-1 matrices. Our LSTM-learned optimizers outperformhand-designed as well learned but unconstrained ones. We demonstrate this fortraining support vector machines and softmax classifiers.",http://arxiv.org/abs/1803.04300v2,,
933,"Deep learning in business analytics and operations research: Models,  applications and managerial implications","Business analytics refers to methods and practices that create value throughdata for individuals, firms, and organizations. This field is currentlyexperiencing a radical shift due to the advent of deep learning: deep neuralnetworks promise improvements in prediction performance as compared to modelsfrom traditional machine learning. However, our research into the existing bodyof literature reveals a scarcity of research works utilizing deep learning inour discipline. Accordingly, the objectives of this work are as follows: (1) wemotivate why researchers and practitioners from business analytics shouldutilize deep neural networks and review potential use cases, necessaryrequirements, and benefits. (2) We investigate the added value to operationsresearch in different case studies with real data from entrepreneurialundertakings. All such cases demonstrate a higher prediction performance incomparison to traditional machine learning and thus direct value gains. (3) Weprovide guidelines and implications for researchers, managers and practitionersin operations research who want to advance their capabilities for businessanalytics with regard to deep learning. (4) We finally discuss directions forfuture research in the field of business analytics.",http://arxiv.org/abs/1806.10897v1,,
934,A hybrid deep learning approach for medical relation extraction,"Mining relationships between treatment(s) and medical problem(s) is vital inthe biomedical domain. This helps in various applications, such as decisionsupport system, safety surveillance, and new treatment discovery. We propose adeep learning approach that utilizes both word level and sentence-levelrepresentations to extract the relationships between treatment and problem.While deep learning techniques demand a large amount of data for training, wemake use of a rule-based system particularly for relationship classes withfewer samples. Our final relations are derived by jointly combining the resultsfrom deep learning and rule-based models. Our system achieved a promisingperformance on the relationship classes of I2b2 2010 relation extraction task.",http://arxiv.org/abs/1806.11189v1,,
935,"Ensemble learning with Conformal Predictors: Targeting credible  predictions of conversion from Mild Cognitive Impairment to Alzheimer's  Disease","Most machine learning classifiers give predictions for new examplesaccurately, yet without indicating how trustworthy predictions are. In themedical domain, this hampers their integration in decision support systems,which could be useful in the clinical practice. We use a supervised learningapproach that combines Ensemble learning with Conformal Predictors to predictconversion from Mild Cognitive Impairment to Alzheimer's Disease. Our goal isto enhance the classification performance (Ensemble learning) and complementeach prediction with a measure of credibility (Conformal Predictors). Ourresults showed the superiority of the proposed approach over a similar ensembleframework with standard classifiers.",http://arxiv.org/abs/1807.01619v2,,
936,Towards Reproducible Empirical Research in Meta-Learning,"Meta-learning is increasingly used to support the recommendation of machinelearning algorithms and their configurations. Such recommendations are madebased on meta-data, consisting of performance evaluations of algorithms onprior datasets, as well as characterizations of these datasets. Thesecharacterizations, also called meta-features, describe properties of the datawhich are predictive for the performance of machine learning algorithms trainedon them. Unfortunately, despite being used in a large number of studies,meta-features are not uniformly described and computed, making many empiricalstudies irreproducible and hard to compare. This paper aims to remedy this bysystematizing and standardizing data characterization measures used inmeta-learning, and performing an in-depth analysis of their utility. Moreover,it presents MFE, a new tool for extracting meta-features from datasets andidentify more subtle reproducibility issues in the literature, proposingguidelines for data characterization that strengthen reproducible empiricalresearch in meta-learning.",http://arxiv.org/abs/1808.10406v1,,
937,"Hyperparameter Learning for Conditional Kernel Mean Embeddings with  Rademacher Complexity Bounds","Conditional kernel mean embeddings are nonparametric models that encodeconditional expectations in a reproducing kernel Hilbert space. While theyprovide a flexible and powerful framework for probabilistic inference, theirperformance is highly dependent on the choice of kernel and regularizationhyperparameters. Nevertheless, current hyperparameter tuning methodspredominantly rely on expensive cross validation or heuristics that is notoptimized for the inference task. For conditional kernel mean embeddings withcategorical targets and arbitrary inputs, we propose a hyperparameter learningframework based on Rademacher complexity bounds to prevent overfitting bybalancing data fit against model complexity. Our approach only requires batchupdates, allowing scalable kernel hyperparameter tuning without invoking kernelapproximations. Experiments demonstrate that our learning framework outperformscompeting methods, and can be further extended to incorporate and learn deepneural network weights to improve generalization.",http://arxiv.org/abs/1809.00175v3,,
938,AutoLoss: Learning Discrete Schedules for Alternate Optimization,"Many machine learning problems involve iteratively and alternately optimizingdifferent task objectives with respect to different sets of parameters.Appropriately scheduling the optimization of a task objective or a set ofparameters is usually crucial to the quality of convergence. In this paper, wepresent AutoLoss, a meta-learning framework that automatically learns anddetermines the optimization schedule. AutoLoss provides a generic way torepresent and learn the discrete optimization schedule from metadata, allowsfor a dynamic and data-driven schedule in ML problems that involve alternatingupdates of different parameters or from different loss objectives. We applyAutoLoss on four ML tasks: d-ary quadratic regression, classification using amulti-layer perceptron (MLP), image generation using GANs, and multi-taskneural machine translation (NMT). We show that the AutoLoss controller is ableto capture the distribution of better optimization schedules that result inhigher quality of convergence on all four tasks. The trained AutoLosscontroller is generalizable -- it can guide and improve the learning of a newtask model with different specifications, or on different datasets.",http://arxiv.org/abs/1810.02442v1,,
939,Provable Gaussian Embedding with One Observation,"The success of machine learning methods heavily relies on having anappropriate representation for data at hand. Traditionally, machine learningapproaches relied on user-defined heuristics to extract features encodingstructural information about data. However, recently there has been a surge inapproaches that learn how to encode the data automatically in a low dimensionalspace. Exponential family embedding provides a probabilistic framework forlearning low-dimensional representation for various types of high-dimensionaldata. Though successful in practice, theoretical underpinnings for exponentialfamily embeddings have not been established. In this paper, we study theGaussian embedding model and develop the first theoretical results forexponential family embedding models. First, we show that, under mild condition,the embedding structure can be learned from one observation by leveraging theparameter sharing between different contexts even though the data are dependentwith each other. Second, we study properties of two algorithms used forlearning the embedding structure and establish convergence results for each ofthem. The first algorithm is based on a convex relaxation, while the othersolved the non-convex formulation of the problem directly. Experimentsdemonstrate the effectiveness of our approach.",http://arxiv.org/abs/1810.11098v1,,
940,Human-like machine learning: limitations and suggestions,"This paper attempts to address the issues of machine learning in its currentimplementation. It is known that machine learning algorithms require asignificant amount of data for training purposes, whereas recent developmentsin deep learning have increased this requirement dramatically. The performanceof an algorithm depends on the quality of data and hence, algorithms are asgood as the data they are trained on. Supervised learning is developed based onhuman learning processes by analysing named (i.e. annotated) objects, scenesand actions. Whether training on large quantities of data (i.e. big data) isthe right or the wrong approach, is debatable. The fact is, that trainingalgorithms the same way we learn ourselves, comes with limitations. This paperdiscusses the issues around applying a human-like approach to train algorithmsand the implications of this approach when using limited data. Several currentstudies involving non-data-driven algorithms and natural examples are alsodiscussed and certain alternative approaches are suggested.",http://arxiv.org/abs/1811.06052v1,,
941,Learning Interpretable Rules for Multi-label Classification,"Multi-label classification (MLC) is a supervised learning problem in which,contrary to standard multiclass classification, an instance can be associatedwith several class labels simultaneously. In this chapter, we advocate arule-based approach to multi-label classification. Rule learning algorithms areoften employed when one is not only interested in accurate predictions, butalso requires an interpretable theory that can be understood, analyzed, andqualitatively evaluated by domain experts. Ideally, by revealing patterns andregularities contained in the data, a rule-based theory yields new insights inthe application domain. Recently, several authors have started to investigatehow rule-based models can be used for modeling multi-label data. Discussingthis task in detail, we highlight some of the problems that make rule learningconsiderably more challenging for MLC than for conventional classification.While mainly focusing on our own previous work, we also provide a shortoverview of related work in this area.",http://arxiv.org/abs/1812.00050v2,,
942,"PD-ML-Lite: Private Distributed Machine Learning from Lighweight  Cryptography","Privacy is a major issue in learning from distributed data. Recently thecryptographic literature has provided several tools for this task. However,these tools either reduce the quality/accuracy of the learningalgorithm---e.g., by adding noise---or they incur a high performance penaltyand/or involve trusting external authorities.  We propose a methodology for {\sl private distributed machine learning fromlight-weight cryptography} (in short, PD-ML-Lite). We apply our methodology totwo major ML algorithms, namely non-negative matrix factorization (NMF) andsingular value decomposition (SVD). Our resulting protocols are communicationoptimal, achieve the same accuracy as their non-private counterparts, andsatisfy a notion of privacy---which we define---that is both intuitive andmeasurable. Our approach is to use lightweight cryptographic protocols (securesum and normalized secure sum) to build learning algorithms rather than wrapcomplex learning algorithms in a heavy-cost MPC framework.  We showcase our algorithms' utility and privacy on several applications: forNMF we consider topic modeling and recommender systems, and for SVD, principalcomponent regression, and low rank approximation.",http://arxiv.org/abs/1901.07986v2,,
943,SecureBoost: A Lossless Federated Learning Framework,"The protection of user privacy is an important concern in machine learning,as evidenced by the rolling out of the General Data Protection Regulation(GDPR) in the European Union (EU) in May 2018. The GDPR is designed to giveusers more control over their personal data, which motivates us to exploremachine learning frameworks with data sharing without violating user privacy.To meet this goal, in this paper, we propose a novel losslessprivacy-preserving tree-boosting system known as SecureBoost in the setting offederated learning. This federated-learning system allows a learning process tobe jointly conducted over multiple parties with partially common user samplesbut different feature sets, which corresponds to a vertically partitionedvirtual data set. An advantage of SecureBoost is that it provides the samelevel of accuracy as the non-privacy-preserving approach while at the sametime, reveal no information of each private data provider. We theoreticallyprove that the SecureBoost framework is as accurate as other non-federatedgradient tree-boosting algorithms that bring the data into one place. Inaddition, along with a proof of security, we discuss what would be required tomake the protocols completely secure.",http://arxiv.org/abs/1901.08755v1,,
944,Learning to Project in Multi-Objective Binary Linear Programming,"In this paper, we investigate the possibility of improving the performance ofmulti-objective optimization solution approaches using machine learningtechniques. Specifically, we focus on multi-objective binary linear programsand employ one of the most effective and recently developed criterion spacesearch algorithms, the so-called KSA, during our study. This algorithm computesall nondominated points of a problem with p objectives by searching on aprojected criterion space, i.e., a (p-1)-dimensional criterion apace. Wepresent an effective and fast learning approach to identify on which projectedspace the KSA should work. We also present several generic features/variablesthat can be used in machine learning techniques for identifying the bestprojected space. Finally, we present an effective bi-objective optimizationbased heuristic for selecting the best subset of the features to overcome theissue of overfitting in learning. Through an extensive computational study over2000 instances of tri-objective Knapsack and Assignment problems, wedemonstrate that an improvement of up to 12% in time can be achieved by theproposed learning method compared to a random selection of the projected space.",http://arxiv.org/abs/1901.10868v1,,
945,Deep Learning Multidimensional Projections,"Dimensionality reduction methods, also known as projections, are frequentlyused for exploring multidimensional data in machine learning, data science, andinformation visualization. Among these, t-SNE and its variants have become verypopular for their ability to visually separate distinct data clusters. However,such methods are computationally expensive for large datasets, suffer fromstability problems, and cannot directly handle out-of-sample data. We propose alearning approach to construct such projections. We train a deep neural networkbased on a collection of samples from a given data universe, and theircorresponding projections, and next use the network to infer projections ofdata from the same, or similar, universes. Our approach generates projectionswith similar characteristics as the learned ones, is computationally two tothree orders of magnitude faster than SNE-class methods, has no complex-to-setuser parameters, handles out-of-sample data in a stable manner, and can be usedto learn any projection technique. We demonstrate our proposal on severalreal-world high dimensional datasets from machine learning.",http://arxiv.org/abs/1902.07958v1,,
946,Scalable Semi-supervised Learning with Graph-based Kernel Machine,"Acquiring labels are often costly, whereas unlabeled data are usually easy toobtain in modern machine learning applications. Semi-supervised learningprovides a principled machine learning framework to address such situations,and has been applied successfully in many real-word applications andindustries. Nonetheless, most of existing semi-supervised learning methodsencounter two serious limitations when applied to modern and large-scaledatasets: computational burden and memory usage demand. To this end, we presentin this paper the Graph-based semi-supervised Kernel Machine (GKM), a methodthat leverages the generalization ability of kernel-based method with thegeometrical and distributive information formulated through a spectral graphinduced from data for semi-supervised learning purpose. Our proposed GKM can besolved directly in the primal form using the Stochastic Gradient Descent methodwith the ideal convergence rate $O(\frac{1}{T})$. Besides, our formulation issuitable for a wide spectrum of important loss functions in the literature ofmachine learning (e.g., Hinge, smooth Hinge, Logistic, L1, and{\epsilon}-insensitive) and smoothness functions (i.e., $l_p(t) = |t|^p$ with$p\ge1$). We further show that the well-known Laplacian Support Vector Machineis a special case of our formulation. We validate our proposed method onseveral benchmark datasets to demonstrate that GKM is appropriate for thelarge-scale datasets since it is optimal in memory usage and yields superiorclassification accuracy whilst simultaneously achieving a significantcomputation speed-up in comparison with the state-of-the-art baselines.",http://arxiv.org/abs/1606.06793v3,,
947,"A high-bias, low-variance introduction to Machine Learning for  physicists","Machine Learning (ML) is one of the most exciting and dynamic areas of modernresearch and application. The purpose of this review is to provide anintroduction to the core concepts and tools of machine learning in a mannereasily understood and intuitive to physicists. The review begins by coveringfundamental concepts in ML and modern statistics such as the bias-variancetradeoff, overfitting, regularization, generalization, and gradient descentbefore moving on to more advanced topics in both supervised and unsupervisedlearning. Topics covered in the review include ensemble models, deep learningand neural networks, clustering and data visualization, energy-based models(including MaxEnt models and Restricted Boltzmann Machines), and variationalmethods. Throughout, we emphasize the many natural connections between ML andstatistical physics. A notable aspect of the review is the use of PythonJupyter notebooks to introduce modern ML/statistical packages to readers usingphysics-inspired datasets (the Ising Model and Monte-Carlo simulations ofsupersymmetric decays of proton-proton collisions). We conclude with anextended outlook discussing possible uses of machine learning for furtheringour understanding of the physical world as well as open problems in ML wherephysicists may be able to contribute. (Notebooks are available athttps://physics.bu.edu/~pankajm/MLnotebooks.html )",http://arxiv.org/abs/1803.08823v2,,
948,"Machine learning materials physics: Surrogate optimization and  multi-fidelity algorithms predict precipitate morphology in an alternative to  phase field dynamics","Machine learning has been effective at detecting patterns and predicting theresponse of systems that behave free of natural laws. Examples include learningcrowd dynamics, recommender systems and autonomous mobility. There also havebeen applications to the search for new materials that bear relations to bigdata classification problems. However, when it comes to physical systemsgoverned by conservation laws, the role of machine learning has been morelimited. Here, we present our recent work in exploring the role of machinelearning methods in discovering, or aiding, the search for physics.Specifically, we focus on using machine learning algorithms to representhigh-dimensional free energy surfaces with the goal of identifying precipitatemorphologies in alloy systems. Traditionally, this problem has been approachedby combining phase field models, which impose first-order dynamics, withelasticity, to traverse a free energy landscape in search of minima.Equilibrium precipitate morphologies occur at these minima. Here, we exploitthe machine learning methods to represent high-dimensional data, combined withsurrogate optimization, sensitivity analysis and multifidelity modelling as analternate framework to explore phenomena controlled by energy extremization.This combination of data-driven methods offers an alternative to the impositionof first-order dynamics via phase field methods, and represents one approach tomachine learning materials physics.",http://arxiv.org/abs/1806.00503v4,,
949,"The impact of imbalanced training data on machine learning for author  name disambiguation","In supervised machine learning for author name disambiguation, negativetraining data are often dominantly larger than positive training data. Thispaper examines how the ratios of negative to positive training data can affectthe performance of machine learning algorithms to disambiguate author names inbibliographic records. On multiple labeled datasets, three classifiers -Logistic Regression, Na\""ive Bayes, and Random Forest - are trained throughrepresentative features such as coauthor names, and title words extracted fromthe same training data but with various positive-negative training data ratios.Results show that increasing negative training data can improve disambiguationperformance but with a few percent of performance gains and sometimes degradeit. Logistic Regression and Na\""ive Bayes learn optimal disambiguation modelseven with a base ratio (1:1) of positive and negative training data. Also, theperformance improvement by Random Forest tends to quickly saturate roughlyafter 1:10 ~ 1:15. These findings imply that contrary to the common practiceusing all training data, name disambiguation algorithms can be trained usingpart of negative training data without degrading much disambiguationperformance while increasing computational efficiency. This study calls formore attention from author name disambiguation scholars to methods for machinelearning from imbalanced data.",http://arxiv.org/abs/1808.00525v2,,
950,"ProteinNet: a standardized data set for machine learning of protein  structure","Rapid progress in deep learning has spurred its application to bioinformaticsproblems including protein structure prediction and design. In classic machinelearning problems like computer vision, progress has been driven bystandardized data sets that facilitate fair assessment of new methods and lowerthe barrier to entry for non-domain experts. While data sets of proteinsequence and structure exist, they lack certain components critical for machinelearning, including high-quality multiple sequence alignments and insulatedtraining / validation splits that account for deep but only weakly detectablehomology across protein space. We have created the ProteinNet series of datasets to provide a standardized mechanism for training and assessing data-drivenmodels of protein sequence-structure relationships. ProteinNet integratessequence, structure, and evolutionary information in programmaticallyaccessible file formats tailored for machine learning frameworks. Multiplesequence alignments of all structurally characterized proteins were createdusing substantial high-performance computing resources. Standardized datasplits were also generated to emulate the difficulty of past CASP (CriticalAssessment of protein Structure Prediction) experiments by resetting proteinsequence and structure space to the historical states that preceded six priorCASPs. Utilizing sensitive evolution-based distance metrics to segregatedistantly related proteins, we have additionally created validation setsdistinct from the official CASP sets that faithfully mimic their difficulty.ProteinNet thus represents a comprehensive and accessible resource for trainingand assessing machine-learned models of protein structure.",http://arxiv.org/abs/1902.00249v1,,
951,"An Easy to Use Repository for Comparing and Improving Machine Learning  Algorithm Usage","The results from most machine learning experiments are used for a specificpurpose and then discarded. This results in a significant loss of informationand requires rerunning experiments to compare learning algorithms. This alsorequires implementation of another algorithm for comparison, that may notalways be correctly implemented. By storing the results from previousexperiments, machine learning algorithms can be compared easily and theknowledge gained from them can be used to improve their performance. Thepurpose of this work is to provide easy access to previous experimental resultsfor learning and comparison. These stored results are comprehensive -- storingthe prediction for each test instance as well as the learning algorithm,hyperparameters, and training set that were used. Previous results areparticularly important for meta-learning, which, in a broad sense, is theprocess of learning from previous machine learning results such that thelearning process is improved. While other experiment databases do exist, one ofour focuses is on easy access to the data. We provide meta-learning data setsthat are ready to be downloaded for meta-learning experiments. In addition,queries to the underlying database can be made if specific information isdesired. We also differ from previous experiment databases in that ourdatabases is designed at the instance level, where an instance is an example ina data set. We store the predictions of a learning algorithm trained on aspecific training set for each instance in the test set. Data set levelinformation can then be obtained by aggregating the results from the instances.The instance level information can be used for many tasks such as determiningthe diversity of a classifier or algorithmically determining the optimal subsetof training instances for a learning algorithm.",http://arxiv.org/abs/1405.7292v2,,
952,Learning Stable Multilevel Dictionaries for Sparse Representations,"Sparse representations using learned dictionaries are being increasingly usedwith success in several data processing and machine learning applications. Theavailability of abundant training data necessitates the development ofefficient, robust and provably good dictionary learning algorithms. Algorithmicstability and generalization are desirable characteristics for dictionarylearning algorithms that aim to build global dictionaries which can efficientlymodel any test data similar to the training samples. In this paper, we proposean algorithm to learn dictionaries for sparse representations from large scaledata, and prove that the proposed learning algorithm is stable andgeneralizable asymptotically. The algorithm employs a 1-D subspace clusteringprocedure, the K-hyperline clustering, in order to learn a hierarchicaldictionary with multiple levels. We also propose an information-theoreticscheme to estimate the number of atoms needed in each level of learning anddevelop an ensemble approach to learn robust dictionaries. Using the proposeddictionaries, the sparse code for novel test data can be computed using alow-complexity pursuit procedure. We demonstrate the stability andgeneralization characteristics of the proposed algorithm using simulations. Wealso evaluate the utility of the multilevel dictionaries in compressed recoveryand subspace learning applications.",http://arxiv.org/abs/1303.0448v2,,
953,Patterns for Learning with Side Information,"Supervised, semi-supervised, and unsupervised learning estimate a functiongiven input/output samples. Generalization of the learned function to unseendata can be improved by incorporating side information into learning. Sideinformation are data that are neither from the input space nor from the outputspace of the function, but include useful information for learning it. In thispaper we show that learning with side information subsumes a variety of relatedapproaches, e.g. multi-task learning, multi-view learning and learning usingprivileged information. Our main contributions are (i) a new perspective thatconnects these previously isolated approaches, (ii) insights about how thesemethods incorporate different types of prior knowledge, and hence implementdifferent patterns, (iii) facilitating the application of these methods innovel tasks, as well as (iv) a systematic experimental evaluation of thesepatterns in two supervised learning tasks.",http://arxiv.org/abs/1511.06429v5,,
954,"Statistical-mechanical analysis of pre-training and fine tuning in deep  learning","In this paper, we present a statistical-mechanical analysis of deep learning.We elucidate some of the essential components of deep learning---pre-trainingby unsupervised learning and fine tuning by supervised learning. We formulatethe extraction of features from the training data as a margin criterion in ahigh-dimensional feature-vector space. The self-organized classifier is thensupplied with small amounts of labelled data, as in deep learning. Although weemploy a simple single-layer perceptron model, rather than directly analyzing amulti-layer neural network, we find a nontrivial phase transition that isdependent on the number of unlabelled data in the generalization error of theresultant classifier. In this sense, we evaluate the efficacy of theunsupervised learning component of deep learning. The analysis is performed bythe replica method, which is a sophisticated tool in statistical mechanics. Wevalidate our result in the manner of deep learning, using a simple iterativealgorithm to learn the weight vector on the basis of belief propagation.",http://arxiv.org/abs/1501.04413v1,,
955,"Common-Description Learning: A Framework for Learning Algorithms and  Generating Subproblems from Few Examples","Current learning algorithms face many difficulties in learning simplepatterns and using them to learn more complex ones. They also require moreexamples than humans do to learn the same pattern, assuming no prior knowledge.In this paper, a new learning framework is introduced that is calledcommon-description learning (CDL). This framework has been tested on 32 smallmulti-task datasets, and the results show that it was able to learn complexalgorithms from a few number of examples. The final model is perfectlyinterpretable and its depth depends on the question. What is meant by depthhere is that whenever needed, the model learns to break down the problem intosimpler subproblems and solves them using previously learned models. Finally,we explain the capabilities of our framework in discovering complex relationsin data and how it can help in improving language understanding in machines.",http://arxiv.org/abs/1605.00241v1,,
956,Learning Bound for Parameter Transfer Learning,"We consider a transfer-learning problem by using the parameter transferapproach, where a suitable parameter of feature mapping is learned through onetask and applied to another objective task. Then, we introduce the notion ofthe local stability and parameter transfer learnability of parametric featuremapping,and thereby derive a learning bound for parameter transfer algorithms.As an application of parameter transfer learning, we discuss the performance ofsparse coding in self-taught learning. Although self-taught learning algorithmswith plentiful unlabeled data often show excellent empirical performance, theirtheoretical analysis has not been studied. In this paper, we also provide thefirst theoretical learning bound for self-taught learning.",http://arxiv.org/abs/1610.08696v3,,
957,"Learning Representations by Stochastic Meta-Gradient Descent in Neural  Networks","Representations are fundamental to artificial intelligence. The performanceof a learning system depends on the type of representation used forrepresenting the data. Typically, these representations are hand-engineeredusing domain knowledge. More recently, the trend is to learn theserepresentations through stochastic gradient descent in multi-layer neuralnetworks, which is called backprop. Learning the representations directly fromthe incoming data stream reduces the human labour involved in designing alearning system. More importantly, this allows in scaling of a learning systemfor difficult tasks. In this paper, we introduce a new incremental learningalgorithm called crossprop, which learns incoming weights of hidden units basedon the meta-gradient descent approach, that was previously introduced by Sutton(1992) and Schraudolph (1999) for learning step-sizes. The final updateequation introduces an additional memory parameter for each of these weightsand generalizes the backprop update equation. From our experiments, we showthat crossprop learns and reuses its feature representation while tackling newand unseen tasks whereas backprop relearns a new feature representation.",http://arxiv.org/abs/1612.02879v2,,
958,Reinforcement Learning for Learning Rate Control,"Stochastic gradient descent (SGD), which updates the model parameters byadding a local gradient times a learning rate at each step, is widely used inmodel training of machine learning algorithms such as neural networks. It isobserved that the models trained by SGD are sensitive to learning rates andgood learning rates are problem specific. We propose an algorithm toautomatically learn learning rates using neural network based actor-criticmethods from deep reinforcement learning (RL).In particular, we train a policynetwork called actor to decide the learning rate at each step during training,and a value network called critic to give feedback about quality of thedecision (e.g., the goodness of the learning rate outputted by the actor) thatthe actor made. The introduction of auxiliary actor and critic networks helpsthe main network achieve better performance. Experiments on different datasetsand network architectures show that our approach leads to better convergence ofSGD than human-designed competitors.",http://arxiv.org/abs/1705.11159v1,,
959,Decoupling Learning Rules from Representations,"In the artificial intelligence field, learning often corresponds to changingthe parameters of a parameterized function. A learning rule is an algorithm ormathematical expression that specifies precisely how the parameters should bechanged. When creating an artificial intelligence system, we must make twodecisions: what representation should be used (i.e., what parameterizedfunction should be used) and what learning rule should be used to searchthrough the resulting set of representable functions. Using most learningrules, these two decisions are coupled in a subtle (and often unintentional)way. That is, using the same learning rule with two different representationsthat can represent the same sets of functions can result in two differentoutcomes. After arguing that this coupling is undesirable, particularly whenusing artificial neural networks, we present a method for partially decouplingthese two decisions for a broad class of learning rules that span unsupervisedlearning, reinforcement learning, and supervised learning.",http://arxiv.org/abs/1706.03100v1,,
960,Discriminative Similarity for Clustering and Semi-Supervised Learning,"Similarity-based clustering and semi-supervised learning methods separate thedata into clusters or classes according to the pairwise similarity between thedata, and the pairwise similarity is crucial for their performance. In thispaper, we propose a novel discriminative similarity learning framework whichlearns discriminative similarity for either data clustering or semi-supervisedlearning. The proposed framework learns classifier from each hypotheticallabeling, and searches for the optimal labeling by minimizing thegeneralization error of the learned classifiers associated with thehypothetical labeling. Kernel classifier is employed in our framework. Bygeneralization analysis via Rademacher complexity, the generalization errorbound for the kernel classifier learned from hypothetical labeling is expressedas the sum of pairwise similarity between the data from different classes,parameterized by the weights of the kernel classifier. Such pairwise similarityserves as the discriminative similarity for the purpose of clustering andsemi-supervised learning, and discriminative similarity with similar form canalso be induced by the integrated squared error bound for kernel densityclassification. Based on the discriminative similarity induced by the kernelclassifier, we propose new clustering and semi-supervised learning methods.",http://arxiv.org/abs/1709.01231v1,,
961,"Multi-Agent Distributed Lifelong Learning for Collective Knowledge  Acquisition","Lifelong machine learning methods acquire knowledge over a series ofconsecutive tasks, continually building upon their experience. Current lifelonglearning algorithms rely upon a single learning agent that has centralizedaccess to all data. In this paper, we extend the idea of lifelong learning froma single agent to a network of multiple agents that collectively learn a seriesof tasks. Each agent faces some (potentially unique) set of tasks; the key ideais that knowledge learned from these tasks may benefit other agents trying tolearn different (but related) tasks. Our Collective Lifelong Learning Algorithm(CoLLA) provides an efficient way for a network of agents to share theirlearned knowledge in a distributed and decentralized manner, while preservingthe privacy of the locally observed data. Note that a decentralized scheme is asubclass of distributed algorithms where a central server does not exist and inaddition to data, computations are also distributed among the agents. Weprovide theoretical guarantees for robust performance of the algorithm andempirically demonstrate that CoLLA outperforms existing approaches fordistributed multi-task learning on a variety of data sets.",http://arxiv.org/abs/1709.05412v2,,
962,Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace,"Gradient-based meta-learning methods leverage gradient descent to learn thecommonalities among various tasks. While previous such methods have beensuccessful in meta-learning tasks, they resort to simple gradient descentduring meta-testing. Our primary contribution is the {\em MT-net}, whichenables the meta-learner to learn on each layer's activation space a subspacethat the task-specific learner performs gradient descent on. Additionally, atask-specific learner of an {\em MT-net} performs gradient descent with respectto a meta-learned distance metric, which warps the activation space to be moresensitive to task identity. We demonstrate that the dimension of this learnedsubspace reflects the complexity of the task-specific learner's adaptationtask, and also that our model is less sensitive to the choice of initiallearning rates than previous gradient-based meta-learning methods. Our methodachieves state-of-the-art or comparable performance on few-shot classificationand regression tasks.",http://arxiv.org/abs/1801.05558v3,,
963,Reinforcement Learning from Imperfect Demonstrations,"Robust real-world learning should benefit from both demonstrations andinteractions with the environment. Current approaches to learning fromdemonstration and reward perform supervised learning on expert demonstrationdata and use reinforcement learning to further improve performance based on thereward received from the environment. These tasks have divergent losses whichare difficult to jointly optimize and such methods can be very sensitive tonoisy demonstrations. We propose a unified reinforcement learning algorithm,Normalized Actor-Critic (NAC), that effectively normalizes the Q-function,reducing the Q-values of actions unseen in the demonstration data. NAC learnsan initial policy network from demonstrations and refines the policy in theenvironment, surpassing the demonstrator's performance. Crucially, bothlearning from demonstration and interactive refinement use the same objective,unlike prior approaches that combine distinct supervised and reinforcementlosses. This makes NAC robust to suboptimal demonstration data since the methodis not forced to mimic all of the examples in the dataset. We show that ourunified reinforcement learning algorithm can learn robustly and outperformexisting baselines when evaluated on several realistic driving games.",http://arxiv.org/abs/1802.05313v1,,
964,Incremental Learning-to-Learn with Statistical Guarantees,"In learning-to-learn the goal is to infer a learning algorithm that workswell on a class of tasks sampled from an unknown meta distribution. In contrastto previous work on batch learning-to-learn, we consider a scenario where tasksare presented sequentially and the algorithm needs to adapt incrementally toimprove its performance on future tasks. Key to this setting is for thealgorithm to rapidly incorporate new observations into the model as theyarrive, without keeping them in memory. We focus on the case where theunderlying algorithm is ridge regression parameterized by a positivesemidefinite matrix. We propose to learn this matrix by applying a stochasticstrategy to minimize the empirical error incurred by ridge regression on futuretasks sampled from the meta distribution. We study the statistical propertiesof the proposed algorithm and prove non-asymptotic bounds on its excesstransfer risk, that is, the generalization performance on new tasks from thesame meta distribution. We compare our online learning-to-learn approach with astate of the art batch method, both theoretically and empirically.",http://arxiv.org/abs/1803.08089v1,,
965,"On the Importance of Attention in Meta-Learning for Few-Shot Text  Classification","Current deep learning based text classification methods are limited by theirability to achieve fast learning and generalization when the data is scarce. Weaddress this problem by integrating a meta-learning procedure that uses theknowledge learned across many tasks as an inductive bias towards better naturallanguage understanding. Based on the Model-Agnostic Meta-Learning framework(MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML)algorithm for text classification. The essential difference between MAML andATAML is in the separation of task-agnostic representation learning andtask-specific attentive adaptation. The proposed ATAML is designed to encouragetask-agnostic representation learning by way of task-agnostic parameterizationand facilitate task-specific adaptation via attention mechanisms. We provideevidence to show that the attention mechanism in ATAML has a synergistic effecton learning performance. In comparisons with models trained from randominitialization, pretrained models and meta trained MAML, our proposed ATAMLmethod generalizes better on single-label and multi-label classification tasksin miniRCV1 and miniReuters-21578 datasets.",http://arxiv.org/abs/1806.00852v1,,
966,"Equivalence Between Wasserstein and Value-Aware Loss for Model-based  Reinforcement Learning","Learning a generative model is a key component of model-based reinforcementlearning. Though learning a good model in the tabular setting is a simple task,learning a useful model in the approximate setting is challenging. In thiscontext, an important question is the loss function used for model learning asvarying the loss function can have a remarkable impact on effectiveness ofplanning. Recently Farahmand et al. (2017) proposed a value-aware modellearning (VAML) objective that captures the structure of value function duringmodel learning. Using tools from Asadi et al. (2018), we show that minimizingthe VAML objective is in fact equivalent to minimizing the Wasserstein metric.This equivalence improves our understanding of value-aware models, and alsocreates a theoretical foundation for applications of Wasserstein in model-basedreinforcement~learning.",http://arxiv.org/abs/1806.01265v2,,
967,"DynMat, a network that can learn after learning","To survive in the dynamically-evolving world, we accumulate knowledge andimprove our skills based on experience. In the process, gaining new knowledgedoes not disrupt our vigilance to external stimuli. In other words, ourlearning process is 'accumulative' and 'online' without interruption. However,despite the recent success, artificial neural networks (ANNs) must be trainedoffline, and they suffer catastrophic interference between old and newlearning, indicating that ANNs' conventional learning algorithms may not besuitable for building intelligent agents comparable to our brain. In thisstudy, we propose a novel neural network architecture (DynMat) consisting ofdual learning systems, inspired by the complementary learning system (CLS)theory suggesting that the brain relies on short- and long-term learningsystems to learn continuously. Our experiments show that 1) DynMat can learn anew class without catastrophic interference and 2) it does not strictly requireoffline training.",http://arxiv.org/abs/1806.06253v2,,
968,Improving Transferability of Deep Neural Networks,"Learning from small amounts of labeled data is a challenge in the area ofdeep learning. This is currently addressed by Transfer Learning where onelearns the small data set as a transfer task from a larger source dataset.Transfer Learning can deliver higher accuracy if the hyperparameters and sourcedataset are chosen well. One of the important parameters is the learning ratefor the layers of the neural network. We show through experiments on theImageNet22k and Oxford Flowers datasets that improvements in accuracy in rangeof 127% can be obtained by proper choice of learning rates. We also show thatthe images/label parameter for a dataset can potentially be used to determineoptimal learning rates for the layers to get the best overall accuracy. Weadditionally validate this method on a sample of real-world imageclassification tasks from a public visual recognition API.",http://arxiv.org/abs/1807.11459v1,,
969,Improvements on Hindsight Learning,"Sparse reward problems are one of the biggest challenges in ReinforcementLearning. Goal-directed tasks are one such sparse reward problems where areward signal is received only when the goal is reached. One promising way totrain an agent to perform goal-directed tasks is to use Hindsight Learningapproaches. In these approaches, even when an agent fails to reach the desiredgoal, the agent learns to reach the goal it achieved instead. Doing this overmultiple trajectories while generalizing the policy learned from the achievedgoals, the agent learns a goal conditioned policy to reach any goal. One suchapproach is Hindsight Experience replay which uses an off-policy ReinforcementLearning algorithm to learn a goal conditioned policy. In this approach, areplay of the past transitions happens in a uniformly random fashion. Anotherapproach is to use a Hindsight version of the policy gradients to directlylearn a policy. In this work, we discuss different ways to replay pasttransitions to improve learning in hindsight experience replay focusing onprioritized variants in particular. Also, we implement the Hindsight Policygradient methods to robotic tasks.",http://arxiv.org/abs/1809.06719v2,,
970,Batch Active Preference-Based Learning of Reward Functions,"Data generation and labeling are usually an expensive part of learning forrobotics. While active learning methods are commonly used to tackle the formerproblem, preference-based learning is a concept that attempts to solve thelatter by querying users with preference questions. In this paper, we willdevelop a new algorithm, batch active preference-based learning, that enablesefficient learning of reward functions using as few data samples as possiblewhile still having short query generation times. We introduce severalapproximations to the batch active learning problem, and provide theoreticalguarantees for the convergence of our algorithms. Finally, we present ourexperimental results for a variety of robotics tasks in simulation. Our resultssuggest that our batch active learning algorithm requires only a few queriesthat are computed in a short amount of time. We then showcase our algorithm ina study to learn human users' preferences.",http://arxiv.org/abs/1810.04303v1,,
971,Optimal Hierarchical Learning Path Design with Reinforcement Learning,"E-learning systems are capable of providing more adaptive and efficientlearning experiences for students than the traditional classroom setting. A keycomponent of such systems is the learning strategy, the algorithm that designsthe learning paths for students based on information such as the students'current progresses, their skills, learning materials, and etc. In this paper,we address the problem of finding the optimal learning strategy for anE-learning system. To this end, we first develop a model for students'hierarchical skills in the E-learning system. Based on the hierarchical skillmodel and the classical cognitive diagnosis model, we further develop aframework to model various proficiency levels of hierarchical skills. Theoptimal learning strategy on top of the hierarchical structure is found byapplying a model-free reinforcement learning method, which does not requireinformation on students' learning transition process. The effectiveness of theproposed framework is demonstrated via numerical experiments.",http://arxiv.org/abs/1810.05347v1,,
972,"Learning to Learn without Forgetting By Maximizing Transfer and  Minimizing Interference","Lack of performance when it comes to continual learning over non-stationarydistributions of data remains a major challenge in scaling neural networklearning to more human realistic settings. In this work we propose a newconceptualization of the continual learning problem in terms of a temporallysymmetric trade-off between transfer and interference that can be optimized byenforcing gradient alignment across examples. We then propose a new algorithm,Meta-Experience Replay (MER), that directly exploits this view by combiningexperience replay with optimization based meta-learning. This method learnsparameters that make interference based on future gradients less likely andtransfer based on future gradients more likely. We conduct experiments acrosscontinual lifelong supervised learning benchmarks and non-stationaryreinforcement learning environments demonstrating that our approachconsistently outperforms recently proposed baselines for continual learning.Our experiments show that the gap between the performance of MER and baselinealgorithms grows both as the environment gets more non-stationary and as thefraction of the total experiences stored gets smaller.",http://arxiv.org/abs/1810.11910v2,,
973,Modularization of End-to-End Learning: Case Study in Arcade Games,"Complex environments and tasks pose a difficult problem for holisticend-to-end learning approaches. Decomposition of an environment intointeracting controllable and non-controllable objects allows supervisedlearning for non-controllable objects and universal value function approximatorlearning for controllable objects. Such decomposition should lead to a shorterlearning time and better generalisation capability. Here, we considerarcade-game environments as sets of interacting objects (controllable,non-controllable) and propose a set of functional modules that are specializedon mastering different types of interactions in a broad range of environments.The modules utilize regression, supervised learning, and reinforcement learningalgorithms. Results of this case study in different Atari games suggest thathuman-level performance can be achieved by a learning agent within a humanamount of game experience (10-15 minutes game time) when a proper decompositionof an environment or a task is provided. However, automatization of suchdecomposition remains a challenging problem. This case study shows how a modelof a causal structure underlying an environment or a task can benefit learningtime and generalization capability of the agent, and argues in favor ofexploiting modular structure in contrast to using pure end-to-end learningapproaches.",http://arxiv.org/abs/1901.09895v1,,
974,A new Potential-Based Reward Shaping for Reinforcement Learning Agent,"Potential-based reward shaping (PBRS) is a particular category of machinelearning methods which aims to improve the learning speed of a reinforcementlearning agent by extracting and utilizing extra knowledge while performing atask. There are two steps in the process of transfer learning: extractingknowledge from previously learned tasks and transferring that knowledge to useit in a target task. The latter step is well discussed in the literature withvarious methods being proposed for it, while the former has been explored less.With this in mind, the type of knowledge that is transmitted is very importantand can lead to considerable improvement. Among the literature of both thetransfer learning and the potential-based reward shaping, a subject that hasnever been addressed is the knowledge gathered during the learning processitself. In this paper, we presented a novel potential-based reward shapingmethod that attempted to extract knowledge from the learning process. Theproposed method extracts knowledge from episodes' cumulative rewards. Theproposed method has been evaluated in the Arcade learning environment and theresults indicate an improvement in the learning process in both the single-taskand the multi-task reinforcement learner agents.",http://arxiv.org/abs/1902.06239v1,,
975,"Complementary Learning for Overcoming Catastrophic Forgetting Using  Experience Replay","Despite huge success, deep networks are unable to learn effectively insequential multitask learning settings as they forget the past learned tasksafter learning new tasks. Inspired from complementary learning systems theory,we address this challenge by learning a generative model that couples thecurrent task to the past learned tasks through a discriminative embeddingspace. We learn an abstract level generative distribution in the embedding thatallows the generation of data points to represent the experience. We samplefrom this distribution and utilize experience replay to avoid forgetting andsimultaneously accumulate new knowledge to the abstract distribution in orderto couple the current task with past experience. We demonstrate theoreticallyand empirically that our framework learns a distribution in the embedding thatis shared across all task and as a result tackles catastrophic forgetting.",http://arxiv.org/abs/1903.04566v1,,
976,Unsupervised Search-based Structured Prediction,"We describe an adaptation and application of a search-based structuredprediction algorithm ""Searn"" to unsupervised learning problems. We show that itis possible to reduce unsupervised learning to supervised learning anddemonstrate a high-quality unsupervised shift-reduce parsing model. Weadditionally show a close connection between unsupervised Searn and expectationmaximization. Finally, we demonstrate the efficacy of a semi-supervisedextension. The key idea that enables this is an application of the predict-selfidea for unsupervised learning.",http://arxiv.org/abs/0906.5151v1,,
977,A Unifying View of Multiple Kernel Learning,"Recent research on multiple kernel learning has lead to a number ofapproaches for combining kernels in regularized risk minimization. The proposedapproaches include different formulations of objectives and varyingregularization strategies. In this paper we present a unifying generaloptimization criterion for multiple kernel learning and show how existingformulations are subsumed as special cases. We also derive the criterion's dualrepresentation, which is suitable for general smooth optimization algorithms.Finally, we evaluate multiple kernel learning in this framework analyticallyusing a Rademacher complexity bound on the generalization error and empiricallyin a set of experiments.",http://arxiv.org/abs/1005.0437v1,,
978,Learning mixed graphical models from data with p larger than n,"Structure learning of Gaussian graphical models is an extensively studiedproblem in the classical multivariate setting where the sample size n is largerthan the number of random variables p, as well as in the more challengingsetting when p>>n. However, analogous approaches for learning the structure ofgraphical models with mixed discrete and continuous variables when p>>n remainlargely unexplored. Here we describe a statistical learning procedure for thisproblem based on limited-order correlations and assess its performance withsynthetic and real data.",http://arxiv.org/abs/1202.3765v1,,
979,"Temporal-Difference Networks for Dynamical Systems with Continuous  Observations and Actions","Temporal-difference (TD) networks are a class of predictive staterepresentations that use well-established TD methods to learn models ofpartially observable dynamical systems. Previous research with TD networks hasdealt only with dynamical systems with finite sets of observations and actions.We present an algorithm for learning TD network representations of dynamicalsystems with continuous observations and actions. Our results show that thealgorithm is capable of learning accurate and robust models of several noisycontinuous dynamical systems. The algorithm presented here is the first fullyincremental method for learning a predictive representation of a continuousdynamical system.",http://arxiv.org/abs/1205.2608v1,,
980,Predicting Parameters in Deep Learning,"We demonstrate that there is significant redundancy in the parameterizationof several deep learning models. Given only a few weight values for eachfeature it is possible to accurately predict the remaining values. Moreover, weshow that not only can the parameter values be predicted, but many of them neednot be learned at all. We train several different architectures by learningonly a small number of weights and predicting the rest. In the best case we areable to predict more than 95% of the weights of a network without any drop inaccuracy.",http://arxiv.org/abs/1306.0543v2,,
981,Boosted Markov Networks for Activity Recognition,"We explore a framework called boosted Markov networks to combine the learningcapacity of boosting and the rich modeling semantics of Markov networks andapplying the framework for video-based activity recognition. Importantly, weextend the framework to incorporate hidden variables. We show how the frameworkcan be applied for both model learning and feature selection. We demonstratethat boosted Markov networks with hidden variables perform comparably with thestandard maximum likelihood estimation. However, our framework is able to learnsparse models, and therefore can provide computational savings when the learnedmodels are used for classification.",http://arxiv.org/abs/1408.1167v1,,
982,Bayesian Multitask Learning with Latent Hierarchies,"We learn multiple hypotheses for related tasks under a latent hierarchicalrelationship between tasks. We exploit the intuition that for domainadaptation, we wish to share classifier structure, but for multitask learning,we wish to share covariance structure. Our hierarchical model is seen tosubsume several previously proposed multitask learning models and performs wellon three distinct real-world data sets.",http://arxiv.org/abs/1408.2032v1,,
983,A Generative Model for Deep Convolutional Learning,"A generative model is developed for deep (multi-layered) convolutionaldictionary learning. A novel probabilistic pooling operation is integrated intothe deep model, yielding efficient bottom-up (pretraining) and top-down(refinement) probabilistic learning. Experimental results demonstrate powerfulcapabilities of the model to learn multi-layer features from images, andexcellent classification results are obtained on the MNIST and Caltech 101datasets.",http://arxiv.org/abs/1504.04054v1,,
984,Deep Exploration via Bootstrapped DQN,"Efficient exploration in complex environments remains a major challenge forreinforcement learning. We propose bootstrapped DQN, a simple algorithm thatexplores in a computationally and statistically efficient manner through use ofrandomized value functions. Unlike dithering strategies such as epsilon-greedyexploration, bootstrapped DQN carries out temporally-extended (or deep)exploration; this can lead to exponentially faster learning. We demonstratethese benefits in complex stochastic MDPs and in the large-scale ArcadeLearning Environment. Bootstrapped DQN substantially improves learning timesand performance across most Atari games.",http://arxiv.org/abs/1602.04621v3,,
985,"Difference of Convex Functions Programming Applied to Control with  Expert Data","This paper reports applications of Difference of Convex functions (DC)programming to Learning from Demonstrations (LfD) and Reinforcement Learning(RL) with expert data. This is made possible because the norm of the OptimalBellman Residual (OBR), which is at the heart of many RL and LfD algorithms, isDC. Improvement in performance is demonstrated on two specific algorithms,namely Reward-regularized Classification for Apprenticeship Learning (RCAL) andReinforcement Learning with Expert Demonstrations (RLED), through experimentson generic Markov Decision Processes (MDP), called Garnets.",http://arxiv.org/abs/1606.01128v2,,
986,Learning to Optimize,"Algorithm design is a laborious process and often requires many iterations ofideation and validation. In this paper, we explore automating algorithm designand present a method to learn an optimization algorithm, which we believe to bethe first method that can automatically discover a better algorithm. Weapproach this problem from a reinforcement learning perspective and representany particular optimization algorithm as a policy. We learn an optimizationalgorithm using guided policy search and demonstrate that the resultingalgorithm outperforms existing hand-engineered algorithms in terms ofconvergence speed and/or the final objective value.",http://arxiv.org/abs/1606.01885v1,,
987,"Theory reconstruction: a representation learning view on predicate  invention","With this positional paper we present a representation learning view onpredicate invention. The intention of this proposal is to bridge the relationaland deep learning communities on the problem of predicate invention. We proposea theory reconstruction approach, a formalism that extends autoencoder approachto representation learning to the relational settings. Our intention is tostart a discussion to define a unifying framework for predicate invention andtheory revision.",http://arxiv.org/abs/1606.08660v2,,
988,Efficient Transductive Online Learning via Randomized Rounding,"Most traditional online learning algorithms are based on variants of mirrordescent or follow-the-leader. In this paper, we present an online algorithmbased on a completely different approach, tailored for transductive settings,which combines ""random playout"" and randomized rounding of loss subgradients.As an application of our approach, we present the first computationallyefficient online algorithm for collaborative filtering with trace-normconstrained matrices. As a second application, we solve an open questionlinking batch learning and transductive online learning",http://arxiv.org/abs/1106.2429v4,,
989,Constraint-free Graphical Model with Fast Learning Algorithm,"In this paper, we propose a simple, versatile model for learning thestructure and parameters of multivariate distributions from a data set.Learning a Markov network from a given data set is not a simple problem,because Markov networks rigorously represent Markov properties, and this rigorimposes complex constraints on the design of the networks. Our proposed modelremoves these constraints, acquiring important aspects from the informationgeometry. The proposed parameter- and structure-learning algorithms are simpleto execute as they are based solely on local computation at each node.Experiments demonstrate that our algorithms work appropriately.",http://arxiv.org/abs/1206.3721v1,,
990,An ensemble-based online learning algorithm for streaming data,"In this study, we introduce an ensemble-based approach for online machinelearning. The ensemble of base classifiers in our approach is obtained bylearning Naive Bayes classifiers on different training sets which are generatedby projecting the original training set to lower dimensional space. We proposea mechanism to learn sequences of data using data chunks paradigm. Theexperiments conducted on a number of UCI datasets and one synthetic datasetdemonstrate that the proposed approach performs significantly better than somewell-known online learning algorithms.",http://arxiv.org/abs/1704.07938v1,,
991,Reinforcement Learning for the Soccer Dribbling Task,"We propose a reinforcement learning solution to the \emph{soccer dribblingtask}, a scenario in which a soccer agent has to go from the beginning to theend of a region keeping possession of the ball, as an adversary attempts togain possession. While the adversary uses a stationary policy, the dribblerlearns the best action to take at each decision point. After definingmeaningful variables to represent the state space, and high-level macro-actionsto incorporate domain knowledge, we describe our application of thereinforcement learning algorithm \emph{Sarsa} with CMAC for functionapproximation. Our experiments show that, after the training period, thedribbler is able to accomplish its task against a strong adversary around 58%of the time.",http://arxiv.org/abs/1305.6568v1,,
992,Structured Learning via Logistic Regression,"A successful approach to structured learning is to write the learningobjective as a joint function of linear parameters and inference messages, anditerate between updates to each. This paper observes that if the inferenceproblem is ""smoothed"" through the addition of entropy terms, for fixedmessages, the learning objective reduces to a traditional (non-structured)logistic regression problem with respect to parameters. In these logisticregression problems, each training example has a bias term determined by thecurrent set of messages. Based on this insight, the structured energy functioncan be extended from linear factors to any function class where an ""oracle""exists to minimize a logistic loss.",http://arxiv.org/abs/1407.0754v1,,
993,Deep Multi-Instance Transfer Learning,"We present a new approach for transferring knowledge from groups toindividuals that comprise them. We evaluate our method in text, by inferringthe ratings of individual sentences using full-review ratings. This approach,which combines ideas from transfer learning, deep learning and multi-instancelearning, reduces the need for laborious human labelling of fine-grained datawhen abundant labels are available at the group level.",http://arxiv.org/abs/1411.3128v2,,
994,Efficient Learning for Undirected Topic Models,"Replicated Softmax model, a well-known undirected topic model, is powerful inextracting semantic representations of documents. Traditional learningstrategies such as Contrastive Divergence are very inefficient. This paperprovides a novel estimator to speed up the learning based on Noise ContrastiveEstimate, extended for documents of variant lengths and weighted inputs.Experiments on two benchmarks show that the new estimator achieves greatlearning efficiency and high accuracy on document retrieval and classification.",http://arxiv.org/abs/1506.07477v1,,
995,Multimodal Sparse Coding for Event Detection,"Unsupervised feature learning methods have proven effective forclassification tasks based on a single modality. We present multimodal sparsecoding for learning feature representations shared across multiple modalities.The shared representations are applied to multimedia event detection (MED) andevaluated in comparison to unimodal counterparts, as well as other featurelearning methods such as GMM supervectors and sparse RBM. We report thecross-validated classification accuracy and mean average precision of the MEDsystem trained on features learned from our unimodal and multimodal settingsfor a subset of the TRECVID MED 2014 dataset.",http://arxiv.org/abs/1605.05212v1,,
996,"Learning Robust Features using Deep Learning for Automatic Seizure  Detection","We present and evaluate the capacity of a deep neural network to learn robustfeatures from EEG to automatically detect seizures. This is a challengingproblem because seizure manifestations on EEG are extremely variable bothinter- and intra-patient. By simultaneously capturing spectral, temporal andspatial information our recurrent convolutional neural network learns a generalspatially invariant representation of a seizure. The proposed approach exceedssignificantly previous results obtained on cross-patient classifiers both interms of sensitivity and false positive rate. Furthermore, our model proves tobe robust to missing channel and variable electrode montage.",http://arxiv.org/abs/1608.00220v1,,
997,"Referential Uncertainty and Word Learning in High-dimensional,  Continuous Meaning Spaces","This paper discusses lexicon word learning in high-dimensional meaning spacesfrom the viewpoint of referential uncertainty. We investigate variousstate-of-the-art Machine Learning algorithms and discuss the impact of scaling,representation and meaning space structure. We demonstrate that current MachineLearning techniques successfully deal with high-dimensional meaning spaces. Inparticular, we show that exponentially increasing dimensions linearly impactlearner performance and that referential uncertainty from word sensitivity hasno impact.",http://arxiv.org/abs/1609.09580v1,,
998,"A Non-generative Framework and Convex Relaxations for Unsupervised  Learning","We give a novel formal theoretical framework for unsupervised learning withtwo distinctive characteristics. First, it does not assume any generative modeland based on a worst-case performance metric. Second, it is comparative, namelyperformance is measured with respect to a given hypothesis class. This allowsto avoid known computational hardness results and improper algorithms based onconvex relaxations. We show how several families of unsupervised learningmodels, which were previously only analyzed under probabilistic assumptions andare otherwise provably intractable, can be efficiently learned in our frameworkby convex optimization.",http://arxiv.org/abs/1610.01132v3,,
999,Improved Strongly Adaptive Online Learning using Coin Betting,"This paper describes a new parameter-free online learning algorithm forchanging environments. In comparing against algorithms with the same timecomplexity as ours, we obtain a strongly adaptive regret bound that is a factorof at least $\sqrt{\log(T)}$ better, where $T$ is the time horizon. Empiricalresults show that our algorithm outperforms state-of-the-art methods inlearning with expert advice and metric learning scenarios.",http://arxiv.org/abs/1610.04578v3,,
1000,Regret Bounds for Lifelong Learning,"We consider the problem of transfer learning in an online setting. Differenttasks are presented sequentially and processed by a within-task algorithm. Wepropose a lifelong learning strategy which refines the underlying datarepresentation used by the within-task algorithm, thereby transferringinformation from one task to the next. We show that when the within-taskalgorithm comes with some regret bound, our strategy inherits this goodproperty. Our bounds are in expectation for a general loss function, anduniform for a convex loss. We discuss applications to dictionary learning andfinite set of predictors. In the latter case, we improve previous$O(1/\sqrt{m})$ bounds to $O(1/m)$ where $m$ is the per task sample size.",http://arxiv.org/abs/1610.08628v1,,
1001,Semi-Supervised Radio Signal Identification,"Radio emitter recognition in dense multi-user environments is an importanttool for optimizing spectrum utilization, identifying and minimizinginterference, and enforcing spectrum policy. Radio data is readily availableand easy to obtain from an antenna, but labeled and curated data is oftenscarce making supervised learning strategies difficult and time consuming inpractice. We demonstrate that semi-supervised learning techniques can be usedto scale learning beyond supervised datasets, allowing for discerning andrecalling new radio signals by using sparse signal representations based onboth unsupervised and supervised methods for nonlinear feature learning andclustering methods.",http://arxiv.org/abs/1611.00303v2,,
1002,Variational Graph Auto-Encoders,"We introduce the variational graph auto-encoder (VGAE), a framework forunsupervised learning on graph-structured data based on the variationalauto-encoder (VAE). This model makes use of latent variables and is capable oflearning interpretable latent representations for undirected graphs. Wedemonstrate this model using a graph convolutional network (GCN) encoder and asimple inner product decoder. Our model achieves competitive results on a linkprediction task in citation networks. In contrast to most existing models forunsupervised learning on graph-structured data and link prediction, our modelcan naturally incorporate node features, which significantly improvespredictive performance on a number of benchmark datasets.",http://arxiv.org/abs/1611.07308v1,,
1003,"Non-Deterministic Policy Improvement Stabilizes Approximated  Reinforcement Learning","This paper investigates a type of instability that is linked to the greedypolicy improvement in approximated reinforcement learning. We show empiricallythat non-deterministic policy improvement can stabilize methods like LSPI bycontrolling the improvements' stochasticity. Additionally we show that asuitable representation of the value function also stabilizes the solution tosome degree. The presented approach is simple and should also be easilytransferable to more sophisticated algorithms like deep reinforcement learning.",http://arxiv.org/abs/1612.07548v1,,
1004,Online Structure Learning for Sum-Product Networks with Gaussian Leaves,"Sum-product networks have recently emerged as an attractive representationdue to their dual view as a special type of deep neural network with clearsemantics and a special type of probabilistic graphical model for whichinference is always tractable. Those properties follow from some conditions(i.e., completeness and decomposability) that must be respected by thestructure of the network. As a result, it is not easy to specify a validsum-product network by hand and therefore structure learning techniques aretypically used in practice. This paper describes the first online structurelearning technique for continuous SPNs with Gaussian leaves. We also introducean accompanying new parameter learning technique.",http://arxiv.org/abs/1701.05265v1,,
1005,Algorithmic stability and hypothesis complexity,"We introduce a notion of algorithmic stability of learning algorithms---thatwe term \emph{argument stability}---that captures stability of the hypothesisoutput by the learning algorithm in the normed space of functions from whichhypotheses are selected. The main result of the paper bounds the generalizationerror of any learning algorithm in terms of its argument stability. The boundsare based on martingale inequalities in the Banach space to which thehypotheses belong. We apply the general bounds to bound the performance of somelearning algorithms based on empirical risk minimization and stochasticgradient descent.",http://arxiv.org/abs/1702.08712v2,,
1006,Atari games and Intel processors,"The asynchronous nature of the state-of-the-art reinforcement learningalgorithms such as the Asynchronous Advantage Actor-Critic algorithm, makesthem exceptionally suitable for CPU computations. However, given the fact thatdeep reinforcement learning often deals with interpreting visual information, alarge part of the train and inference time is spent performing convolutions. Inthis work we present our results on learning strategies in Atari games using aConvolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0machine learning framework. We also analyze effects of asynchronouscomputations on the convergence of reinforcement learning algorithms.",http://arxiv.org/abs/1705.06936v1,,
1007,Kernel Scaling for Manifold Learning and Classification,"Kernel methods play a critical role in many dimensionality reductionalgorithms. They are useful in manifold learning, classification, clusteringand other machine learning tasks. Setting the kernel's scale parameter, alsoreferred as the kernel's bandwidth, highly affects the extractedlow-dimensional representation. We propose to set a scale parameter that istailored to the desired application such as classification and manifoldlearning. The scale computation for the manifold learning task enables that thedimension of the extracted embedding equals the intrinsic dimension estimation.Three methods are proposed for scale computation in a classification task. Theproposed frameworks are simulated on artificial and real datasets. The resultsshow a high correlation between optimal classification rates and the computedscaling.",http://arxiv.org/abs/1707.01093v1,,
1008,"Human in the Loop: Interactive Passive Automata Learning via  Evidence-Driven State-Merging Algorithms","We present an interactive version of an evidence-driven state-merging (EDSM)algorithm for learning variants of finite state automata. Learning theseautomata often amounts to recovering or reverse engineering the modelgenerating the data despite noisy, incomplete, or imperfectly sampled datasources rather than optimizing a purely numeric target function. Domainexpertise and human knowledge about the target domain can guide this process,and typically is captured in parameter settings. Often, domain expertise issubconscious and not expressed explicitly. Directly interacting with thelearning algorithm makes it easier to utilize this knowledge effectively.",http://arxiv.org/abs/1707.09430v1,,
1009,Learning non-parametric Markov networks with mutual information,"We propose a method for learning Markov network structures for continuousdata without invoking any assumptions about the distribution of the variables.The method makes use of previous work on a non-parametric estimator for mutualinformation which is used to create a non-parametric test for multivariateconditional independence. This independence test is then combined with anefficient constraint-based algorithm for learning the graph structure. Theperformance of the method is evaluated on several synthetic data sets and it isshown to learn considerably more accurate structures than competing methodswhen the dependencies between the variables involve non-linearities.",http://arxiv.org/abs/1708.02497v1,,
1010,Deep Incremental Boosting,"This paper introduces Deep Incremental Boosting, a new technique derived fromAdaBoost, specifically adapted to work with Deep Learning methods, that reducesthe required training time and improves generalisation. We draw inspirationfrom Transfer of Learning approaches to reduce the start-up time to trainingeach incremental Ensemble member. We show a set of experiments that outlinessome preliminary results on some common Deep Learning datasets and discuss thepotential improvements Deep Incremental Boosting brings to traditional Ensemblemethods in Deep Learning.",http://arxiv.org/abs/1708.03704v1,,
1011,Semi-supervised Conditional GANs,"We introduce a new model for building conditional generative models in asemi-supervised setting to conditionally generate data given attributes byadapting the GAN framework. The proposed semi-supervised GAN (SS-GAN) modeluses a pair of stacked discriminators to learn the marginal distribution of thedata, and the conditional distribution of the attributes given the datarespectively. In the semi-supervised setting, the marginal distribution (whichis often harder to learn) is learned from the labeled + unlabeled data, and theconditional distribution is learned purely from the labeled data. Ourexperimental results demonstrate that this model performs significantly bettercompared to existing semi-supervised conditional GAN models.",http://arxiv.org/abs/1708.05789v1,,
1012,"Symmetric Variational Autoencoder and Connections to Adversarial  Learning","A new form of the variational autoencoder (VAE) is proposed, based on thesymmetric Kullback-Leibler divergence. It is demonstrated that learning of theresulting symmetric VAE (sVAE) has close connections to previously developedadversarial-learning methods. This relationship helps unify the previouslydistinct techniques of VAE and adversarially learning, and provides insightsthat allow us to ameliorate shortcomings with some previously developedadversarial methods. In addition to an analysis that motivates and explains thesVAE, an extensive set of experiments validate the utility of the approach.",http://arxiv.org/abs/1709.01846v2,,
1013,Is Epicurus the father of Reinforcement Learning?,"The Epicurean Philosophy is commonly thought as simplistic and hedonistic.Here I discuss how this is a misconception and explore its link toReinforcement Learning. Based on the letters of Epicurus, I construct anobjective function for hedonism which turns out to be equivalent of theReinforcement Learning objective function when omitting the discount factor. Ithen discuss how Plato and Aristotle 's views that can be also loosely linkedto Reinforcement Learning, as well as their weaknesses in relationship to it.Finally, I emphasise the close affinity of the Epicurean views and the Bellmanequation.",http://arxiv.org/abs/1710.04582v1,,
1014,"Biologically Inspired Feedforward Supervised Learning for Deep  Self-Organizing Map Networks","In this study, we propose a novel deep neural network and its supervisedlearning method that uses a feedforward supervisory signal. The method isinspired by the human visual system and performs human-like association-basedlearning without any backward error propagation. The feedforward supervisorysignal that produces the correct result is preceded by the target signal andassociates its confirmed label with the classification result of the targetsignal. It effectively uses a large amount of information from the feedforwardsignal, and forms a continuous and rich learning representation. The method isvalidated using visual recognition tasks on the MNIST handwritten dataset.",http://arxiv.org/abs/1710.09574v1,,
1015,"Double Q($$) and Q($, $): Unifying Reinforcement  Learning Control Algorithms","Temporal-difference (TD) learning is an important field in reinforcementlearning. Sarsa and Q-Learning are among the most used TD algorithms. TheQ($\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paperextends the Q($\sigma$) algorithm to an online multi-step algorithm Q($\sigma,\lambda$) using eligibility traces and introduces Double Q($\sigma$) as theextension of Q($\sigma$) to double learning. Experiments suggest that the newQ($\sigma, \lambda$) algorithm can outperform the classical TD control methodsSarsa($\lambda$), Q($\lambda$) and Q($\sigma$).",http://arxiv.org/abs/1711.01569v1,,
1016,Learning Less-Overlapping Representations,"In representation learning (RL), how to make the learned representations easyto interpret and less overfitted to training data are two important butchallenging issues. To address these problems, we study a new type ofregulariza- tion approach that encourages the supports of weight vectors in RLmodels to have small overlap, by simultaneously promoting near-orthogonalityamong vectors and sparsity of each vector. We apply the proposed regularizer totwo models: neural networks (NNs) and sparse coding (SC), and develop anefficient ADMM-based algorithm for regu- larized SC. Experiments on variousdatasets demonstrate that weight vectors learned under our regularizer are moreinterpretable and have better generalization performance.",http://arxiv.org/abs/1711.09300v1,,
1017,A generalized concept-cognitive learning: A machine learning viewpoint,"Concept-cognitive learning (CCL) is a hot topic in recent years, and it hasattracted much attention from the communities of formal concept analysis,granular computing and cognitive computing. However, the relationship amongcognitive computing (CC), concept-cognitive computing (CCC), CCL andconcept-cognitive learning model (CCLM) is not clearly described. To this end,we first explain the relationship of CC, CCC, CCL and CCLM. Then, we propose ageneralized concept-cognitive learning (GCCL) from the point of view of machinelearning. Finally, experiments on some data sets are conducted to verify thefeasibility of concept formation and concept-cognitive process of GCCL.",http://arxiv.org/abs/1801.02334v3,,
1018,Learning with Abandonment,"Consider a platform that wants to learn a personalized policy for each user,but the platform faces the risk of a user abandoning the platform if she isdissatisfied with the actions of the platform. For example, a platform isinterested in personalizing the number of newsletters it sends, but faces therisk that the user unsubscribes forever. We propose a general thresholdedlearning model for scenarios like this, and discuss the structure of optimalpolicies. We describe salient features of optimal personalization algorithmsand how feedback the platform receives impacts the results. Furthermore, weinvestigate how the platform can efficiently learn the heterogeneity acrossusers by interacting with a population and provide performance guarantees.",http://arxiv.org/abs/1802.08718v1,,
1019,"An Online Algorithm for Learning Buyer Behavior under Realistic Pricing  Restrictions","We propose a new efficient online algorithm to learn the parameters governingthe purchasing behavior of a utility maximizing buyer, who responds to prices,in a repeated interaction setting. The key feature of our algorithm is that itcan learn even non-linear buyer utility while working with arbitrary priceconstraints that the seller may impose. This overcomes a major shortcoming ofprevious approaches, which use unrealistic prices to learn these parametersmaking them unsuitable in practice.",http://arxiv.org/abs/1803.01968v1,,
1020,Gotta Learn Fast: A New Benchmark for Generalization in RL,"In this report, we present a new reinforcement learning (RL) benchmark basedon the Sonic the Hedgehog (TM) video game franchise. This benchmark is intendedto measure the performance of transfer learning and few-shot learningalgorithms in the RL domain. We also present and evaluate some baselinealgorithms on the new benchmark.",http://arxiv.org/abs/1804.03720v2,,
1021,A Unified Framework of Deep Neural Networks by Capsules,"With the growth of deep learning, how to describe deep neural networksunifiedly is becoming an important issue. We first formalize neural networksmathematically with their directed graph representations, and prove ageneration theorem about the induced networks of connected directed acyclicgraphs. Then, we set up a unified framework for deep learning with capsulenetworks. This capsule framework could simplify the description of existingdeep neural networks, and provide a theoretical basis of graphic designing andprogramming techniques for deep learning models, thus would be of greatsignificance to the advancement of deep learning.",http://arxiv.org/abs/1805.03551v2,,
1022,k-Space Deep Learning for Accelerated MRI,"The annihilating filter-based low-rank Hanel matrix approach (ALOHA) is oneof the state-of-the-art compressed sensing approaches that directlyinterpolates the missing k-space data using low-rank Hankel matrix completion.Inspired by the recent mathematical discovery that links deep neural networksto Hankel matrix decomposition using data-driven framelet basis, here wepropose a fully data-driven deep learning algorithm for k-space interpolation.Our network can be also easily applied to non-Cartesian k-space trajectories bysimply adding an additional re-gridding layer. Extensive numerical experimentsshow that the proposed deep learning method significantly outperforms theexisting image-domain deep learning approaches.",http://arxiv.org/abs/1805.03779v1,,
1023,Do Outliers Ruin Collaboration?,"We consider the problem of learning a binary classifier from $n$ differentdata sources, among which at most an $\eta$ fraction are adversarial. Theoverhead is defined as the ratio between the sample complexity of learning inthis setting and that of learning the same hypothesis class on a single datadistribution. We present an algorithm that achieves an $O(\eta n + \ln n)$overhead, which is proved to be worst-case optimal. We also discuss thepotential challenges to the design of a computationally efficient learningalgorithm with a small overhead.",http://arxiv.org/abs/1805.04720v1,,
1024,Learning convex polytopes with margin,"We present an improved algorithm for properly learning convex polytopes inthe realizable PAC setting from data with a margin. Our learning algorithmconstructs a consistent polytope as an intersection of about $t \log t$halfspaces with margins in time polynomial in $t$ (where $t$ is the number ofhalfspaces forming an optimal polytope).  We also identify distinct generalizations of the notion of margin fromhyperplanes to polytopes and investigate how they relate geometrically; thisresult may be of interest beyond the learning setting.",http://arxiv.org/abs/1805.09719v2,,
1025,Private PAC learning implies finite Littlestone dimension,"We show that every approximately differentially private learning algorithm(possibly improper) for a class $H$ with Littlestone dimension~$d$ requires$\Omega\bigl(\log^*(d)\bigr)$ examples. As a corollary it follows that theclass of thresholds over $\mathbb{N}$ can not be learned in a private manner;this resolves open question due to [Bun et al., 2015, Feldman and Xiao, 2015].We leave as an open question whether every class with a finite Littlestonedimension can be learned by an approximately differentially private algorithm.",http://arxiv.org/abs/1806.00949v3,,
1026,Diffeomorphic Learning,"We introduce in this paper a learning paradigm in which the training data istransformed by a diffeomorphic transformation before prediction. The learningalgorithm minimizes a cost function evaluating the prediction error on thetraining set penalized by the distance between the diffeomorphism and theidentity. The approach borrows ideas from shape analysis, in the waydiffeomorphisms are estimated for shape and image alignment, and brings them ina previously unexplored setting, estimating, in particular diffeomorphisms inmuch larger dimensions. After introducing the concept and describing a learningalgorithm, we present diverse applications, mostly with synthetic examples,demonstrating the potential of the approach, as well as some of its currentroom for improvement.",http://arxiv.org/abs/1806.01240v2,,
1027,"Learning in Integer Latent Variable Models with Nested Automatic  Differentiation","We develop nested automatic differentiation (AD) algorithms for exactinference and learning in integer latent variable models. Recently, Winner,Sujono, and Sheldon showed how to reduce marginalization in a class of integerlatent variable models to evaluating a probability generating function whichcontains many levels of nested high-order derivatives. We contribute faster andmore stable AD algorithms for this challenging problem and a novel algorithm tocompute exact gradients for learning. These contributions lead to significantlyfaster and more accurate learning algorithms, and are the first AD algorithmswhose running time is polynomial in the number of levels of nesting.",http://arxiv.org/abs/1806.03207v1,,
1028,Modular meta-learning,"Many prediction problems, such as those that arise in the context ofrobotics, have a simplifying underlying structure that could acceleratelearning. In this paper, we present a strategy for learning a set of neuralnetwork modules that can be combined in different ways. We train differentmodular structures on a set of related tasks and generalize to new tasks bycomposing the learned modules in new ways. We show this improves performance intwo robotics-related problems.",http://arxiv.org/abs/1806.10166v1,,
1029,Single Index Latent Variable Models for Network Topology Inference,"A semi-parametric, non-linear regression model in the presence of latentvariables is applied towards learning network graph structure. These latentvariables can correspond to unmodeled phenomena or unmeasured agents in acomplex system of interacting entities. This formulation jointly estimatesnon-linearities in the underlying data generation, the direct interactionsbetween measured entities, and the indirect effects of unmeasured processes onthe observed data. The learning is posed as regularized empirical riskminimization. Details of the algorithm for learning the model are outlined.Experiments demonstrate the performance of the learned model on real data.",http://arxiv.org/abs/1807.00002v1,,
1030,"Navigating Diverse Data Science Learning: Critical Reflections Towards  Future Practice","Data Science is currently a popular field of science attracting expertisefrom very diverse backgrounds. Current learning practices need to acknowledgethis and adapt to it. This paper summarises some experiences relating to suchlearning approaches from teaching a postgraduate Data Science module, and drawssome learned lessons that are of relevance to others teaching Data Science.",http://arxiv.org/abs/1807.03750v1,,
1031,Robbins-Monro conditions for persistent exploration learning strategies,"We formulate simple assumptions, implying the Robbins-Monro conditions forthe $Q$-learning algorithm with the local learning rate, depending on thenumber of visits of a particular state-action pair (local clock) and the numberof iteration (global clock). It is assumed that the Markov decision process iscommunicating and the learning policy ensures the persistent exploration. Therestrictions are imposed on the functional dependence of the learning rate onthe local and global clocks. The result partially confirms the conjecture ofBradkte (1994).",http://arxiv.org/abs/1808.00245v3,,
1032,"Learning Overparameterized Neural Networks via Stochastic Gradient  Descent on Structured Data","Neural networks have many successful applications, while much lesstheoretical understanding has been gained. Towards bridging this gap, we studythe problem of learning a two-layer overparameterized ReLU neural network formulti-class classification via stochastic gradient descent (SGD) from randominitialization. In the overparameterized setting, when the data comes frommixtures of well-separated distributions, we prove that SGD learns a networkwith a small generalization error, albeit the network has enough capacity tofit arbitrary labels. Furthermore, the analysis provides interesting insightsinto several aspects of learning neural networks and can be verified based onempirical studies on synthetic data and on the MNIST dataset.",http://arxiv.org/abs/1808.01204v2,,
1033,Robust Implicit Backpropagation,"Arguably the biggest challenge in applying neural networks is tuning thehyperparameters, in particular the learning rate. The sensitivity to thelearning rate is due to the reliance on backpropagation to train the network.In this paper we present the first application of Implicit Stochastic GradientDescent (ISGD) to train neural networks, a method known in convex optimizationto be unconditionally stable and robust to the learning rate. Our keycontribution is a novel layer-wise approximation of ISGD which makes itsupdates tractable for neural networks. Experiments show that our method is morerobust to high learning rates and generally outperforms standardbackpropagation on a variety of tasks.",http://arxiv.org/abs/1808.02433v1,,
1034,Reinforcement Learning under Threats,"In several reinforcement learning (RL) scenarios, mainly in securitysettings, there may be adversaries trying to interfere with the rewardgenerating process. In this paper, we introduce Threatened Markov DecisionProcesses (TMDPs), which provide a framework to support a decision makeragainst a potential adversary in RL. Furthermore, we propose a level-$k$thinking scheme resulting in a new learning framework to deal with TMDPs. Afterintroducing our framework and deriving theoretical results, relevant empiricalevidence is given via extensive experiments, showing the benefits of accountingfor adversaries while the agent learns.",http://arxiv.org/abs/1809.01560v1,,
1035,Anderson Acceleration for Reinforcement Learning,"Anderson acceleration is an old and simple method for accelerating thecomputation of a fixed point. However, as far as we know and quitesurprisingly, it has never been applied to dynamic programming or reinforcementlearning. In this paper, we explain briefly what Anderson acceleration is andhow it can be applied to value iteration, this being supported by preliminaryexperiments showing a significant speed up of convergence, that we criticallydiscuss. We also discuss how this idea could be applied more generally to(deep) reinforcement learning.",http://arxiv.org/abs/1809.09501v1,,
1036,Optimal Learning with Anisotropic Gaussian SVMs,"This paper investigates the nonparametric regression problem using SVMs withanisotropic Gaussian RBF kernels. Under the assumption that the targetfunctions are resided in certain anisotropic Besov spaces, we establish thealmost optimal learning rates, more precisely, optimal up to some logarithmicfactor, presented by the effective smoothness. By taking the effectivesmoothness into consideration, our almost optimal learning rates are fasterthan those obtained with the underlying RKHSs being certain anisotropic Sobolevspaces. Moreover, if the target function depends only on fewer dimensions,faster learning rates can be further achieved.",http://arxiv.org/abs/1810.02321v1,,
1037,Sharp Analysis of Learning with Discrete Losses,"The problem of devising learning strategies for discrete losses (e.g.,multilabeling, ranking) is currently addressed with methods and theoreticalanalyses ad-hoc for each loss. In this paper we study a least-squares frameworkto systematically design learning algorithms for discrete losses, withquantitative characterizations in terms of statistical and computationalcomplexity. In particular we improve existing results by providing explicitdependence on the number of labels for a wide class of losses and fasterlearning rates in conditions of low-noise. Theoretical results are complementedwith experiments on real datasets, showing the effectiveness of the proposedgeneral approach.",http://arxiv.org/abs/1810.06839v1,,
1038,RLgraph: Modular Computation Graphs for Deep Reinforcement Learning,"Reinforcement learning (RL) tasks are challenging to implement, execute andtest due to algorithmic instability, hyper-parameter sensitivity, andheterogeneous distributed communication patterns. We argue for the separationof logical component composition, backend graph definition, and distributedexecution. To this end, we introduce RLgraph, a library for designing andexecuting reinforcement learning tasks in both static graph and define-by-runparadigms. The resulting implementations are robust, incrementally testable,and yield high performance across different deep learning frameworks anddistributed backends.",http://arxiv.org/abs/1810.09028v2,,
1039,Adversarial Online Learning with noise,"We present and study models of adversarial online learning where the feedbackobserved by the learner is noisy, and the feedback is either full informationfeedback or bandit feedback. Specifically, we consider binary losses xored withthe noise, which is a Bernoulli random variable. We consider both a constantnoise rate and a variable noise rate. Our main results are tight regret boundsfor learning with noise in the adversarial online learning model.",http://arxiv.org/abs/1810.09346v3,,
1040,Learning to Represent Edits,"We introduce the problem of learning distributed representations of edits. Bycombining a ""neural editor"" with an ""edit encoder"", our models learn torepresent the salient information of an edit and can be used to apply edits tonew inputs. We experiment on natural language and source code edit data. Ourevaluation yields promising results that suggest that our neural network modelslearn to capture the structure and semantics of edits. We hope that thisinteresting task and data source will inspire other researchers to work furtheron this problem.",http://arxiv.org/abs/1810.13337v2,,
1041,"Towards a Simple Approach to Multi-step Model-based Reinforcement  Learning","When environmental interaction is expensive, model-based reinforcementlearning offers a solution by planning ahead and avoiding costly mistakes.Model-based agents typically learn a single-step transition model. In thispaper, we propose a multi-step model that predicts the outcome of an actionsequence with variable length. We show that this model is easy to learn, andthat the model can make policy-conditional predictions. We report preliminaryresults that show a clear advantage for the multi-step model compared to itsone-step counterpart.",http://arxiv.org/abs/1811.00128v1,,
1042,"Improving Generalization for Abstract Reasoning Tasks Using Disentangled  Feature Representations","In this work we explore the generalization characteristics of unsupervisedrepresentation learning by leveraging disentangled VAE's to learn a usefullatent space on a set of relational reasoning problems derived from RavenProgressive Matrices. We show that the latent representations, learned byunsupervised training using the right objective function, significantlyoutperform the same architectures trained with purely supervised learning,especially when it comes to generalization.",http://arxiv.org/abs/1811.04784v1,,
1043,Deep Inverse Optimization,"Given a set of observations generated by an optimization process, the goal ofinverse optimization is to determine likely parameters of that process. We castinverse optimization as a form of deep learning. Our method, called deepinverse optimization, is to unroll an iterative optimization process and thenuse backpropagation to learn parameters that generate the observations. Wedemonstrate that by backpropagating through the interior point algorithm we canlearn the coefficients determining the cost vector and the constraints,independently or jointly, for both non-parametric and parametric linearprograms, starting from one or multiple observations. With this approach,inverse optimization can leverage concepts and algorithms from deep learning.",http://arxiv.org/abs/1812.00804v1,,
1044,Efficient Model-Free Reinforcement Learning Using Gaussian Process,"Efficient Reinforcement Learning usually takes advantage of demonstration orgood exploration strategy. By applying posterior sampling in model-free RLunder the hypothesis of GP, we propose Gaussian Process Posterior SamplingReinforcement Learning(GPPSTD) algorithm in continuous state space, givingtheoretical justifications and empirical results. We also provide theoreticaland empirical results that various demonstration could lower expecteduncertainty and benefit posterior sampling exploration. In this way, wecombined the demonstration and exploration process together to achieve a moreefficient reinforcement learning.",http://arxiv.org/abs/1812.04359v1,,
1045,"Learning representations of molecules and materials with atomistic  neural networks","Deep Learning has been shown to learn efficient representations forstructured data such as image, text or audio. In this chapter, we presentneural network architectures that are able to learn efficient representationsof molecules and materials. In particular, the continuous-filter convolutionalnetwork SchNet accurately predicts chemical properties across compositional andconfigurational space on a variety of datasets. Beyond that, we analyze theobtained representations to find evidence that their spatial and chemicalproperties agree with chemical intuition.",http://arxiv.org/abs/1812.04690v1,,
1046,A short review on Applications of Deep learning for Cyber security,"Deep learning is an advanced model of traditional machine learning. This hasthe capability to extract optimal feature representation from raw inputsamples. This has been applied towards various use cases in cyber security suchas intrusion detection, malware classification, android malware detection, spamand phishing detection and binary analysis. This paper outlines the survey ofall the works related to deep learning based solutions for various cybersecurity use cases. Keywords: Deep learning, intrusion detection, malwaredetection, Android malware detection, spam & phishing detection, trafficanalysis, binary analysis.",http://arxiv.org/abs/1812.06292v2,,
1047,Ophthalmic Diagnosis and Deep Learning -- A Survey,"This review presents an overview of the applications of deep learninginophthalmic diagnosis using retinal images and a review of the availableretinalimage datasets. Applications of deep learning for segmentation of opticdisk,blood vessels and retinal layer as well as detection of red lesions arereviewed.Recent deep learning models for classification of retinal diseaseincludingage-related macular degeneration, glaucoma, diabetic macular edemaanddiabetic retinopathy are also reported",http://arxiv.org/abs/1812.07101v2,,
1048,Max-Diversity Distributed Learning: Theory and Algorithms,"We study the risk performance of distributed learning for the regularizationempirical risk minimization with fast convergence rate, substantially improvingthe error analysis of the existing divide-and-conquer based distributedlearning. An interesting theoretical finding is that the larger the diversityof each local estimate is, the tighter the risk bound is. This theoreticalanalysis motivates us to devise an effective maxdiversity distributed learningalgorithm (MDD). Experimental results show that MDD can outperform the existingdivide-andconquer methods but with a bit more time. Theoretical analysis andempirical results demonstrate that our proposed MDD is sound and effective.",http://arxiv.org/abs/1812.07738v2,,
1049,"Modular meta-learning in abstract graph networks for combinatorial  generalization","Modular meta-learning is a new framework that generalizes to unseen datasetsby combining a small set of neural modules in different ways. In this work wepropose abstract graph networks: using graphs as abstractions of a system'ssubparts without a fixed assignment of nodes to system subparts, for which wewould need supervision. We combine this idea with modular meta-learning to geta flexible framework with combinatorial generalization to new tasks built in.We then use it to model the pushing of arbitrarily shaped objects from littleor no training data.",http://arxiv.org/abs/1812.07768v1,,
1050,Optimizing Market Making using Multi-Agent Reinforcement Learning,"In this paper, reinforcement learning is applied to the problem of optimizingmarket making. A multi-agent reinforcement learning framework is used tooptimally place limit orders that lead to successful trades. The frameworkconsists of two agents. The macro-agent optimizes on making the decision tobuy, sell, or hold an asset. The micro-agent optimizes on placing limit orderswithin the limit order book. For the context of this paper, the proposedframework is applied and studied on the Bitcoin cryptocurrency market. The goalof this paper is to show that reinforcement learning is a viable strategy thatcan be applied to complex problems (with complex environments) such as marketmaking.",http://arxiv.org/abs/1812.10252v1,,
1051,Adversarial Learning of a Sampler Based on an Unnormalized Distribution,"We investigate adversarial learning in the case when only an unnormalizedform of the density can be accessed, rather than samples. With insights sogarnered, adversarial learning is extended to the case for which one has accessto an unnormalized form u(x) of the target density function, but no samples.Further, new concepts in GAN regularization are developed, based on learningfrom samples or from u(x). The proposed method is compared to alternativeapproaches, with encouraging results demonstrated across a range ofapplications, including deep soft Q-learning.",http://arxiv.org/abs/1901.00612v1,,
1052,"Geometrization of deep networks for the interpretability of deep  learning systems","How to understand deep learning systems remains an open problem. In thispaper we propose that the answer may lie in the geometrization of deepnetworks. Geometrization is a bridge to connect physics, geometry, deep networkand quantum computation and this may result in a new scheme to reveal the ruleof the physical world. By comparing the geometry of image matching and deepnetworks, we show that geometrization of deep networks can be used tounderstand existing deep learning systems and it may also help to solve theinterpretability problem of deep learning systems.",http://arxiv.org/abs/1901.02354v2,,
1053,Diverse mini-batch Active Learning,"We study the problem of reducing the amount of labeled training data requiredto train supervised classification models. We approach it by leveraging ActiveLearning, through sequential selection of examples which benefit the modelmost. Selecting examples one by one is not practical for the amount of trainingexamples required by the modern Deep Learning models. We consider themini-batch Active Learning setting, where several examples are selected atonce. We present an approach which takes into account both informativeness ofthe examples for the model, as well as the diversity of the examples in amini-batch. By using the well studied K-means clustering algorithm, thisapproach scales better than the previously proposed approaches, and achievescomparable or better performance.",http://arxiv.org/abs/1901.05954v1,,
1054,"Activation Functions for Generalized Learning Vector Quantization - A  Performance Comparison","An appropriate choice of the activation function (like ReLU, sigmoid orswish) plays an important role in the performance of (deep) multilayerperceptrons (MLP) for classification and regression learning. Prototype-basedclassification learning methods like (generalized) learning vector quantization(GLVQ) are powerful alternatives. These models also deal with activationfunctions but here they are applied to the so-called classifier functioninstead. In this paper we investigate successful candidates of activationfunctions known for MLPs for application in GLVQ and their influence on theperformance.",http://arxiv.org/abs/1901.05995v1,,
1055,"NIF: A Framework for Quantifying Neural Information Flow in Deep  Networks","In this paper, we present a new approach to interpreting deep learningmodels. More precisely, by coupling mutual information with network science, weexplore how information flows through feed forward networks. We show thatefficiently approximating mutual information via the dual representation ofKullback-Leibler divergence allows us to create an information measure thatquantifies how much information flows between any two neurons of a deeplearning model. To that end, we propose NIF, Neural Information Flow, a newmetric for codifying information flow which exposes the internals of a deeplearning model while providing feature attributions.",http://arxiv.org/abs/1901.08557v1,,
1056,Fairness with Dynamics,"It has recently been shown that if feedback effects of decisions are ignored,then imposing fairness constraints such as demographic parity or equality ofopportunity can actually exacerbate unfairness. We propose to address thischallenge by modeling feedback effects as the dynamics of a Markov decisionprocesses (MDPs). First, we define analogs of fairness properties that havebeen proposed for supervised learning. Second, we propose algorithms forlearning fair decision-making policies for MDPs. We also explore extensions toreinforcement learning, where parts of the dynamical system are unknown andmust be learned without violating fairness. Finally, we demonstrate the need toaccount for dynamical effects using simulations on a loan applicant MDP.",http://arxiv.org/abs/1901.08568v1,,
1057,Policy Consolidation for Continual Reinforcement Learning,"We propose a method for tackling catastrophic forgetting in deepreinforcement learning that is \textit{agnostic} to the timescale of changes inthe distribution of experiences, does not require knowledge of task boundaries,and can adapt in \textit{continuously} changing environments. In our\textit{policy consolidation} model, the policy network interacts with acascade of hidden networks that simultaneously remember the agent's policy at arange of timescales and regularise the current policy by its own history,thereby improving its ability to learn without forgetting. We find that themodel improves continual learning relative to baselines on a number ofcontinuous control tasks in single-task, alternating two-task, and multi-agentcompetitive self-play settings.",http://arxiv.org/abs/1902.00255v1,,
1058,"Learning Taxonomies of Concepts and not Words using Contextualized Word  Representations: A Position Paper","Taxonomies are semantic hierarchies of concepts. One limitation of currenttaxonomy learning systems is that they define concepts as single words. Thisposition paper argues that contextualized word representations, which recentlyachieved state-of-the-art results on many competitive NLP tasks, are apromising method to address this limitation. We outline a novel approach fortaxonomy learning that (1) defines concepts as synsets, (2) learnsdensity-based approximations of contextualized word representations, and (3)can measure similarity and hypernymy among them.",http://arxiv.org/abs/1902.02169v1,,
1059,"Riemannian joint dimensionality reduction and dictionary learning on  symmetric positive definite manifold","Dictionary leaning (DL) and dimensionality reduction (DR) are powerful toolsto analyze high-dimensional noisy signals. This paper presents a proposal of anovel Riemannian joint dimensionality reduction and dictionary learning(R-JDRDL) on symmetric positive definite (SPD) manifolds for classificationtasks. The joint learning considers the interaction between dimensionalityreduction and dictionary learning procedures by connecting them into a unifiedframework. We exploit a Riemannian optimization framework for solving DL and DRproblems jointly. Finally, we demonstrate that the proposed R-JDRDL outperformsexisting state-of-the-arts algorithms when used for image classification tasks.",http://arxiv.org/abs/1902.04186v1,,
1060,Gaussian Mean Field Regularizes by Limiting Learned Information,"Variational inference with a factorized Gaussian posterior estimate is awidely used approach for learning parameters and hidden variables. Empirically,a regularizing effect can be observed that is poorly understood. In this work,we show how mean field inference improves generalization by limiting mutualinformation between learned parameters and the data through noise. We quantifya maximum capacity when the posterior variance is either fixed or learned andconnect it to generalization error, even when the KL-divergence in theobjective is rescaled. Our experiments demonstrate that bounding informationbetween parameters and data effectively regularizes neural networks on bothsupervised and unsupervised tasks.",http://arxiv.org/abs/1902.04340v1,,
1061,Interpolation Consistency Training for Semi-Supervised Learning,"We introduce Interpolation Consistency Training (ICT), a simple andcomputation efficient algorithm for training Deep Neural Networks in thesemi-supervised learning paradigm. ICT encourages the prediction at aninterpolation of unlabeled points to be consistent with the interpolation ofthe predictions at those points. In classification problems, ICT moves thedecision boundary to low-density regions of the data distribution. Ourexperiments show that ICT achieves state-of-the-art performance when applied tostandard neural network architectures on the CIFAR-10 and SVHN benchmarkdatasets.",http://arxiv.org/abs/1903.03825v1,,
1062,"Privacy-preserving Active Learning on Sensitive Data for User Intent  Classification","Active learning holds promise of significantly reducing data annotation costswhile maintaining reasonable model performance. However, it requires sendingdata to annotators for labeling. This presents a possible privacy leak when thetraining set includes sensitive user data. In this paper, we describe anapproach for carrying out privacy preserving active learning with quantifiableguarantees. We evaluate our approach by showing the tradeoff between privacy,utility and annotation budget on a binary classification task in a activelearning setting.",http://arxiv.org/abs/1903.11112v1,,
1063,Learning convex bodies is hard,"We show that learning a convex body in $\RR^d$, given random samples from thebody, requires $2^{\Omega(\sqrt{d/\eps})}$ samples. By learning a convex bodywe mean finding a set having at most $\eps$ relative symmetric difference withthe input body. To prove the lower bound we construct a hard to learn family ofconvex bodies. Our construction of this family is very simple and based onerror correcting codes.",http://arxiv.org/abs/0904.1227v1,,
1064,Bayesian Multitask Learning with Latent Hierarchies,"We learn multiple hypotheses for related tasks under a latent hierarchicalrelationship between tasks. We exploit the intuition that for domainadaptation, we wish to share classifier structure, but for multitask learning,we wish to share covariance structure. Our hierarchical model is seen tosubsume several previously proposed multitask learning models and performs wellon three distinct real-world data sets.",http://arxiv.org/abs/0907.0783v1,,
1065,Cross-Task Knowledge-Constrained Self Training,"We present an algorithmic framework for learning multiple related tasks. Ourframework exploits a form of prior knowledge that relates the output spaces ofthese tasks. We present PAC learning results that analyze the conditions underwhich such learning is possible. We present results on learning a shallowparser and named-entity recognition system that exploits our framework, showingconsistent improvements over baseline methods.",http://arxiv.org/abs/0907.0784v1,,
1066,Agnostic Active Learning Without Constraints,"We present and analyze an agnostic active learning algorithm that workswithout keeping a version space. This is unlike all previous approaches where arestricted set of candidate hypotheses is maintained throughout learning, andonly hypotheses from this set are ever returned. By avoiding this version spaceapproach, our algorithm sheds the computational burden and brittlenessassociated with maintaining version spaces, yet still allows for substantialimprovements over supervised learning for classification.",http://arxiv.org/abs/1006.2588v1,,
1067,"Multiple Closed-Form Local Metric Learning for K-Nearest Neighbor  Classifier","Many researches have been devoted to learn a Mahalanobis distance metric,which can effectively improve the performance of kNN classification. Mostapproaches are iterative and computational expensive and linear rigidity stillcritically limits metric learning algorithm to perform better. We proposed acomputational economical framework to learn multiple metrics in closed-form.",http://arxiv.org/abs/1311.3157v1,,
1068,Negative Learning Rates and P-Learning,"We present a method of training a differentiable function approximator for aregression task using negative examples. We effect this training using negativelearning rates. We also show how this method can be used to perform directpolicy learning in a reinforcement learning setting.",http://arxiv.org/abs/1603.08253v3,,
1069,Non-Adaptive Learning a Hidden Hipergraph,"We give a new deterministic algorithm that non-adaptively learns a hiddenhypergraph from edge-detecting queries. All previous non-adaptive algorithmseither run in exponential time or have non-optimal query complexity. We givethe first polynomial time non-adaptive learning algorithm for learninghypergraph that asks almost optimal number of queries.",http://arxiv.org/abs/1502.04137v1,,
1070,When deep learning meets security,"Deep learning is an emerging research field that has proven its effectivenesstowards deploying more efficient intelligent systems. Security, on the otherhand, is one of the most essential issues in modern communication systems.Recently many papers have shown that using deep learning models can achievepromising results when applied to the security domain. In this work, we providean overview for the recent studies that apply deep learning techniques to thefield of security.",http://arxiv.org/abs/1807.04739v1,,
1071,"Face valuing: Training user interfaces with facial expressions and  reinforcement learning","An important application of interactive machine learning is extending oramplifying the cognitive and physical capabilities of a human. To accomplishthis, machines need to learn about their human users' intentions and adapt totheir preferences. In most current research, a user has conveyed preferences toa machine using explicit corrective or instructive feedback; explicit feedbackimposes a cognitive load on the user and is expensive in terms of human effort.The primary objective of the current work is to demonstrate that a learningagent can reduce the amount of explicit feedback required for adapting to theuser's preferences pertaining to a task by learning to perceive a value of itsbehavior from the human user, particularly from the user's facialexpressions---we call this face valuing. We empirically evaluate face valuingon a grip selection task. Our preliminary results suggest that an agent canquickly adapt to a user's changing preferences with minimal explicit feedbackby learning a value function that maps facial features extracted from a cameraimage to expected future reward. We believe that an agent learning to perceivea value from the body language of its human user is complementary to existinginteractive machine learning approaches and will help in creating successfulhuman-machine interactive applications.",http://arxiv.org/abs/1606.02807v1,,
1072,Interpreting Deep Learning: The Machine Learning Rorschach Test?,"Theoretical understanding of deep learning is one of the most important tasksfacing the statistics and machine learning communities. While deep neuralnetworks (DNNs) originated as engineering methods and models of biologicalnetworks in neuroscience and psychology, they have quickly become a centerpieceof the machine learning toolbox. Unfortunately, DNN adoption powered by recentsuccesses combined with the open-source nature of the machine learningcommunity, has outpaced our theoretical understanding. We cannot reliablyidentify when and why DNNs will make mistakes. In some applications like texttranslation these mistakes may be comical and provide for fun fodder inresearch talks, a single error can be very costly in tasks like medicalimaging. As we utilize DNNs in increasingly sensitive applications, a betterunderstanding of their properties is thus imperative. Recent advances in DNNtheory are numerous and include many different sources of intuition, such aslearning theory, sparse signal analysis, physics, chemistry, and psychology. Aninteresting pattern begins to emerge in the breadth of possibleinterpretations. The seemingly limitless approaches are mostly constrained bythe lens with which the mathematical operations are viewed. Ultimately, theinterpretation of DNNs appears to mimic a type of Rorschach test --- apsychological test wherein subjects interpret a series of seemingly ambiguousink-blots. Validation for DNN theory requires a convergence of the literature.We must distinguish between universal results that are invariant to theanalysis perspective and those that are specific to a particular networkconfiguration. Simultaneously we must deal with the fact that many standardstatistical tools for quantifying generalization or empirically assessingimportant network features are difficult to apply to DNNs.",http://arxiv.org/abs/1806.00148v1,,
1073,Heteroscedastic Relevance Vector Machine,"In this work we propose a heteroscedastic generalization to RVM, a fastBayesian framework for regression, based on some recent similar works. We usevariational approximation and expectation propagation to tackle the problem.The work is still under progress and we are examining the results and comparingwith the previous works.",http://arxiv.org/abs/1301.2015v1,,
1074,Implicit Bias of Gradient Descent on Linear Convolutional Networks,"We show that gradient descent on full-width linear convolutional networks ofdepth $L$ converges to a linear predictor related to the $\ell_{2/L}$ bridgepenalty in the frequency domain. This is in contrast to linearly fullyconnected networks, where gradient descent converges to the hard margin linearsupport vector machine solution, regardless of depth.",http://arxiv.org/abs/1806.00468v2,,
1075,"Regularize, Expand and Compress: Multi-task based Lifelong Learning via  NonExpansive AutoML","Lifelong learning, the problem of continual learning where tasks arrive insequence, has been lately attracting more attention in the computer visioncommunity. The aim of lifelong learning is to develop a system that can learnnew tasks while maintaining the performance on the previously learned tasks.However, there are two obstacles for lifelong learning of deep neural networks:catastrophic forgetting and capacity limitation. To solve the above issues,inspired by the recent breakthroughs in automatically learning good neuralnetwork architectures, we develop a Multi-task based lifelong learning vianonexpansive AutoML framework termed Regularize, Expand and Compress (REC). RECis composed of three stages: 1) continually learns the sequential tasks withoutthe learned tasks' data via a newly proposed multi-task weight consolidation(MWC) algorithm; 2) expands the network to help the lifelong learning withpotentially improved model capability and performance by network-transformationbased AutoML; 3) compresses the expanded model after learning every new task tomaintain model efficiency and performance. The proposed MWC and REC algorithmsachieve superior performance over other lifelong learning algorithms on fourdifferent datasets.",http://arxiv.org/abs/1903.08362v1,,
1076,"Representative Task Self-selection for Flexible Clustered Lifelong  Learning","Consider the lifelong learning paradigm whose objective is to learn asequence of tasks depending on previous experiences, e.g., knowledge library ordeep network weights. However, the knowledge libraries or deep networks formost recent lifelong learning models are with prescribed size, and candegenerate the performance for both learned tasks and coming ones when facingwith a new task environment (cluster). To address this challenge, we propose anovel incremental clustered lifelong learning framework with two knowledgelibraries: feature learning library and model knowledge library, calledFlexible Clustered Lifelong Learning (FCL3). Specifically, the feature learninglibrary modeled by an autoencoder architecture maintains a set ofrepresentation common across all the observed tasks, and the model knowledgelibrary can be self-selected by identifying and adding new representativemodels (clusters). When a new task arrives, our proposed FCL3 model firstlytransfers knowledge from these libraries to encode the new task, i.e.,effectively and selectively soft-assigning this new task to multiplerepresentative models over feature learning library. Then, 1) the new task witha higher outlier probability will then be judged as a new representative, andused to redefine both feature learning library and representative models overtime; or 2) the new task with lower outlier probability will only refine thefeature learning library. For model optimization, we cast this lifelonglearning problem as an alternating direction minimization problem as a new taskcomes. Finally, we evaluate the proposed framework by analyzing severalmulti-task datasets, and the experimental results demonstrate that our FCL3model can achieve better performance than most lifelong learning frameworks,even batch clustered multi-task learning models.",http://arxiv.org/abs/1903.02173v1,,
1077,"Learning with Differential Privacy: Stability, Learnability and the  Sufficiency and Necessity of ERM Principle","While machine learning has proven to be a powerful data-driven solution tomany real-life problems, its use in sensitive domains has been limited due toprivacy concerns. A popular approach known as **differential privacy** offersprovable privacy guarantees, but it is often observed in practice that it couldsubstantially hamper learning accuracy. In this paper we study the learnability(whether a problem can be learned by any algorithm) under Vapnik's generallearning setting with differential privacy constraint, and reveal someintricate relationships between privacy, stability and learnability.  In particular, we show that a problem is privately learnable **if an onlyif** there is a private algorithm that asymptotically minimizes the empiricalrisk (AERM). In contrast, for non-private learning AERM alone is not sufficientfor learnability. This result suggests that when searching for private learningalgorithms, we can restrict the search to algorithms that are AERM. In light ofthis, we propose a conceptual procedure that always finds a universallyconsistent algorithm whenever the problem is learnable under privacyconstraint. We also propose a generic and practical algorithm and show thatunder very general conditions it privately learns a wide class of learningproblems. Lastly, we extend some of the results to the more practical$(\epsilon,\delta)$-differential privacy and establish the existence of aphase-transition on the class of problems that are approximately privatelylearnable with respect to how small $\delta$ needs to be.",http://arxiv.org/abs/1502.06309v3,,
1078,Putting a bug in ML: The moth olfactory network learns to read MNIST,"We seek to (i) characterize the learning architectures exploited inbiological neural networks for training on very few samples, and (ii) portthese algorithmic structures to a machine learning context. The Moth OlfactoryNetwork is among the simplest biological neural systems that can learn, and itsarchitecture includes key structural elements and mechanisms widespread inbiological neural nets, such as cascaded networks, competitive inhibition, highintrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. Thesestructural biological elements, in combination, enable rapid learning.  MothNet is a computational model of the Moth Olfactory Network, closelyaligned with the moth's known biophysics and with in vivo electrode datacollected from moths learning new odors. We assign this model the task oflearning to read the MNIST digits. We show that MothNet successfully learns toread given very few training samples (1 to 10 samples per class). In thisfew-samples regime, it outperforms standard machine learning methods such asnearest-neighbors, support-vector machines, and neural networks (NNs), andmatches specialized one-shot transfer-learning methods but without the need forpre-training. The MothNet architecture illustrates how algorithmic structuresderived from biological brains can be used to build alternative NNs that mayavoid some of the learning rate limitations of current engineered NNs.",http://arxiv.org/abs/1802.05405v3,,
1079,Notes on stable learning with piecewise-linear basis functions,"We discuss technical results on learning function approximations usingpiecewise-linear basis functions, and analyze their stability and convergenceusing nonlinear contraction theory.",http://arxiv.org/abs/1804.10085v1,,
1080,"One-Shot Generation of Near-Optimal Topology through Theory-Driven  Machine Learning","We introduce a theory-driven mechanism for learning a neural network modelthat performs generative topology design in one shot given a problem setting,circumventing the conventional iterative process that computational designtasks usually entail. The proposed mechanism can lead to machines that quicklyresponse to new design requirements based on its knowledge accumulated throughpast experiences of design generation. Achieving such a mechanism throughsupervised learning would require an impractically large amount ofproblem-solution pairs for training, due to the known limitation of deep neuralnetworks in knowledge generalization. To this end, we introduce an interactionbetween a student (the neural network) and a teacher (the optimality conditionsunderlying topology optimization): The student learns from existing data and istested on unseen problems. Deviation of the student's solutions from theoptimality conditions is quantified, and used for choosing new data points tolearn from. We call this learning mechanism ""theory-driven"", as it explicitlyuses domain-specific theories to guide the learning, thus distinguishing itselffrom purely data-driven supervised learning. We show through a complianceminimization problem that the proposed learning mechanism leads to topologygeneration with near-optimal structural compliance, much improved from standardsupervised learning under the same computational budget.",http://arxiv.org/abs/1807.10787v3,,
1081,Machine learning-based Raman amplifier design,"A multi-layer neural network is employed to learn the mapping between Ramangain profile and pump powers and wavelengths. The learned model predicts withhigh-accuracy, low-latency and low-complexity the pumping setup for any gainprofile.",http://arxiv.org/abs/1811.10381v1,,
1082,Gaussian Processes for Data-Efficient Learning in Robotics and Control,"Autonomous learning has been a promising direction in control and roboticsfor more than a decade since data-driven learning allows to reduce the amountof engineering knowledge, which is otherwise required. However, autonomousreinforcement learning (RL) approaches typically require many interactions withthe system to learn controllers, which is a practical limitation in realsystems, such as robots, where many interactions can be impractical and timeconsuming. To address this problem, current learning approaches typicallyrequire task-specific knowledge in form of expert demonstrations, realisticsimulators, pre-shaped policies, or specific knowledge about the underlyingdynamics. In this article, we follow a different approach and speed up learningby extracting more information from data. In particular, we learn aprobabilistic, non-parametric Gaussian process transition model of the system.By explicitly incorporating model uncertainty into long-term planning andcontroller learning our approach reduces the effects of model errors, a keyproblem in model-based learning. Compared to state-of-the art RL ourmodel-based policy search method achieves an unprecedented speed of learning.We demonstrate its applicability to autonomous learning in real robot andcontrol tasks.",http://arxiv.org/abs/1502.02860v2,,
1083,"Leveraging Crowdsourcing Data For Deep Active Learning - An Application:  Learning Intents in Alexa","This paper presents a generic Bayesian framework that enables any deeplearning model to actively learn from targeted crowds. Our framework inheritsfrom recent advances in Bayesian deep learning, and extends existing work byconsidering the targeted crowdsourcing approach, where multiple annotators withunknown expertise contribute an uncontrolled amount (often limited) ofannotations. Our framework leverages the low-rank structure in annotations tolearn individual annotator expertise, which then helps to infer the true labelsfrom noisy and sparse annotations. It provides a unified Bayesian model tosimultaneously infer the true labels and train the deep learning model in orderto reach an optimal learning efficacy. Finally, our framework exploits theuncertainty of the deep learning model during prediction as well as theannotators' estimated expertise to minimize the number of required annotationsand annotators for optimally training the deep learning model.  We evaluate the effectiveness of our framework for intent classification inAlexa (Amazon's personal assistant), using both synthetic and real-worlddatasets. Experiments show that our framework can accurately learn annotatorexpertise, infer true labels, and effectively reduce the amount of annotationsin model training as compared to state-of-the-art approaches. We furtherdiscuss the potential of our proposed framework in bridging machine learningand crowdsourcing towards improved human-in-the-loop systems.",http://arxiv.org/abs/1803.04223v1,,
1084,"SupportNet: solving catastrophic forgetting in class incremental  learning with support data","A plain well-trained deep learning model often does not have the ability tolearn new knowledge without forgetting the previously learned knowledge, whichis known as catastrophic forgetting. Here we propose a novel method,SupportNet, to efficiently and effectively solve the catastrophic forgettingproblem in the class incremental learning scenario. SupportNet combines thestrength of deep learning and support vector machine (SVM), where SVM is usedto identify the support data from the old data, which are fed to the deeplearning model together with the new data for further training so that themodel can review the essential information of the old data when learning thenew information. Two powerful consolidation regularizers are applied tostabilize the learned representation and ensure the robustness of the learnedmodel. We validate our method with comprehensive experiments on various tasks,which show that SupportNet drastically outperforms the state-of-the-artincremental learning methods and even reaches similar performance as the deeplearning model trained from scratch on both old and new data. Our program isaccessible at: https://github.com/lykaust15/SupportNet",http://arxiv.org/abs/1806.02942v3,,
1085,Learning Actionable Representations with Goal-Conditioned Policies,"Representation learning is a central challenge across a range of machinelearning areas. In reinforcement learning, effective and functionalrepresentations have the potential to tremendously accelerate learning progressand solve more challenging problems. Most prior work on representation learninghas focused on generative approaches, learning representations that capture allunderlying factors of variation in the observation space in a more disentangledor well-ordered manner. In this paper, we instead aim to learn functionallysalient representations: representations that are not necessarily complete interms of capturing all factors of variation in the observation space, butrather aim to capture those factors of variation that are important fordecision making -- that are ""actionable."" These representations are aware ofthe dynamics of the environment, and capture only the elements of theobservation that are necessary for decision making rather than all factors ofvariation, without explicit reconstruction of the observation. We show howthese representations can be useful to improve exploration for sparse rewardproblems, to enable long horizon hierarchical reinforcement learning, and as astate representation for learning policies for downstream tasks. We evaluateour method on a number of simulated environments, and compare it to priormethods for representation learning, exploration, and hierarchicalreinforcement learning.",http://arxiv.org/abs/1811.07819v2,,
1086,"Ensemble-based kernel learning for a class of data assimilation problems  with imperfect forward simulators","Simulator imperfection, often known as model error, is ubiquitous inpractical data assimilation problems. Despite the enormous efforts dedicated toaddressing this problem, properly handling simulator imperfection in dataassimilation remains to be a challenging task. In this work, we propose anapproach to dealing with simulator imperfection from a point of view offunctional approximation that can be implemented through a certain machinelearning method, such as kernel-based learning adopted in the current work. Tothis end, we start from considering a class of supervised learning problems,and then identify similarities between supervised learning and variational dataassimilation. These similarities found the basis for us to develop anensemble-based learning framework to tackle supervised learning problems, whileachieving various advantages of ensemble-based methods over the variationalones. After establishing the ensemble-based learning framework, we proceed toinvestigate the integration of ensemble-based learning into an ensemble-baseddata assimilation framework to handle simulator imperfection. In the course ofour investigations, we also develop a strategy to tackle the issue ofmulti-modality in supervised-learning problems, and transfer this strategy todata assimilation problems to help improve assimilation performance. Fordemonstration, we apply the ensemble-based learning framework and theintegrated, ensemble-based data assimilation framework to a supervised learningproblem and a data assimilation problem with an imperfect forward simulator,respectively. The experiment results indicate that both frameworks achieve goodperformance in relevant case studies, and that functional approximation throughmachine learning may serve as a viable way to account for simulatorimperfection in data assimilation problems.",http://arxiv.org/abs/1901.10758v1,,
1087,Heterogeneous Multi-task Metric Learning across Multiple Domains,"Distance metric learning (DML) plays a crucial role in diverse machinelearning algorithms and applications. When the labeled information in targetdomain is limited, transfer metric learning (TML) helps to learn the metric byleveraging the sufficient information from other related domains. Multi-taskmetric learning (MTML), which can be regarded as a special case of TML,performs transfer across all related domains. Current TML tools usually assumethat the same feature representation is exploited for different domains.However, in real-world applications, data may be drawn from heterogeneousdomains. Heterogeneous transfer learning approaches can be adopted to remedythis drawback by deriving a metric from the learned transformation acrossdifferent domains. But they are often limited in that only two domains can behandled. To appropriately handle multiple domains, we develop a novelheterogeneous multi-task metric learning (HMTML) framework. In HMTML, themetrics of all different domains are learned together. The transformationsderived from the metrics are utilized to induce a common subspace, and thehigh-order covariance among the predictive structures of these domains ismaximized in this subspace. There do exist a few heterogeneous transferlearning approaches that deal with multiple domains, but the high-orderstatistics (correlation information), which can only be exploited bysimultaneously examining all domains, is ignored in these approaches. Comparedwith them, the proposed HMTML can effectively explore such high-orderinformation, thus obtaining more reliable feature transformations and metrics.Effectiveness of our method is validated by the extensive and intensiveexperiments on text categorization, scene classification, and social imageannotation.",http://arxiv.org/abs/1904.04081v1,,
1088,Hedging predictions in machine learning,"Recent advances in machine learning make it possible to design efficientprediction algorithms for data sets with huge numbers of parameters. This paperdescribes a new technique for ""hedging"" the predictions output by many suchalgorithms, including support vector machines, kernel ridge regression, kernelnearest neighbours, and by many other state-of-the-art methods. The hedgedpredictions for the labels of new objects include quantitative measures oftheir own accuracy and reliability. These measures are provably valid under theassumption of randomness, traditional in machine learning: the objects andtheir labels are assumed to be generated independently from the sameprobability distribution. In particular, it becomes possible to control (up tostatistical fluctuations) the number of erroneous predictions by selecting asuitable confidence level. Validity being achieved automatically, the remaininggoal of hedged prediction is efficiency: taking full account of the newobjects' features and other available information to produce as accuratepredictions as possible. This can be done successfully using the powerfulmachinery of modern machine learning.",http://arxiv.org/abs/cs/0611011v1,,
1089,Tensor machines for learning target-specific polynomial features,"Recent years have demonstrated that using random feature maps cansignificantly decrease the training and testing times of kernel-basedalgorithms without significantly lowering their accuracy. Regrettably, becauserandom features are target-agnostic, typically thousands of such features arenecessary to achieve acceptable accuracies. In this work, we consider theproblem of learning a small number of explicit polynomial features. Ourapproach, named Tensor Machines, finds a parsimonious set of features byoptimizing over the hypothesis class introduced by Kar and Karnick for randomfeature maps in a target-specific manner. Exploiting a natural connectionbetween polynomials and tensors, we provide bounds on the generalization errorof Tensor Machines. Empirically, Tensor Machines behave favorably on severalreal-world datasets compared to other state-of-the-art techniques for learningpolynomial features, and deliver significantly more parsimonious models.",http://arxiv.org/abs/1504.01697v1,,
1090,M3: Scaling Up Machine Learning via Memory Mapping,"To process data that do not fit in RAM, conventional wisdom would suggestusing distributed approaches. However, recent research has demonstrated virtualmemory's strong potential in scaling up graph mining algorithms on a singlemachine. We propose to use a similar approach for general machine learning. Wecontribute: (1) our latest finding that memory mapping is also a feasibletechnique for scaling up general machine learning algorithms like logisticregression and k-means, when data fits in or exceeds RAM (we tested datasets upto 190GB); (2) an approach, called M3, that enables existing machine learningalgorithms to work with out-of-core datasets through memory mapping, achievinga speed that is significantly faster than a 4-instance Spark cluster, andcomparable to an 8-instance cluster.",http://arxiv.org/abs/1604.03034v1,,
1091,"Fast Bayesian Optimization of Machine Learning Hyperparameters on Large  Datasets","Bayesian optimization has become a successful tool for hyperparameteroptimization of machine learning algorithms, such as support vector machines ordeep neural networks. Despite its success, for large datasets, training andvalidating a single configuration often takes hours, days, or even weeks, whichlimits the achievable performance. To accelerate hyperparameter optimization,we propose a generative model for the validation error as a function oftraining set size, which is learned during the optimization process and allowsexploration of preliminary configurations on small subsets, by extrapolating tothe full dataset. We construct a Bayesian optimization procedure, dubbedFabolas, which models loss and training time as a function of dataset size andautomatically trades off high information gain about the global optimum againstcomputational cost. Experiments optimizing support vector machines and deepneural networks show that Fabolas often finds high-quality solutions 10 to 100times faster than other state-of-the-art Bayesian optimization methods or therecently proposed bandit strategy Hyperband.",http://arxiv.org/abs/1605.07079v2,,
1092,"Performance Prediction and Optimization of Solar Water Heater via a  Knowledge-Based Machine Learning Method","Measuring the performance of solar energy and heat transfer systems requiresa lot of time, economic cost and manpower. Meanwhile, directly predicting theirperformance is challenging due to the complicated internal structures.Fortunately, a knowledge-based machine learning method can provide a promisingprediction and optimization strategy for the performance of energy systems. Inthis Chapter, the authors will show how they utilize the machine learningmodels trained from a large experimental database to perform precise predictionand optimization on a solar water heater (SWH) system. A new energy systemoptimization strategy based on a high-throughput screening (HTS) process isproposed. This Chapter consists of: i) Comparative studies on varieties ofmachine learning models (artificial neural networks (ANNs), support vectormachine (SVM) and extreme learning machine (ELM)) to predict the performancesof SWHs; ii) Development of an ANN-based software to assist the quickprediction and iii) Introduction of a computational HTS method to design ahigh-performance SWH system.",http://arxiv.org/abs/1710.02511v1,,
1093,"On Breast Cancer Detection: An Application of Machine Learning  Algorithms on the Wisconsin Diagnostic Dataset","This paper presents a comparison of six machine learning (ML) algorithms:GRU-SVM (Agarap, 2017), Linear Regression, Multilayer Perceptron (MLP), NearestNeighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) onthe Wisconsin Diagnostic Breast Cancer (WDBC) dataset (Wolberg, Street, &Mangasarian, 1992) by measuring their classification test accuracy and theirsensitivity and specificity values. The said dataset consists of features whichwere computed from digitized images of FNA tests on a breast mass (Wolberg,Street, & Mangasarian, 1992). For the implementation of the ML algorithms, thedataset was partitioned in the following fashion: 70% for training phase, and30% for the testing phase. The hyper-parameters used for all the classifierswere manually assigned. Results show that all the presented ML algorithmsperformed well (all exceeded 90% test accuracy) on the classification task. TheMLP algorithm stands out among the implemented algorithms with a test accuracyof ~99.04%.",http://arxiv.org/abs/1711.07831v4,,
1094,"A Human-Grounded Evaluation Benchmark for Local Explanations of Machine  Learning","In order for people to be able to trust and take advantage of the results ofadvanced machine learning and artificial intelligence solutions for realdecision making, people need to be able to understand the machine rationale forgiven output. Research in explain artificial intelligence (XAI) addresses theaim, but there is a need for evaluation of human relevance andunderstandability of explanations. Our work contributes a novel methodology forevaluating the quality or human interpretability of explanations for machinelearning models. We present an evaluation benchmark for instance explanationsfrom text and image classifiers. The explanation meta-data in this benchmark isgenerated from user annotations of image and text samples. We describe thebenchmark and demonstrate its utility by a quantitative evaluation onexplanations generated from a recent machine learning algorithm. This researchdemonstrates how human-grounded evaluation could be used as a measure toqualify local machine-learning explanations.",http://arxiv.org/abs/1801.05075v1,,
1095,"2P-DNN : Privacy-Preserving Deep Neural Networks Based on Homomorphic  Cryptosystem","Machine Learning as a Service (MLaaS), such as Microsoft Azure, Amazon AWS,offers an effective DNN model to complete the machine learning task for smallbusinesses and individuals who are restricted to the lacking data and computingpower. However, here comes an issue that user privacy is ex-posed to the MLaaSserver, since users need to upload their sensitive data to the MLaaS server. Inorder to preserve their privacy, users can encrypt their data before uploadingit. This makes it difficult to run the DNN model because it is not designed forrunning in ciphertext domain. In this paper, using the Paillier homomorphiccryptosystem we present a new Privacy-Preserving Deep Neural Network model thatwe called 2P-DNN. This model can fulfill the machine leaning task in ciphertextdomain. By using 2P-DNN, MLaaS is able to provide a Privacy-Preserving machinelearning ser-vice for users. We build our 2P-DNN model based on LeNet-5, andtest it with the encrypted MNIST dataset. The classification accuracy is morethan 97%, which is close to the accuracy of LeNet-5 running with the MNISTdataset and higher than that of other existing Privacy-Preserving machinelearning models",http://arxiv.org/abs/1807.08459v1,,
1096,Speaker Fluency Level Classification Using Machine Learning Techniques,"Level assessment for foreign language students is necessary for putting themin the right level group, furthermore, interviewing students is a verytime-consuming task, so we propose to automate the evaluation of speakerfluency level by implementing machine learning techniques. This work presentsan audio processing system capable of classifying the level of fluency ofnon-native English speakers using five different machine learning models. As afirst step, we have built our own dataset, which consists of labeled audioconversations in English between people ranging in different fluencydomains/classes (low, intermediate, high). We segment the audio conversationsinto 5s non-overlapped audio clips to perform feature extraction on them. Westart by extracting Mel cepstral coefficients from the audios, selecting 20coefficients is an appropriate quantity for our data. We thereafter extractedzero-crossing rate, root mean square energy and spectral flux features, provingthat this improves model performance. Out of a total of 1424 audio segments,with 70% training data and 30% test data, one of our trained models (supportvector machine) achieved a classification accuracy of 94.39%, whereas the otherfour models passed an 89% classification accuracy threshold.",http://arxiv.org/abs/1808.10556v1,,
1097,Tree Tensor Networks for Generative Modeling,"Matrix product states (MPS), a tensor network designed for one-dimensionalquantum systems, has been recently proposed for generative modeling of naturaldata (such as images) in terms of `Born machine'. However, the exponentialdecay of correlation in MPS restricts its representation power heavily formodeling complex data such as natural images. In this work, we push forward theeffort of applying tensor networks to machine learning by employing the TreeTensor Network (TTN) which exhibits balanced performance in expressibility andefficient training and sampling. We design the tree tensor network to utilizethe 2-dimensional prior of the natural images and develop sweeping learning andsampling algorithms which can be efficiently implemented utilizing GraphicalProcessing Units (GPU). We apply our model to random binary patterns and thebinary MNIST datasets of handwritten digits. We show that TTN is superior toMPS for generative modeling in keeping correlation of pixels in natural images,as well as giving better log-likelihood scores in standard datasets ofhandwritten digits. We also compare its performance with state-of-the-artgenerative models such as the Variational AutoEncoders, Restricted Boltzmannmachines, and PixelCNN. Finally, we discuss the future development of TensorNetwork States in machine learning problems.",http://arxiv.org/abs/1901.02217v1,,
1098,Learning and generalization theories of large committee--machines,"The study of the distribution of volumes associated to the internalrepresentations of learning examples allows us to derive the critical learningcapacity ($\alpha_c=\frac{16}{\pi} \sqrt{\ln K}$) of large committee machines,to verify the stability of the solution in the limit of a large number $K$ ofhidden units and to find a Bayesian generalization cross--over at $\alpha=K$.",http://arxiv.org/abs/cond-mat/9601122v1,,
1099,On the Job Training,"We propose a new framework for building and evaluating machine learningalgorithms. We argue that many real-world problems require an agent which mustquickly learn to respond to demands, yet can continue to perform and respond tonew training throughout its useful life. We give a framework for how suchagents can be built, describe several metrics for evaluating them, and showthat subtle changes in system construction can significantly affect agentperformance.",http://arxiv.org/abs/cs/0506085v1,,
1100,Visualization of Manifold-Valued Elements by Multidimensional Scaling,"The present contribution suggests the use of a multidimensional scaling (MDS)algorithm as a visualization tool for manifold-valued elements. A visualizationtool of this kind is useful in signal processing and machine learning wheneverlearning/adaptation algorithms insist on high-dimensional parameter manifolds.",http://arxiv.org/abs/1004.0314v2,,
1101,Variational Optimization,"We discuss a general technique that can be used to form a differentiablebound on the optima of non-differentiable or discrete objective functions. Weform a unified description of these methods and consider under whichcircumstances the bound is concave. In particular we consider two concreteapplications of the method, namely sparse learning and support vectorclassification.",http://arxiv.org/abs/1212.4507v2,,
1102,"Tight Lower Bound on the Probability of a Binomial Exceeding its  Expectation","We give the proof of a tight lower bound on the probability that a binomialrandom variable exceeds its expected value. The inequality plays an importantrole in a variety of contexts, including the analysis of relative deviationbounds in learning theory and generalization bounds for unbounded lossfunctions.",http://arxiv.org/abs/1306.1433v3,,
1103,On Ranking Senators By Their Votes,"The problem of ranking a set of objects given some measure of similarity isone of the most basic in machine learning. Recently Agarwal proposed a methodbased on techniques in semi-supervised learning utilizing the graph Laplacian.In this work we consider a novel application of this technique to rankingbinary choice data and apply it specifically to ranking US Senators by theirideology.",http://arxiv.org/abs/0909.1418v1,,
1104,Scalable Inference for Latent Dirichlet Allocation,"We investigate the problem of learning a topic model - the well-known LatentDirichlet Allocation - in a distributed manner, using a cluster of C processorsand dividing the corpus to be learned equally among them. We propose a simpleapproximated method that can be tuned, trading speed for accuracy according tothe task at hand. Our approach is asynchronous, and therefore suitable forclusters of heterogenous machines.",http://arxiv.org/abs/0909.4603v1,,
1105,Experiments with Random Projection,"Recent theoretical work has identified random projection as a promisingdimensionality reduction technique for learning mixtures of Gausians. Here wesummarize these results and illustrate them by a wide variety of experiments onsynthetic and real data.",http://arxiv.org/abs/1301.3849v1,,
1106,Generalized Risk-Aversion in Stochastic Multi-Armed Bandits,"We consider the problem of minimizing the regret in stochastic multi-armedbandit, when the measure of goodness of an arm is not the mean return, but somegeneral function of the mean and the variance.We characterize the conditionsunder which learning is possible and present examples for which no naturalalgorithm can achieve sublinear regret.",http://arxiv.org/abs/1405.0833v1,,
1107,Improved graph Laplacian via geometric self-consistency,"We address the problem of setting the kernel bandwidth used by ManifoldLearning algorithms to construct the graph Laplacian. Exploiting the connectionbetween manifold geometry, represented by the Riemannian metric, and theLaplace-Beltrami operator, we set the bandwidth by optimizing the Laplacian'sability to preserve the geometry of the data. Experiments show that thisprincipled approach is effective and robust.",http://arxiv.org/abs/1406.0118v1,,
1108,Differentially Private Policy Evaluation,"We present the first differentially private algorithms for reinforcementlearning, which apply to the task of evaluating a fixed policy. We establishtwo approaches for achieving differential privacy, provide a theoreticalanalysis of the privacy and utility of the two algorithms, and show promisingresults on simple empirical examples.",http://arxiv.org/abs/1603.02010v1,,
1109,Multi-GPU Training of ConvNets,"In this work we evaluate different approaches to parallelize computation ofconvolutional neural networks across several GPUs.",http://arxiv.org/abs/1312.5853v4,,
1110,A Framework for Distributed Deep Learning Layer Design in Python,"In this paper, a framework for testing Deep Neural Network (DNN) design inPython is presented. First, big data, machine learning (ML), and ArtificialNeural Networks (ANNs) are discussed to familiarize the reader with theimportance of such a system. Next, the benefits and detriments of implementingsuch a system in Python are presented. Lastly, the specifics of the system areexplained, and some experimental results are presented to prove theeffectiveness of the system.",http://arxiv.org/abs/1510.07303v1,,
1111,"A Batch, Off-Policy, Actor-Critic Algorithm for Optimizing the Average  Reward","We develop an off-policy actor-critic algorithm for learning an optimalpolicy from a training set composed of data from multiple individuals. Thisalgorithm is developed with a view towards its use in mobile health.",http://arxiv.org/abs/1607.05047v1,,
1112,Stochastic Canonical Correlation Analysis,"We tightly analyze the sample complexity of CCA, provide a learning algorithmthat achieves optimal statistical performance in time linear in the requirednumber of samples (up to log factors), as well as a streaming algorithm withsimilar guarantees.",http://arxiv.org/abs/1702.06533v1,,
1113,Deep Learning Methods for Efficient Large Scale Video Labeling,"We present a solution to ""Google Cloud and YouTube-8M Video UnderstandingChallenge"" that ranked 5th place. The proposed model is an ensemble of threemodel families, two frame level and one video level. The training was performedon augmented dataset, with cross validation.",http://arxiv.org/abs/1706.04572v1,,
1114,Linking Generative Adversarial Learning and Binary Classification,"In this note, we point out a basic link between generative adversarial (GA)training and binary classification -- any powerful discriminator essentiallycomputes an (f-)divergence between real and generated samples. The result,repeatedly re-derived in decision theory, has implications for GA Networks(GANs), providing an alternative perspective on training f-GANs by designingthe discriminator loss function.",http://arxiv.org/abs/1709.01509v1,,
1115,"On the exact relationship between the denoising function and the data  distribution","We prove an exact relationship between the optimal denoising function and thedata distribution in the case of additive Gaussian noise, showing thatdenoising implicitly models the structure of data allowing it to be exploitedin the unsupervised learning of representations. This result generalizes aknown relationship [2], which is valid only in the limit of small corruptionnoise.",http://arxiv.org/abs/1709.02797v1,,
1116,Learning to Detect Entanglement,"This paper introduces the forest algorithm, an algorithm that can detectentanglement through the use of decision trees generated by machine learning.Tests against similar tomography-based detection algorithms using experimentaldata and numerical simulations indicate that, once trained, the proposedalgorithm outperforms previous approaches. The results identify entanglementdetection as another area of quantum information where machine learning canplay a helpful role.",http://arxiv.org/abs/1709.03617v1,,
1117,"EnergyNet: Energy-based Adaptive Structural Learning of Artificial  Neural Network Architectures","We present E NERGY N ET , a new framework for analyzing and buildingartificial neural network architectures. Our approach adaptively learns thestructure of the networks in an unsupervised manner. The methodology is basedupon the theoretical guarantees of the energy function of restricted Boltzmannmachines (RBM) of infinite number of nodes. We present experimental results toshow that the final network adapts to the complexity of a given problem.",http://arxiv.org/abs/1711.03130v1,,
1118,Predicting readmission risk from doctors' notes,"We develop a model using deep learning techniques and natural languageprocessing on unstructured text from medical records to predict hospital-wide$30$-day unplanned readmission, with c-statistic $.70$. Our model isconstructed to allow physicians to interpret the significant features forprediction.",http://arxiv.org/abs/1711.10663v2,,
1119,On the Latent Space of Wasserstein Auto-Encoders,"We study the role of latent space dimensionality in Wasserstein auto-encoders(WAEs). Through experimentation on synthetic and real datasets, we argue thatrandom encoders should be preferred over deterministic encoders. We highlightthe potential of WAEs for representation learning with promising results on abenchmark disentanglement task.",http://arxiv.org/abs/1802.03761v1,,
1120,Variational Inference for Policy Gradient,"Inspired by the seminal work on Stein Variational Inference and SteinVariational Policy Gradient, we derived a method to generate samples from theposterior variational parameter distribution by \textit{explicitly} minimizingthe KL divergence to match the target distribution in an amortize fashion.Consequently, we applied this varational inference technique into vanillapolicy gradient, TRPO and PPO with Bayesian Neural Network parameterizationsfor reinforcement learning problems.",http://arxiv.org/abs/1802.07833v2,,
1121,Learning to Recognize Musical Genre from Audio,"We here summarize our experience running a challenge with open data formusical genre recognition. Those notes motivate the task and the challengedesign, show some statistics about the submissions, and present the results.",http://arxiv.org/abs/1803.05337v1,,
1122,Information Theoretic Interpretation of Deep learning,"We interpret part of the experimental results of Shwartz-Ziv and Tishby[2017]. Inspired by these results, we established a conjecture of the dynamicsof the machinary of deep neural network. This conjecture can be used to explainthe counterpart result by Saxe et al. [2018].",http://arxiv.org/abs/1803.07980v2,,
1123,"Learning Latent Events from Network Message Logs: A Decomposition Based  Approach","In this communication, we describe a novel technique for event mining using adecomposition based approach that combines non-parametric change-pointdetection with LDA. We prove theoretical guarantees about sample-complexity andconsistency of the approach. In a companion paper, we will perform a thoroughevaluation of our approach with detailed experiments.",http://arxiv.org/abs/1804.03346v1,,
1124,Variational Composite Autoencoders,"Learning in the latent variable model is challenging in the presence of thecomplex data structure or the intractable latent variable. Previous variationalautoencoders can be low effective due to the straightforward encoder-decoderstructure. In this paper, we propose a variational composite autoencoder tosidestep this issue by amortizing on top of the hierarchical latent variablemodel. The experimental results confirm the advantages of our model.",http://arxiv.org/abs/1804.04435v1,,
1125,Online Bandit Linear Optimization: A Study,"This article introduces the concepts around Online Bandit Linear Optimizationand explores an efficient setup called SCRiBLe (Self-Concordant Regularizationin Bandit Learning) created by Abernethy et. al.\cite{abernethy}. The SCRiBLesetup and algorithm yield a $O(\sqrt{T})$ regret bound and polynomial run timecomplexity bound on the dimension of the input space. In this article we buildup to the bandit linear optimization case and study SCRiBLe.",http://arxiv.org/abs/1805.05773v1,,
1126,Machine learning for bounce calculation,"We study the possibility of using machine learning for the calculation of thebounce action in quantum tunneling. Adopting supervised learning, we trainneural network to give the bounce action from a given potential. It is foundthat, for one-dimensional tunneling, even a simple neural network performs at apercent level.",http://arxiv.org/abs/1805.12153v3,,
1127,"Defense Against the Dark Arts: An overview of adversarial example  security research and future research directions","This article presents a summary of a keynote lecture at the Deep LearningSecurity workshop at IEEE Security and Privacy 2018. This lecture summarizesthe state of the art in defenses against adversarial examples and providesrecommendations for future research directions on this topic.",http://arxiv.org/abs/1806.04169v1,,
1128,Equalizing Financial Impact in Supervised Learning,"Notions of ""fair classification"" that have arisen in computer sciencegenerally revolve around equalizing certain statistics across protected groups.This approach has been criticized as ignoring societal issues, including howerrors can hurt certain groups disproportionately. We pose a modification ofone of the fairness criteria from Hardt, Price, and Srebro [NIPS, 2016] thatmakes a small step towards addressing this issue in the case of financialdecisions like giving loans. We call this new notion ""equalized financialimpact.""",http://arxiv.org/abs/1806.09211v1,,
1129,Using NLP on news headlines to predict index trends,"This paper attempts to provide a state of the art in trend prediction usingnews headlines. We present the research done on predicting DJIA trends usingNatural Language Processing. We will explain the different algorithms we haveused as well as the various embedding techniques attempted. We rely onstatistical and deep learning models in order to extract information from thecorpuses.",http://arxiv.org/abs/1806.09533v1,,
1130,Uncertainty in the Variational Information Bottleneck,"We present a simple case study, demonstrating that Variational InformationBottleneck (VIB) can improve a network's classification calibration as well asits ability to detect out-of-distribution data. Without explicitly beingdesigned to do so, VIB gives two natural metrics for handling and quantifyinguncertainty.",http://arxiv.org/abs/1807.00906v1,,
1131,On the Computational Power of Online Gradient Descent,"We prove that the evolution of weight vectors in online gradient descent canencode arbitrary polynomial-space computations, even in very simple learningsettings. Our results imply that, under weak complexity-theoretic assumptions,it is impossible to reason efficiently about the fine-grained behavior ofonline gradient descent.",http://arxiv.org/abs/1807.01280v2,,
1132,Learning Graph Representations by Dendrograms,"Hierarchical graph clustering is a common technique to reveal the multi-scalestructure of complex networks. We propose a novel metric for assessing thequality of a hierarchical clustering. This metric reflects the ability toreconstruct the graph from the dendrogram, which encodes the hierarchy. Theoptimal representation of the graph defines a class of reducible linkagesleading to regular dendrograms by greedy agglomerative clustering.",http://arxiv.org/abs/1807.05087v1,,
1133,Multiclass Universum SVM,"We introduce Universum learning for multiclass problems and propose a novelformulation for multiclass universum SVM (MU-SVM). We also propose an analyticspan bound for model selection with almost 2-4x faster computation times thanstandard resampling techniques. We empirically demonstrate the efficacy of theproposed MUSVM formulation on several real world datasets achieving > 20%improvement in test accuracies compared to multi-class SVM.",http://arxiv.org/abs/1808.08111v1,,
1134,Regret vs. Bandwidth Trade-off for Recommendation Systems,"We consider recommendation systems that need to operate under wirelessbandwidth constraints, measured as number of broadcast transmissions, anddemonstrate a (tight for some instances) tradeoff between regret and bandwidthfor two scenarios: the case of multi-armed bandit with context, and the casewhere there is a latent structure in the message space that we can exploit toreduce the learning phase.",http://arxiv.org/abs/1810.06313v1,,
1135,Black-Box Autoregressive Density Estimation for State-Space Models,"State-space models (SSMs) provide a flexible framework for modellingtime-series data. Consequently, SSMs are ubiquitously applied in areas such asengineering, econometrics and epidemiology. In this paper we provide a fastapproach for approximate Bayesian inference in SSMs using the tools of deeplearning and variational inference.",http://arxiv.org/abs/1811.08337v2,,
1136,$_$-VAEs can retain label information even at high compression,"In this paper, we investigate the degree to which the encoding of a$\beta$-VAE captures label information across multiple architectures on BinaryStatic MNIST and Omniglot. Even though they are trained in a completelyunsupervised manner, we demonstrate that a $\beta$-VAE can retain a largeamount of label information, even when asked to learn a highly compressedrepresentation.",http://arxiv.org/abs/1812.02682v1,,
1137,"On the Ineffectiveness of Variance Reduced Optimization for Deep  Learning","The application of stochastic variance reduction to optimization has shownremarkable recent theoretical and practical success. The applicability of thesetechniques to the hard non-convex optimization problems encountered duringtraining of modern deep neural networks is an open problem. We show that naiveapplication of the SVRG technique and related approaches fail, and explore why.",http://arxiv.org/abs/1812.04529v1,,
1138,Recurrent Neural Networks for Time Series Forecasting,"Time series forecasting is difficult. It is difficult even for recurrentneural networks with their inherent ability to learn sequentiality. Thisarticle presents a recurrent neural network based time series forecastingframework covering feature engineering, feature importances, point and intervalpredictions, and forecast evaluation. The description of the method is followedby an empirical study using both LSTM and GRU networks.",http://arxiv.org/abs/1901.00069v1,,
1139,On Connected Sublevel Sets in Deep Learning,"We study sublevel sets of the loss function in training deep neural networks.For linearly independent data, we prove that every sublevel set of the loss isconnected and unbounded. We then apply this result to prove similar propertieson the loss surface of deep over-parameterized neural nets with piecewiselinear activation functions.",http://arxiv.org/abs/1901.07417v1,,
1140,Augmented Neural ODEs,"We show that Neural Ordinary Differential Equations (ODEs) learnrepresentations that preserve the topology of the input space and prove thatthis implies the existence of functions Neural ODEs cannot represent. Toaddress these limitations, we introduce Augmented Neural ODEs which, inaddition to being more expressive models, are empirically more stable,generalize better and have a lower computational cost than Neural ODEs.",http://arxiv.org/abs/1904.01681v1,,
1141,"Domain Adaptation Extreme Learning Machines for Drift Compensation in  E-nose Systems","This paper addresses an important issue, known as sensor drift that behaves anonlinear dynamic property in electronic nose (E-nose), from the viewpoint ofmachine learning. Traditional methods for drift compensation are laborious andcostly due to the frequent acquisition and labeling process for gases samplesrecalibration. Extreme learning machines (ELMs) have been confirmed to beefficient and effective learning techniques for pattern recognition andregression. However, ELMs primarily focus on the supervised, semi-supervisedand unsupervised learning problems in single domain (i.e. source domain). Toour best knowledge, ELM with cross-domain learning capability has never beenstudied. This paper proposes a unified framework, referred to as DomainAdaptation Extreme Learning Machine (DAELM), which learns a robust classifierby leveraging a limited number of labeled data from target domain for driftcompensation as well as gases recognition in E-nose systems, without loss ofthe computational efficiency and learning ability of traditional ELM. In theunified framework, two algorithms called DAELM-S and DAELM-T are proposed forthe purpose of this paper, respectively. In order to percept the differencesamong ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on thepopular sensor drift data with multiple batches collected by E-nose systemclearly demonstrate that the proposed DAELM significantly outperforms existingdrift compensation methods without cumbersome measures, and also bring newperspectives for ELM.",http://arxiv.org/abs/1505.06405v1,,
1142,Deep Reinforcement Learning framework for Autonomous Driving,"Reinforcement learning is considered to be a strong AI paradigm which can beused to teach machines through interaction with the environment and learningfrom their mistakes. Despite its perceived utility, it has not yet beensuccessfully applied in automotive applications. Motivated by the successfuldemonstrations of learning of Atari games and Go by Google DeepMind, we proposea framework for autonomous driving using deep reinforcement learning. This isof particular relevance as it is difficult to pose autonomous driving as asupervised learning problem due to strong interactions with the environmentincluding other vehicles, pedestrians and roadworks. As it is a relatively newarea of research for autonomous driving, we provide a short overview of deepreinforcement learning and then describe our proposed framework. Itincorporates Recurrent Neural Networks for information integration, enablingthe car to handle partially observable scenarios. It also integrates the recentwork on attention models to focus on relevant information, thereby reducing thecomputational complexity for deployment on embedded hardware. The framework wastested in an open source 3D car racing simulator called TORCS. Our simulationresults demonstrate learning of autonomous maneuvering in a scenario of complexroad curvatures and simple interaction of other vehicles.",http://arxiv.org/abs/1704.02532v1,,
1143,The Learnability of Unknown Quantum Measurements,"Quantum machine learning has received significant attention in recent years,and promising progress has been made in the development of quantum algorithmsto speed up traditional machine learning tasks. In this work, however, we focuson investigating the information-theoretic upper bounds of sample complexity -how many training samples are sufficient to predict the future behaviour of anunknown target function. This kind of problem is, arguably, one of the mostfundamental problems in statistical learning theory and the bounds forpractical settings can be completely characterised by a simple measure ofcomplexity.  Our main result in the paper is that, for learning an unknown quantummeasurement, the upper bound, given by the fat-shattering dimension, islinearly proportional to the dimension of the underlying Hilbert space.Learning an unknown quantum state becomes a dual problem to ours, and as abyproduct, we can recover Aaronson's famous result [Proc. R. Soc. A463:3089-3144 (2007)] solely using a classical machine learning technique. Inaddition, other famous complexity measures like covering numbers and Rademachercomplexities are derived explicitly. We are able to connect measures of samplecomplexity with various areas in quantum information science, e.g. quantumstate/measurement tomography, quantum state discrimination and quantum randomaccess codes, which may be of independent interest. Lastly, with the assistanceof general Bloch-sphere representation, we show that learning quantummeasurements/states can be mathematically formulated as a neural network.Consequently, classical ML algorithms can be applied to efficiently accomplishthe two quantum learning tasks.",http://arxiv.org/abs/1501.00559v1,,
1144,The Wilson Machine for Image Modeling,"Learning the distribution of natural images is one of the hardest and mostimportant problems in machine learning. The problem remains open, because theenormous complexity of the structures in natural images spans all lengthscales. We break down the complexity of the problem and show that the hierarchyof structures in natural images fuels a new class of learning algorithms basedon the theory of critical phenomena and stochastic processes. We approach thisproblem from the perspective of the theory of critical phenomena, which wasdeveloped in condensed matter physics to address problems with infinitelength-scale fluctuations, and build a framework to integrate the criticalityof natural images into a learning algorithm. The problem is broken down bymapping images into a hierarchy of binary images, called bitplanes. In thisrepresentation, the top bitplane is critical, having fluctuations in structuresover a vast range of scales. The bitplanes below go through a gradualstochastic heating process to disorder. We turn this representation into adirected probabilistic graphical model, transforming the learning problem intothe unsupervised learning of the distribution of the critical bitplane and thesupervised learning of the conditional distributions for the remainingbitplanes. We learnt the conditional distributions by logistic regression in aconvolutional architecture. Conditioned on the critical binary image, thissimple architecture can generate large, natural-looking images, with manyshades of gray, without the use of hidden units, unprecedented in the studiesof natural images. The framework presented here is a major step in bringingcriticality and stochastic processes to machine learning and in studyingnatural image statistics.",http://arxiv.org/abs/1510.07740v2,,
1145,Human-aided Multi-Entity Bayesian Networks Learning from Relational Data,"An Artificial Intelligence (AI) system is an autonomous system which emulateshuman mental and physical activities such as Observe, Orient, Decide, and Act,called the OODA process. An AI system performing the OODA process requires asemantically rich representation to handle a complex real world situation andability to reason under uncertainty about the situation. Multi-Entity BayesianNetworks (MEBNs) combines First-Order Logic with Bayesian Networks forrepresenting and reasoning about uncertainty in complex, knowledge-richdomains. MEBN goes beyond standard Bayesian networks to enable reasoning aboutan unknown number of entities interacting with each other in various types ofrelationships, a key requirement for the OODA process of an AI system. MEBNmodels have heretofore been constructed manually by a domain expert. However,manual MEBN modeling is labor-intensive and insufficiently agile. To addressthese problems, an efficient method is needed for MEBN modeling. One of themethods is to use machine learning to learn a MEBN model in whole or in partfrom data. In the era of Big Data, data-rich environments, characterized byuncertainty and complexity, have become ubiquitous. The larger the data sampleis, the more accurate the results of the machine learning approach can be.Therefore, machine learning has potential to improve the quality of MEBN modelsas well as the effectiveness for MEBN modeling. In this research, we study aMEBN learning framework to develop a MEBN model from a combination of domainexpert's knowledge and data. To evaluate the MEBN learning framework, weconduct an experiment to compare the MEBN learning framework and the existingmanual MEBN modeling in terms of development efficiency.",http://arxiv.org/abs/1806.02421v1,,
1146,DP-ADMM: ADMM-based Distributed Learning with Differential Privacy,"Alternating Direction Method of Multipliers (ADMM) is a widely used tool formachine learning in distributed settings, where a machine learning model istrained over distributed data sources through an interactive process of localcomputation and message passing. Such an iterative process could cause privacyconcerns of data owners. The goal of this paper is to provide differentialprivacy for ADMM-based distributed machine learning. Prior approaches ondifferentially private ADMM exhibit low utility under high privacy guaranteeand often assume the objective functions of the learning problems to be smoothand strongly convex. To address these concerns, we propose a noveldifferentially private ADMM-based distributed learning algorithm calledDP-ADMM, which combines an approximate augmented Lagrangian function withtime-varying Gaussian noise addition in the iterative process to achieve higherutility for general objective functions under the same differential privacyguarantee. We also apply the moments accountant method to bound the end-to-endprivacy loss. The theoretical analysis shows that DP-ADMM can be applied to awider class of distributed learning problems, is provably convergent, andoffers an explicit utility-privacy tradeoff. To our knowledge, this is thefirst paper to provide explicit convergence and utility properties fordifferentially private ADMM-based distributed learning algorithms. Theevaluation results demonstrate that our approach can achieve good convergenceand model accuracy under high end-to-end differential privacy guarantee.",http://arxiv.org/abs/1808.10101v5,,
1147,"MT-CGCNN: Integrating Crystal Graph Convolutional Neural Network with  Multitask Learning for Material Property Prediction","Developing accurate, transferable and computationally inexpensive machinelearning models can rapidly accelerate the discovery and development of newmaterials. Some of the major challenges involved in developing such models are,(i) limited availability of materials data as compared to other fields, (ii)lack of universal descriptor of materials to predict its various properties.The limited availability of materials data can be addressed through transferlearning, while the generic representation was recently addressed by Xie andGrossman [1], where they developed a crystal graph convolutional neural network(CGCNN) that provides a unified representation of crystals. In this work, wedevelop a new model (MT-CGCNN) by integrating CGCNN with transfer learningbased on multi-task (MT) learning. We demonstrate the effectiveness of MT-CGCNNby simultaneous prediction of various material properties such as FormationEnergy, Band Gap and Fermi Energy for a wide range of inorganic crystals (46774materials). MT-CGCNN is able to reduce the test error when employed oncorrelated properties by upto 8%. The model prediction has lower test errorcompared to CGCNN, even when the training data is reduced by 10%. We alsodemonstrate our model's better performance through prediction of end userscenario related to metal/non-metal classification. These results encouragefurther development of machine learning approaches which leverage multi-tasklearning to address the aforementioned challenges in the discovery of newmaterials. We make MT-CGCNN's source code available to encourage reproducibleresearch.",http://arxiv.org/abs/1811.05660v1,,
1148,"Learning User Preferences via Reinforcement Learning with Spatial  Interface Valuing","Interactive Machine Learning is concerned with creating systems that operatein environments alongside humans to achieve a task. A typical use is to extendor amplify the capabilities of a human in cognitive or physical ways, requiringthe machine to adapt to the users' intentions and preferences. Often, thistakes the form of a human operator providing some type of feedback to the user,which can be explicit feedback, implicit feedback, or a combination of both.Explicit feedback, such as through a mouse click, carries a high cognitiveload. The focus of this study is to extend the current state of the art ininteractive machine learning by demonstrating that agents can learn a humanuser's behavior and adapt to preferences with a reduced amount of explicithuman feedback in a mixed feedback setting. The learning agent perceives avalue of its own behavior from hand gestures given via a spatial interface.This feedback mechanism is termed Spatial Interface Valuing. This method isevaluated experimentally in a simulated environment for a grasping task using arobotic arm with variable grip settings. Preliminary results indicate thatlearning agents using spatial interface valuing can learn a value functionmapping spatial gestures to expected future rewards much more quickly ascompared to those same agents just receiving explicit feedback, demonstratingthat an agent perceiving feedback from a human user via a spatial interface canserve as an effective complement to existing approaches.",http://arxiv.org/abs/1902.00719v1,,
1149,Classification of Sets using Restricted Boltzmann Machines,"We consider the problem of classification when inputs correspond to sets ofvectors. This setting occurs in many problems such as the classification ofpieces of mail containing several pages, of web sites with several sections orof images that have been pre-segmented into smaller regions. We proposegeneralizations of the restricted Boltzmann machine (RBM) that are appropriatein this context and explore how to incorporate different assumptions about therelationship between the input sets and the target class within the RBM. Inexperiments on standard multiple-instance learning datasets, we demonstrate thecompetitiveness of approaches based on RBMs and apply the proposed variants tothe problem of incoming mail classification.",http://arxiv.org/abs/1103.4896v1,,
1150,Conditional Restricted Boltzmann Machines for Cold Start Recommendations,"Restricted Boltzman Machines (RBMs) have been successfully used inrecommender systems. However, as with most of other collaborative filteringtechniques, it cannot solve cold start problems for there is no rating for anew item. In this paper, we first apply conditional RBM (CRBM) which could takeextra information into account and show that CRBM could solve cold startproblem very well, especially for rating prediction task. CRBM naturallycombine the content and collaborative data under a single framework which couldbe fitted effectively. Experiments show that CRBM can be compared favourablywith matrix factorization models, while hidden features learned from the formermodels are more easy to be interpreted.",http://arxiv.org/abs/1408.0096v1,,
1151,Learning by Transduction,"We describe a method for predicting a classification of an object givenclassifications of the objects in the training set, assuming that the pairsobject/classification are generated by an i.i.d. process from a continuousprobability distribution. Our method is a modification of Vapnik'ssupport-vector machine; its main novelty is that it gives not only theprediction itself but also a practicable measure of the evidence found insupport of that prediction. We also describe a procedure for assigning degreesof confidence to predictions made by the support vector machine. Someexperimental results are presented, and possible extensions of the algorithmsare discussed.",http://arxiv.org/abs/1301.7375v1,,
1152,"Accelerating Minibatch Stochastic Gradient Descent using Stratified  Sampling","Stochastic Gradient Descent (SGD) is a popular optimization method which hasbeen applied to many important machine learning tasks such as Support VectorMachines and Deep Neural Networks. In order to parallelize SGD, minibatchtraining is often employed. The standard approach is to uniformly sample aminibatch at each step, which often leads to high variance. In this paper wepropose a stratified sampling strategy, which divides the whole dataset intoclusters with low within-cluster variance; we then take examples from theseclusters using a stratified sampling technique. It is shown that theconvergence rate can be significantly improved by the algorithm. Encouragingexperimental results confirm the effectiveness of the proposed method.",http://arxiv.org/abs/1405.3080v1,,
1153,On the consistency of Multithreshold Entropy Linear Classifier,"Multithreshold Entropy Linear Classifier (MELC) is a recent classifier ideawhich employs information theoretic concept in order to create a multithresholdmaximum margin model. In this paper we analyze its consistency overmultithreshold linear models and show that its objective function upper boundsthe amount of misclassified points in a similar manner like hinge loss does insupport vector machines. For further confirmation we also conduct somenumerical experiments on five datasets.",http://arxiv.org/abs/1504.04740v1,,
1154,Machine olfaction using time scattering of sensor multiresolution graphs,"In this paper we construct a learning architecture for high dimensional timeseries sampled by sensor arrangements. Using a redundant wavelet decompositionon a graph constructed over the sensor locations, our algorithm is able toconstruct discriminative features that exploit the mutual information betweenthe sensors. The algorithm then applies scattering networks to the time seriesgraphs to create the feature space. We demonstrate our method on a machineolfaction problem, where one needs to classify the gas type and the locationwhere it originates from data sampled by an array of sensors. Our experimentalresults clearly demonstrate that our method outperforms classical machinelearning techniques used in previous studies.",http://arxiv.org/abs/1602.04358v1,,
1155,Higher-Order Factorization Machines,"Factorization machines (FMs) are a supervised learning approach that can usesecond-order feature combinations even when the data is very high-dimensional.Unfortunately, despite increasing interest in FMs, there exists to date noefficient training algorithm for higher-order FMs (HOFMs). In this paper, wepresent the first generic yet efficient algorithms for training arbitrary-orderHOFMs. We also present new variants of HOFMs with shared parameters, whichgreatly reduce model size and prediction times while maintaining similaraccuracy. We demonstrate the proposed approaches on four different linkprediction tasks.",http://arxiv.org/abs/1607.07195v2,,
1156,"Polynomial Networks and Factorization Machines: New Insights and  Efficient Training Algorithms","Polynomial networks and factorization machines are two recently-proposedmodels that can efficiently use feature interactions in classification andregression tasks. In this paper, we revisit both models from a unifiedperspective. Based on this new view, we study the properties of both models andpropose new efficient training algorithms. Key to our approach is to castparameter learning as a low-rank symmetric tensor estimation problem, which wesolve by multi-convex optimization. We demonstrate our approach on regressionand recommender system tasks.",http://arxiv.org/abs/1607.08810v1,,
1157,"Random matrices meet machine learning: a large dimensional analysis of  LS-SVM","This article proposes a performance analysis of kernel least squares supportvector machines (LS-SVMs) based on a random matrix approach, in the regimewhere both the dimension of data $p$ and their number $n$ grow large at thesame rate. Under a two-class Gaussian mixture model for the input data, weprove that the LS-SVM decision function is asymptotically normal with means andcovariances shown to depend explicitly on the derivatives of the kernelfunction. This provides improved understanding along with new insights into theinternal workings of SVM-type methods for large datasets.",http://arxiv.org/abs/1609.02020v2,,
1158,Learning Operations on a Stack with Neural Turing Machines,"Multiple extensions of Recurrent Neural Networks (RNNs) have been proposedrecently to address the difficulty of storing information over long timeperiods. In this paper, we experiment with the capacity of Neural TuringMachines (NTMs) to deal with these long-term dependencies on well-balancedstrings of parentheses. We show that not only does the NTM emulate a stack withits heads and learn an algorithm to recognize such words, but it is alsocapable of strongly generalizing to much longer sequences.",http://arxiv.org/abs/1612.00827v1,,
1159,Supervised Machine Learning for Signals Having RRC Shaped Pulses,"Classification performances of the supervised machine learning techniquessuch as support vector machines, neural networks and logistic regression arecompared for modulation recognition purposes. The simple and robust featuresare used to distinguish continuous-phase FSK from QAM-PSK signals. Signalshaving root-raised-cosine shaped pulses are simulated in extreme noisyconditions having joint impurities of block fading, lack of symbol and samplingsynchronization, carrier offset, and additive white Gaussian noise. Thefeatures are based on sample mean and sample variance of the imaginary part ofthe product of two consecutive complex signal values.",http://arxiv.org/abs/1705.06299v1,,
1160,"An Asynchronous Distributed Framework for Large-scale Learning Based on  Parameter Exchanges","In many distributed learning problems, the heterogeneous loading of computingmachines may harm the overall performance of synchronous strategies. In thispaper, we propose an effective asynchronous distributed framework for theminimization of a sum of smooth functions, where each machine performsiterations in parallel on its local function and updates a shared parameterasynchronously. In this way, all machines can continuously work even thoughthey do not have the latest version of the shared parameter. We prove theconvergence of the consistency of this general distributed asynchronous methodfor gradient iterations then show its efficiency on the matrix factorizationproblem for recommender systems and on binary classification.",http://arxiv.org/abs/1705.07751v1,,
1161,"Gradient Sparsification for Communication-Efficient Distributed  Optimization","Modern large scale machine learning applications require stochasticoptimization algorithms to be implemented on distributed computationalarchitectures. A key bottleneck is the communication overhead for exchanginginformation such as stochastic gradients among different workers. In thispaper, to reduce the communication cost we propose a convex optimizationformulation to minimize the coding length of stochastic gradients. To solve theoptimal sparsification efficiently, several simple and fast algorithms areproposed for approximate solution, with theoretical guaranteed for sparseness.Experiments on $\ell_2$ regularized logistic regression, support vectormachines, and convolutional neural networks validate our sparsificationapproaches.",http://arxiv.org/abs/1710.09854v1,,
1162,"Using stochastic computation graphs formalism for optimization of  sequence-to-sequence model","Variety of machine learning problems can be formulated as an optimizationtask for some (surrogate) loss function. Calculation of loss function can beviewed in terms of stochastic computation graphs (SCG). We use this formalismto analyze a problem of optimization of famous sequence-to-sequence model withattention and propose reformulation of the task. Examples are given for machinetranslation (MT). Our work provides a unified view on different optimizationapproaches for sequence-to-sequence models and could help researchers indeveloping new network architectures with embedded stochastic nodes.",http://arxiv.org/abs/1711.07724v2,,
1163,Tensor2Tensor for Neural Machine Translation,"Tensor2Tensor is a library for deep learning models that is well-suited forneural machine translation and includes the reference implementation of thestate-of-the-art Transformer model.",http://arxiv.org/abs/1803.07416v1,,
1164,Big Data Quantum Support Vector Clustering,"Clustering is a complex process in finding the relevant hidden patterns inunlabeled datasets, broadly known as unsupervised learning. Support vectorclustering algorithm is a well-known clustering algorithm based on supportvector machines and Gaussian kernels. In this paper, we have investigated thesupport vector clustering algorithm in quantum paradigm. We have developed aquantum algorithm which is based on quantum support vector machine and thequantum kernel (Gaussian kernel and polynomial kernel) formulation. Theinvestigation exhibits approximately exponential speed up in the quantumversion with respect to the classical counterpart.",http://arxiv.org/abs/1804.10905v1,,
1165,Deep Factorization Machines for Knowledge Tracing,"This paper introduces our solution to the 2018 Duolingo Shared Task on SecondLanguage Acquisition Modeling (SLAM). We used deep factorization machines, awide and deep learning model of pairwise relationships between users, items,skills, and other entities considered. Our solution (AUC 0.815) hopefullymanaged to beat the logistic regression baseline (AUC 0.774) but not the topperforming model (AUC 0.861) and reveals interesting strategies to build uponitem response theory models.",http://arxiv.org/abs/1805.00356v1,,
1166,"A Fast, Principled Working Set Algorithm for Exploiting Piecewise Linear  Structure in Convex Problems","By reducing optimization to a sequence of smaller subproblems, working setalgorithms achieve fast convergence times for many machine learning problems.Despite such performance, working set implementations often resort toheuristics to determine subproblem size, makeup, and stopping criteria. Wepropose BlitzWS, a working set algorithm with useful theoretical guarantees.Our theory relates subproblem size and stopping criteria to the amount ofprogress during each iteration. This result motivates strategies for optimizingalgorithmic parameters and discarding irrelevant components as BlitzWSprogresses toward a solution. BlitzWS applies to many convex problems,including training L1-regularized models and support vector machines. Weshowcase this versatility with empirical comparisons, which demonstrate BlitzWSis indeed a fast algorithm.",http://arxiv.org/abs/1807.08046v1,,
1167,An Introductory Survey on Attention Mechanisms in NLP Problems,"First derived from human intuition, later adapted to machine translation forautomatic token alignment, attention mechanism, a simple method that can beused for encoding sequence data based on the importance score each element isassigned, has been widely applied to and attained significant improvement invarious tasks in natural language processing, including sentimentclassification, text summarization, question answering, dependency parsing,etc. In this paper, we survey through recent works and conduct an introductorysummary of the attention mechanism in different NLP problems, aiming to provideour readers with basic knowledge on this widely used method, discuss itsdifferent variants for different tasks, explore its association with othertechniques in machine learning, and examine methods for evaluating itsperformance.",http://arxiv.org/abs/1811.05544v1,,
1168,"To bee or not to bee: Investigating machine learning approaches for  beehive sound recognition","In this work, we aim to explore the potential of machine learning methods tothe problem of beehive sound recognition. A major contribution of this work isthe creation and release of annotations for a selection of beehive recordings.By experimenting with both support vector machines and convolutional neuralnetworks, we explore important aspects to be considered in the development ofbeehive sound recognition systems using machine learning approaches.",http://arxiv.org/abs/1811.06016v1,,
1169,Quantum speedup for twin support vector machine,"In this work, we investigate how to speed up machine learning tasks based onquantum computation. We show an important classifier in machine learning, thetwin support vector machine, can be exponentially speeded up on quantumcomputers. Specifically, for a training set with $m$ samples which arerepresented by $n$-dimensional vectors, the proposed quantum algorithm canlearn two non-parallel hyperplanes in $O(\mathrm{log}\ mn)$ time, and thenclassify a new sample in $O(\mathrm{log}\ n)$ time by comparing the distancesfrom the simple to the two hyperplanes. Note that the classical algorithmrequires polynomial time both in the training and classification procedures.",http://arxiv.org/abs/1902.08907v1,,
1170,"Asymptotically Exact, Embarrassingly Parallel MCMC","Communication costs, resulting from synchronization requirements duringlearning, can greatly slow down many parallel machine learning algorithms. Inthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm inwhich subsets of data are processed independently, with very littlecommunication. First, we arbitrarily partition data onto multiple machines.Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may beused to draw samples from a posterior distribution given the data subset.Finally, the samples from each machine are combined to form samples from thefull posterior. This embarrassingly parallel algorithm allows each machine toact independently on a subset of the data (without communication) until thefinal combination stage. We prove that our algorithm generates asymptoticallyexact samples and empirically demonstrate its ability to parallelize burn-inand sampling in several models.",http://arxiv.org/abs/1311.4780v2,,
1171,General Vector Machine,"The support vector machine (SVM) is an important class of learning machinesfor function approach, pattern recognition, and time-serious prediction, etc.It maps samples into the feature space by so-called support vectors of selectedsamples, and then feature vectors are separated by maximum margin hyperplane.The present paper presents the general vector machine (GVM) to replace the SVM.The support vectors are replaced by general project vectors selected from theusual vector space, and a Monte Carlo (MC) algorithm is developed to find thegeneral vectors. The general project vectors improves the feature-extractionability, and the MC algorithm can control the width of the separation margin ofthe hyperplane. By controlling the separation margin, we show that the maximummargin hyperplane can usually induce the overlearning, and the best learningmachine is achieved with a proper separation margin. Applications in functionapproach, pattern recognition, and classification indicate that the developedmethod is very successful, particularly for small-set training problems.Additionally, our algorithm may induce some particular applications, such asfor the transductive inference.",http://arxiv.org/abs/1602.03950v1,,
1172,Iterative Machine Teaching,"In this paper, we consider the problem of machine teaching, the inverseproblem of machine learning. Different from traditional machine teaching whichviews the learners as batch algorithms, we study a new paradigm where thelearner uses an iterative algorithm and a teacher can feed examplessequentially and intelligently based on the current performance of the learner.We show that the teaching complexity in the iterative case is very differentfrom that in the batch case. Instead of constructing a minimal training set forlearners, our iterative machine teaching focuses on achieving fast convergencein the learner model. Depending on the level of information the teacher hasfrom the learner model, we design teaching algorithms which can provably reducethe number of teaching examples and achieve faster convergence than learningwithout teachers. We also validate our theoretical findings with extensiveexperiments on different data distribution and real image datasets.",http://arxiv.org/abs/1705.10470v3,,
1173,Instance-based entropy fuzzy support vector machine for imbalanced data,"Imbalanced classification has been a major challenge for machine learningbecause many standard classifiers mainly focus on balanced datasets and tend tohave biased results towards the majority class. We modify entropy fuzzy supportvector machine (EFSVM) and introduce instance-based entropy fuzzy supportvector machine (IEFSVM). Both EFSVM and IEFSVM use the entropy information ofk-nearest neighbors to determine the fuzzy membership value for each samplewhich prioritizes the importance of each sample. IEFSVM considers the diversityof entropy patterns for each sample when increasing the size of neighbors, k,while EFSVM uses single entropy information of the fixed size of neighbors forall samples. By varying k, we can reflect the component change of sample'sneighbors from near to far distance in the determination of fuzzy valuemembership. Numerical experiments on 35 public and 12 real-world imbalanceddatasets are performed to validate IEFSVM and area under the receiver operatingcharacteristic curve (AUC) is used to compare its performance with other SVMsand machine learning methods. IEFSVM shows a much higher AUC value for datasetswith high imbalance ratio, implying that IEFSVM is effective in dealing withthe class imbalance problem.",http://arxiv.org/abs/1807.03933v1,,
1174,Support Feature Machines,"Support Vector Machines (SVMs) with various kernels have played dominant rolein machine learning for many years, finding numerous applications. Althoughthey have many attractive features interpretation of their solutions is quitedifficult, the use of a single kernel type may not be appropriate in all areasof the input space, convergence problems for some kernels are not uncommon, thestandard quadratic programming solution has $O(m^3)$ time and $O(m^2)$ spacecomplexity for $m$ training patterns. Kernel methods work because theyimplicitly provide new, useful features. Such features, derived from variouskernels and other vector transformations, may be used directly in any machinelearning algorithm, facilitating multiresolution, heterogeneous models of data.Therefore Support Feature Machines (SFM) based on linear models in the extendedfeature spaces, enabling control over selection of support features, give atleast as good results as any kernel-based SVMs, removing all problems relatedto interpretation, scaling and convergence. This is demonstrated for a numberof benchmark datasets analyzed with linear discrimination, SVM, decision treesand nearest neighbor methods.",http://arxiv.org/abs/1901.09643v1,,
1175,"A Theory of Local Learning, the Learning Channel, and the Optimality of  Backpropagation","In a physical neural system, where storage and processing are intimatelyintertwined, the rules for adjusting the synaptic weights can only depend onvariables that are available locally, such as the activity of the pre- andpost-synaptic neurons, resulting in local learning rules. A systematicframework for studying the space of local learning rules is obtained by firstspecifying the nature of the local variables, and then the functional form thatties them together into each learning rule. Such a framework enables also thesystematic discovery of new learning rules and exploration of relationshipsbetween learning rules and group symmetries. We study polynomial local learningrules stratified by their degree and analyze their behavior and capabilities inboth linear and non-linear units and networks. Stacking local learning rules indeep feedforward networks leads to deep local learning. While deep locallearning can learn interesting representations, it cannot learn complexinput-output functions, even when targets are available for the top layer.Learning complex input-output functions requires local deep learning wheretarget information is communicated to the deep layers through a backwardlearning channel. The nature of the communicated information about the targetsand the structure of the learning channel partition the space of learningalgorithms. We estimate the learning channel capacity associated with severalalgorithms and show that backpropagation outperforms them by simultaneouslymaximizing the information rate and minimizing the computational cost, even inrecurrent networks. The theory clarifies the concept of Hebbian learning,establishes the power and limitations of local learning rules, introduces thelearning channel which enables a formal analysis of the optimality ofbackpropagation, and explains the sparsity of the space of learning rulesdiscovered so far.",http://arxiv.org/abs/1506.06472v2,,
1176,Scikit-learn: Machine Learning in Python,"Scikit-learn is a Python module integrating a wide range of state-of-the-artmachine learning algorithms for medium-scale supervised and unsupervisedproblems. This package focuses on bringing machine learning to non-specialistsusing a general-purpose high-level language. Emphasis is put on ease of use,performance, documentation, and API consistency. It has minimal dependenciesand is distributed under the simplified BSD license, encouraging its use inboth academic and commercial settings. Source code, binaries, and documentationcan be downloaded from http://scikit-learn.org.",http://arxiv.org/abs/1201.0490v4,,
1177,"Nonparametric Divergence Estimation with Applications to Machine  Learning on Distributions","Low-dimensional embedding, manifold learning, clustering, classification, andanomaly detection are among the most important problems in machine learning.The existing methods usually consider the case when each instance has a fixed,finite-dimensional feature representation. Here we consider a differentsetting. We assume that each instance corresponds to a continuous probabilitydistribution. These distributions are unknown, but we are given some i.i.d.samples from each distribution. Our goal is to estimate the distances betweenthese distributions and use these distances to perform low-dimensionalembedding, clustering/classification, or anomaly detection for thedistributions. We present estimation algorithms, describe how to apply them formachine learning tasks on distributions, and show empirical results onsynthetic data, real word images, and astronomical data sets.",http://arxiv.org/abs/1202.3758v1,,
1178,Learning from Distributions via Support Measure Machines,"This paper presents a kernel-based discriminative learning framework onprobability measures. Rather than relying on large collections of vectorialtraining examples, our framework learns using a collection of probabilitydistributions that have been constructed to meaningfully represent trainingdata. By representing these probability distributions as mean embeddings in thereproducing kernel Hilbert space (RKHS), we are able to apply many standardkernel-based learning techniques in straightforward fashion. To accomplishthis, we construct a generalization of the support vector machine (SVM) calleda support measure machine (SMM). Our analyses of SMMs provides several insightsinto their relationship to traditional SVMs. Based on such insights, we proposea flexible SVM (Flex-SVM) that places different kernel functions on eachtraining example. Experimental results on both synthetic and real-world datademonstrate the effectiveness of our proposed framework.",http://arxiv.org/abs/1202.6504v2,,
1179,Learning Interpretable Musical Compositional Rules and Traces,"Throughout music history, theorists have identified and documentedinterpretable rules that capture the decisions of composers. This paper asks,""Can a machine behave like a music theorist?"" It presents MUS-ROVER, aself-learning system for automatically discovering rules from symbolic music.MUS-ROVER performs feature learning via $n$-gram models to extractcompositional rules --- statistical patterns over the resulting features. Weevaluate MUS-ROVER on Bach's (SATB) chorales, demonstrating that it can recoverknown rules, as well as identify new, characteristic patterns for furtherstudy. We discuss how the extracted rules can be used in both machine and humancomposition.",http://arxiv.org/abs/1606.05572v1,,
1180,"Big Universe, Big Data: Machine Learning and Image Analysis for  Astronomy","Astrophysics and cosmology are rich with data. The advent of wide-areadigital cameras on large aperture telescopes has led to ever more ambitioussurveys of the sky. Data volumes of entire surveys a decade ago can now beacquired in a single night and real-time analysis is often desired. Thus,modern astronomy requires big data know-how, in particular it demands highlyefficient machine learning and image analysis algorithms. But scalability isnot the only challenge: Astronomy applications touch several current machinelearning research questions, such as learning from biased data and dealing withlabel and measurement noise. We argue that this makes astronomy a great domainfor computer science research, as it pushes the boundaries of data analysis. Inthe following, we will present this exciting application area for datascientists. We will focus on exemplary results, discuss main challenges, andhighlight some recent methodological advancements in machine learning and imageanalysis triggered by astronomical applications.",http://arxiv.org/abs/1704.04650v1,,
1181,"Using a Machine Learning Approach to Implement and Evaluate Product Line  Features","Bike-sharing systems are a means of smart transportation in urbanenvironments with the benefit of a positive impact on urban mobility. In thispaper we are interested in studying and modeling the behavior of features thatpermit the end user to access, with her/his web browser, the status of theBike-Sharing system. In particular, we address features able to make aprediction on the system state. We propose to use a machine learning approachto analyze usage patterns and learn computational models of such features fromlogs of system usage.  On the one hand, machine learning methodologies provide a powerful andgeneral means to implement a wide choice of predictive features. On the otherhand, trained machine learning models are provided with a measure of predictiveperformance that can be used as a metric to assess the cost-performancetrade-off of the feature. This provides a principled way to assess the runtimebehavior of different components before putting them into operation.",http://arxiv.org/abs/1508.03906v1,,
1182,"Learning dynamic Boltzmann machines with spike-timing dependent  plasticity","We propose a particularly structured Boltzmann machine, which we refer to asa dynamic Boltzmann machine (DyBM), as a stochastic model of amulti-dimensional time-series. The DyBM can have infinitely many layers ofunits but allows exact and efficient inference and learning when its parametershave a proposed structure. This proposed structure is motivated by postulatesand observations, from biological neural networks, that the synaptic weight isstrengthened or weakened, depending on the timing of spikes (i.e., spike-timingdependent plasticity or STDP). We show that the learning rule of updating theparameters of the DyBM in the direction of maximizing the likelihood of giventime-series can be interpreted as STDP with long term potentiation and longterm depression. The learning rule has a guarantee of convergence and can beperformed in a distributed matter (i.e., local in space) with limited memory(i.e., local in time).",http://arxiv.org/abs/1509.08634v1,,
1183,"A Bootstrap Machine Learning Approach to Identify Rare Disease Patients  from Electronic Health Records","Rare diseases are very difficult to identify among large number of otherpossible diagnoses. Better availability of patient data and improvement inmachine learning algorithms empower us to tackle this problem computationally.In this paper, we target one such rare disease - cardiac amyloidosis. We aim toautomate the process of identifying potential cardiac amyloidosis patients withthe help of machine learning algorithms and also learn most predictive factors.With the help of experienced cardiologists, we prepared a gold standard with 73positive (cardiac amyloidosis) and 197 negative instances. We achieved highaverage cross-validation F1 score of 0.98 using an ensemble machine learningclassifier. Some of the predictive variables were: Age and Diagnosis of cardiacarrest, chest pain, congestive heart failure, hypertension, prim open angleglaucoma, and shoulder arthritis. Further studies are needed to validate theaccuracy of the system across an entire health system and its generalizabilityfor other diseases.",http://arxiv.org/abs/1609.01586v1,,
1184,Modeling Scalability of Distributed Machine Learning,"Present day machine learning is computationally intensive and processes largeamounts of data. It is implemented in a distributed fashion in order to addressthese scalability issues. The work is parallelized across a number of computingnodes. It is usually hard to estimate in advance how many nodes to use for aparticular workload. We propose a simple framework for estimating thescalability of distributed machine learning algorithms. We measure thescalability by means of the speedup an algorithm achieves with more nodes. Wepropose time complexity models for gradient descent and graphical modelinference. We validate our models with experiments on deep learning trainingand belief propagation. This framework was used to study the scalability ofmachine learning algorithms in Apache Spark.",http://arxiv.org/abs/1610.06276v2,,
1185,Reinforcement Learning Using Quantum Boltzmann Machines,"We investigate whether quantum annealers with select chip layouts canoutperform classical computers in reinforcement learning tasks. We associate atransverse field Ising spin Hamiltonian with a layout of qubits similar to thatof a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) tonumerically simulate quantum sampling from this system. We design areinforcement learning algorithm in which the set of visible nodes representingthe states and actions of an optimal policy are the first and last layers ofthe deep network. In absence of a transverse field, our simulations show thatDBMs are trained more effectively than restricted Boltzmann machines (RBM) withthe same number of nodes. We then develop a framework for training the networkas a quantum Boltzmann machine (QBM) in the presence of a significanttransverse field for reinforcement learning. This method also outperforms thereinforcement learning method that uses RBMs.",http://arxiv.org/abs/1612.05695v3,,
1186,Multimodal Machine Learning: A Survey and Taxonomy,"Our experience of the world is multimodal - we see objects, hear sounds, feeltexture, smell odors, and taste flavors. Modality refers to the way in whichsomething happens or is experienced and a research problem is characterized asmultimodal when it includes multiple such modalities. In order for ArtificialIntelligence to make progress in understanding the world around us, it needs tobe able to interpret such multimodal signals together. Multimodal machinelearning aims to build models that can process and relate information frommultiple modalities. It is a vibrant multi-disciplinary field of increasingimportance and with extraordinary potential. Instead of focusing on specificmultimodal applications, this paper surveys the recent advances in multimodalmachine learning itself and presents them in a common taxonomy. We go beyondthe typical early and late fusion categorization and identify broaderchallenges that are faced by multimodal machine learning, namely:representation, translation, alignment, fusion, and co-learning. This newtaxonomy will enable researchers to better understand the state of the fieldand identify directions for future research.",http://arxiv.org/abs/1705.09406v2,,
1187,"Financial Series Prediction: Comparison Between Precision of Time Series  Models and Machine Learning Methods","Precise financial series predicting has long been a difficult problem becauseof unstableness and many noises within the series. Although Traditional timeseries models like ARIMA and GARCH have been researched and proved to beeffective in predicting, their performances are still far from satisfying.Machine Learning, as an emerging research field in recent years, has broughtabout many incredible improvements in tasks such as regressing and classifying,and it's also promising to exploit the methodology in financial time seriespredicting. In this paper, the predicting precision of financial time seriesbetween traditional time series models and mainstream machine learning modelsincluding some state-of-the-art ones of deep learning are compared throughexperiment using real stock index data from history. The result shows thatmachine learning as a modern method far surpasses traditional models inprecision.",http://arxiv.org/abs/1706.00948v5,,
1188,Is it ethical to avoid error analysis?,"Machine learning algorithms tend to create more accurate models with theavailability of large datasets. In some cases, highly accurate models can hidethe presence of bias in the data. There are several studies published thattackle the development of discriminatory-aware machine learning algorithms. Wecenter on the further evaluation of machine learning models by doing erroranalysis, to understand under what conditions the model is not working asexpected. We focus on the ethical implications of avoiding error analysis, froma falsification of results and discrimination perspective. Finally, we showdifferent ways to approach error analysis in non-interpretable machine learningalgorithms such as deep learning.",http://arxiv.org/abs/1706.10237v1,,
1189,"Foolbox: A Python toolbox to benchmark the robustness of machine  learning models","Even todays most advanced machine learning models are easily fooled by almostimperceptible perturbations of their inputs. Foolbox is a new Python package togenerate such adversarial perturbations and to quantify and compare therobustness of machine learning models. It is build around the idea that themost comparable robustness measure is the minimum perturbation needed to craftan adversarial example. To this end, Foolbox provides reference implementationsof most published adversarial attack methods alongside some new ones, all ofwhich perform internal hyperparameter tuning to find the minimum adversarialperturbation. Additionally, Foolbox interfaces with most popular deep learningframeworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allowsdifferent adversarial criteria such as targeted misclassification and top-kmisclassification as well as different distance measures. The code is licensedunder the MIT license and is openly available athttps://github.com/bethgelab/foolbox . The most up-to-date documentation can befound at http://foolbox.readthedocs.io .",http://arxiv.org/abs/1707.04131v3,,
1190,Enhanced Quantum Synchronization via Quantum Machine Learning,"We study the quantum synchronization between a pair of two-level systemsinside two coupled cavities. By using a digital-analog decomposition of themaster equation that rules the system dynamics, we show that this approachleads to quantum synchronization between both two-level systems. Moreover, wecan identify in this digital-analog block decomposition the fundamentalelements of a quantum machine learning protocol, in which the agent and theenvironment (learning units) interact through a mediating system, namely, theregister. If we can additionally equip this algorithm with a classical feedbackmechanism, which consists of projective measurements in the register,reinitialization of the register state and local conditional operations on theagent and environment subspace, a powerful and flexible quantum machinelearning protocol emerges. Indeed, numerical simulations show that thisprotocol enhances the synchronization process, even when every subsystemexperience different loss/decoherence mechanisms, and give us the flexibilityto choose the synchronization state. Finally, we propose an implementationbased on current technologies in superconducting circuits.",http://arxiv.org/abs/1709.08519v2,,
1191,A Machine Learning Framework to Forecast Wave Conditions,"A~machine learning framework is developed to estimate ocean-wave conditions.By supervised training of machine learning models on many thousands ofiterations of a physics-based wave model, accurate representations ofsignificant wave heights and period can be used to predict ocean conditions. Amodel of Monterey Bay was used as the example test site; it was forced bymeasured wave conditions, ocean-current nowcasts, and reported winds. Theseinput data along with model outputs of spatially variable wave heights andcharacteristic period were aggregated into supervised learning training andtest data sets, which were supplied to machine learning models. These machinelearning models replicated wave heights with a root-mean-squared error of 9cmand correctly identify over 90% of the characteristic periods for the test-datasets. Impressively, transforming model inputs to outputs through matrixoperations requires only a fraction (<1/1,000) of the computation time comparedto forecasting with the physics-based model.",http://arxiv.org/abs/1709.08725v1,,
1192,"Towards Deep Learning Models for Psychological State Prediction using  Smartphone Data: Challenges and Opportunities","There is an increasing interest in exploiting mobile sensing technologies andmachine learning techniques for mental health monitoring and intervention.Researchers have effectively used contextual information, such as mobility,communication and mobile phone usage patterns for quantifying individuals' moodand wellbeing. In this paper, we investigate the effectiveness of neuralnetwork models for predicting users' level of stress by using the locationinformation collected by smartphones. We characterize the mobility patterns ofindividuals using the GPS metrics presented in the literature and employ thesemetrics as input to the network. We evaluate our approach on the open-sourceStudentLife dataset. Moreover, we discuss the challenges and trade-offsinvolved in building machine learning models for digital mental health andhighlight potential future work in this direction.",http://arxiv.org/abs/1711.06350v1,,
1193,Evaluation of Interactive Machine Learning Systems,"The evaluation of interactive machine learning systems remains a difficulttask. These systems learn from and adapt to the human, but at the same time,the human receives feedback and adapts to the system. Getting a clearunderstanding of these subtle mechanisms of co-operation and co-adaptation ischallenging. In this chapter, we report on our experience in designing andevaluating various interactive machine learning applications from differentdomains. We argue for coupling two types of validation: algorithm-centeredanalysis, to study the computational behaviour of the system; andhuman-centered evaluation, to observe the utility and effectiveness of theapplication for end-users. We use a visual analytics application for guidedsearch, built using an interactive evolutionary approach, as an exemplar of ourwork. Our observation is that human-centered design and evaluation complementalgorithmic analysis, and can play an important role in addressing the""black-box"" effect of machine learning. Finally, we discuss researchopportunities that require human-computer interaction methodologies, in orderto support both the visible and hidden roles that humans play in interactivemachine learning.",http://arxiv.org/abs/1801.07964v1,,
1194,"A Stochastic Large-scale Machine Learning Algorithm for Distributed  Features and Observations","As the size of modern data sets exceeds the disk and memory capacities of asingle computer, machine learning practitioners have resorted to parallel anddistributed computing. Given that optimization is one of the pillars of machinelearning and predictive modeling, distributed optimization methods haverecently garnered ample attention, in particular when either observations orfeatures are distributed, but not both. We propose a general stochasticalgorithm where observations, features, and gradient components can be sampledin a double distributed setting, i.e., with both features and observationsdistributed. Very technical analyses establish convergence properties of thealgorithm under different conditions on the learning rate (diminishing to zeroor constant). Computational experiments in Spark demonstrate a superiorperformance of our algorithm versus a benchmark in early iterations of thealgorithm, which is due to the stochastic components of the algorithm.",http://arxiv.org/abs/1803.11287v1,,
1195,Differentiable Learning of Quantum Circuit Born Machine,"Quantum circuit Born machines are generative models which represent theprobability distribution of classical dataset as quantum pure states.Computational complexity considerations of the quantum sampling problem suggestthat the quantum circuits exhibit stronger expressibility compared to classicalneural networks. One can efficiently draw samples from the quantum circuits viaprojective measurements on qubits. However, similar to the leading implicitgenerative models in deep learning, such as the generative adversarialnetworks, the quantum circuits cannot provide the likelihood of the generatedsamples, which poses a challenge to the training. We devise an efficientgradient-based learning algorithm for the quantum circuit Born machine byminimizing the kerneled maximum mean discrepancy loss. We simulated generativemodeling of the Bars-and-Stripes dataset and Gaussian mixture distributionsusing deep quantum circuits. Our experiments show the importance of circuitdepth and gradient-based optimization algorithm. The proposed learningalgorithm is runnable on near-term quantum device and can exhibit quantumadvantages for generative modeling.",http://arxiv.org/abs/1804.04168v1,,
1196,geomstats: a Python Package for Riemannian Geometry in Machine Learning,"We introduce geomstats, a python package that performs computations onmanifolds such as hyperspheres, hyperbolic spaces, spaces of symmetric positivedefinite matrices and Lie groups of transformations. We provide efficient andextensively unit-tested implementations of these manifolds, together withuseful Riemannian metrics and associated Exponential and Logarithm maps. Thecorresponding geodesic distances provide a range of intuitive choices ofMachine Learning loss functions. We also give the corresponding Riemanniangradients. The operations implemented in geomstats are available with differentcomputing backends such as numpy, tensorflow and keras. We have enabled GPUimplementation and integrated geomstats manifold computations into keras deeplearning framework. This paper also presents a review of manifolds in machinelearning and an overview of the geomstats package with examples demonstratingits use for efficient and user-friendly Riemannian geometry.",http://arxiv.org/abs/1805.08308v2,,
1197,"Collective Online Learning of Gaussian Processes in Massive Multi-Agent  Systems","Distributed machine learning (ML) is a modern computation paradigm thatdivides its workload into independent tasks that can be simultaneously achievedby multiple machines (i.e., agents) for better scalability. However, a typicaldistributed system is usually implemented with a central server that collectsdata statistics from multiple independent machines operating on differentsubsets of data to build a global analytic model. This centralizedcommunication architecture however exposes a single choke point for operationalfailure and places severe bottlenecks on the server's communication andcomputation capacities as it has to process a growing volume of communicationfrom a crowd of learning agents. To mitigate these bottlenecks, this paperintroduces a novel Collective Online Learning Gaussian Process framework formassive distributed systems that allows each agent to build its local model,which can be exchanged and combined efficiently with others via peer-to-peercommunication to converge on a global model of higher quality. Finally, ourempirical results consistently demonstrate the efficiency of our framework onboth synthetic and real-world datasets.",http://arxiv.org/abs/1805.09266v2,,
1198,Flexible and accurate inference and learning for deep generative models,"We introduce a new approach to learning in hierarchical latent-variablegenerative models called the ""distributed distributional code Helmholtzmachine"", which emphasises flexibility and accuracy in the inferential process.In common with the original Helmholtz machine and later variational autoencoderalgorithms (but unlike adverserial methods) our approach learns an explicitinference or ""recognition"" model to approximate the posterior distribution overthe latent variables. Unlike in these earlier methods, the posteriorrepresentation is not limited to a narrow tractable parameterised form (nor isit represented by samples). To train the generative and recognition models wedevelop an extended wake-sleep algorithm inspired by the original HelmholtzMachine. This makes it possible to learn hierarchical latent models with bothdiscrete and continuous variables, where an accurate posterior representationis essential. We demonstrate that the new algorithm outperforms currentstate-of-the-art methods on synthetic, natural image patch and the MNIST datasets.",http://arxiv.org/abs/1805.11051v1,,
1199,"Machine Learning for Yield Curve Feature Extraction: Application to  Illiquid Corporate Bonds (Preliminary Draft)","This paper studies the application of machine learning in extracting themarket implied features from historical risk neutral corporate bond yields. Weconsider the example of a hypothetical illiquid fixed income market. Afterchoosing a surrogate liquid market, we apply the Denoising Autoencoderalgorithm from the field of computer vision and pattern recognition to learnthe features of the missing yield parameters from the historically implied dataof the instruments traded in the chosen liquid market. The results of thetrained machine learning algorithm are compared with the outputs of a point in-time 2 dimensional interpolation algorithm known as the Thin Plate Spline.Finally, the performances of the two algorithms are compared.",http://arxiv.org/abs/1806.01731v1,,
1200,Analytic continuation via domain-knowledge free machine learning,"We present a machine-learning approach to a long-standing issue in quantummany-body physics, namely, analytic continuation. This notoriousill-conditioned problem of obtaining spectral function from imaginary timeGreen's function has been a focus of new method developments for past decades.Here we demonstrate the usefulness of modern machine-learning techniquesincluding convolutional neural networks and the variants of stochastic gradientdescent optimiser. Machine-learning continuation kernel is successfullyrealized without any 'domain-knowledge', which means that any physical 'prior'is not utilized in the kernel construction and the neural networks 'learn' theknowledge solely from 'training'. The outstanding performance is achieved forboth insulating and metallic band structure. Our machine-learning-basedapproach not only provides the more accurate spectrum than the conventionalmethods in terms of peak positions and heights, but is also more robust againstthe noise which is the required key feature for any continuation technique tobe successful. Furthermore, its computation speed is 10$^4$-10$^5$ times fasterthan maximum entropy method.",http://arxiv.org/abs/1806.03841v2,,
1201,Addressing the Fundamental Tension of PCGML with Discriminative Learning,"Procedural content generation via machine learning (PCGML) is typicallyframed as the task of fitting a generative model to full-scale examples of adesired content distribution. This approach presents a fundamental tension: themore design effort expended to produce detailed training examples for shaping agenerator, the lower the return on investment from applying PCGML in the firstplace. In response, we propose the use of discriminative models (which capturethe validity of a design rather the distribution of the content) trained onpositive and negative examples. Through a modest modification ofWaveFunctionCollapse, a commercially-adopted PCG approach that we characterizeas using elementary machine learning, we demonstrate a new mode of control forlearning-based generators. We demonstrate how an artist might craft a focusedset of additional positive and negative examples by critique of the generator'sprevious outputs. This interaction mode bridges PCGML with mixed-initiativedesign assistance tools by working with a machine to define a space of validdesigns rather than just one new design.",http://arxiv.org/abs/1809.04432v1,,
1202,Training Machine Learning Models by Regularizing their Explanations,"Neural networks are among the most accurate supervised learning methods inuse today. However, their opacity makes them difficult to trust in criticalapplications, especially when conditions in training may differ from those inpractice. Recent efforts to develop explanations for neural networks andmachine learning models more generally have produced tools to shed light on theimplicit rules behind predictions. These tools can help us identify when modelsare right for the wrong reasons. However, they do not always scale toexplaining predictions for entire datasets, are not always at the right levelof abstraction, and most importantly cannot correct the problems they reveal.In this thesis, we explore the possibility of training machine learning models(with a particular focus on neural networks) using explanations themselves. Weconsider approaches where models are penalized not only for making incorrectpredictions but also for providing explanations that are either inconsistentwith domain knowledge or overly complex. These methods let us train modelswhich can not only provide more interpretable rationales for their predictionsbut also generalize better when training data is confounded or meaningfullydifferent from test data (even adversarially so).",http://arxiv.org/abs/1810.00869v1,,
1203,Preprocessor Selection for Machine Learning Pipelines,"Much of the work in metalearning has focused on classifier selection,combined more recently with hyperparameter optimization, with little concernfor data preprocessing. Yet, it is generally well accepted that machinelearning applications require not only model building, but also datapreprocessing. In other words, practical solutions consist of pipelines ofmachine learning operators rather than single algorithms. Interestingly, ourexperiments suggest that, on average, data preprocessing hinders accuracy,while the best performing pipelines do actually make use of preprocessors.Here, we conduct an extensive empirical study over a wide range of learningalgorithms and preprocessors, and use metalearning to determine when one shouldmake use of preprocessors in ML pipeline design.",http://arxiv.org/abs/1810.09942v1,,
1204,Robust Classification of Financial Risk,"Algorithms are increasingly common components of high-impact decision-making,and a growing body of literature on adversarial examples in laboratory settingsindicates that standard machine learning models are not robust. This suggeststhat real-world systems are also susceptible to manipulation ormisclassification, which especially poses a challenge to machine learningmodels used in financial services. We use the loan grade classification problemto explore how machine learning models are sensitive to small changes inuser-reported data, using adversarial attacks documented in the literature andan original, domain-specific attack. Our work shows that a robust optimizationalgorithm can build models for financial services that are resistant tomisclassification on perturbations. To the best of our knowledge, this is thefirst study of adversarial attacks and defenses for deep learning in financialservices.",http://arxiv.org/abs/1811.11079v1,,
1205,"Communication-Efficient On-Device Machine Learning: Federated  Distillation and Augmentation under Non-IID Private Data","On-device machine learning (ML) enables the training process to exploit amassive amount of user-generated private data samples. To enjoy this benefit,inter-device communication overhead should be minimized. With this end, wepropose federated distillation (FD), a distributed model training algorithmwhose communication payload size is much smaller than a benchmark scheme,federated learning (FL), particularly when the model size is large. Moreover,user-generated data samples are likely to become non-IID across devices, whichcommonly degrades the performance compared to the case with an IID dataset. Tocope with this, we propose federated augmentation (FAug), where each devicecollectively trains a generative model, and thereby augments its local datatowards yielding an IID dataset. Empirical studies demonstrate that FD withFAug yields around 26x less communication overhead while achieving 95-98% testaccuracy compared to FL.",http://arxiv.org/abs/1811.11479v1,,
1206,Deep Learning for Classical Japanese Literature,"Much of machine learning research focuses on producing models which performwell on benchmark tasks, in turn improving our understanding of the challengesassociated with those tasks. From the perspective of ML researchers, thecontent of the task itself is largely irrelevant, and thus there haveincreasingly been calls for benchmark tasks to more heavily focus on problemswhich are of social or cultural relevance. In this work, we introduceKuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), aswell as two larger, more challenging datasets, Kuzushiji-49 andKuzushiji-Kanji. Through these datasets, we wish to engage the machine learningcommunity into the world of classical Japanese literature. Dataset available athttps://github.com/rois-codh/kmnist",http://arxiv.org/abs/1812.01718v1,,
1207,GADGET SVM: A Gossip-bAseD sub-GradiEnT Solver for Linear SVMs,"In the era of big data, an important weapon in a machine learningresearcher's arsenal is a scalable Support Vector Machine (SVM) algorithm. SVMsare extensively used for solving classification problems. Traditionalalgorithms for learning SVMs often scale super linearly with training set sizewhich becomes infeasible very quickly for large data sets. In recent years,scalable algorithms have been designed which study the primal or dualformulations of the problem. This often suggests a way to decompose the problemand facilitate development of distributed algorithms. In this paper, we presenta distributed algorithm for learning linear Support Vector Machines in theprimal form for binary classification called Gossip-bAseD sub-GradiEnT (GADGET)SVM. The algorithm is designed such that it can be executed locally on nodes ofa distributed system. Each node processes its local homogeneously partitioneddata and learns a primal SVM model. It then gossips with random neighbors aboutthe classifier learnt and uses this information to update the model. Extensivetheoretical and empirical results suggest that this anytime algorithm hasperformance comparable to its centralized and online counterparts.",http://arxiv.org/abs/1812.02261v1,,
1208,Three Tools for Practical Differential Privacy,"Differentially private learning on real-world data poses challenges forstandard machine learning practice: privacy guarantees are difficult tointerpret, hyperparameter tuning on private data reduces the privacy budget,and ad-hoc privacy attacks are often required to test model privacy. Weintroduce three tools to make differentially private machine learning morepractical: (1) simple sanity checks which can be carried out in a centralizedmanner before training, (2) an adaptive clipping bound to reduce the effectivenumber of tuneable privacy parameters, and (3) we show that large-batchtraining improves model performance.",http://arxiv.org/abs/1812.02890v1,,
1209,Helix: Holistic Optimization for Accelerating Iterative Machine Learning,"Machine learning workflow development is a process of trial-and-error:developers iterate on workflows by testing out small modifications until thedesired accuracy is achieved. Unfortunately, existing machine learning systemsfocus narrowly on model training---a small fraction of the overall developmenttime---and neglect to address iterative development. We propose Helix, amachine learning system that optimizes the execution acrossiterations---intelligently caching and reusing, or recomputing intermediates asappropriate. Helix captures a wide variety of application needs within itsScala DSL, with succinct syntax defining unified processes for datapreprocessing, model specification, and learning. We demonstrate that the reuseproblem can be cast as a Max-Flow problem, while the caching problem isNP-Hard. We develop effective lightweight heuristics for the latter. Empiricalevaluation shows that Helix is not only able to handle a wide variety of usecases in one unified workflow but also much faster, providing run timereductions of up to 19x over state-of-the-art systems, such as DeepDive orKeystoneML, on four real-world applications in natural language processing,computer vision, social and natural sciences.",http://arxiv.org/abs/1812.05762v1,,
1210,Molecular Dynamics with Neural-Network Potentials,"Molecular dynamics simulations are an important tool for describing theevolution of a chemical system with time. However, these simulations areinherently held back either by the prohibitive cost of accurate electronicstructure theory computations or the limited accuracy of classical empiricalforce fields. Machine learning techniques can help to overcome theselimitations by providing access to potential energies, forces and othermolecular properties modeled directly after an electronic structure referenceat only a fraction of the original computational cost. The present textdiscusses several practical aspects of conducting machine learning drivenmolecular dynamics simulations. First, we study the efficient selection ofreference data points on the basis of an active learning inspired adaptivesampling scheme. This is followed by the analysis of a machine-learning basedmodel for simulating molecular dipole moments in the framework of predictinginfrared spectra via molecular dynamics simulations. Finally, we show thatmachine learning models can offer valuable aid in understanding chemicalsystems beyond a simple prediction of quantities.",http://arxiv.org/abs/1812.07676v1,,
1211,Identifying and Correcting Label Bias in Machine Learning,"Datasets often contain biases which unfairly disadvantage certain groups, andclassifiers trained on such datasets can inherit these biases. In this paper,we provide a mathematical formulation of how this bias can arise. We do so byassuming the existence of underlying, unknown, and unbiased labels which areoverwritten by an agent who intends to provide accurate labels but may havebiases against certain groups. Despite the fact that we only observe the biasedlabels, we are able to show that the bias may nevertheless be corrected byre-weighting the data points without changing the labels. We show, withtheoretical guarantees, that training on the re-weighted dataset corresponds totraining on the unobserved but unbiased labels, thus leading to an unbiasedmachine learning classifier. Our procedure is fast and robust and can be usedwith virtually any learning algorithm. We evaluate on a number of standardmachine learning fairness datasets and a variety of fairness notions, findingthat our method outperforms standard approaches in achieving fairclassification.",http://arxiv.org/abs/1901.04966v1,,
1212,Yet another zeta function and learning,"We study the convergence speed of the batch learning algorithm, and compareits speed to that of the memoryless learning algorithm and of learning withmemory (as analyzed in joint work with N. Komarova). We obtain precise resultsand show in particular that the batch learning algorithm is never worse thanthe memoryless learning algorithm (at least asymptotically). Its performancevis-a-vis learning with full memory is less clearcut, and depends oncertainprobabilistic assumptions. These results necessitate theintroduction ofthe moment zeta function of a probability distribution and the study of some ofits properties.",http://arxiv.org/abs/cs/0107033v1,,
1213,Learning Theory in the Arithmetic Hierarchy,"We consider the arithmetic complexity of index sets of uniformly computablyenumerable families learnable under different learning criteria. We determinethe exact complexity of these sets for the standard notions of finite learning,learning in the limit, behaviorally correct learning and anomalous learning inthe limit. In proving the $\Sigma_5^0$-completeness result for behaviorallycorrect learning we prove a result of independent interest; if a uniformlycomputably enumerable family is not learnable, then for any computable learnerthere is a $\Delta_2^0$ enumeration witnessing failure.",http://arxiv.org/abs/1302.7069v1,,
1214,Learning Multi-Scale Representations for Material Classification,"The recent progress in sparse coding and deep learning has made unsupervisedfeature learning methods a strong competitor to hand-crafted descriptors. Incomputer vision, success stories of learned features have been predominantlyreported for object recognition tasks. In this paper, we investigate if and howfeature learning can be used for material recognition. We propose twostrategies to incorporate scale information into the learning procedureresulting in a novel multi-scale coding procedure. Our results show that ourlearned features for material recognition outperform hand-crafted descriptorson the FMD and the KTH-TIPS2 material classification benchmarks.",http://arxiv.org/abs/1408.2938v1,,
1215,A Map of Update Constraints in Inductive Inference,"We investigate how different learning restrictions reduce learning power andhow the different restrictions relate to one another. We give a complete mapfor nine different restrictions both for the cases of complete informationlearning and set-driven learning. This completes the picture for thesewell-studied \emph{delayable} learning restrictions. A further insight isgained by different characterizations of \emph{conservative} learning in termsof variants of \emph{cautious} learning.  Our analyses greatly benefit from general theorems we give, for exampleshowing that learners with exclusively delayable restrictions can always beassumed total.",http://arxiv.org/abs/1404.7527v2,,
1216,IDS: An Incremental Learning Algorithm for Finite Automata,"We present a new algorithm IDS for incremental learning of deterministicfinite automata (DFA). This algorithm is based on the concept of distinguishingsequences introduced in (Angluin81). We give a rigorous proof that two versionsof this learning algorithm correctly learn in the limit. Finally we present anempirical performance analysis that compares these two algorithms, focussing onlearning times and different types of learning queries. We conclude that IDS isan efficient algorithm for software engineering applications of automatalearning, such as testing and model inference.",http://arxiv.org/abs/1206.2691v1,,
1217,Deep Learning of Representations: Looking Forward,"Deep learning research aims at discovering learning algorithms that discovermultiple levels of distributed representations, with higher levels representingmore abstract concepts. Although the study of deep learning has already led toimpressive theoretical results, learning algorithms and breakthroughexperiments, several challenges lie ahead. This paper proposes to examine someof these challenges, centering on the questions of scaling deep learningalgorithms to much larger models and datasets, reducing optimizationdifficulties due to ill-conditioning or local minima, designing more efficientand powerful inference and sampling procedures, and learning to disentangle thefactors of variation underlying the observed data. It also proposes a fewforward-looking research directions aimed at overcoming these challenges.",http://arxiv.org/abs/1305.0445v2,,
1218,Playing Atari with Deep Reinforcement Learning,"We present the first deep learning model to successfully learn controlpolicies directly from high-dimensional sensory input using reinforcementlearning. The model is a convolutional neural network, trained with a variantof Q-learning, whose input is raw pixels and whose output is a value functionestimating future rewards. We apply our method to seven Atari 2600 games fromthe Arcade Learning Environment, with no adjustment of the architecture orlearning algorithm. We find that it outperforms all previous approaches on sixof the games and surpasses a human expert on three of them.",http://arxiv.org/abs/1312.5602v1,,
1219,Le Cam meets LeCun: Deficiency and Generic Feature Learning,"""Deep Learning"" methods attempt to learn generic features in an unsupervisedfashion from a large unlabelled data set. These generic features should performas well as the best hand crafted features for any learning problem that makesuse of this data. We provide a definition of generic features, characterizewhen it is possible to learn them and provide methods closely related to theautoencoder and deep belief network of deep learning. In order to do so we usethe notion of deficiency and illustrate its value in studying certain generallearning problems.",http://arxiv.org/abs/1402.4884v2,,
1220,Learning to select data for transfer learning with Bayesian Optimization,"Domain similarity measures can be used to gauge adaptability and selectsuitable data for transfer learning, but existing approaches define ad hocmeasures that are deemed suitable for respective tasks. Inspired by work oncurriculum learning, we propose to \emph{learn} data selection measures usingBayesian Optimization and evaluate them across models, domains and tasks. Ourlearned measures outperform existing domain similarity measures significantlyon three tasks: sentiment analysis, part-of-speech tagging, and parsing. Weshow the importance of complementing similarity with diversity, and thatlearned measures are -- to some degree -- transferable across models, domains,and even tasks.",http://arxiv.org/abs/1707.05246v1,,
1221,Faster Deep Q-learning using Neural Episodic Control,"The research on deep reinforcement learning which estimates Q-value by deeplearning has been attracted the interest of researchers recently. In deepreinforcement learning, it is important to efficiently learn the experiencesthat an agent has collected by exploring environment. We propose NEC2DQN thatimproves learning speed of a poor sample efficiency algorithm such as DQN byusing good one such as NEC at the beginning of learning. We show it is able tolearn faster than Double DQN or N-step DQN in the experiments of Pong.",http://arxiv.org/abs/1801.01968v4,,
1222,Comparing Bayesian Network Classifiers,"In this paper, we empirically evaluate algorithms for learning four types ofBayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BNaugmented Naive-Bayes and general BNs, where the latter two are learned usingtwo variants of a conditional-independence (CI) based BN-learning algorithm.Experimental results show the obtained classifiers, learned using the CI basedalgorithms, are competitive with (or superior to) the best known classifiers,based on both Bayesian networks and other formalisms; and that thecomputational time for learning and using these classifiers is relativelysmall. Moreover, these results also suggest a way to learn yet more effectiveclassifiers; we demonstrate empirically that this new algorithm does work asexpected. Collectively, these results argue that BN classifiers deserve moreattention in machine learning and data mining communities.",http://arxiv.org/abs/1301.6684v1,,
1223,Geodesic Distance Function Learning via Heat Flow on Vector Fields,"Learning a distance function or metric on a given data manifold is of greatimportance in machine learning and pattern recognition. Many of the previousworks first embed the manifold to Euclidean space and then learn the distancefunction. However, such a scheme might not faithfully preserve the distancefunction if the original manifold is not Euclidean. Note that the distancefunction on a manifold can always be well-defined. In this paper, we propose tolearn the distance function directly on the manifold without embedding. Wefirst provide a theoretical characterization of the distance function by itsgradient field. Based on our theoretical analysis, we propose to first learnthe gradient field of the distance function and then learn the distancefunction itself. Specifically, we set the gradient field of a local distancefunction as an initial vector field. Then we transport it to the whole manifoldvia heat flow on vector fields. Finally, the geodesic distance function can beobtained by requiring its gradient field to be close to the normalized vectorfield. Experimental results on both synthetic and real data demonstrate theeffectiveness of our proposed algorithm.",http://arxiv.org/abs/1405.0133v2,,
1224,A PAC-Bayesian bound for Lifelong Learning,"Transfer learning has received a lot of attention in the machine learningcommunity over the last years, and several effective algorithms have beendeveloped. However, relatively little is known about their theoreticalproperties, especially in the setting of lifelong learning, where the goal isto transfer information to tasks for which no data have been observed so far.In this work we study lifelong learning from a theoretical perspective. Ourmain result is a PAC-Bayesian generalization bound that offers a unified viewon existing paradigms for transfer learning, such as the transfer of parametersor the transfer of low-dimensional representations. We also use the bound toderive two principled lifelong learning algorithms, and we show that theseyield results comparable with existing methods.",http://arxiv.org/abs/1311.2838v2,,
1225,"Learning Kernels for Structured Prediction using Polynomial Kernel  Transformations","Learning the kernel functions used in kernel methods has been a vastlyexplored area in machine learning. It is now widely accepted that to obtain'good' performance, learning a kernel function is the key challenge. In thiswork we focus on learning kernel representations for structured regression. Wepropose use of polynomials expansion of kernels, referred to as Schoenbergtransforms and Gegenbaur transforms, which arise from the seminal result ofSchoenberg (1938). These kernels can be thought of as polynomial combination ofinput features in a high dimensional reproducing kernel Hilbert space (RKHS).We learn kernels over input and output for structured data, such that,dependency between kernel features is maximized. We use Hilbert-SchmidtIndependence Criterion (HSIC) to measure this. We also give an efficient,matrix decomposition-based algorithm to learn these kernel transformations, anddemonstrate state-of-the-art results on several real-world datasets.",http://arxiv.org/abs/1601.01411v1,,
1226,"Exploiting random projections and sparsity with random forests and  gradient boosting methods -- Application to multi-label and multi-output  learning, random forest model compression and leveraging input sparsity","Within machine learning, the supervised learning field aims at modeling theinput-output relationship of a system, from past observations of its behavior.Decision trees characterize the input-output relationship through a series ofnested $if-then-else$ questions, the testing nodes, leading to a set ofpredictions, the leaf nodes. Several of such trees are often combined togetherfor state-of-the-art performance: random forest ensembles average thepredictions of randomized decision trees trained independently in parallel,while tree boosting ensembles train decision trees sequentially to refine thepredictions made by the previous ones.  The emergence of new applications requires scalable supervised learningalgorithms in terms of computational power and memory space with respect to thenumber of inputs, outputs, and observations without sacrificing accuracy. Inthis thesis, we identify three main areas where decision tree methods could beimproved for which we provide and evaluate original algorithmic solutions: (i)learning over high dimensional output spaces, (ii) learning with large sampledatasets and stringent memory constraints at prediction time and (iii) learningover high dimensional sparse input spaces.",http://arxiv.org/abs/1704.08067v1,,
1227,An Incidence Geometry approach to Dictionary Learning,"We study the Dictionary Learning (aka Sparse Coding) problem of obtaining asparse representation of data points, by learning \emph{dictionary vectors}upon which the data points can be written as sparse linear combinations. Weview this problem from a geometry perspective as the spanning set of a subspacearrangement, and focus on understanding the case when the underlying hypergraphof the subspace arrangement is specified. For this Fitted Dictionary Learningproblem, we completely characterize the combinatorics of the associatedsubspace arrangements (i.e.\ their underlying hypergraphs). Specifically, acombinatorial rigidity-type theorem is proven for a type of geometric incidencesystem. The theorem characterizes the hypergraphs of subspace arrangements thatgenerically yield (a) at least one dictionary (b) a locally unique dictionary(i.e.\ at most a finite number of isolated dictionaries) of the specified size.We are unaware of prior application of combinatorial rigidity techniques in thesetting of Dictionary Learning, or even in machine learning. We also provide asystematic classification of problems related to Dictionary Learning togetherwith various algorithms, their assumptions and performance.",http://arxiv.org/abs/1402.7344v2,,
1228,"Generalized Gradient Learning on Time Series under Elastic  Transformations","The majority of machine learning algorithms assumes that objects arerepresented as vectors. But often the objects we want to learn on are morenaturally represented by other data structures such as sequences and timeseries. For these representations many standard learning algorithms areunavailable. We generalize gradient-based learning algorithms to time seriesunder dynamic time warping. To this end, we introduce elastic functions, whichextend functions on time series to matrix spaces. Necessary conditions arepresented under which generalized gradient learning on time series isconsistent. We indicate how results carry over to arbitrary elastic distancefunctions and to sequences consisting of symbolic elements. Specifically, fourlinear classifiers are extended to time series under dynamic time warping andapplied to benchmark datasets. Results indicate that generalized gradientlearning via elastic functions have the potential to complement thestate-of-the-art in statistical pattern recognition on time series.",http://arxiv.org/abs/1502.04843v2,,
1229,Learning with hidden variables,"Learning and inferring features that generate sensory input is a taskcontinuously performed by cortex. In recent years, novel algorithms andlearning rules have been proposed that allow neural network models to learnsuch features from natural images, written text, audio signals, etc. Thesenetworks usually involve deep architectures with many layers of hidden neurons.Here we review recent advancements in this area emphasizing, amongst otherthings, the processing of dynamical inputs by networks with hidden nodes andthe role of single neuron models. These points and the questions they arise canprovide conceptual advancements in understanding of learning in the cortex andthe relationship between machine learning approaches to learning with hiddennodes and those in cortical circuits.",http://arxiv.org/abs/1506.00354v2,,
1230,Accelerating Deep Learning with Shrinkage and Recall,"Deep Learning is a very powerful machine learning model. Deep Learning trainsa large number of parameters for multiple layers and is very slow when data isin large scale and the architecture size is large. Inspired from the shrinkingtechnique used in accelerating computation of Support Vector Machines (SVM)algorithm and screening technique used in LASSO, we propose a shrinking DeepLearning with recall (sDLr) approach to speed up deep learning computation. Weexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 datasets. Results show that the speedup using shrinking Deep Learning with recall(sDLr) can reach more than 2.0 while still giving competitive classificationperformance.",http://arxiv.org/abs/1605.01369v2,,
1231,"Stochastic Averaging for Constrained Optimization with Application to  Online Resource Allocation","Existing approaches to resource allocation for nowadays stochastic networksare challenged to meet fast convergence and tolerable delay requirements. Thepresent paper leverages online learning advances to facilitate stochasticresource allocation tasks. By recognizing the central role of Lagrangemultipliers, the underlying constrained optimization problem is formulated as amachine learning task involving both training and operational modes, with thegoal of learning the sought multipliers in a fast and efficient manner. To thisend, an order-optimal offline learning approach is developed first for batchtraining, and it is then generalized to the online setting with a proceduretermed learn-and-adapt. The novel resource allocation protocol permeatesbenefits of stochastic approximation and statistical learning to obtainlow-complexity online updates with learning errors close to the statisticalaccuracy limits, while still preserving adaptation performance, which in thestochastic network optimization context guarantees queue stability. Analysisand simulated tests demonstrate that the proposed data-driven approach improvesthe delay and convergence performance of existing resource allocation schemes.",http://arxiv.org/abs/1610.02143v2,,
1232,Constraint Selection in Metric Learning,"A number of machine learning algorithms are using a metric, or a distance, inorder to compare individuals. The Euclidean distance is usually employed, butit may be more efficient to learn a parametric distance such as Mahalanobismetric. Learning such a metric is a hot topic since more than ten years now,and a number of methods have been proposed to efficiently learn it. However,the nature of the problem makes it quite difficult for large scale data, aswell as data for which classes overlap. This paper presents a simple way ofimproving accuracy and scalability of any iterative metric learning algorithm,where constraints are obtained prior to the algorithm. The proposed approachrelies on a loss-dependent weighted selection of constraints that are used forlearning the metric. Using the corresponding dedicated loss function, themethod clearly allows to obtain better results than state-of-the-art methods,both in terms of accuracy and time complexity. Some experimental results onreal world, and potentially large, datasets are demonstrating the effectivenessof our proposition.",http://arxiv.org/abs/1612.04853v1,,
1233,Multi-Task Learning Using Neighborhood Kernels,"This paper introduces a new and effective algorithm for learning kernels in aMulti-Task Learning (MTL) setting. Although, we consider a MTL scenario here,our approach can be easily applied to standard single task learning, as well.As shown by our empirical results, our algorithm consistently outperforms thetraditional kernel learning algorithms such as uniform combination solution,convex combinations of base kernels as well as some kernel alignment-basedmodels, which have been proven to give promising results in the past. Wepresent a Rademacher complexity bound based on which a new Multi-Task MultipleKernel Learning (MT-MKL) model is derived. In particular, we propose a SupportVector Machine-regularized model in which, for each task, an optimal kernel islearned based on a neighborhood-defining kernel that is not restricted to bepositive semi-definite. Comparative experimental results are showcased thatunderline the merits of our neighborhood-defining framework in bothclassification and regression problems.",http://arxiv.org/abs/1707.03426v1,,
1234,"ByRDiE: Byzantine-resilient distributed coordinate descent for  decentralized learning","Distributed machine learning algorithms enable learning of models fromdatasets that are distributed over a network without gathering the data at acentralized location. While efficient distributed algorithms have beendeveloped under the assumption of faultless networks, failures that can renderthese algorithms nonfunctional occur frequently in the real world. This paperfocuses on the problem of Byzantine failures, which are the hardest tosafeguard against in distributed algorithms. While Byzantine fault tolerancehas a rich history, existing work does not translate into efficient andpractical algorithms for high-dimensional distributed learning. In this paper,an algorithm termed Byzantine-resilient distributed coordinate descent (ByRDiE)is developed and analyzed that enables distributed learning in the presence ofByzantine failures. Theoretical analysis and numerical experiments highlightthe usefulness of ByRDiE for high-dimensional (convex and nonconvex)distributed learning problems in the presence of Byzantine failures.",http://arxiv.org/abs/1708.08155v3,,
1235,"Predict Responsibly: Improving Fairness and Accuracy by Learning to  Defer","In many machine learning applications, there are multiple decision-makersinvolved, both automated and human. The interaction between these agents oftengoes unaddressed in algorithmic development. In this work, we explore a simpleversion of this interaction with a two-stage framework containing an automatedmodel and an external decision-maker. The model can choose to say ""Pass"", andpass the decision downstream, as explored in rejection learning. We extend thisconcept by proposing ""learning to defer"", which generalizes rejection learningby considering the effect of other agents in the decision-making process. Wepropose a learning algorithm which accounts for potential biases held byexternal decision-makers in a system. Experiments demonstrate that learning todefer can make systems not only more accurate but also less biased. Even whenworking with inconsistent or biased users, we show that deferring models stillgreatly improve the accuracy and/or fairness of the entire system.",http://arxiv.org/abs/1711.06664v3,,
1236,Non-linear motor control by local learning in spiking neural networks,"Learning weights in a spiking neural network with hidden neurons, usinglocal, stable and online rules, to control non-linear body dynamics is an openproblem. Here, we employ a supervised scheme, Feedback-based Online LocalLearning Of Weights (FOLLOW), to train a network of heterogeneous spikingneurons with hidden layers, to control a two-link arm so as to reproduce adesired state trajectory. The network first learns an inverse model of thenon-linear dynamics, i.e. from state trajectory as input to the network, itlearns to infer the continuous-time command that produced the trajectory.Connection weights are adjusted via a local plasticity rule that involvespre-synaptic firing and post-synaptic feedback of the error in the inferredcommand. We choose a network architecture, termed differential feedforward,that gives the lowest test error from different feedforward and recurrentarchitectures. The learned inverse model is then used to generate acontinuous-time motor command to control the arm, given a desired trajectory.",http://arxiv.org/abs/1712.10158v1,,
1237,"Tunneling Neural Perception and Logic Reasoning through Abductive  Learning","Perception and reasoning are basic human abilities that are seamlesslyconnected as part of human intelligence. However, in current machine learningsystems, the perception and reasoning modules are incompatible. Tasks requiringjoint perception and reasoning ability are difficult to accomplish autonomouslyand still demand human intervention. Inspired by the way language expertsdecoded Mayan scripts by joining two abilities in an abductive manner, thispaper proposes the abductive learning framework. The framework learnsperception and reasoning simultaneously with the help of a trial-and-errorabductive process. We present the Neural-Logical Machine as an implementationof this novel learning framework. We demonstrate that--using human-likeabductive learning--the machine learns from a small set of simple hand-writtenequations and then generalizes well to complex equations, a feat that is beyondthe capability of state-of-the-art neural network models. The abductivelearning framework explores a new direction for approaching human-levellearning ability.",http://arxiv.org/abs/1802.01173v2,,
1238,Hierarchical Imitation and Reinforcement Learning,"We study how to effectively leverage expert feedback to learn sequentialdecision-making policies. We focus on problems with sparse rewards and longtime horizons, which typically pose significant challenges in reinforcementlearning. We propose an algorithmic framework, called hierarchical guidance,that leverages the hierarchical structure of the underlying problem tointegrate different modes of expert interaction. Our framework can incorporatedifferent combinations of imitation learning (IL) and reinforcement learning(RL) at different levels, leading to dramatic reductions in both expert effortand cost of exploration. Using long-horizon benchmarks, including Montezuma'sRevenge, we demonstrate that our approach can learn significantly faster thanhierarchical RL, and be significantly more label-efficient than standard IL. Wealso theoretically analyze labeling cost for certain instantiations of ourframework.",http://arxiv.org/abs/1803.00590v2,,
1239,Differentiable Submodular Maximization,"We consider learning of submodular functions from data. These functions areimportant in machine learning and have a wide range of applications, e.g. datasummarization, feature selection and active learning. Despite theircombinatorial nature, submodular functions can be maximized approximately withstrong theoretical guarantees in polynomial time. Typically, learning thesubmodular function and optimization of that function are treated separately,i.e. the function is first learned using a proxy objective and subsequentlymaximized. In contrast, we show how to perform learning and optimizationjointly. By interpreting the output of greedy maximization algorithms asdistributions over sequences of items and smoothening these distributions, weobtain a differentiable objective. In this way, we can differentiate throughthe maximization algorithms and optimize the model to work well with theoptimization algorithm. We theoretically characterize the error made by ourapproach, yielding insights into the tradeoff of smoothness and accuracy. Wedemonstrate the effectiveness of our approach for jointly learning andoptimizing on synthetic maximum cut data, and on real world applications suchas product recommendation and image collection summarization.",http://arxiv.org/abs/1803.01785v2,,
1240,"Learning the Localization Function: Machine Learning Approach to  Fingerprinting Localization","Considered as a data-driven approach, Fingerprinting Localization Solutions(FPSs) enjoy huge popularity due to their good performance and minimalenvironment information requirement. This papers addresses applications ofartificial intelligence to solve two problems in Received Signal StrengthIndicator (RSSI) based FPS, first the cumbersome training database constructionand second the extrapolation of fingerprinting algorithm for similar buildingswith slight environmental changes. After a concise overview of deep learningdesign techniques, two main techniques widely used in deep learning areexploited for the above mentioned issues namely data augmentation and transferlearning. We train a multi-layer neural network that learns the mapping fromthe observations to the locations. A data augmentation method is proposed toincrease the training database size based on the structure of RSSI measurementsand hence reducing effectively the amount of training data. Then it is shownexperimentally how a model trained for a particular building can be transferredto a similar one by fine tuning with significantly smaller training numbers.The paper implicitly discusses the new guidelines to consider about deeplearning designs when they are employed in a new application context.",http://arxiv.org/abs/1803.08153v1,,
1241,RMDL: Random Multimodel Deep Learning for Classification,"The continually increasing number of complex datasets each year necessitatesever improving machine learning methods for robust and accurate categorizationof these data. This paper introduces Random Multimodel Deep Learning (RMDL): anew ensemble, deep learning approach for classification. Deep learning modelshave achieved state-of-the-art results across many domains. RMDL solves theproblem of finding the best deep learning structure and architecture whilesimultaneously improving robustness and accuracy through ensembles of deeplearning architectures. RDML can accept as input a variety data to includetext, video, images, and symbolic. This paper describes RMDL and shows testresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,and 20newsgroup. These test results show that RDML produces consistently betterperformance than standard methods over a broad range of data types andclassification problems.",http://arxiv.org/abs/1805.01890v2,,
1242,Multi-Agent Deep Reinforcement Learning with Human Strategies,"Deep learning has enabled traditional reinforcement learning methods to dealwith high-dimensional problems. However, one of the disadvantages of deepreinforcement learning methods is the limited exploration capacity of learningagents. In this paper, we introduce an approach that integrates humanstrategies to increase the exploration capacity of multiple deep reinforcementlearning agents. We also report the development of our own multi-agentenvironment called Multiple Tank Defence to simulate the proposed approach. Theresults show the significant performance improvement of multiple agents thathave learned cooperatively with human strategies. This implies that there is acritical need for human intellect teamed with machines to solve complexproblems. In addition, the success of this simulation indicates that ourdeveloped multi-agent environment can be used as a testbed platform to developand validate other multi-agent control algorithms. Details of the environmentimplementation can be referred tohttp://www.deakin.edu.au/~thanhthi/madrl_human.htm",http://arxiv.org/abs/1806.04562v1,,
1243,Separability is not the best goal for machine learning,"Neural networks use their hidden layers to transform input data into linearlyseparable data clusters, with a linear or a perceptron type output layer makingthe final projection on the line perpendicular to the discriminatinghyperplane. For complex data with multimodal distributions this transformationis difficult to learn. Projection on $k\geq 2$ line segments is the simplestextension of linear separability, defining much easier goal for the learningprocess. Simple problems are 2-separable, but problems with inherent complexlogic may be solved in a simple way by $k$-separable projections. Thedifficulty of learning non-linear data distributions is shifted to separationof line intervals, simplifying the transformation of data by hidden networklayers. For classification of difficult Boolean problems, such as the parityproblem, linear projection combined with \ksep is sufficient and provides apowerful new target for learning. More complex targets may also be defined,changing the goal of learning from linear discrimination to creation of datadistributions that can easily be handled by specialized models selected toanalyze output distributions. This approach can replace many layers oftransformation required by deep learning models.",http://arxiv.org/abs/1807.02873v1,,
1244,Learning Data-adaptive Nonparametric Kernels,"Traditional kernels or their combinations are often not sufficiently flexibleto fit the data in complicated practical tasks. In this paper, we present aData-Adaptive Nonparametric Kernel (DANK) learning framework by imposing anadaptive matrix on the kernel/Gram matrix in an entry-wise strategy. Since wedo not specify the formulation of the adaptive matrix, each entry in it can bedirectly and flexibly learned from the data. Therefore, the solution space ofthe learned kernel is largely expanded, which makes DANK flexible to adapt tothe data. Specifically, the proposed kernel learning framework can beseamlessly embedded to support vector machines (SVM) and support vectorregression (SVR), which has the capability of enlarging the margin betweenclasses and reducing the model generalization error. Theoretically, wedemonstrate that the objective function of our devised model isgradient-Lipschitz continuous. Thereby, the training process for kernel andparameter learning in SVM/SVR can be efficiently optimized in a unifiedframework. Further, to address the scalability issue in DANK, adecomposition-based scalable approach is developed, of which the effectivenessis demonstrated by both empirical studies and theoretical guarantees.Experimentally, our method outperforms other representative kernel learningbased algorithms on various classification and regression benchmark datasets.",http://arxiv.org/abs/1808.10724v1,,
1245,"PILAE: A Non-gradient Descent Learning Scheme for Deep Feedforward  Neural Networks","In this work, a non-gradient descent learning scheme is proposed for deepfeedforward neural networks (DNN). As we known, autoencoder can be used as thebuilding blocks of the multi-layer perceptron (MLP) deep neural network. So,the MLP will be taken as an example to illustrate the proposed scheme ofpseudoinverse learning algorithm for autoencoder (PILAE) training. The PILAEwith low rank approximation is a non-gradient based learning algorithm, and theencoder weight matrix is set to be the low rank approximation of thepseudoinverse of the input matrix, while the decoder weight matrix iscalculated by the pseudoinverse learning algorithm. It is worth to note thatonly few network structure hyperparameters need to be tuned. Hence, theproposed algorithm can be regarded as a quasi-automated training algorithmwhich can be utilized in autonomous machine learning research field. Theexperimental results show that the proposed learning scheme for DNN can achievebetter performance on considering the tradeoff between training efficiency andclassification accuracy.",http://arxiv.org/abs/1811.01545v1,,
1246,Multiple Instance Learning for ECG Risk Stratification,"In this paper, we apply a multiple instance learning paradigm to signal-basedrisk stratification for cardiovascular outcomes. In contrast to methods thatrequire hand-crafted features or domain knowledge, our method learns arepresentation with state-of-the-art predictive power from the raw ECG signal.We accomplish this by leveraging the multiple instance learning framework. Thisframework is particularly valuable to learning from biometric signals, wherepatient-level labels are available but signal segments are rarely annotated. Wemake two contributions in this paper: 1) reframing risk stratification forcardiovascular death (CVD) as a multiple instance learning problem, and 2)using this framework to design a new risk score, for which patients in thehighest quartile are 15.9 times more likely to die of CVD within 90 days ofhospital admission for an acute coronary syndrome.",http://arxiv.org/abs/1812.00475v1,,
1247,"A Model for Learned Bloom Filters, and Optimizing by Sandwiching","Recent work has suggested enhancing Bloom filters by using a pre-filter,based on applying machine learning to determine a function that models the dataset the Bloom filter is meant to represent. Here we model such learned Bloomfilters,, with the following outcomes: (1) we clarify what guarantees can andcannot be associated with such a structure; (2) we show how to estimate whatsize the learning function must obtain in order to obtain improved performance;(3) we provide a simple method, sandwiching, for optimizing learned Bloomfilters; and (4) we propose a design and analysis approach for a learnedBloomier filter, based on our modeling approach.",http://arxiv.org/abs/1901.00902v1,,
1248,Interpretable Reinforcement Learning via Differentiable Decision Trees,"Decision trees are ubiquitous in machine learning for their ease of use andinterpretability; however, they are not typically implemented in reinforcementlearning because they cannot be updated via stochastic gradient descent.Traditional applications of decision trees for reinforcement learning havefocused instead on making commitments to decision boundaries as the tree isgrown one layer at a time. We overcome this critical limitation by allowing fora gradient update over the entire tree structure that improves samplecomplexity when a tree is fuzzy and interpretability when sharp. We offer threekey contributions towards this goal. First, we motivate the need for policygradient-based learning by examining the theoretical properties of gradientdescent over differentiable decision trees. Second, we introduce aregularization framework that yields interpretability via sparsity in the treestructure. Third, we demonstrate the ability to construct a decision tree viapolicy gradient in canonical reinforcement learning domains and supervisedlearning benchmarks.",http://arxiv.org/abs/1903.09338v1,,
1249,Computational and Robotic Models of Early Language Development: A Review,"We review computational and robotics models of early language learning anddevelopment. We first explain why and how these models are used to understandbetter how children learn language. We argue that they provide concretetheories of language learning as a complex dynamic system, complementingtraditional methods in psychology and linguistics. We review different modelingformalisms, grounded in techniques from machine learning and artificialintelligence such as Bayesian and neural network approaches. We then discusstheir role in understanding several key mechanisms of language development:cross-situational statistical learning, embodiment, situated socialinteraction, intrinsically motivated learning, and cultural evolution. Weconclude by discussing future challenges for research, including modeling oflarge-scale empirical data about language acquisition in real-worldenvironments.  Keywords: Early language learning, Computational and robotic models, machinelearning, development, embodiment, social interaction, intrinsic motivation,self-organization, dynamical systems, complexity.",http://arxiv.org/abs/1903.10246v1,,
1250,Domain Independent SVM for Transfer Learning in Brain Decoding,"Brain imaging data are important in brain sciences yet expensive to obtain,with big volume (i.e., large p) but small sample size (i.e., small n). Totackle this problem, transfer learning is a promising direction that leveragessource data to improve performance on related, target data. Most transferlearning methods focus on minimizing data distribution mismatch. However, a bigchallenge in brain imaging is the large domain discrepancies in cognitiveexperiment designs and subject-specific structures and functions. A recenttransfer learning approach minimizes domain dependence to learn common featuresacross domains, via the Hilbert-Schmidt Independence Criterion (HSIC). Inspiredby this method, we propose a new Domain Independent Support Vector Machine(DI-SVM) for transfer learning in brain condition decoding. Specifically,DI-SVM simultaneously minimizes the SVM empirical risk and the dependence ondomain information via a simplified HSIC. We use public data to construct 13transfer learning tasks in brain decoding, including three interestingmulti-source transfer tasks. Experiments show that DI-SVM's superiorperformance over eight competing methods on these tasks, particularly animprovement of more than 24% on multi-source transfer tasks.",http://arxiv.org/abs/1903.11020v1,,
1251,Coded Elastic Computing,"Cloud providers have recently introduced low-priority machines to reduce thecost of computations. Exploiting such opportunity for machine learning tasks ischallenging inasmuch as low-priority machines can elastically leave (throughpreemption) and join the computation at any time. In this paper, we design anew technique called coded elastic computing enabling distributed machinelearning computations over elastic resources. The proposed technique allowsmachines to transparently leave the computation without sacrificing thealgorithm-level performance, and, at the same time, flexibly reduce theworkload at existing machines when new machines join the computation. Thanks tothe redundancy provided by encoding, our approach is able to achieve similarcomputational cost as the original (uncoded) method when all machines arepresent; the cost gracefully increases when machines are preempted and reduceswhen machines join. We test the performance of the proposed technique on twomini-benchmark experiments, namely elastic matrix multiplications and linearregression. Our preliminary experimental results show improvements over severalexisting techniques.",http://arxiv.org/abs/1812.06411v2,,
1252,"Analysis of on-line learning when a moving teacher goes around a true  teacher","In the framework of on-line learning, a learning machine might move around ateacher due to the differences in structures or output functions between theteacher and the learning machine or due to noises. The generalizationperformance of a new student supervised by a moving machine has been analyzed.A model composed of a true teacher, a moving teacher and a student that are alllinear perceptrons with noises has been treated analytically using statisticalmechanics. It has been proven that the generalization errors of a student canbe smaller than that of a moving teacher, even if the student only usesexamples from the moving teacher.",http://arxiv.org/abs/physics/0509050v1,,
1253,Predictive Hypothesis Identification,"While statistics focusses on hypothesis testing and on estimating (propertiesof) the true sampling distribution, in machine learning the performance oflearning algorithms on future data is the primary issue. In this paper webridge the gap with a general principle (PHI) that identifies hypotheses withbest predictive performance. This includes predictive point and intervalestimation, simple and composite hypothesis testing, (mixture) model selection,and others as special cases. For concrete instantiations we will recoverwell-known methods, variations thereof, and new ones. PHI nicely justifies,reconciles, and blends (a reparametrization invariant variation of) MAP, ML,MDL, and moment estimation. One particular feature of PHI is that it cangenuinely deal with nested hypotheses.",http://arxiv.org/abs/0809.1270v1,,
1254,Teraflop-scale Incremental Machine Learning,"We propose a long-term memory design for artificial general intelligencebased on Solomonoff's incremental machine learning methods. We use R5RS Schemeand its standard library with a few omissions as the reference machine. Weintroduce a Levin Search variant based on Stochastic Context Free Grammartogether with four synergistic update algorithms that use the same grammar as aguiding probability distribution of programs. The update algorithms includeadjusting production probabilities, re-using previous solutions, learningprogramming idioms and discovery of frequent subprograms. Experiments with twotraining sequences demonstrate that our approach to incremental learning iseffective.",http://arxiv.org/abs/1103.1003v1,,
1255,A Method for Comparing Hedge Funds,"The paper presents new machine learning methods: signal composition, whichclassifies time-series regardless of length, type, and quantity; andself-labeling, a supervised-learning enhancement. The paper describes furtherthe implementation of the methods on a financial search engine system toidentify behavioral similarities among time-series representing monthly returnsof 11,312 hedge funds operated during approximately one decade (2000 - 2010).The presented approach of cross-category and cross-location classificationassists the investor to identify alternative investments.",http://arxiv.org/abs/1303.0073v2,,
1256,Inverse Signal Classification for Financial Instruments,"The paper presents new machine learning methods: signal composition, whichclassifies time-series regardless of length, type, and quantity; andself-labeling, a supervised-learning enhancement. The paper describes furtherthe implementation of the methods on a financial search engine system using acollection of 7,881 financial instruments traded during 2011 to identifyinverse behavior among the time-series.",http://arxiv.org/abs/1303.0283v2,,
1257,The algorithm of noisy k-means,"In this note, we introduce a new algorithm to deal with finite dimensionalclustering with errors in variables. The design of this algorithm is based onrecent theoretical advances (see Loustau (2013a,b)) in statistical learningwith errors in variables. As the previous mentioned papers, the algorithm mixesdifferent tools from the inverse problem literature and the machine learningcommunity. Coarsely, it is based on a two-step procedure: (1) a deconvolutionstep to deal with noisy inputs and (2) Newton's iterations as the populark-means.",http://arxiv.org/abs/1308.3314v1,,
1258,Predicting User Actions in Software Processes,"This paper describes an approach for user (e.g. SW architect) assisting insoftware processes. The approach observes the user's action and tries topredict his next step. For this we use approaches in the area of machinelearning (sequence learning) and adopt these for the use in software processes.  Keywords: Software engineering, Software process description languages,Software processes, Machine learning, Sequence prediction",http://arxiv.org/abs/1110.1301v1,,
1259,Estimating a Causal Order among Groups of Variables in Linear Models,"The machine learning community has recently devoted much attention to theproblem of inferring causal relationships from statistical data. Most of thiswork has focused on uncovering connections among scalar random variables. Wegeneralize existing methods to apply to collections of multi-dimensional randomvectors, focusing on techniques applicable to linear models. The performance ofthe resulting algorithms is evaluated and compared in simulations, which showthat our methods can, in many cases, provide useful information on causalrelationships even for relatively small sample sizes.",http://arxiv.org/abs/1207.1977v1,,
1260,Matrix Approximation under Local Low-Rank Assumption,"Matrix approximation is a common tool in machine learning for buildingaccurate prediction models for recommendation systems, text mining, andcomputer vision. A prevalent assumption in constructing matrix approximationsis that the partially observed matrix is of low-rank. We propose a new matrixapproximation model where we assume instead that the matrix is only locally oflow-rank, leading to a representation of the observed matrix as a weighted sumof low-rank matrices. We analyze the accuracy of the proposed local low-rankmodeling. Our experiments show improvements in prediction accuracy inrecommendation tasks.",http://arxiv.org/abs/1301.3192v1,,
1261,A Hybrid Monte Carlo Architecture for Parameter Optimization,"Much recent research has been conducted in the area of Bayesian learning,particularly with regard to the optimization of hyper-parameters via Gaussianprocess regression. The methodologies rely chiefly on the method of maximizingthe expected improvement of a score function with respect to adjustments in thehyper-parameters. In this work, we present a novel algorithm that exploitsnotions of confidence intervals and uncertainties to enable the discovery ofthe best optimal within a targeted region of the parameter space. Wedemonstrate the efficacy of our algorithm with respect to machine learningproblems and show cases where our algorithm is competitive with the method ofmaximizing expected improvement.",http://arxiv.org/abs/1405.2377v1,,
1262,Semi-Supervised Learning with Generative Adversarial Networks,"We extend Generative Adversarial Networks (GANs) to the semi-supervisedcontext by forcing the discriminator network to output class labels. We train agenerative model G and a discriminator D on a dataset with inputs belonging toone of N classes. At training time, D is made to predict which of N+1 classesthe input belongs to, where an extra class is added to correspond to theoutputs of G. We show that this method can be used to create a moredata-efficient classifier and that it allows for generating higher qualitysamples than a regular GAN.",http://arxiv.org/abs/1606.01583v2,,
1263,Distributed Parameter Estimation via Pseudo-likelihood,"Estimating statistical models within sensor networks requires distributedalgorithms, in which both data and computation are distributed across the nodesof the network. We propose a general approach for distributed learning based oncombining local estimators defined by pseudo-likelihood components,encompassing a number of combination methods, and provide both theoretical andexperimental analysis. We show that simple linear combination or max-votingmethods, when combined with second-order information, are statisticallycompetitive with more advanced and costly joint optimization. Our algorithmshave many attractive properties including low communication and computationalcost and ""any-time"" behavior.",http://arxiv.org/abs/1206.6420v1,,
1264,"apsis - Framework for Automated Optimization of Machine Learning Hyper  Parameters","The apsis toolkit presented in this paper provides a flexible framework forhyperparameter optimization and includes both random search and a bayesianoptimizer. It is implemented in Python and its architecture featuresadaptability to any desired machine learning code. It can easily be used withcommon Python ML frameworks such as scikit-learn. Published under the MITLicense other researchers are heavily encouraged to check out the code,contribute or raise any suggestions. The code can be found atgithub.com/FrederikDiehl/apsis.",http://arxiv.org/abs/1503.02946v2,,
1265,XGBoost: A Scalable Tree Boosting System,"Tree boosting is a highly effective and widely used machine learning method.In this paper, we describe a scalable end-to-end tree boosting system calledXGBoost, which is used widely by data scientists to achieve state-of-the-artresults on many machine learning challenges. We propose a novel sparsity-awarealgorithm for sparse data and weighted quantile sketch for approximate treelearning. More importantly, we provide insights on cache access patterns, datacompression and sharding to build a scalable tree boosting system. By combiningthese insights, XGBoost scales beyond billions of examples using far fewerresources than existing systems.",http://arxiv.org/abs/1603.02754v3,,
1266,"Functional Mixture Discriminant Analysis with hidden process regression  for curve classification","We present a new mixture model-based discriminant analysis approach forfunctional data using a specific hidden process regression model. The approachallows for fitting flexible curve-models to each class of complex-shaped curvespresenting regime changes. The model parameters are learned by maximizing theobserved-data log-likelihood for each class by using a dedicatedexpectation-maximization (EM) algorithm. Comparisons on simulated data withalternative approaches show that the proposed approach provides better results.",http://arxiv.org/abs/1312.7007v1,,
1267,Local Nonstationarity for Efficient Bayesian Optimization,"Bayesian optimization has shown to be a fundamental global optimizationalgorithm in many applications: ranging from automatic machine learning,robotics, reinforcement learning, experimental design, simulations, etc. Themost popular and effective Bayesian optimization relies on a surrogate model inthe form of a Gaussian process due to its flexibility to represent a prior overfunction. However, many algorithms and setups relies on the stationarityassumption of the Gaussian process. In this paper, we present a novelnonstationary strategy for Bayesian optimization that is able to outperform thestate of the art in Bayesian optimization both in stationary and nonstationaryproblems.",http://arxiv.org/abs/1506.02080v1,,
1268,"Use it or Lose it: Selective Memory and Forgetting in a Perpetual  Learning Machine","In a recent article we described a new type of deep neural network - aPerpetual Learning Machine (PLM) - which is capable of learning 'on the fly'like a brain by existing in a state of Perpetual Stochastic Gradient Descent(PSGD). Here, by simulating the process of practice, we demonstrate bothselective memory and selective forgetting when we introduce statistical recallbiases during PSGD. Frequently recalled memories are remembered, whilstmemories recalled rarely are forgotten. This results in a 'use it or lose it'stimulus driven memory process that is similar to human memory.",http://arxiv.org/abs/1509.03185v1,,
1269,Survey on Feature Selection,"Feature selection plays an important role in the data mining process. It isneeded to deal with the excessive number of features, which can become acomputational burden on the learning algorithms. It is also necessary, evenwhen computational resources are not scarce, since it improves the accuracy ofthe machine learning tasks, as we will see in the upcoming sections. In thisreview, we discuss the different feature selection approaches, and the relationbetween them and the various machine learning algorithms.",http://arxiv.org/abs/1510.02892v1,,
1270,"On the Relationship between Online Gaussian Process Regression and  Kernel Least Mean Squares Algorithms","We study the relationship between online Gaussian process (GP) regression andkernel least mean squares (KLMS) algorithms. While the latter have no capacityof storing the entire posterior distribution during online learning, wediscover that their operation corresponds to the assumption of a fixedposterior covariance that follows a simple parametric model. Interestingly,several well-known KLMS algorithms correspond to specific cases of this model.The probabilistic perspective allows us to understand how each of them handlesuncertainty, which could explain some of their performance differences.",http://arxiv.org/abs/1609.03164v1,,
1271,A Perspective on Deep Imaging,"The combination of tomographic imaging and deep learning, or machine learningin general, promises to empower not only image analysis but also imagereconstruction. The latter aspect is considered in this perspective articlewith an emphasis on medical imaging to develop a new generation of imagereconstruction theories and techniques. This direction might lead tointelligent utilization of domain knowledge from big data, innovativeapproaches for image reconstruction, and superior performance in clinical andpreclinical applications. To realize the full impact of machine learning onmedical imaging, major challenges must be addressed.",http://arxiv.org/abs/1609.04375v2,,
1272,The RNN-ELM Classifier,"In this paper we examine learning methods combining the Random NeuralNetwork, a biologically inspired neural network and the Extreme LearningMachine that achieve state of the art classification performance whilerequiring much shorter training time. The Random Neural Network is a integrateand fire computational model of a neural network whose mathematical structurepermits the efficient analysis of large ensembles of neurons. An activationfunction is derived from the RNN and used in an Extreme Learning Machine. Wecompare the performance of this combination against the ELM with variousactivation functions, we reduce the input dimensionality via PCA and compareits performance vs. autoencoder based versions of the RNN-ELM.",http://arxiv.org/abs/1609.07724v1,,
1273,Missing Data Imputation for Supervised Learning,"Missing data imputation can help improve the performance of prediction modelsin situations where missing data hide useful information. This paper comparesmethods for imputing missing categorical data for supervised classificationtasks. We experiment on two machine learning benchmark datasets with missingcategorical data, comparing classifiers trained on non-imputed (i.e., one-hotencoded) or imputed data with different levels of additional missing-dataperturbation. We show imputation methods can increase predictive accuracy inthe presence of missing-data perturbation, which can actually improveprediction accuracy by regularizing the classifier. We achieve thestate-of-the-art on the Adult dataset with missing-data perturbation andk-nearest-neighbors (k-NN) imputation.",http://arxiv.org/abs/1610.09075v2,,
1274,"TorchCraft: a Library for Machine Learning Research on Real-Time  Strategy Games","We present TorchCraft, a library that enables deep learning research onReal-Time Strategy (RTS) games such as StarCraft: Brood War, by making iteasier to control these games from a machine learning framework, here Torch.This white paper argues for using RTS games as a benchmark for AI research, anddescribes the design and components of TorchCraft.",http://arxiv.org/abs/1611.00625v2,,
1275,"Reinforcement Learning Approach for Parallelization in Filters  Aggregation Based Feature Selection Algorithms","One of the classical problems in machine learning and data mining is featureselection. A feature selection algorithm is expected to be quick, and at thesame time it should show high performance. MeLiF algorithm effectively solvesthis problem using ensembles of ranking filters. This article describes twodifferent ways to improve MeLiF algorithm performance with parallelization.Experiments show that proposed schemes significantly improves algorithmperformance and increase feature selection quality.",http://arxiv.org/abs/1611.02047v1,,
1276,"Low Latency Anomaly Detection and Bayesian Network Prediction of Anomaly  Likelihood","We develop a supervised machine learning model that detects anomalies insystems in real time. Our model processes unbounded streams of data into timeseries which then form the basis of a low-latency anomaly detection model.Moreover, we extend our preliminary goal of just anomaly detection tosimultaneous anomaly prediction. We approach this very challenging problem bydeveloping a Bayesian Network framework that captures the information about theparameters of the lagged regressors calibrated in the first part of ourapproach and use this structure to learn local conditional probabilitydistributions.",http://arxiv.org/abs/1611.03898v1,,
1277,"Learning Interpretability for Visualizations using Adapted Cox Models  through a User Experiment","In order to be useful, visualizations need to be interpretable. This paperuses a user-based approach to combine and assess quality measures in order tobetter model user preferences. Results show that cluster separability measuresare outperformed by a neighborhood conservation measure, even though the formerare usually considered as intuitively representative of user motives. Moreover,combining measures, as opposed to using a single measure, further improvesprediction performances.",http://arxiv.org/abs/1611.06175v1,,
1278,Intra-day Activity Better Predicts Chronic Conditions,"In this work we investigate intra-day patterns of activity on a population of7,261 users of mobile health wearable devices and apps. We show that: (1) usingintra-day step and sleep data recorded from passive trackers significantlyimproves classification performance on self-reported chronic conditions relatedto mental health and nervous system disorders, (2) Convolutional NeuralNetworks achieve top classification performance vs. baseline models whentrained directly on multivariate time series of activity data, and (3) jointlypredicting all condition classes via multi-task learning can be leveraged toextract features that generalize across data sets and achieve the highestclassification performance.",http://arxiv.org/abs/1612.01200v1,,
1279,Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe,"We consider the problem of bandit optimization, inspired by stochasticoptimization and online learning problems with bandit feedback. In thisproblem, the objective is to minimize a global loss function of all theactions, not necessarily a cumulative loss. This framework allows us to study avery general class of problems, with applications in statistics, machinelearning, and other fields. To solve this problem, we analyze theUpper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits andconvex optimization. We give theoretical guarantees for the performance of thisalgorithm over various classes of functions, and discuss the optimality ofthese results.",http://arxiv.org/abs/1702.06917v2,,
1280,Iterative Machine Learning for Output Tracking,"This article develops iterative machine learning (IML) for output tracking.The input-output data generated during iterations to develop the model used inthe iterative update. The main contribution of this article to propose the useof kernel-based machine learning to iteratively update both the model and themodel-inversion-based input simultaneously. Additionally, augmented inputs withpersistency of excitation are proposed to promote learning of the model duringthe iteration process. The proposed approach is illustrated with a simulationexample.",http://arxiv.org/abs/1705.07826v2,,
1281,Techniques for visualizing LSTMs applied to electrocardiograms,"This paper explores four different visualization techniques for longshort-term memory (LSTM) networks applied to continuous-valued time series. Onthe datasets analysed, we find that the best visualization technique is tolearn an input deletion mask that optimally reduces the true class score. Witha specific focus on single-lead electrocardiograms from the MIT-BIH arrhythmiadataset, we show that salient input features for the LSTM classifier align wellwith medical theory.",http://arxiv.org/abs/1705.08153v3,,
1282,Machine Learning Algorithms for $b$-Jet Tagging at the ATLAS Experiment,"The separation of $b$-quark initiated jets from those coming from lighterquark flavors ($b$-tagging) is a fundamental tool for the ATLAS physics programat the CERN Large Hadron Collider. The most powerful $b$-tagging algorithmscombine information from low-level taggers, exploiting reconstructed track andvertex information, into machine learning classifiers. The potential of moderndeep learning techniques is explored using simulated events, and compared tothat achievable from more traditional classifiers such as boosted decisiontrees.",http://arxiv.org/abs/1711.08811v1,,
1283,Structured Optimal Transport,"Optimal Transport has recently gained interest in machine learning forapplications ranging from domain adaptation, sentence similarities to deeplearning. Yet, its ability to capture frequently occurring structure beyond the""ground metric"" is limited. In this work, we develop a nonlinear generalizationof (discrete) optimal transport that is able to reflect much additionalstructure. We demonstrate how to leverage the geometry of this new model forfast algorithms, and explore connections and properties. Illustrativeexperiments highlight the benefit of the induced structured couplings for tasksin domain adaptation and natural language processing.",http://arxiv.org/abs/1712.06199v1,,
1284,"Deep Learning for Identifying Potential Conceptual Shifts for  Co-creative Drawing","We present a system for identifying conceptual shifts between visualcategories, which will form the basis for a co-creative drawing system to helpusers draw more creative sketches. The system recognizes human sketches andmatches them to structurally similar sketches from categories to which they donot belong. This would allow a co-creative drawing system to produce anambiguous sketch that blends features from both categories.",http://arxiv.org/abs/1801.00723v1,,
1285,Model Theory and Machine Learning,"About 25 years ago, it came to light that a single combinatorial propertydetermines both an important dividing line in model theory (NIP) and machinelearning (PAC-learnability). The following years saw a fruitful exchange ofideas between PAC learning and the model theory of NIP structures. In thisarticle, we point out a new and similar connection between model theory andmachine learning, this time developing a correspondence between\emph{stability} and learnability in various settings of \emph{onlinelearning.} In particular, this gives many new examples of mathematicallyinteresting classes which are learnable in the online setting.",http://arxiv.org/abs/1801.06566v1,,
1286,"UMAP: Uniform Manifold Approximation and Projection for Dimension  Reduction","UMAP (Uniform Manifold Approximation and Projection) is a novel manifoldlearning technique for dimension reduction. UMAP is constructed from atheoretical framework based in Riemannian geometry and algebraic topology. Theresult is a practical scalable algorithm that applies to real world data. TheUMAP algorithm is competitive with t-SNE for visualization quality, andarguably preserves more of the global structure with superior run timeperformance. Furthermore, UMAP has no computational restrictions on embeddingdimension, making it viable as a general purpose dimension reduction techniquefor machine learning.",http://arxiv.org/abs/1802.03426v2,,
1287,Towards Deep Representation Learning with Genetic Programming,"Genetic Programming (GP) is an evolutionary algorithm commonly used formachine learning tasks. In this paper we present a method that allows GP totransform the representation of a large-scale machine learning dataset into amore compact representation, by means of processing features from the originalrepresentation at individual level. We develop as a proof of concept of thismethod an autoencoder. We tested a preliminary version of our approach in avariety of well-known machine learning image datasets. We speculate that thismethod, used in an iterative manner, can produce results competitive withstate-of-art deep neural networks.",http://arxiv.org/abs/1802.07133v1,,
1288,Generating Artificial Data for Private Deep Learning,"In this paper, we propose generating artificial data that retain statisticalproperties of real data as the means of providing privacy with respect to theoriginal dataset. We use generative adversarial network to drawprivacy-preserving artificial data samples and derive an empirical method toassess the risk of information disclosure in a differential-privacy-like way.Our experiments show that we are able to generate artificial data of highquality and successfully train and validate machine learning models on thisdata while limiting potential privacy loss.",http://arxiv.org/abs/1803.03148v2,,
1289,"A comparative study of feature selection methods for stress hotspot  classification in materials","The first step in constructing a machine learning model is defining thefeatures of the data set that can be used for optimal learning. In this work wediscuss feature selection methods, which can be used to build better models, aswell as achieve model interpretability. We applied these methods in the contextof stress hotspot classification problem, to determine what microstructuralcharacteristics can cause stress to build up in certain grains during uniaxialtensile deformation. The results show how some feature selection techniques arebiased and demonstrate a preferred technique to get feature rankings forphysical interpretations.",http://arxiv.org/abs/1804.09604v1,,
1290,Improved Classification Based on Deep Belief Networks,"For better classification generative models are used to initialize the modeland model features before training a classifier. Typically it is needed tosolve separate unsupervised and supervised learning problems. Generativerestricted Boltzmann machines and deep belief networks are widely used forunsupervised learning. We developed several supervised models based on DBN inorder to improve this two-phase strategy. Modifying the loss function toaccount for expectation with respect to the underlying generative model,introducing weight bounds, and multi-level programming are applied in modeldevelopment. The proposed models capture both unsupervised and supervisedobjectives effectively. The computational study verifies that our modelsperform better than the two-phase training approach.",http://arxiv.org/abs/1804.09812v1,,
1291,Real-time regression analysis with deep convolutional neural networks,"We discuss the development of novel deep learning algorithms to enablereal-time regression analysis for time series data. We showcase the applicationof this new method with a timely case study, and then discuss the applicabilityof this approach to tackle similar challenges across science domains.",http://arxiv.org/abs/1805.02716v1,,
1292,A One-Class Decision Tree Based on Kernel Density Estimation,"One-Class Classification (OCC) is a domain of machine learning which achievestraining by means of a single class sample. The present work aims at developinga one-class model which addresses concerns of both performance and readability.To this end, we propose a hybrid OCC method which relies on density estimationas part of a tree-based learning algorithm. Within a greedy and recursiveapproach, our proposal rests on kernel density estimation to split a datasubset on the basis of one or several intervals of interest. Our method showsfavorable performance in comparison with common methods of the literature on arange of benchmark datasets.",http://arxiv.org/abs/1805.05021v1,,
1293,Comparing Fairness Criteria Based on Social Outcome,"Fairness in algorithmic decision-making processes is attracting increasingconcern. When an algorithm is applied to human-related decision-making anestimator solely optimizing its predictive power can learn biases on theexisting data, which motivates us the notion of fairness in machine learning.while several different notions are studied in the literature, little studiesare done on how these notions affect the individuals. We demonstrate such acomparison between several policies induced by well-known fairness criteria,including the color-blind (CB), the demographic parity (DP), and the equalizedodds (EO). We show that the EO is the only criterion among them that removesgroup-level disparity. Empirical studies on the social welfare and disparity ofthese policies are conducted.",http://arxiv.org/abs/1806.05112v1,,
1294,"Machine learning with controllable quantum dynamics of a nuclear spin  ensemble in a solid","We experimentally demonstrate quantum machine learning using NMR based on aframework of quantum reservoir computing. Reservoir computing is for exploitingnatural nonlinear dynamics with large degrees of freedom, which is called areservoir, for a machine learning purpose. Here we propose a concrete physicalimplementation of a quantum reservoir using controllable dynamics of a nuclearspin ensemble in a molecular solid. In this implementation, we demonstratelearning of nonlinear functions with binary or continuous variable inputs withlow mean squared errors. Our implementation and demonstration paves a roadtoward exploiting quantum computational supremacy in NMR ensemble systems forinformation processing with reachable technologies.",http://arxiv.org/abs/1806.10910v1,,
1295,"Algorithms for solving optimization problems arising from deep neural  net models: smooth problems","Machine Learning models incorporating multiple layered learning networks havebeen seen to provide effective models for various classification problems. Theresulting optimization problem to solve for the optimal vector minimizing theempirical risk is, however, highly nonlinear. This presents a challenge toapplication and development of appropriate optimization algorithms for solvingthe problem. In this paper, we summarize the primary challenges involved andpresent the case for a Newton-based method incorporating directions of negativecurvature, including promising numerical results on data arising from securityanomally deetection.",http://arxiv.org/abs/1807.00172v1,,
1296,"Algorithms for solving optimization problems arising from deep neural  net models: nonsmooth problems","Machine Learning models incorporating multiple layered learning networks havebeen seen to provide effective models for various classification problems. Theresulting optimization problem to solve for the optimal vector minimizing theempirical risk is, however, highly nonconvex. This alone presents a challengeto application and development of appropriate optimization algorithms forsolving the problem. However, in addition, there are a number of interestingproblems for which the objective function is non- smooth and nonseparable. Inthis paper, we summarize the primary challenges involved, the state of the art,and present some numerical results on an interesting and representative classof problems.",http://arxiv.org/abs/1807.00173v1,,
1297,Confidence Intervals for Testing Disparate Impact in Fair Learning,"We provide the asymptotic distribution of the major indexes used in thestatistical literature to quantify disparate treatment in machine learning. Weaim at promoting the use of confidence intervals when testing the so-calledgroup disparate impact. We illustrate on some examples the importance of usingconfidence intervals and not a single value.",http://arxiv.org/abs/1807.06362v1,,
1298,"Comment on ""All-optical machine learning using diffractive deep neural  networks""","Lin et al. (Reports, 7 September 2018, p. 1004) reported a remarkableproposal that employs a passive, strictly linear optical setup to performpattern classifications. But interpreting the multilayer diffractive setup as adeep neural network and advocating it as an all-optical deep learning frameworkare not well justified and represent a mischaracterization of the system byoverlooking its defining characteristics of perfect linearity and strictpassivity.",http://arxiv.org/abs/1809.08360v2,,
1299,Learning Confidence Sets using Support Vector Machines,"The goal of confidence-set learning in the binary classification setting isto construct two sets, each with a specific probability guarantee to cover aclass. An observation outside the overlap of the two sets is deemed to be fromone of the two classes, while the overlap is an ambiguity region which couldbelong to either class. Instead of plug-in approaches, we propose a supportvector classifier to construct confidence sets in a flexible manner.Theoretically, we show that the proposed learner can control the non-coveragerates and minimize the ambiguity with high probability. Efficient algorithmsare developed and numerical studies illustrate the effectiveness of theproposed method.",http://arxiv.org/abs/1809.10818v1,,
1300,"Learning an internal representation of the end-effector configuration  space","Current machine learning techniques proposed to automatically discover arobot kinematics usually rely on a priori information about the robot'sstructure, sensors properties or end-effector position. This paper proposes amethod to estimate a certain aspect of the forward kinematics model with nosuch information. An internal representation of the end-effector configurationis generated from unstructured proprioceptive and exteroceptive data flow undervery limited assumptions. A mapping from the proprioceptive space to thisrepresentational space can then be used to control the robot.",http://arxiv.org/abs/1810.01866v1,,
1301,Machine Learning Suites for Online Toxicity Detection,"To identify and classify toxic online commentary, the modern tools of datascience transform raw text into key features from which either thresholding orlearning algorithms can make predictions for monitoring offensiveconversations. We systematically evaluate 62 classifiers representing 19 majoralgorithmic families against features extracted from the Jigsaw dataset ofWikipedia comments. We compare the classifiers based on statisticallysignificant differences in accuracy and relative execution time. Among theseclassifiers for identifying toxic comments, tree-based algorithms provide themost transparently explainable rules and rank-order the predictive contributionof each feature. Among 28 features of syntax, sentiment, emotion and outlierword dictionaries, a simple bad word list proves most predictive of offensivecommentary.",http://arxiv.org/abs/1810.01869v1,,
1302,Generalizing the theory of cooperative inference,"Cooperation information sharing is important to theories of human learningand has potential implications for machine learning. Prior work derivedconditions for achieving optimal Cooperative Inference given strong, relativelyrestrictive assumptions. We relax these assumptions by demonstratingconvergence for any discrete joint distribution, robustness through equivalenceclasses and stability under perturbation, and effectiveness by deriving boundsfrom structural properties of the original joint distribution. We providegeometric interpretations, connections to and implications for optimaltransport, and connections to importance sampling, and conclude by outliningopen questions and challenges to realizing the promise of CooperativeInference.",http://arxiv.org/abs/1810.02423v2,,
1303,Statistical Optimality of Interpolated Nearest Neighbor Algorithms,"In the era of deep learning, understanding over-fitting phenomenon becomesincreasingly important. It is observed that carefully designed deep neuralnetworks achieve small testing error even when the training error is close tozero. One possible explanation is that for many modern machine learningalgorithms, over-fitting can greatly reduce the estimation bias, while notincreasing the estimation variance too much. To illustrate the above idea, weprove that the proposed interpolated nearest neighbor algorithm achieves theminimax optimal rate in both regression and classification regimes, and observethat they are empirically better than the traditional $k$ nearest neighbormethod in some cases.",http://arxiv.org/abs/1810.02814v2,,
1304,A Simple Baseline Algorithm for Graph Classification,"Graph classification has recently received a lot of attention from variousfields of machine learning e.g. kernel methods, sequential modeling or graphembedding. All these approaches offer promising results with differentrespective strengths and weaknesses. However, most of them rely on complexmathematics and require heavy computational power to achieve their bestperformance. We propose a simple and fast algorithm based on the spectraldecomposition of graph Laplacian to perform graph classification and get afirst reference score for a dataset. We show that this method obtainscompetitive results compared to state-of-the-art algorithms.",http://arxiv.org/abs/1810.09155v2,,
1305,Planning in Dynamic Environments with Conditional Autoregressive Models,"We demonstrate the use of conditional autoregressive generative models (vanden Oord et al., 2016a) over a discrete latent space (van den Oord et al.,2017b) for forward planning with MCTS. In order to test this method, weintroduce a new environment featuring varying difficulty levels, along withmoving goals and obstacles. The combination of high-quality frame generationand classical planning approaches nearly matches true environment performancefor our task, demonstrating the usefulness of this method for model-basedplanning in dynamic environments.",http://arxiv.org/abs/1811.10097v1,,
1306,"Unsupervised learning with GLRM feature selection reveals novel  traumatic brain injury phenotypes","Baseline injury categorization is important to traumatic brain injury (TBI)research and treatment. Current categorization is dominated by symptom-basedscores that insufficiently capture injury heterogeneity. In this work, we applyunsupervised clustering to identify novel TBI phenotypes. Our approach uses ageneralized low-rank model (GLRM) model for feature selection in a procedureanalogous to wrapper methods. The resulting clusters reveal four novel TBIphenotypes with distinct feature profiles and that correlate to 90-dayfunctional and cognitive status.",http://arxiv.org/abs/1812.00030v1,,
1307,"Semi-supervised Rare Disease Detection Using Generative Adversarial  Network","Rare diseases affect a relatively small number of people, which limitsinvestment in research for treatments and cures. Developing an efficient methodfor rare disease detection is a crucial first step towards subsequent clinicalresearch. In this paper, we present a semi-supervised learning framework forrare disease detection using generative adversarial networks. Our method takesadvantage of the large amount of unlabeled data for disease detection andachieves the best results in terms of precision-recall score compared tobaseline techniques.",http://arxiv.org/abs/1812.00547v1,,
1308,Memory-Efficient Adaptive Optimization for Large-Scale Learning,"Adaptive gradient-based optimizers such as AdaGrad and Adam are among themethods of choice in modern machine learning. These methods maintainsecond-order statistics of each parameter, thus doubling the memory footprintof the optimizer. In behemoth-size applications, this memory overhead restrictsthe size of the model being used as well as the number of examples in amini-batch. We describe a novel, simple, and flexible adaptive optimizationmethod with sublinear memory cost that retains the benefits of per-parameteradaptivity while allowing for larger models and mini-batches. We giveconvergence guarantees for our method and demonstrate its effectiveness intraining very large deep models.",http://arxiv.org/abs/1901.11150v1,,
1309,Orthogonal Estimation of Wasserstein Distances,"Wasserstein distances are increasingly used in a wide variety of applicationsin machine learning. Sliced Wasserstein distances form an important subclasswhich may be estimated efficiently through one-dimensional sorting operations.In this paper, we propose a new variant of sliced Wasserstein distance, studythe use of orthogonal coupling in Monte Carlo estimation of Wassersteindistances and draw connections with stratified sampling, and evaluate ourapproaches experimentally in a range of large-scale experiments in generativemodelling and reinforcement learning.",http://arxiv.org/abs/1903.03784v2,,
1310,SLSGD: Secure and Efficient Distributed On-device Machine Learning,"We consider distributed on-device learning with limited communication andsecurity requirements. We propose a new robust distributed optimizationalgorithm with efficient communication and attack tolerance. The proposedalgorithm has provable convergence and robustness under non-IID settings.Empirical results show that the proposed algorithm stabilizes the convergenceand tolerates data poisoning on a small number of workers.",http://arxiv.org/abs/1903.06996v2,,
1311,"Improving image classifiers for small datasets by learning rate  adaptations","Our paper introduces an efficient combination of established techniques toimprove classifier performance, in terms of accuracy and training time. Weachieve two-fold to ten-fold speedup in nearing state of the art accuracy, overdifferent model architectures, by dynamically tuning the learning rate. We findit especially beneficial in the case of a small dataset, where reliability ofmachine reasoning is lower. We validate our approach by comparing our methodversus vanilla training on CIFAR-10. We also demonstrate its practicalviability by implementing on an unbalanced corpus of diagnostic images.",http://arxiv.org/abs/1903.10726v2,,
1312,"Data Science and Digital Systems: The 3Ds of Machine Learning Systems  Design","Machine learning solutions, in particular those based on deep learningmethods, form an underpinning of the current revolution in ""artificialintelligence"" that has dominated popular press headlines and is having asignificant influence on the wider tech agenda. Here we give an overview of the3Ds of ML systems design: Data, Design and Deployment. By considering the 3Dswe can move towards \emph{data first} design.",http://arxiv.org/abs/1903.11241v1,,
1313,Alternating Projections for Learning with Expectation Constraints,"We present an objective function for learning with unlabeled data thatutilizes auxiliary expectation constraints. We optimize this objective functionusing a procedure that alternates between information and moment projections.Our method provides an alternate interpretation of the posterior regularizationframework (Graca et al., 2008), maintains uncertainty during optimizationunlike constraint-driven learning (Chang et al., 2007), and is more efficientthan generalized expectation criteria (Mann & McCallum, 2008). Applications ofthis framework include minimally supervised learning, semisupervised learning,and learning with constraints that are more expressive than the underlyingmodel. In experiments, we demonstrate comparable accuracy to generalizedexpectation criteria for minimally supervised learning, and use expressivestructural constraints to guide semi-supervised learning, providing a 3%-6%improvement over stateof-the-art constraint-driven learning.",http://arxiv.org/abs/1205.2660v1,,
1314,Active Imitation Learning via Reduction to I.I.D. Active Learning,"In standard passive imitation learning, the goal is to learn a target policyby passively observing full execution trajectories of it. Unfortunately,generating such trajectories can require substantial expert effort and beimpractical in some cases. In this paper, we consider active imitation learningwith the goal of reducing this effort by querying the expert about the desiredaction at individual states, which are selected based on answers to pastqueries and the learner's interactions with an environment simulator. Weintroduce a new approach based on reducing active imitation learning to i.i.d.active learning, which can leverage progress in the i.i.d. setting. Our firstcontribution, is to analyze reductions for both non-stationary and stationarypolicies, showing that the label complexity (number of queries) of activeimitation learning can be substantially less than passive learning. Our secondcontribution, is to introduce a practical algorithm inspired by the reductions,which is shown to be highly effective in four test domains compared to a numberof alternatives.",http://arxiv.org/abs/1210.4876v1,,
1315,Learning Bayesian Networks with Local Structure,"In this paper we examine a novel addition to the known methods for learningBayesian networks from data that improves the quality of the learned networks.Our approach explicitly represents and learns the local structure in theconditional probability tables (CPTs), that quantify these networks. Thisincreases the space of possible models, enabling the representation of CPTswith a variable number of parameters that depends on the learned localstructures. The resulting learning procedure is capable of inducing models thatbetter emulate the real complexity of the interactions present in the data. Wedescribe the theoretical foundations and practical aspects of learning localstructures, as well as an empirical evaluation of the proposed method. Thisevaluation indicates that learning curves characterizing the procedure thatexploits the local structure converge faster than these of the standardprocedure. Our results also show that networks learned with local structuretend to be more complex (in terms of arcs), yet require less parameters.",http://arxiv.org/abs/1302.3577v1,,
1316,"Multi-class Generalized Binary Search for Active Inverse Reinforcement  Learning","This paper addresses the problem of learning a task from demonstration. Weadopt the framework of inverse reinforcement learning, where tasks arerepresented in the form of a reward function. Our contribution is a novelactive learning algorithm that enables the learning agent to query the expertfor more informative demonstrations, thus leading to more sample-efficientlearning. For this novel algorithm (Generalized Binary Search for InverseReinforcement Learning, or GBS-IRL), we provide a theoretical bound on samplecomplexity and illustrate its applicability on several different tasks. To ourknowledge, GBS-IRL is the first active IRL algorithm with provable samplecomplexity bounds. We also discuss our method in light of other existingmethods in the literature and its general applicability in multi-classclassification problems. Finally, motivated by recent work on learning fromdemonstration in robots, we also discuss how different forms of human feedbackcan be integrated in a transparent manner in our learning framework.",http://arxiv.org/abs/1301.5488v1,,
1317,"Learning to Make Predictions In Partially Observable Environments  Without a Generative Model","When faced with the problem of learning a model of a high-dimensionalenvironment, a common approach is to limit the model to make only a restrictedset of predictions, thereby simplifying the learning problem. These partialmodels may be directly useful for making decisions or may be combined togetherto form a more complete, structured model. However, in partially observable(non-Markov) environments, standard model-learning methods learn generativemodels, i.e. models that provide a probability distribution over all possiblefutures (such as POMDPs). It is not straightforward to restrict such models tomake only certain predictions, and doing so does not always simplify thelearning problem. In this paper we present prediction profile models:non-generative partial models for partially observable systems that make only agiven set of predictions, and are therefore far simpler than generative modelsin some cases. We formalize the problem of learning a prediction profile modelas a transformation of the original model-learning problem, and showempirically that one can learn prediction profile models that make a small setof important predictions even in systems that are too complex for standardgenerative models.",http://arxiv.org/abs/1401.3870v1,,
1318,Differentially Private Distributed Online Learning,"Online learning has been in the spotlight from the machine learning societyfor a long time. To handle massive data in Big Data era, one single learnercould never efficiently finish this heavy task. Hence, in this paper, wepropose a novel distributed online learning algorithm to solve the problem.Comparing to typical centralized online learner, the distributed learnersoptimize their own learning parameters based on local data sources and timelycommunicate with neighbors. However, communication may lead to a privacybreach. Thus, we use differential privacy to preserve the privacy of learners,and study the influence of guaranteeing differential privacy on the utility ofthe distributed online learning algorithm. Furthermore, by using the resultsfrom Kakade and Tewari (2009), we use the regret bounds of online learning toachieve fast convergence rates for offline learning algorithms in distributedscenarios, which provides tighter utility performance than the existingstate-of-the-art results. In simulation, we demonstrate that the differentiallyprivate offline learning algorithm has high variance, but we can use mini-batchto improve the performance. Finally, the simulations show that the analyticalresults of our proposed theorems are right and our private distributed onlinelearning algorithm is a general framework.",http://arxiv.org/abs/1505.06556v2,,
1319,Representation Learning: A Review and New Perspectives,"The success of machine learning algorithms generally depends on datarepresentation, and we hypothesize that this is because differentrepresentations can entangle and hide more or less the different explanatoryfactors of variation behind the data. Although specific domain knowledge can beused to help design representations, learning with generic priors can also beused, and the quest for AI is motivating the design of more powerfulrepresentation-learning algorithms implementing such priors. This paper reviewsrecent work in the area of unsupervised feature learning and deep learning,covering advances in probabilistic models, auto-encoders, manifold learning,and deep networks. This motivates longer-term unanswered questions about theappropriate objectives for learning good representations, for computingrepresentations (i.e., inference), and the geometrical connections betweenrepresentation learning, density estimation and manifold learning.",http://arxiv.org/abs/1206.5538v3,,
1320,"Data-Efficient Learning of Feedback Policies from Image Pixels using  Deep Dynamical Models","Data-efficient reinforcement learning (RL) in continuous state-action spacesusing very high-dimensional observations remains a key challenge in developingfully autonomous systems. We consider a particularly important instance of thischallenge, the pixels-to-torques problem, where an RL agent learns aclosed-loop control policy (""torques"") from pixel information only. Weintroduce a data-efficient, model-based reinforcement learning algorithm thatlearns such a closed-loop policy directly from pixel information. The keyingredient is a deep dynamical model for learning a low-dimensional featureembedding of images jointly with a predictive model in this low-dimensionalfeature space. Joint learning is crucial for long-term predictions, which lieat the core of the adaptive nonlinear model predictive control strategy that weuse for closed-loop control. Compared to state-of-the-art RL methods forcontinuous states and actions, our approach learns quickly, scales tohigh-dimensional state spaces, is lightweight and an important step towardfully autonomous end-to-end learning from pixels to torques.",http://arxiv.org/abs/1510.02173v2,,
1321,Can Active Learning Experience Be Transferred?,"Active learning is an important machine learning problem in reducing thehuman labeling effort. Current active learning strategies are designed fromhuman knowledge, and are applied on each dataset in an immutable manner. Inother words, experience about the usefulness of strategies cannot be updatedand transferred to improve active learning on other datasets. This paperinitiates a pioneering study on whether active learning experience can betransferred. We first propose a novel active learning model that linearlyaggregates existing strategies. The linear weights can then be used torepresent the active learning experience. We equip the model with the popularlinear upper- confidence-bound (LinUCB) algorithm for contextual bandit toupdate the weights. Finally, we extend our model to transfer the experienceacross datasets with the technique of biased regularization. Empirical studiesdemonstrate that the learned experience not only is competitive with existingstrategies on most single datasets, but also can be transferred across datasetsto improve the performance on future learning tasks.",http://arxiv.org/abs/1608.00667v1,,
1322,Multi-Task Multiple Kernel Relationship Learning,"This paper presents a novel multitask multiple kernel learning framework thatefficiently learns the kernel weights leveraging the relationship acrossmultiple tasks. The idea is to automatically infer this task relationship inthe \textit{RKHS} space corresponding to the given base kernels. The problem isformulated as a regularization-based approach called \textit{Multi-TaskMultiple Kernel Relationship Learning} (\textit{MK-MTRL}), which models thetask relationship matrix from the weights learned from latent feature spaces oftask-specific base kernels. Unlike in previous work, the proposed formulationallows one to incorporate prior knowledge for simultaneously learning severalrelated tasks. We propose an alternating minimization algorithm to learn themodel parameters, kernel weights and task relationship matrix. In order totackle large-scale problems, we further propose a two-stage \textit{MK-MTRL}online learning algorithm and show that it significantly reduces thecomputational time, and also achieves performance comparable to that of thejoint learning framework. Experimental results on benchmark datasets show thatthe proposed formulations outperform several state-of-the-art multitasklearning methods.",http://arxiv.org/abs/1611.03427v2,,
1323,Learning to Learn without Gradient Descent by Gradient Descent,"We learn recurrent neural network optimizers trained on simple syntheticfunctions by gradient descent. We show that these learned optimizers exhibit aremarkable degree of transfer in that they can be used to efficiently optimizea broad range of derivative-free black-box functions, including Gaussianprocess bandits, simple control objectives, global optimization benchmarks andhyper-parameter tuning tasks. Up to the training horizon, the learnedoptimizers learn to trade-off exploration and exploitation, and comparefavourably with heavily engineered Bayesian optimization packages forhyper-parameter tuning.",http://arxiv.org/abs/1611.03824v6,,
1324,Why & When Deep Learning Works: Looking Inside Deep Learnings,"The Intel Collaborative Research Institute for Computational Intelligence(ICRI-CI) has been heavily supporting Machine Learning and Deep Learningresearch from its foundation in 2012. We have asked six leading ICRI-CI DeepLearning researchers to address the challenge of ""Why & When Deep Learningworks"", with the goal of looking inside Deep Learning, providing insights onhow deep networks function, and uncovering key observations on theirexpressiveness, limitations, and potential. The output of this challengeresulted in five papers that address different facets of deep learning. Thesedifferent facets include a high-level understating of why and when deepnetworks work (and do not work), the impact of geometry on the expressivenessof deep networks, and making deep networks interpretable.",http://arxiv.org/abs/1705.03921v1,,
1325,"Classifying Documents within Multiple Hierarchical Datasets using  Multi-Task Learning","Multi-task learning (MTL) is a supervised learning paradigm in which theprediction models for several related tasks are learned jointly to achievebetter generalization performance. When there are only a few training examplesper task, MTL considerably outperforms the traditional Single task learning(STL) in terms of prediction accuracy. In this work we develop an MTL basedapproach for classifying documents that are archived within dual concepthierarchies, namely, DMOZ and Wikipedia. We solve the multi-classclassification problem by defining one-versus-rest binary classification tasksfor each of the different classes across the two hierarchical datasets. Insteadof learning a linear discriminant for each of the different tasksindependently, we use a MTL approach with relationships between the differenttasks across the datasets established using the non-parametric, lazy, nearestneighbor approach. We also develop and evaluate a transfer learning (TL)approach and compare the MTL (and TL) methods against the standard single tasklearning and semi-supervised learning approaches. Our empirical resultsdemonstrate the strength of our developed methods that show an improvementespecially when there are fewer number of training examples per classificationtask.",http://arxiv.org/abs/1706.01583v1,,
1326,Variational Adaptive-Newton Method for Explorative Learning,"We present the Variational Adaptive Newton (VAN) method which is a black-boxoptimization method especially suitable for explorative-learning tasks such asactive learning and reinforcement learning. Similar to Bayesian methods, VANestimates a distribution that can be used for exploration, but requirescomputations that are similar to continuous optimization methods. Ourtheoretical contribution reveals that VAN is a second-order method that unifiesexisting methods in distinct fields of continuous optimization, variationalinference, and evolution strategies. Our experimental results show that VANperforms well on a wide-variety of learning tasks. This work presents ageneral-purpose explorative-learning method that has the potential to improvelearning in areas such as active learning and reinforcement learning.",http://arxiv.org/abs/1711.05560v1,,
1327,A Bridge Between Hyperparameter Optimization and Larning-to-learn,"We consider a class of a nested optimization problems involving inner andouter objectives. We observe that by taking into explicit account theoptimization dynamics for the inner objective it is possible to derive ageneral framework that unifies gradient-based hyperparameter optimization andmeta-learning (or learning-to-learn). Depending on the specific setting, thevariables of the outer objective take either the meaning of hyperparameters ina supervised learning problem or parameters of a meta-learner. We show thatsome recently proposed methods in the latter setting can be instantiated in ourframework and tackled with the same gradient-based algorithms. Finally, wediscuss possible design patterns for learning-to-learn and present encouragingpreliminary experiments for few-shot learning.",http://arxiv.org/abs/1712.06283v2,,
1328,"Deep learning in radiology: an overview of the concepts and a survey of  the state of the art","Deep learning is a branch of artificial intelligence where networks of simpleinterconnected units are used to extract patterns from data in order to solvecomplex problems. Deep learning algorithms have shown groundbreakingperformance in a variety of sophisticated tasks, especially those related toimages. They have often matched or exceeded human performance. Since themedical field of radiology mostly relies on extracting useful information fromimages, it is a very natural application area for deep learning, and researchin this area has rapidly grown in recent years. In this article, we review theclinical reality of radiology and discuss the opportunities for application ofdeep learning algorithms. We also introduce basic concepts of deep learningincluding convolutional neural networks. Then, we present a survey of theresearch in deep learning applied to radiology. We organize the studies by thetypes of specific tasks that they attempt to solve and review the broad rangeof utilized deep learning algorithms. Finally, we briefly discuss opportunitiesand challenges for incorporating deep learning in the radiology practice of thefuture.",http://arxiv.org/abs/1802.08717v1,,
1329,Setting up a Reinforcement Learning Task with a Real-World Robot,"Reinforcement learning is a promising approach to developing hard-to-engineeradaptive solutions for complex and diverse robotic tasks. However, learningwith real-world robots is often unreliable and difficult, which resulted intheir low adoption in reinforcement learning research. This difficulty isworsened by the lack of guidelines for setting up learning tasks with robots.In this work, we develop a learning task with a UR5 robotic arm to bring tolight some key elements of a task setup and study their contributions to thechallenges with robots. We find that learning performance can be highlysensitive to the setup, and thus oversights and omissions in setup details canmake effective learning, reproducibility, and fair comparison hard. Our studysuggests some mitigating steps to help future experimenters avoid difficultiesand pitfalls. We show that highly reliable and repeatable experiments can beperformed in our setup, indicating the possibility of reinforcement learningresearch extensively based on real-world robots.",http://arxiv.org/abs/1803.07067v1,,
1330,Towards Symbolic Reinforcement Learning with Common Sense,"Deep Reinforcement Learning (deep RL) has made several breakthroughs inrecent years in applications ranging from complex control tasks in unmannedvehicles to game playing. Despite their success, deep RL still lacks severalimportant capacities of human intelligence, such as transfer learning,abstraction and interpretability. Deep Symbolic Reinforcement Learning (DSRL)seeks to incorporate such capacities to deep Q-networks (DQN) by learning arelevant symbolic representation prior to using Q-learning. In this paper, wepropose a novel extension of DSRL, which we call Symbolic ReinforcementLearning with Common Sense (SRL+CS), offering a better balance betweengeneralization and specialization, inspired by principles of common sense whenassigning rewards and aggregating Q-values. Experiments reported in this papershow that SRL+CS learns consistently faster than Q-learning and DSRL, achievingalso a higher accuracy. In the hardest case, where agents were trained in adeterministic environment and tested in a random environment, SRL+CS achievesnearly 100% average accuracy compared to DSRL's 70% and DQN's 50% accuracy. Tothe best of our knowledge, this is the first case of near perfect zero-shottransfer learning using Reinforcement Learning.",http://arxiv.org/abs/1804.08597v1,,
1331,Verifiable Reinforcement Learning via Policy Extraction,"While deep reinforcement learning has successfully solved many challengingcontrol tasks, its real-world applicability has been limited by the inabilityto ensure the safety of learned policies. We propose an approach to verifiablereinforcement learning by training decision tree policies, which can representcomplex policies (since they are nonparametric), yet can be efficientlyverified using existing techniques (since they are highly structured). Thechallenge is that decision tree policies are difficult to train. We proposeVIPER, an algorithm that combines ideas from model compression and imitationlearning to learn decision tree policies guided by a DNN policy (called theoracle) and its Q-function, and show that it substantially outperforms twobaselines. We use VIPER to (i) learn a provably robust decision tree policy fora variant of Atari Pong with a symbolic state space, (ii) learn a decision treepolicy for a toy game based on Pong that provably never loses, and (iii) learna provably stable decision tree policy for cart-pole. In each case, thedecision tree policy achieves performance equal to that of the original DNNpolicy.",http://arxiv.org/abs/1805.08328v2,,
1332,Probabilistic Model-Agnostic Meta-Learning,"Meta-learning for few-shot learning entails acquiring a prior over previoustasks and experiences, such that new tasks be learned from small amounts ofdata. However, a critical challenge in few-shot learning is task ambiguity:even when a powerful prior can be meta-learned from a large number of priortasks, a small dataset for a new task can simply be too ambiguous to acquire asingle model (e.g., a classifier) for that task that is accurate. In thispaper, we propose a probabilistic meta-learning algorithm that can samplemodels for a new task from a model distribution. Our approach extendsmodel-agnostic meta-learning, which adapts to new tasks via gradient descent,to incorporate a parameter distribution that is trained via a variational lowerbound. At meta-test time, our algorithm adapts via a simple procedure thatinjects noise into gradient descent, and at meta-training time, the model istrained such that this stochastic adaptation procedure produces samples fromthe approximate model posterior. Our experimental results show that our methodcan sample plausible classifiers and regressors in ambiguous few-shot learningproblems.",http://arxiv.org/abs/1806.02817v1,,
1333,"Meta-Learning Transferable Active Learning Policies by Deep  Reinforcement Learning","Active learning (AL) aims to enable training high performance classifierswith low annotation cost by predicting which subset of unlabelled instanceswould be most beneficial to label. The importance of AL has motivated extensiveresearch, proposing a wide variety of manually designed AL algorithms withdiverse theoretical and intuitive motivations. In contrast to this body ofresearch, we propose to treat active learning algorithm design as ameta-learning problem and learn the best criterion from data. We model anactive learning algorithm as a deep neural network that inputs the base learnerstate and the unlabelled point set and predicts the best point to annotatenext. Training this active query policy network with reinforcement learning,produces the best non-myopic policy for a given dataset. The key challenge inachieving a general solution to AL then becomes that of learner generalisation,particularly across heterogeneous datasets. We propose a multi-taskdataset-embedding approach that allows dataset-agnostic active learners to betrained. Our evaluation shows that AL algorithms trained in this way candirectly generalise across diverse problems.",http://arxiv.org/abs/1806.04798v1,,
1334,A Meta-Learning Approach for Custom Model Training,"Transfer-learning and meta-learning are two effective methods to applyknowledge learned from large data sources to new tasks. In few-class, few-shottarget task settings (i.e. when there are only a few classes and trainingexamples available in the target task), meta-learning approaches that optimizefor future task learning have outperformed the typical transfer approach ofinitializing model weights from a pre-trained starting point. But as weexperimentally show, meta-learning algorithms that work well in the few-classsetting do not generalize well in many-shot and many-class cases. In thispaper, we propose a joint training approach that combines bothtransfer-learning and meta-learning. Benefiting from the advantages of each,our method obtains improved generalization performance on unseen target tasksin both few- and many-class and few- and many-shot scenarios.",http://arxiv.org/abs/1809.08346v2,,
1335,Reinforcement Learning for Improving Agent Design,"In many reinforcement learning tasks, the goal is to learn a policy tomanipulate an agent, whose design is fixed, to maximize some notion ofcumulative reward. The design of the agent's physical structure is rarelyoptimized for the task at hand. In this work, we explore the possibility oflearning a version of the agent's design that is better suited for its task,jointly with the policy. We propose a minor alteration to the OpenAI Gymframework, where we parameterize parts of an environment, and allow an agent tojointly learn to modify these environment parameters along with its policy. Wedemonstrate that an agent can learn a better structure of its body that is notonly better suited for the task, but also facilitates policy learning. Jointlearning of policy and structure may even uncover design principles that areuseful for assisted-design applications. Videos of results athttps://designrl.github.io/",http://arxiv.org/abs/1810.03779v2,,
1336,Transferring Knowledge across Learning Processes,"In complex transfer learning scenarios new tasks might not be tightly linkedto previous tasks. Approaches that transfer information contained only in thefinal parameters of a source model will therefore struggle. Instead, transferlearning at a higher level of abstraction is needed. We propose Leap, aframework that achieves this by transferring knowledge across learningprocesses. We associate each task with a manifold on which the training processtravels from initialization to final parameters and construct a meta-learningobjective that minimizes the expected length of this path. Our frameworkleverages only information obtained during training and can be computed on thefly at negligible cost. We demonstrate that our framework outperforms competingmethods, both in meta-learning and transfer learning, on a set of computervision tasks. Finally, we demonstrate that Leap can transfer knowledge acrosslearning processes in demanding reinforcement learning environments (Atari)that involve millions of gradient steps.",http://arxiv.org/abs/1812.01054v3,,
1337,Domain Adaptation for Reinforcement Learning on the Atari,"Deep reinforcement learning agents have recently been successful across avariety of discrete and continuous control tasks; however, they can be slow totrain and require a large number of interactions with the environment to learna suitable policy. This is borne out by the fact that a reinforcement learningagent has no prior knowledge of the world, no pre-existing data to depend onand so must devote considerable time to exploration. Transfer learning canalleviate some of the problems by leveraging learning done on some source taskto help learning on some target task. Our work presents an algorithm forinitialising the hidden feature representation of the target task. We propose adomain adaptation method to transfer state representations and demonstratetransfer across domains, tasks and action spaces. We utilise adversarial domainadaptation ideas combined with an adversarial autoencoder architecture. Wealign our new policies' representation space with a pre-trained source policy,taking target task data generated from a random policy. We demonstrate thatthis initialisation step provides significant improvement when learning a newreinforcement learning task, which highlights the wide applicability ofadversarial adaptation methods; even as the task and label/action space alsochanges.",http://arxiv.org/abs/1812.07452v1,,
1338,Multi-objective Evolutionary Federated Learning,"Federated learning is an emerging technique used to prevent the leakage ofprivate information. Unlike centralized learning that needs to collect datafrom users and store them collectively on a cloud server, federated learningmakes it possible to learn a global model while the data are distributed on theusers' devices. However, compared with the traditional centralized approach,the federated setting consumes considerable communication resources of theclients, which is indispensable for updating global models and prevents thistechnique from being widely used. In this paper, we aim to optimize thestructure of the neural network models in federated learning using amulti-objective evolutionary algorithm to simultaneously minimize thecommunication costs and the global model test errors. A scalable method forencoding network connectivity is adapted to federated learning to enhance theefficiency in evolving deep neural networks. Experimental results on bothmultilayer perceptrons and convolutional neural networks indicate that theproposed optimization method is able to find optimized neural network modelsthat can not only significantly reduce communication costs but also improve thelearning performance of federated learning compared with the standard fullyconnected neural networks.",http://arxiv.org/abs/1812.07478v1,,
1339,"Subspace Match Probably Does Not Accurately Assess the Similarity of  Learned Representations","Learning informative representations of data is one of the primary goals ofdeep learning, but there is still little understanding as to whatrepresentations a neural network actually learns. To better understand this,subspace match was recently proposed as a method for assessing the similarityof the representations learned by neural networks. It has been shown that twonetworks with the same architecture trained from different initializationslearn representations that at hidden layers show low similarity when assessedwith subspace match, even when the output layers show high similarity and thenetworks largely exhibit similar performance on classification tasks. In thisnote, we present a simple example motivated by standard results in commutativealgebra to illustrate how this can happen, and show that although the subspacematch at a hidden layer may be 0, the representations learned may be isomorphicas vector spaces. This leads us to conclude that a subspace match comparison oflearned representations may well be uninformative, and it points to the needfor better methods of understanding learned representations.",http://arxiv.org/abs/1901.00884v1,,
1340,Integrating Learning and Reasoning with Deep Logic Models,"Deep learning is very effective at jointly learning feature representationsand classification models, especially when dealing with high dimensional inputpatterns. Probabilistic logic reasoning, on the other hand, is capable to takeconsistent and robust decisions in complex environments. The integration ofdeep learning and logic reasoning is still an open-research problem and it isconsidered to be the key for the development of real intelligent agents. Thispaper presents Deep Logic Models, which are deep graphical models integratingdeep learning and logic reasoning both for learning and inference. Deep LogicModels create an end-to-end differentiable architecture, where deep learnersare embedded into a network implementing a continuous relaxation of the logicknowledge. The learning process allows to jointly learn the weights of the deeplearners and the meta-parameters controlling the high-level reasoning. Theexperimental results show that the proposed methodology overtakes thelimitations of the other approaches that have been proposed to bridge deeplearning and reasoning.",http://arxiv.org/abs/1901.04195v1,,
1341,Towards Physically Safe Reinforcement Learning under Supervision,"This paper addresses the question of how a previously available controlpolicy $\pi_s$ can be used as a supervisor to more quickly and safely train anew learned control policy $\pi_L$ for a robot. A weighted average of thesupervisor and learned policies is used during trials, with a heavier weightinitially on the supervisor, in order to allow safe and useful physical trialswhile the learned policy is still ineffective. During the process, the weightis adjusted to favor the learned policy. As weights are adjusted, the learnednetwork must compensate so as to give safe and reasonable outputs under thedifferent weights. A pioneer network is introduced that pre-learns a policythat performs similarly to the current learned policy under the planned nextstep for new weights; this pioneer network then replaces the currently learnednetwork in the next set of trials. Experiments in OpenAI Gym demonstrate theeffectiveness of the proposed method.",http://arxiv.org/abs/1901.06576v1,,
1342,"Decoupling feature extraction from policy learning: assessing benefits  of state representation learning in goal based robotics","Scaling end-to-end reinforcement learning to control real robots from visionpresents a series of challenges, in particular in terms of sample efficiency.Against end-to-end learning, state representation learning can help learn acompact, efficient and relevant representation of states that speeds up policylearning, reducing the number of samples needed, and that is easier tointerpret. We evaluate several state representation learning methods on goalbased robotics tasks and propose a new unsupervised model that stacksrepresentations and combines strengths of several of these approaches. Thismethod encodes all the relevant features, performs on par or better thanend-to-end learning, and is robust to hyper-parameters change.",http://arxiv.org/abs/1901.08651v2,,
1343,"Bayesian semi-supervised learning for uncertainty-calibrated prediction  of molecular properties and active learning","Predicting bioactivity and physical properties of small molecules is acentral challenge in drug discovery. Deep learning is becoming the method ofchoice but studies to date focus on mean accuracy as the main metric. However,to replace costly and mission-critical experiments by models, a high meanaccuracy is not enough: Outliers can derail a discovery campaign, thus modelsneed reliably predict when it will fail, even when the training data is biased;experiments are expensive, thus models need to be data-efficient and suggestinformative training sets using active learning. We show that uncertaintyquantification and active learning can be achieved by Bayesian semi-supervisedgraph convolutional neural networks. The Bayesian approach estimatesuncertainty in a statistically principled way through sampling from theposterior distribution. Semi-supervised learning disentangles representationlearning and regression, keeping uncertainty estimates accurate in the low datalimit and allowing the model to start active learning from a small initial poolof training data. Our study highlights the promise of Bayesian deep learningfor chemistry.",http://arxiv.org/abs/1902.00925v1,,
1344,Unsupervised Visuomotor Control through Distributional Planning Networks,"While reinforcement learning (RL) has the potential to enable robots toautonomously acquire a wide range of skills, in practice, RL usually requiresmanual, per-task engineering of reward functions, especially in real worldsettings where aspects of the environment needed to compute progress are notdirectly accessible. To enable robots to autonomously learn skills, we insteadconsider the problem of reinforcement learning without access to rewards. Weaim to learn an unsupervised embedding space under which the robot can measureprogress towards a goal for itself. Our approach explicitly optimizes for ametric space under which action sequences that reach a particular state areoptimal when the goal is the final state reached. This enables learningeffective and control-centric representations that lead to more autonomousreinforcement learning algorithms. Our experiments on three simulatedenvironments and two real-world manipulation problems show that our method canlearn effective goal metrics from unlabeled interaction, and use the learnedgoal metrics for autonomous reinforcement learning.",http://arxiv.org/abs/1902.05542v1,,
1345,Using World Models for Pseudo-Rehearsal in Continual Learning,"The utility of learning a dynamics/world model of the environment inreinforcement learning has been shown in a many ways. When using neuralnetworks, however, these models suffer catastrophic forgetting when learned ina lifelong or continual fashion. Current solutions to the continual learningproblem require experience to be segmented and labeled as discrete tasks,however, in continuous experience it is generally unclear what a sufficientsegmentation of tasks would be. Here we propose a method to continually learnthese internal world models through the interleaving of internally generatedrollouts from past experiences (i.e., pseudo-rehearsal). We show this methodcan sequentially learn unsupervised temporal prediction, without task labels,in a disparate set of Atari games. Empirically, this interleaving of theinternally generated rollouts with the external environment's observationsleads to an average 4.5x reduction in temporal prediction loss compared tonon-interleaved learning. Similarly, we show that the representations of thisinternal model remain stable across learned environments. Here, an agenttrained using an initial version of the internal model can perform equally wellwhen using a subsequent version that has successfully incorporated experiencefrom multiple new environments.",http://arxiv.org/abs/1903.02647v1,,
1346,Learning-to-Learn Stochastic Gradient Descent with Biased Regularization,"We study the problem of learning-to-learn: inferring a learning algorithmthat works well on tasks sampled from an unknown distribution. As class ofalgorithms we consider Stochastic Gradient Descent on the true risk regularizedby the square euclidean distance to a bias vector. We present an average excessrisk bound for such a learning algorithm. This result quantifies the potentialbenefit of using a bias vector with respect to the unbiased case. We thenaddress the problem of estimating the bias from a sequence of tasks. We proposea meta-algorithm which incrementally updates the bias, as new tasks areobserved. The low space and time complexity of this approach makes it appealingin practice. We provide guarantees on the learning ability of themeta-algorithm. A key feature of our results is that, when the number of tasksgrows and their variance is relatively small, our learning-to-learn approachhas a significant advantage over learning each task in isolation by StochasticGradient Descent without a bias term. We report on numerical experiments whichdemonstrate the effectiveness of our approach.",http://arxiv.org/abs/1903.10399v1,,
1347,"On Better Exploring and Exploiting Task Relationships in Multi-Task  Learning: Joint Model and Feature Learning","Multitask learning (MTL) aims to learn multiple tasks simultaneously throughthe interdependence between different tasks. The way to measure the relatednessbetween tasks is always a popular issue. There are mainly two ways to measurerelatedness between tasks: common parameters sharing and common featuressharing across different tasks. However, these two types of relatedness aremainly learned independently, leading to a loss of information. In this paper,we propose a new strategy to measure the relatedness that jointly learns sharedparameters and shared feature representations. The objective of our proposedmethod is to transform the features from different tasks into a common featurespace in which the tasks are closely related and the shared parameters can bebetter optimized. We give a detailed introduction to our proposed multitasklearning method. Additionally, an alternating algorithm is introduced tooptimize the nonconvex objection. A theoretical bound is given to demonstratethat the relatedness between tasks can be better measured by our proposedmultitask learning algorithm. We conduct various experiments to verify thesuperiority of the proposed joint model and feature a multitask learningmethod.",http://arxiv.org/abs/1904.01747v1,,
1348,A Selective Overview of Deep Learning,"Deep learning has arguably achieved tremendous success in recent years. Insimple words, deep learning uses the composition of many nonlinear functions tomodel the complex dependency between input features and labels. While neuralnetworks have a long history, recent advances have greatly improved theirperformance in computer vision, natural language processing, etc. From thestatistical and scientific perspective, it is natural to ask: What is deeplearning? What are the new characteristics of deep learning, compared withclassical methods? What are the theoretical foundations of deep learning? Toanswer these questions, we introduce common neural network models (e.g.,convolutional neural nets, recurrent neural nets, generative adversarial nets)and training techniques (e.g., stochastic gradient descent, dropout, batchnormalization) from a statistical point of view. Along the way, we highlightnew characteristics of deep learning (including depth and over-parametrization)and explain their practical and theoretical benefits. We also sample recentresults on theories of deep learning, many of which are only suggestive. Whilea complete understanding of deep learning remains elusive, we hope that ourperspectives and discussions serve as a stimulus for new statistical research.",http://arxiv.org/abs/1904.05526v1,,
1349,"Predicting the Plant Root-Associated Ecological Niche of 21 Pseudomonas  Species Using Machine Learning and Metabolic Modeling","Plants rarely occur in isolated systems. Bacteria can inhabit either theendosphere, the region inside the plant root, or the rhizosphere, the soilregion just outside the plant root. Our goal is to understand if using genomicdata and media dependent metabolic model information is better for trainingmachine learning of predicting bacterial ecological niche than mediaindependent models or pure genome based species trees. We considered threemachine learning techniques: support vector machine, non-negative matrixfactorization, and artificial neural networks. In all three machine-learningapproaches, the media-based metabolic models and flux balance analyses weremore effective at predicting bacterial niche than the genome or PRMT models.Support Vector Machine trained on a minimal media base with Mannose, Prolineand Valine was most predictive of all models and media types with an f-score of0.8 for rhizosphere and 0.97 for endosphere. Thus we can conclude thatmedia-based metabolic modeling provides a holistic view of the metabolome,allowing machine learning algorithms to highlight the differences between andcategorize endosphere and rhizosphere bacteria. There was no single media typethat best highlighted differences between endosphere and rhizosphere bacteriametabolism and therefore no single enzyme, reaction, or compound that definedwhether a bacteria's origin was of the endosphere or rhizosphere.",http://arxiv.org/abs/1701.03220v1,,
1350,"A comparison of machine learning techniques for taxonomic classification  of teeth from the Family Bovidae","This study explores the performance of modern, accurate machine learningalgorithms on the classification of fossil teeth in the Family Bovidae.Isolated bovid teeth are typically the most common fossils found in southernAfrica and they often constitute the basis for paleoenvironmentalreconstructions. Taxonomic identification of fossil bovid teeth, however, isoften imprecise and subjective. Using modern teeth with known taxons, machinelearning algorithms can be trained to classify fossils. Previous work by Brophyet. al. 2014 uses elliptical Fourier analysis of the form (size and shape) ofthe outline of the occlusal surface of each tooth as features in a lineardiscriminant analysis framework. This manuscript expands on that previous workby exploring how different machine learning approaches classify the teeth andtesting which technique is best for classification. Five different machinelearning techniques including linear discriminant analysis, neural networks,nuclear penalized multinomial regression, random forests, and support vectormachines were used to estimate these models. Support vector machines and randomforests perform the best in terms of both log-loss and misclassification rate;both of these methods are improvements over linear discriminant analysis. Withthe identification and application of these superior methods, bovid teeth canbe classified with higher accuracy.",http://arxiv.org/abs/1802.05778v1,,
1351,"Chatter Classification in Turning Using Machine Learning and Topological  Data Analysis","Chatter identification and detection in machining processes has been anactive area of research in the past two decades. Part of the challenge instudying chatter is that machining equations that describe its occurrence areoften nonlinear delay differential equations. The majority of the availabletools for chatter identification rely on defining a metric that captures thecharacteristics of chatter, and a threshold that signals its occurrence. Thedifficulty in choosing these parameters can be somewhat alleviated by utilizingmachine learning techniques. However, even with a successful classificationalgorithm, the transferability of typical machine learning methods from onedata set to another remains very limited. In this paper we combine supervisedmachine learning with Topological Data Analysis (TDA) to obtain a descriptor ofthe process which can detect chatter. The features we use are derived from thepersistence diagram of an attractor reconstructed from the time series viaTakens embedding. We test the approach using deterministic and stochasticturning models, where the stochasticity is introduced via the cuttingcoefficient term. Our results show a 97% successful classification rate on thedeterministic model labeled by the stability diagram obtained using thespectral element method. The features gleaned from the deterministic model arethen utilized for characterization of chatter in a stochastic turning modelwhere there are very limited analysis methods.",http://arxiv.org/abs/1804.02261v1,,
1352,Machine learning algorithms based on generalized Gibbs ensembles,"Machine learning algorithms often take inspiration from established resultsand knowledge from statistical physics. A prototypical example is the Boltzmannmachine algorithm for supervised learning, which utilizes knowledge ofclassical thermal partition functions and the Boltzmann distribution. Recently,a quantum version of the Boltzmann machine was introduced by Amin, et. al.,however, non-commutativity of quantum operators renders the training process byminimizing a cost function inefficient. Recent advances in the study ofnon-equilibrium quantum integrable systems, which never thermalize, have leadto the exploration of a wider class of statistical ensembles. These systems maybe described by the so-called generalized Gibbs ensemble (GGE), whichincorporates a number of ""effective temperatures"". We propose that these GGE'scan be successfully applied as the basis of a Boltzmann-machine-like learningalgorithm, which operates by learning the optimal values of effectivetemperatures. We show that the GGE algorithm is an optimal quantum Boltzmannmachine: it is the only quantum machine that circumvents the quantumtraining-process problem. We apply a simplified version of the GGE algorithm,where quantum effects are suppressed, to the classification of handwrittendigits in the MNIST database. While lower error rates can be found with otherstate-of-the-art algorithms, we find that our algorithm reaches relatively lowerror rates while learning a much smaller number of parameters than would beneeded in a traditional Boltzmann machine, thereby reducing computational cost.",http://arxiv.org/abs/1804.03546v3,,
1353,Engineering Safety in Machine Learning,"Machine learning algorithms are increasingly influencing our decisions andinteracting with us in all parts of our daily lives. Therefore, just like forpower plants, highways, and myriad other engineered sociotechnical systems, wemust consider the safety of systems involving machine learning. In this paper,we first discuss the definition of safety in terms of risk, epistemicuncertainty, and the harm incurred by unwanted outcomes. Then we examinedimensions, such as the choice of cost function and the appropriateness ofminimizing the empirical average training cost, along which certain real-worldapplications may not be completely amenable to the foundational principle ofmodern statistical machine learning: empirical risk minimization. Inparticular, we note an emerging dichotomy of applications: ones in which safetyis important and risk minimization is not the complete story (we name theseType A applications), and ones in which safety is not so critical and riskminimization is sufficient (we name these Type B applications). Finally, wediscuss how four different strategies for achieving safety in engineering(inherently safe design, safety reserves, safe fail, and procedural safeguards)can be mapped to the machine learning context through interpretability andcausality of predictive models, objectives beyond expected prediction accuracy,human involvement for labeling difficult or rare examples, and user experiencedesign of software.",http://arxiv.org/abs/1601.04126v1,,
1354,Global Gene Expression Analysis Using Machine Learning Methods,"Microarray is a technology to quantitatively monitor the expression of largenumber of genes in parallel. It has become one of the main tools for globalgene expression analysis in molecular biology research in recent years. Thelarge amount of expression data generated by this technology makes the study ofcertain complex biological problems possible and machine learning methods areplaying a crucial role in the analysis process. At present, many machinelearning methods have been or have the potential to be applied to major areasof gene expression analysis. These areas include clustering, classification,dynamic modeling and reverse engineering.  In this thesis, we focus our work on using machine learning methods to solvethe classification problems arising from microarray data. We first identify themajor types of the classification problems; then apply several machine learningmethods to solve the problems and perform systematic tests on real andartificial datasets. We propose improvement to existing methods. Specifically,we develop a multivariate and a hybrid feature selection method to obtain highclassification performance for high dimension classification problems. Usingthe hybrid feature selection method, we are able to identify small sets offeatures that give predictive accuracy that is as good as that from othermethods which require many more features.",http://arxiv.org/abs/1506.02087v1,,
1355,"Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and  Their Combination","This paper covers the two approaches for sentiment analysis: i) lexicon basedmethod; ii) machine learning method. We describe several techniques toimplement these approaches and discuss how they can be adopted for sentimentclassification of Twitter messages. We present a comparative study of differentlexicon combinations and show that enhancing sentiment lexicons with emoticons,abbreviations and social-media slang expressions increases the accuracy oflexicon-based classification for Twitter. We discuss the importance of featuregeneration and feature selection processes for machine learning sentimentclassification. To quantify the performance of the main sentiment analysismethods over Twitter we run these algorithms on a benchmark Twitter datasetfrom the SemEval-2013 competition, task 2-B. The results show that machinelearning method based on SVM and Naive Bayes classifiers outperforms thelexicon method. We present a new ensemble method that uses a lexicon basedsentiment score as input feature for the machine learning approach. Thecombined method proved to produce more precise classifications. We also showthat employing a cost-sensitive classifier for highly unbalanced datasetsyields an improvement of sentiment classification performance up to 7%.",http://arxiv.org/abs/1507.00955v3,,
1356,"Transferability in Machine Learning: from Phenomena to Black-Box Attacks  using Adversarial Samples","Many machine learning models are vulnerable to adversarial examples: inputsthat are specially crafted to cause a machine learning model to produce anincorrect output. Adversarial examples that affect one model often affectanother model, even if the two models have different architectures or weretrained on different training sets, so long as both models were trained toperform the same task. An attacker may therefore train their own substitutemodel, craft adversarial examples against the substitute, and transfer them toa victim model, with very little information about the victim. Recent work hasfurther developed a technique that uses the victim model as an oracle to labela synthetic training set for the substitute, so the attacker need not evencollect a training set to mount the attack. We extend these recent techniquesusing reservoir sampling to greatly enhance the efficiency of the trainingprocedure for the substitute model. We introduce new transferability attacksbetween previously unexplored (substitute, victim) pairs of machine learningmodel classes, most notably SVMs and decision trees. We demonstrate our attackson two commercial machine learning classification systems from Amazon (96.19%misclassification rate) and Google (88.94%) using only 800 queries of thevictim model, thereby showing that existing machine learning approaches are ingeneral vulnerable to systematic black-box attacks regardless of theirstructure.",http://arxiv.org/abs/1605.07277v1,,
1357,Adversarial examples in the physical world,"Most existing machine learning classifiers are highly vulnerable toadversarial examples. An adversarial example is a sample of input data whichhas been modified very slightly in a way that is intended to cause a machinelearning classifier to misclassify it. In many cases, these modifications canbe so subtle that a human observer does not even notice the modification atall, yet the classifier still makes a mistake. Adversarial examples posesecurity concerns because they could be used to perform an attack on machinelearning systems, even if the adversary has no access to the underlying model.Up to now, all previous work have assumed a threat model in which the adversarycan feed data directly into the machine learning classifier. This is not alwaysthe case for systems operating in the physical world, for example those whichare using signals from cameras and other sensors as an input. This paper showsthat even in such physical world scenarios, machine learning systems arevulnerable to adversarial examples. We demonstrate this by feeding adversarialimages obtained from cell-phone camera to an ImageNet Inception classifier andmeasuring the classification accuracy of the system. We find that a largefraction of adversarial examples are classified incorrectly even when perceivedthrough the camera.",http://arxiv.org/abs/1607.02533v4,,
1358,"Toward the automated analysis of complex diseases in genome-wide  association studies using genetic programming","Machine learning has been gaining traction in recent years to meet the demandfor tools that can efficiently analyze and make sense of the ever-growingdatabases of biomedical data in health care systems around the world. However,effectively using machine learning methods requires considerable domainexpertise, which can be a barrier of entry for bioinformaticians new tocomputational data science methods. Therefore, off-the-shelf tools that makemachine learning more accessible can prove invaluable for bioinformaticians. Tothis end, we have developed an open source pipeline optimization tool(TPOT-MDR) that uses genetic programming to automatically design machinelearning pipelines for bioinformatics studies. In TPOT-MDR, we implementMultifactor Dimensionality Reduction (MDR) as a feature construction method formodeling higher-order feature interactions, and combine it with a new expertknowledge-guided feature selector for large biomedical data sets. Wedemonstrate TPOT-MDR's capabilities using a combination of simulated and realworld data sets from human genetics and find that TPOT-MDR significantlyoutperforms modern machine learning methods such as logistic regression andeXtreme Gradient Boosting (XGBoost). We further analyze the best pipelinediscovered by TPOT-MDR for a real world problem and highlight TPOT-MDR'sability to produce a high-accuracy solution that is also easily interpretable.",http://arxiv.org/abs/1702.01780v1,,
1359,"Applying Deep Machine Learning for psycho-demographic profiling of  Internet users using O.C.E.A.N. model of personality","In the modern era, each Internet user leaves enormous amounts of auxiliarydigital residuals (footprints) by using a variety of on-line services. All thisdata is already collected and stored for many years. In recent works, it wasdemonstrated that it's possible to apply simple machine learning methods toanalyze collected digital footprints and to create psycho-demographic profilesof individuals. However, while these works clearly demonstrated theapplicability of machine learning methods for such an analysis, created simpleprediction models still lacks accuracy necessary to be successfully applied forpractical needs. We have assumed that using advanced deep machine learningmethods may considerably increase the accuracy of predictions. We started withsimple machine learning methods to estimate basic prediction performance andmoved further by applying advanced methods based on shallow and deep neuralnetworks. Then we compared prediction power of studied models and madeconclusions about its performance. Finally, we made hypotheses how predictionaccuracy can be further improved. As result of this work, we provide fullsource code used in the experiments for all interested researchers andpractitioners in corresponding GitHub repository. We believe that applying deepmachine learning for psycho-demographic profiling may have an enormous impacton the society (for good or worse) and provides means for ArtificialIntelligence (AI) systems to better understand humans by creating theirpsychological profiles. Thus AI agents may achieve the human-like ability toparticipate in conversation (communication) flow by anticipating humanopponents' reactions, expectations, and behavior.",http://arxiv.org/abs/1703.06914v2,,
1360,Optimizing Kernel Machines using Deep Learning,"Building highly non-linear and non-parametric models is central to severalstate-of-the-art machine learning systems. Kernel methods form an importantclass of techniques that induce a reproducing kernel Hilbert space (RKHS) forinferring non-linear models through the construction of similarity functionsfrom data. These methods are particularly preferred in cases where the trainingdata sizes are limited and when prior knowledge of the data similarities isavailable. Despite their usefulness, they are limited by the computationalcomplexity and their inability to support end-to-end learning with atask-specific objective. On the other hand, deep neural networks have becomethe de facto solution for end-to-end inference in several learning paradigms.In this article, we explore the idea of using deep architectures to performkernel machine optimization, for both computational efficiency and end-to-endinferencing. To this end, we develop the DKMO (Deep Kernel MachineOptimization) framework, that creates an ensemble of dense embeddings usingNystrom kernel approximations and utilizes deep learning to generatetask-specific representations through the fusion of the embeddings.Intuitively, the filters of the network are trained to fuse information from anensemble of linear subspaces in the RKHS. Furthermore, we introduce the kerneldropout regularization to enable improved training convergence. Finally, weextend this framework to the multiple kernel case, by coupling a global fusionlayer with pre-trained deep kernel machines for each of the constituentkernels. Using case studies with limited training data, and lack of explicitfeature sources, we demonstrate the effectiveness of our framework overconventional model inferencing techniques.",http://arxiv.org/abs/1711.05374v1,,
1361,Using Machine Learning to Improve Cylindrical Algebraic Decomposition,"Cylindrical Algebraic Decomposition (CAD) is a key tool in computationalalgebraic geometry, best known as a procedure to enable Quantifier Eliminationover real-closed fields. However, it has a worst case complexity doublyexponential in the size of the input, which is often encountered in practice.It has been observed that for many problems a change in algorithm settings orproblem formulation can cause huge differences in runtime costs, changingproblem instances from intractable to easy. A number of heuristics have beendeveloped to help with such choices, but the complicated nature of thegeometric relationships involved means these are imperfect and can sometimesmake poor choices. We investigate the use of machine learning (specificallysupport vector machines) to make such choices instead.  Machine learning is the process of fitting a computer model to a complexfunction based on properties learned from measured data. In this paper we applyit in two case studies: the first to select between heuristics for choosing aCAD variable ordering; the second to identify when a CAD problem instance wouldbenefit from Groebner Basis preconditioning. These appear to be the first suchapplications of machine learning to Symbolic Computation. We demonstrate inboth cases that the machine learned choice outperforms human developedheuristics.",http://arxiv.org/abs/1804.10520v1,,
1362,Privacy-preserving Machine Learning through Data Obfuscation,"As machine learning becomes a practice and commodity, numerous cloud-basedservices and frameworks are provided to help customers develop and deploymachine learning applications. While it is prevalent to outsource modeltraining and serving tasks in the cloud, it is important to protect the privacyof sensitive samples in the training dataset and prevent information leakage tountrusted third parties. Past work have shown that a malicious machine learningservice provider or end user can easily extract critical information about thetraining samples, from the model parameters or even just model outputs.  In this paper, we propose a novel and generic methodology to preserve theprivacy of training data in machine learning applications. Specifically weintroduce an obfuscate function and apply it to the training data beforefeeding them to the model training task. This function adds random noise toexisting samples, or augments the dataset with new samples. By doing sosensitive information about the properties of individual samples, orstatistical properties of a group of samples, is hidden. Meanwhile the modeltrained from the obfuscated dataset can still achieve high accuracy. With thisapproach, the customers can safely disclose the data or models to third-partyproviders or end users without the need to worry about data privacy. Ourexperiments show that this approach can effective defeat four existing types ofmachine learning privacy attacks at negligible accuracy cost.",http://arxiv.org/abs/1807.01860v2,,
1363,"Shedding Light on Black Box Machine Learning Algorithms: Development of  an Axiomatic Framework to Assess the Quality of Methods that Explain  Individual Predictions","From self-driving vehicles and back-flipping robots to virtual assistants whobook our next appointment at the hair salon or at that restaurant for dinner -machine learning systems are becoming increasingly ubiquitous. The main reasonfor this is that these methods boast remarkable predictive capabilities.However, most of these models remain black boxes, meaning that it is verychallenging for humans to follow and understand their intricate inner workings.Consequently, interpretability has suffered under this ever-increasingcomplexity of machine learning models. Especially with regards to newregulations, such as the General Data Protection Regulation (GDPR), thenecessity for plausibility and verifiability of predictions made by these blackboxes is indispensable. Driven by the needs of industry and practice, theresearch community has recognised this interpretability problem and focussed ondeveloping a growing number of so-called explanation methods over the past fewyears. These methods explain individual predictions made by black box machinelearning models and help to recover some of the lost interpretability. With theproliferation of these explanation methods, it is, however, often unclear,which explanation method offers a higher explanation quality, or is generallybetter-suited for the situation at hand. In this thesis, we thus propose anaxiomatic framework, which allows comparing the quality of differentexplanation methods amongst each other. Through experimental validation, wefind that the developed framework is useful to assess the explanation qualityof different explanation methods and reach conclusions that are consistent withindependent research.",http://arxiv.org/abs/1808.05054v1,,
1364,Traffic Density Estimation using a Convolutional Neural Network,"The goal of this project is to introduce and present a machine learningapplication that aims to improve the quality of life of people in Singapore. Inparticular, we investigate the use of machine learning solutions to tackle theproblem of traffic congestion in Singapore. In layman's terms, we seek to makeSingapore (or any other city) a smoother place. To accomplish this aim, wepresent an end-to-end system comprising of 1. A traffic density estimationalgorithm at traffic lights/junctions and 2. a suitable traffic signal controlalgorithms that make use of the density information for better traffic control.Traffic density estimation can be obtained from traffic junction images usingvarious machine learning techniques (combined with CV tools). After researchinto various advanced machine learning methods, we decided on convolutionalneural networks (CNNs). We conducted experiments on our algorithms, using thepublicly available traffic camera dataset published by the Land TransportAuthority (LTA) to demonstrate the feasibility of this approach. With thesetraffic density estimates, different traffic algorithms can be applied tominimize congestion at traffic junctions in general.",http://arxiv.org/abs/1809.01564v1,,
1365,A Marauder's Map of Security and Privacy in Machine Learning,"There is growing recognition that machine learning (ML) exposes new securityand privacy vulnerabilities in software systems, yet the technical community'sunderstanding of the nature and extent of these vulnerabilities remains limitedbut expanding. In this talk, we explore the threat model space of ML algorithmsthrough the lens of Saltzer and Schroeder's principles for the design of securecomputer systems. This characterization of the threat space prompts aninvestigation of current and future research directions. We structure ourdiscussion around three of these directions, which we believe are likely tolead to significant progress. The first encompasses a spectrum of approaches toverification and admission control, which is a prerequisite to enable fail-safedefaults in machine learning systems. The second seeks to design mechanisms forassembling reliable records of compromise that would help understand the degreeto which vulnerabilities are exploited by adversaries, as well as favorpsychological acceptability of machine learning applications. The third pursuesformal frameworks for security and privacy in machine learning, which we argueshould strive to align machine learning goals such as generalization withsecurity and privacy desiderata like robustness or privacy. Key insightsresulting from these three directions pursued both in the ML and securitycommunities are identified and the effectiveness of approaches are related tostructural elements of ML algorithms and the data used to train them. Weconclude by systematizing best practices in our community.",http://arxiv.org/abs/1811.01134v1,,
1366,BCCNet: Bayesian classifier combination neural network,"Machine learning research for developing countries can demonstrate clearsustainable impact by delivering actionable and timely information toin-country government organisations (GOs) and NGOs in response to theircritical information requirements. We co-create products with UK and in-countrycommercial, GO and NGO partners to ensure the machine learning algorithmsaddress appropriate user needs whether for tactical decision making orevidence-based policy decisions. In one particular case, we developed anddeployed a novel algorithm, BCCNet, to quickly process large quantities ofunstructured data to prevent and respond to natural disasters. Crowdsourcingprovides an efficient mechanism to generate labels from unstructured data toprime machine learning algorithms for large scale data analysis. However, theselabels are often imperfect with qualities varying among different citizenscientists, which prohibits their direct use with many state-of-the-art machinelearning techniques. We describe BCCNet, a framework that simultaneouslyaggregates biased and contradictory labels from the crowd and trains anautomatic classifier to process new data. Our case studies, mosquito sounddetection for malaria prevention and damage detection for disaster response,show the efficacy of our method in the challenging context of developing worldapplications.",http://arxiv.org/abs/1811.12258v1,,
1367,Analysis of Machine Learning for Link Quality Estimation,"Since the emergence of wireless communication networks, quality aspects ofwireless links have been among the main focus of many studies.  The analysis of the rich body of existing literature on link qualityestimation that uses models built from data traces indicates that thetechniques used for modeling are becoming increasingly complex. Several recentestimators use machine learning techniques that require a complex design anddevelopment process, and each step of this process has the potential tosignificantly impact the final performance.  The aim of this paper is to provide an in-depth study of how each step in theprocess of designing and developing a link quality estimator based on machinelearning techniques affects its overall performance. Based on the analysis ofthe state-of-the-art, we selected a representative subset of machine learningmodels used in the literature and a representative publicly available dataset.We then performed a systematic study on the influence of the design decisionstaken in each step of the machine learning process on the performance ofmachine learning-based link quality estimators. The results indicate thatmeasurement data preprocessing and feature engineering have a higher influenceon the performance of the model than the choice of the algorithm, calling forcautious use and parameterization of the algorithm.",http://arxiv.org/abs/1812.08856v3,,
1368,"Unsupervised machine learning in atomistic simulations, between  predictions and understanding","Automated analyses of the outcome of a simulation have been an important partof atomistic modeling since the early days, addressing the need of linking thebehavior of individual atoms and the collective properties that are usually thefinal quantity of interest. Methods such as clustering and dimensionalityreduction have been used to provide a simplified, coarse-grained representationof the structure and dynamics of complex systems, from proteins tonanoparticles. In recent years, the rise of machine learning has led to an evenmore widespread use of these algorithms in atomistic modeling, and to considerdifferent classification and inference techniques as part of a coherent toolboxof data-driven approaches.  This perspective briefly reviews some of the unsupervised machine-learningmethods -- that are geared towards classification and coarse-graining ofmolecular simulations -- seen in relation to the fundamental mathematicalconcepts that underlie all machine-learning techniques. It discusses theimportance of using concise yet complete representations of atomic structuresas the starting point of the analyses, and highlights the risk of introducingpreconceived biases when using machine learning to rationalize and understandstructure-property relations. Supervised machine-learning techniques, thatexplicitly attempt to predict the properties of a material given its structure,are less susceptible to such biases. Current developments in the field suggestthat using these two classes of approaches side-by-side and in a fullyintegrated mode, while keeping in mind the relations between the data analysisframework and the fundamental physical principles, will be key for realizingthe full potential of machine learning to help understanding the behavior ofcomplex molecules and materials.",http://arxiv.org/abs/1902.05158v2,,
1369,Machine learning in policy evaluation: new tools for causal inference,"While machine learning (ML) methods have received a lot of attention inrecent years, these methods are primarily for prediction. Empirical researchersconducting policy evaluations are, on the other hand, pre-occupied with causalproblems, trying to answer counterfactual questions: what would have happenedin the absence of a policy? Because these counterfactuals can never be directlyobserved (described as the ""fundamental problem of causal inference"")prediction tools from the ML literature cannot be readily used for causalinference. In the last decade, major innovations have taken place incorporatingsupervised ML tools into estimators for causal parameters such as the averagetreatment effect (ATE). This holds the promise of attenuating modelmisspecification issues, and increasing of transparency in model selection. Oneparticularly mature strand of the literature include approaches thatincorporate supervised ML approaches in the estimation of the ATE of a binarytreatment, under the \textit{unconfoundedness} and positivity assumptions (alsoknown as exchangeability and overlap assumptions).  This article reviews popular supervised machine learning algorithms,including the Super Learner. Then, some specific uses of machine learning fortreatment effect estimation are introduced and illustrated, namely (1) tocreate balance among treated and control groups, (2) to estimate so-callednuisance models (e.g. the propensity score, or conditional expectations of theoutcome) in semi-parametric estimators that target causal parameters (e.g.targeted maximum likelihood estimation or the double ML estimator), and (3) theuse of machine learning for variable selection in situations with a high numberof covariates.",http://arxiv.org/abs/1903.00402v1,,
1370,A Research Agenda: Dynamic Models to Defend Against Correlated Attacks,"In this article I describe a research agenda for securing machine learningmodels against adversarial inputs at test time. This article does not presentresults but instead shares some of my thoughts about where I think that thefield needs to go. Modern machine learning works very well on I.I.D. data: datafor which each example is drawn {\em independently} and for which thedistribution generating each example is {\em identical}. When these assumptionsare relaxed, modern machine learning can perform very poorly. When machinelearning is used in contexts where security is a concern, it is desirable todesign models that perform well even when the input is designed by a maliciousadversary. So far most research in this direction has focused on an adversarywho violates the {\em identical} assumption, and imposes some kind ofrestricted worst-case distribution shift. I argue that machine learningsecurity researchers should also address the problem of relaxing the {\emindependence} assumption and that current strategies designed for robustness todistribution shift will not do so. I recommend {\em dynamic models} that changeeach time they are run as a potential solution path to this problem, and showan example of a simple attack using correlated data that can be mitigated by asimple dynamic defense. This is not intended as a real-world security measure,but as a recommendation to explore this research direction and develop morerealistic defenses.",http://arxiv.org/abs/1903.06293v1,,
1371,Bayesian machine learning for quantum molecular dynamics,"This article discusses applications of Bayesian machine learning for quantummolecular dynamics. One particular formulation of quantum dynamics advocatedhere is in the form of a machine learning simulator of the Schr\""{o}dingerequation. If combined with the Bayesian statistics, such a simulator allows oneto obtain not only the quantum predictions but also the error bars of thedynamical results associated with uncertainties of inputs (such as thepotential energy surface or non-adiabatic couplings) into the nuclearSchr\""{o}dinger equation. Instead of viewing atoms as undergoing dynamics on agiven potential energy surface, Bayesian machine learning allows one toformulate the problem as the Schr\""{o}dinger equation with a non-parametricdistribution of potential energy surfaces that becomes conditioned by thedesired dynamical properties (such as the experimental measurements). Machinelearning models of the Schr\""{o}dinger equation solutions can identify thesensitivity of the dynamical properties to different parts of the potentialsurface, the collision energy, angular momentum, external field parameters andbasis sets used for the calculations. This can be used to inform the design ofefficient quantum dynamics calculations. Machine learning models can also beused to correlate rigorous results with approximate calculations, providingaccurate interpolation of exact results. Finally, there is evidence that it ispossible to build Bayesian machine learning models capable of physicallyextrapolating the solutions of the Schr\""{o}dinger equation. This isparticularly valuable as such models could complement common discovery tools toexplore physical properties at Hamiltonian parameters not accessible byrigorous quantum calculations or experiments, and potentially be used toaccelerate the numerical integration of the nuclear Schr\""{o}dinger equation.",http://arxiv.org/abs/1904.03730v1,,
1372,"FairVis: Visual Analytics for Discovering Intersectional Bias in Machine  Learning","The growing capability and accessibility of machine learning has led to itsapplication to many real-world domains and data about people. Despite thebenefits algorithmic systems may bring, models can reflect, inject, orexacerbate implicit and explicit societal biases into their outputs,disadvantaging certain demographic subgroups. Discovering which biases amachine learning model has introduced is a great challenge, due to the numerousdefinitions of fairness and the large number of potentially impacted subgroups.We present FairVis, a mixed-initiative visual analytics system that integratesa novel subgroup discovery technique for users to audit the fairness of machinelearning models. Through FairVis, users can apply domain knowledge to generateand investigate known subgroups, and explore suggested and similar subgroups.FairVis' coordinated views enable users to explore a high-level overview ofsubgroup performance and subsequently drill down into detailed investigation ofspecific subgroups. We show how FairVis helps to discover biases in two realdatasets used in predicting income and recidivism. As a visual analytics systemdevoted to discovering bias in machine learning, FairVis demonstrates howinteractive visualization may help data scientists and the general public inunderstanding and creating more equitable algorithmic systems.",http://arxiv.org/abs/1904.05419v1,,
1373,Dynamic Control Flow in Large-Scale Machine Learning,"Many recent machine learning models rely on fine-grained dynamic control flowfor training and inference. In particular, models based on recurrent neuralnetworks and on reinforcement learning depend on recurrence relations,data-dependent conditional execution, and other features that call for dynamiccontrol flow. These applications benefit from the ability to make rapidcontrol-flow decisions across a set of computing devices in a distributedsystem. For performance, scalability, and expressiveness, a machine learningsystem must support dynamic control flow in distributed and heterogeneousenvironments.  This paper presents a programming model for distributed machine learning thatsupports dynamic control flow. We describe the design of the programming model,and its implementation in TensorFlow, a distributed machine learning system.Our approach extends the use of dataflow graphs to represent machine learningmodels, offering several distinctive features. First, the branches ofconditionals and bodies of loops can be partitioned across many machines to runon a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.Second, programs written in our model support automatic differentiation anddistributed gradient computations, which are necessary for training machinelearning models that use control flow. Third, our choice of non-strictsemantics enables multiple loop iterations to execute in parallel acrossmachines, and to overlap compute and I/O operations.  We have done our work in the context of TensorFlow, and it has been usedextensively in research and production. We evaluate it using several real-worldapplications, and demonstrate its performance and scalability.",http://arxiv.org/abs/1805.01772v1,,
1374,Reinforcement Evolutionary Learning Method for self-learning,"In statistical modelling the biggest threat is concept drift which makes themodel gradually showing deteriorating performance over time. There are state ofthe art methodologies to detect the impact of concept drift, however generalstrategy considered to overcome the issue in performance is to rebuild orre-calibrate the model periodically as the variable patterns for the modelchanges significantly due to market change or consumer behavior change etc.Quantitative research is the most widely spread application of data science inMarketing or financial domain where applicability of state of the artreinforcement learning for auto-learning is less explored paradigm.Reinforcement learning is heavily dependent on having a simulated environmentwhich is majorly available for gaming or online systems, to learn from the livefeedback. However, there are some research happened on the area of onlineadvertisement, pricing etc where due to the nature of the online learningenvironment scope of reinforcement learning is explored. Our proposed solutionis a reinforcement learning based, true self-learning algorithm which can adaptto the data change or concept drift and auto learn and self-calibrate for thenew patterns of the data solving the problem of concept drift.  Keywords - Reinforcement learning, Genetic Algorithm, Q-learning,Classification modelling, CMA-ES, NES, Multi objective optimization, Conceptdrift, Population stability index, Incremental learning, F1-measure, PredictiveModelling, Self-learning, MCTS, AlphaGo, AlphaZero",http://arxiv.org/abs/1810.03198v1,,
1375,Supervised Neural Networks for Helioseismic Ring-Diagram Inversions,"The inversion of ring fit parameters to obtain subsurface flow maps inring-diagram analysis for 8 years of SDO observations is computationallyexpensive, requiring ~3200 CPU hours. In this paper we apply machine learningtechniques to the inversion in order to speed up calculations. Specifically, wetrain a predictor for subsurface flows using the mode fit parameters and theprevious inversion results, to replace future inversion requirements. Weutilize Artificial Neural Networks as a supervised learning method forpredicting the flows in 15 degree ring tiles. To demonstrate that the machinelearning results still contain the subtle signatures key to local helioseismicstudies, we use the machine learning results to study the recently discoveredsolar equatorial Rossby waves. The Artificial Neural Network is computationallyefficient, able to make future flow predictions of an entire Carringtonrotation in a matter of seconds, which is much faster than the current ~31 CPUhours. Initial training of the networks requires ~3 CPU hours. The trainedArtificial Neural Network can achieve a root mean-square error equal toapproximately half that reported for the velocity inversions, demonstrating theaccuracy of the machine learning (and perhaps the overestimation of theoriginal errors from the ring-diagram pipeline). We find the signature ofequatorial Rossby waves in the machine learning flows covering six years ofdata, demonstrating that small-amplitude signals are maintained. The recoveryof Rossby waves in the machine learning flow maps can be achieved with only oneCarrington rotation (27.275 days) of training data. We have shown that machinelearning can be applied to, and perform more efficiently than the currentring-diagram inversion. The computation burden of the machine learning includes3 CPU hours for initial training, then around 0.0001 CPU hours for futurepredictions.",http://arxiv.org/abs/1901.01505v1,,
1376,Human experts vs. machines in taxa recognition,"The step of expert taxa recognition currently slows down the response time ofmany bioassessments. Shifting to quicker and cheaper state-of-the-art machinelearning approaches is still met with expert scepticism towards the ability andlogic of machines. In our study, we investigate both the differences inaccuracy and in the identification logic of taxonomic experts and machines. Wepropose a systematic approach utilizing deep Convolutional Neural Nets with thetransfer learning paradigm and extensively evaluate it over a multi-label andmulti-pose taxonomic dataset specifically created for this comparison. We alsostudy the prediction accuracy on different ranks of taxonomic hierarchy indetail. Our results revealed that human experts using actual specimens yieldthe lowest classification error. However, our proposed, much faster, automatedapproach using deep Convolutional Neural Nets comes very close to humanaccuracy. Contrary to previous findings in the literature, we find thatmachines following a typical flat classification approach commonly used inmachine learning performs better than forcing machines to adopt a hierarchical,local per parent node approach used by human taxonomic experts. Finally, wepublicly share our unique dataset to serve as a public benchmark dataset inthis field.",http://arxiv.org/abs/1708.06899v3,,
1377,A Formal Measure of Machine Intelligence,"A fundamental problem in artificial intelligence is that nobody really knowswhat intelligence is. The problem is especially acute when we need to considerartificial systems which are significantly different to humans. In this paperwe approach this problem in the following way: We take a number of well knowninformal definitions of human intelligence that have been given by experts, andextract their essential features. These are then mathematically formalised toproduce a general measure of intelligence for arbitrary machines. We believethat this measure formally captures the concept of machine intelligence in thebroadest reasonable sense.",http://arxiv.org/abs/cs/0605024v1,,
1378,A Roadmap towards Machine Intelligence,"The development of intelligent machines is one of the biggest unsolvedchallenges in computer science. In this paper, we propose some fundamentalproperties these machines should have, focusing in particular on communicationand learning. We discuss a simple environment that could be used toincrementally teach a machine the basics of natural-language-basedcommunication, as a prerequisite to more complex interaction with human users.We also present some conjectures on the sort of algorithms the machine shouldsupport in order to profitably learn from the environment.",http://arxiv.org/abs/1511.08130v2,,
1379,Does Neural Machine Translation Benefit from Larger Context?,"We propose a neural machine translation architecture that models thesurrounding text in addition to the source sentence. These models lead tobetter performance, both in terms of general translation quality and pronounprediction, when trained on small corpora, although this improvement largelydisappears when trained with a larger corpus. We also discover thatattention-based neural machine translation is well suited for pronounprediction and compares favorably with other approaches that were specificallydesigned for this task.",http://arxiv.org/abs/1704.05135v1,,
1380,Relaxations for inference in restricted Boltzmann machines,"We propose a relaxation-based approximate inference algorithm that samplesnear-MAP configurations of a binary pairwise Markov random field. We experimenton MAP inference tasks in several restricted Boltzmann machines. We also useour underlying sampler to estimate the log-partition function of restrictedBoltzmann machines and compare against other sampling-based methods.",http://arxiv.org/abs/1312.6205v2,,
1381,Randomized Kernel Methods for Least-Squares Support Vector Machines,"The least-squares support vector machine is a frequently used kernel methodfor non-linear regression and classification tasks. Here we discuss severalapproximation algorithms for the least-squares support vector machineclassifier. The proposed methods are based on randomized block kernel matrices,and we show that they provide good accuracy and reliable scaling formulti-class classification problems with relatively large data sets. Also, wepresent several numerical experiments that illustrate the practicalapplicability of the proposed methods.",http://arxiv.org/abs/1703.07830v1,,
1382,Boltzmann Encoded Adversarial Machines,"Restricted Boltzmann Machines (RBMs) are a class of generative neural networkthat are typically trained to maximize a log-likelihood objective function. Weargue that likelihood-based training strategies may fail because the objectivedoes not sufficiently penalize models that place a high probability in regionswhere the training data distribution has low probability. To overcome thisproblem, we introduce Boltzmann Encoded Adversarial Machines (BEAMs). A BEAM isan RBM trained against an adversary that uses the hidden layer activations ofthe RBM to discriminate between the training data and the probabilitydistribution generated by the model. We present experiments demonstrating thatBEAMs outperform RBMs and GANs on multiple benchmarks.",http://arxiv.org/abs/1804.08682v1,,
1383,"Exploring the Use of Attention within an Neural Machine Translation  Decoder States to Translate Idioms","Idioms pose problems to almost all Machine Translation systems. This type oflanguage is very frequent in day-to-day language use and cannot be simplyignored. The recent interest in memory augmented models in the field ofLanguage Modelling has aided the systems to achieve good results by bridginglong-distance dependencies. In this paper we explore the use of such techniquesinto a Neural Machine Translation system to help in translation of idiomaticlanguage.",http://arxiv.org/abs/1810.06695v1,,
1384,Towards Neural Machine Translation for African Languages,"Given that South African education is in crisis, strategies for improvementand sustainability of high-quality, up-to-date education must be explored. Inthe migration of education online, inclusion of machine translation forlow-resourced local languages becomes necessary. This paper aims to spur theuse of current neural machine translation (NMT) techniques for low-resourcedlocal languages. The paper demonstrates state-of-the-art performance onEnglish-to-Setswana translation using the Autshumato dataset. The use of theTransformer architecture beat previous techniques by 5.33 BLEU points. Thisdemonstrates the promise of using current NMT techniques for African languages.",http://arxiv.org/abs/1811.05467v1,,
1385,Sparse Least Squares Low Rank Kernel Machines,"A general framework of least squares support vector machine with low rankkernels, referred to as LR-LSSVM, is introduced in this paper. The specialstructure of low rank kernels with a controlled model size brings sparsity aswell as computational efficiency to the proposed model. Meanwhile, a two-stepoptimization algorithm with three different criteria is proposed and variousexperiments are carried out using the example of the so-call robust RBF kernelto validate the model. The experiment results show that the performance of theproposed algorithm is comparable or superior to several existing kernelmachines.",http://arxiv.org/abs/1901.10098v1,,
1386,"The speed of quantum and classical learning for performing the k-th root  of NOT","We consider quantum learning machines -- quantum computers that modifythemselves in order to improve their performance in some way -- that aretrained to perform certain classical task, i.e. to execute a function whichtakes classical bits as input and returns classical bits as output. This allowsa fair comparison between learning efficiency of quantum and classical learningmachine in terms of the number of iterations required for completion oflearning. We find an explicit example of the task for which numericalsimulations show that quantum learning is faster than its classicalcounterpart. The task is extraction of the k-th root of NOT (NOT = logicalnegation), with k=2^m and m \in N. The reason for this speed-up is thatclassical machine requires memory of size log k=m to accomplish the learning,while the memory of a single qubit is sufficient for the quantum machine forany k.",http://arxiv.org/abs/0904.4571v2,,
1387,"Randomized co-training: from cortical neurons to machine learning and  back again","Despite its size and complexity, the human cortex exhibits strikinganatomical regularities, suggesting there may simple meta-algorithms underlyingcortical learning and computation. We expect such meta-algorithms to be ofinterest since they need to operate quickly, scalably and effectively withlittle-to-no specialized assumptions.  This note focuses on a specific question: How can neurons use vast quantitiesof unlabeled data to speed up learning from the comparatively rare labelsprovided by reward systems? As a partial answer, we propose randomizedco-training as a biologically plausible meta-algorithm satisfying the aboverequirements. As evidence, we describe a biologically-inspired algorithm,Correlated Nystrom Views (XNV) that achieves state-of-the-art performance insemi-supervised learning, and sketch work in progress on a neuronalimplementation.",http://arxiv.org/abs/1310.6536v1,,
1388,"Learning a hyperplane regressor by minimizing an exact bound on the VC  dimension","The capacity of a learning machine is measured by its Vapnik-Chervonenkisdimension, and learning machines with a low VC dimension generalize better. Itis well known that the VC dimension of SVMs can be very large or unbounded,even though they generally yield state-of-the-art learning performance. In thispaper, we show how to learn a hyperplane regressor by minimizing an exact, or\boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed asthe Minimal Complexity Machine (MCM) Regressor, involves solving a simplelinear programming problem. Experimental results show, that on a number ofbenchmark datasets, the proposed approach yields regressors with error ratesmuch less than those obtained with conventional SVM regresssors, while oftenusing fewer support vectors. On some benchmark datasets, the number of supportvectors is less than one tenth the number used by SVMs, indicating that the MCMdoes indeed learn simpler representations.",http://arxiv.org/abs/1410.4573v1,,
1389,"API design for machine learning software: experiences from the  scikit-learn project","Scikit-learn is an increasingly popular machine learning li- brary. Writtenin Python, it is designed to be simple and efficient, accessible tonon-experts, and reusable in various contexts. In this paper, we present anddiscuss our design choices for the application programming interface (API) ofthe project. In particular, we describe the simple and elegant interface sharedby all learning and processing units in the library and then discuss itsadvantages in terms of composition and reusability. The paper also comments onimplementation details specific to the Python ecosystem and analyzes obstaclesfaced by users and developers of the library.",http://arxiv.org/abs/1309.0238v1,,
1390,Bayesian Reinforcement Learning: A Survey,"Bayesian methods for machine learning have been widely investigated, yieldingprincipled methods for incorporating prior information into inferencealgorithms. In this survey, we provide an in-depth review of the role ofBayesian methods for the reinforcement learning (RL) paradigm. The majorincentives for incorporating Bayesian reasoning in RL are: 1) it provides anelegant approach to action-selection (exploration/exploitation) as a functionof the uncertainty in learning; and 2) it provides a machinery to incorporateprior knowledge into the algorithms. We first discuss models and methods forBayesian inference in the simple single-step Bandit model. We then review theextensive recent literature on Bayesian methods for model-based RL, where priorinformation can be expressed on the parameters of the Markov model. We alsopresent Bayesian methods for model-free RL, where priors are expressed over thevalue function or policy class. The objective of the paper is to provide acomprehensive survey on Bayesian RL algorithms and their theoretical andempirical properties.",http://arxiv.org/abs/1609.04436v1,,
1391,Online Learning to Rank in Stochastic Click Models,"Online learning to rank is a core problem in information retrieval andmachine learning. Many provably efficient algorithms have been recentlyproposed for this problem in specific click models. The click model is a modelof how the user interacts with a list of documents. Though these results aresignificant, their impact on practice is limited, because all proposedalgorithms are designed for specific click models and lack convergenceguarantees in other models. In this work, we propose BatchRank, the firstonline learning to rank algorithm for a broad class of click models. The classencompasses two most fundamental click models, the cascade and position-basedmodels. We derive a gap-dependent upper bound on the $T$-step regret ofBatchRank and evaluate it on a range of web search queries. We observe thatBatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, anexisting algorithm for the cascade model.",http://arxiv.org/abs/1703.02527v2,,
1392,"Towards well-specified semi-supervised model-based classifiers via  structural adaptation","Semi-supervised learning plays an important role in large-scale machinelearning. Properly using additional unlabeled data (largely available nowadays)often can improve the machine learning accuracy. However, if the machinelearning model is misspecified for the underlying true data distribution, themodel performance could be seriously jeopardized. This issue is known as modelmisspecification. To address this issue, we focus on generative models andpropose a criterion to detect the onset of model misspecification by measuringthe performance difference between models obtained using supervised andsemi-supervised learning. Then, we propose to automatically modify thegenerative models during model training to achieve an unbiased generativemodel. Rigorous experiments were carried out to evaluate the proposed methodusing two image classification data sets PASCAL VOC'07 and MIR Flickr. Ourproposed method has been demonstrated to outperform a number ofstate-of-the-art semi-supervised learning approaches for the classificationtask.",http://arxiv.org/abs/1705.00597v1,,
1393,"Learning Representations of Emotional Speech with Deep Convolutional  Generative Adversarial Networks","Automatically assessing emotional valence in human speech has historicallybeen a difficult task for machine learning algorithms. The subtle changes inthe voice of the speaker that are indicative of positive or negative emotionalstates are often ""overshadowed"" by voice characteristics relating to emotionalintensity or emotional activation. In this work we explore a representationlearning approach that automatically derives discriminative representations ofemotional speech. In particular, we investigate two machine learning strategiesto improve classifier performance: (1) utilization of unlabeled data using adeep convolutional generative adversarial network (DCGAN), and (2) multitasklearning. Within our extensive experiments we leverage a multitask annotatedemotional corpus as well as a large unlabeled meeting corpus (around 100hours). Our speaker-independent classification experiments show that inparticular the use of unlabeled data in our investigations improves performanceof the classifiers and both fully supervised baseline approaches areoutperformed considerably. We improve the classification of emotional valenceon a discrete 5-point scale to 43.88% and on a 3-point scale to 49.80%, whichis competitive to state-of-the-art performance.",http://arxiv.org/abs/1705.02394v1,,
1394,Pycobra: A Python Toolbox for Ensemble Learning and Visualisation,"We introduce \texttt{pycobra}, a Python library devoted to ensemble learning(regression and classification) and visualisation. Its main assets are theimplementation of several ensemble learning algorithms, a flexible and genericinterface to compare and blend any existing machine learning algorithmavailable in Python libraries (as long as a \texttt{predict} method is given),and visualisation tools such as Voronoi tessellations. \texttt{pycobra} isfully \texttt{scikit-learn} compatible and is released under the MITopen-source license. \texttt{pycobra} can be downloaded from the Python PackageIndex (PyPi) and Machine Learning Open Source Software (MLOSS). The currentversion (along with Jupyter notebooks, extensive documentation, and continuousintegration tests) is available at\href{https://github.com/bhargavvader/pycobra}{https://github.com/bhargavvader/pycobra}.",http://arxiv.org/abs/1707.00558v2,,
1395,Deep Learning with Topological Signatures,"Inferring topological and geometrical information from data can offer analternative perspective on machine learning problems. Methods from topologicaldata analysis, e.g., persistent homology, enable us to obtain such information,typically in the form of summary representations of topological features.However, such topological signatures often come with an unusual structure(e.g., multisets of intervals) that is highly impractical for most machinelearning techniques. While many strategies have been proposed to map thesetopological signatures into machine learning compatible representations, theysuffer from being agnostic to the target learning task. In contrast, we proposea technique that enables us to input topological signatures to deep neuralnetworks and learn a task-optimal representation during training. Our approachis realized as a novel input layer with favorable theoretical properties.Classification experiments on 2D object shapes and social network graphsdemonstrate the versatility of the approach and, in case of the latter, we evenoutperform the state-of-the-art by a large margin.",http://arxiv.org/abs/1707.04041v3,,
1396,"HTM-MAT: An online prediction software toolbox based on cortical machine  learning algorithm","HTM-MAT is a MATLAB based toolbox for implementing cortical learningalgorithms (CLA) including related cortical-like algorithms that possessesspatiotemporal properties. CLA is a suite of predictive machine learningalgorithms developed by Numenta Inc. and is based on the hierarchical temporalmemory (HTM). This paper presents an implementation of HTM-MAT with severalillustrative examples including several toy datasets and compared with twosequence learning applications employing state-of-the-art algorithms - therecurrentjs based on the Long Short-Term Memory (LSTM) algorithm and OS-ELMwhich is based on an online sequential version of the Extreme Learning Machine.The performance of HTM-MAT using two historical benchmark datasets and one realworld dataset is also compared with one of the existing sequence learningapplications, the OS-ELM. The results indicate that HTM-MAT predictions areindeed competitive and can outperform OS-ELM in sequential prediction tasks.",http://arxiv.org/abs/1708.01659v1,,
1397,Boltzmann machines for time-series,"We review Boltzmann machines extended for time-series. These models oftenhave recurrent structure, and back propagration through time (BPTT) is used tolearn their parameters. The per-step computational complexity of BPTT in onlinelearning, however, grows linearly with respect to the length of precedingtime-series (i.e., learning rule is not local in time), which limits theapplicability of BPTT in online learning. We then review dynamic Boltzmannmachines (DyBMs), whose learning rule is local in time. DyBM's learning rulerelates to spike-timing dependent plasticity (STDP), which has been postulatedand experimentally confirmed for biological neural networks.",http://arxiv.org/abs/1708.06004v3,,
1398,Medical Image Analysis using Convolutional Neural Networks: A Review,"Medical image analysis is the science of analyzing or solving medicalproblems using different image analysis techniques for affective and efficientextraction of information. It has emerged as one of the top research area inthe field of engineering and medicine. Recent years have witnessed rapid use ofmachine learning algorithms in medical image analysis. These machine learningtechniques are used to extract compact information for improved performance ofmedical image analysis system, when compared to the traditional methods thatuse extraction of handcrafted features. Deep learning is a breakthrough inmachine learning techniques that has overwhelmed the field of patternrecognition and computer vision research by providing state-of-the-art results.Deep learning provides different machine learning algorithms that model highlevel data abstractions and do not rely on handcrafted features. Recently, deeplearning methods utilizing deep convolutional neural networks have been appliedto medical image analysis providing promising results. The application areacovers the whole spectrum of medical image analysis including detection,segmentation, classification, and computer aided diagnosis. This paper presentsa review of the state-of-the-art convolutional neural network based techniquesused for medical image analysis.",http://arxiv.org/abs/1709.02250v1,,
1399,Robust Decentralized Learning Using ADMM with Unreliable Agents,"Many machine learning problems can be formulated as consensus optimizationproblems which can be solved efficiently via a cooperative multi-agent system.However, the agents in the system can be unreliable due to a variety ofreasons: noise, faults and attacks. Providing erroneous updates leads theoptimization process in a wrong direction, and degrades the performance ofdistributed machine learning algorithms. This paper considers the problem ofdecentralized learning using ADMM in the presence of unreliable agents. First,we rigorously analyze the effect of erroneous updates (in ADMM learningiterations) on the convergence behavior of multi-agent system. We show that thealgorithm linearly converges to a neighborhood of the optimal solution undercertain conditions and characterize the neighborhood size analytically. Next,we provide guidelines for network design to achieve a faster convergence. Wealso provide conditions on the erroneous updates for exact convergence to theoptimal solution. Finally, to mitigate the influence of unreliable agents, wepropose \textsf{ROAD}, a robust variant of ADMM, and show its resilience tounreliable agents with an exact convergence to the optimum.",http://arxiv.org/abs/1710.05241v3,,
1400,Extreme Learning Machine with Local Connections,"This paper is concerned with the sparsification of the input-hidden weightsof ELM (Extreme Learning Machine). For ordinary feedforward neural networks,the sparsification is usually done by introducing certain regularizationtechnique into the learning process of the network. But this strategy can notbe applied for ELM, since the input-hidden weights of ELM are supposed to berandomly chosen rather than to be learned. To this end, we propose a modifiedELM, called ELM-LC (ELM with local connections), which is designed for thesparsification of the input-hidden weights as follows: The hidden nodes and theinput nodes are divided respectively into several corresponding groups, and aninput node group is fully connected with its corresponding hidden node group,but is not connected with any other hidden node group. As in the usual ELM, thehidden-input weights are randomly given, and the hidden-output weights areobtained through a least square learning. In the numerical simulations on somebenchmark problems, the new ELM-CL behaves better than the traditional ELM.",http://arxiv.org/abs/1801.06975v1,,
1401,"Hybrid Gradient Boosting Trees and Neural Networks for Forecasting  Operating Room Data","Time series data constitutes a distinct and growing problem in machinelearning. As the corpus of time series data grows larger, deep models thatsimultaneously learn features and classify with these features can beintractable or suboptimal. In this paper, we present feature learning via longshort term memory (LSTM) networks and prediction via gradient boosting trees(XGB). Focusing on the consequential setting of electronic health record data,we predict the occurrence of hypoxemia five minutes into the future based onpast features. We make two observations: 1) long short term memory networks areeffective at capturing long term dependencies based on a single feature and 2)gradient boosting trees are capable of tractably combining a large number offeatures including static features like height and weight. With theseobservations in mind, we generate features by performing ""supervised""representation learning with LSTM networks. Augmenting the original XGB modelwith these features gives significantly better performance than eitherindividual method.",http://arxiv.org/abs/1801.07384v2,,
1402,Training Set Debugging Using Trusted Items,"Training set bugs are flaws in the data that adversely affect machinelearning. The training set is usually too large for man- ual inspection, butone may have the resources to verify a few trusted items. The set of trusteditems may not by itself be adequate for learning, so we propose an algorithmthat uses these items to identify bugs in the training set and thus im- proveslearning. Specifically, our approach seeks the smallest set of changes to thetraining set labels such that the model learned from this corrected trainingset predicts labels of the trusted items correctly. We flag the items whoselabels are changed as potential bugs, whose labels can be checked for veracityby human experts. To find the bugs in this way is a challenging combinatorialbilevel optimization problem, but it can be relaxed into a continuousoptimization problem. Ex- periments on toy and real data demonstrate that ourapproach can identify training set bugs effectively and suggest appro- priatechanges to the labels. Our algorithm is a step toward trustworthy machinelearning.",http://arxiv.org/abs/1801.08019v1,,
1403,Quantum Entanglement in Deep Learning Architectures,"Modern deep learning has enabled unprecedented achievements in variousdomains. Nonetheless, employment of machine learning for wave functionrepresentations is focused on more traditional architectures such as restrictedBoltzmann machines (RBMs) and fully-connected neural networks. In this letter,we establish that contemporary deep learning architectures, in the form of deepconvolutional and recurrent networks, can efficiently represent highlyentangled quantum systems. By constructing Tensor Network equivalents of thesearchitectures, we identify an inherent reuse of information in the networkoperation as a key trait which distinguishes them from standard Tensor Networkbased representations, and which enhances their entanglement capacity. Ourresults show that such architectures can support volume-law entanglementscaling, polynomially more efficiently than presently employed RBMs. Thus,beyond a quantification of the entanglement capacity of leading deep learningarchitectures, our analysis formally motivates a shift of trendingneural-network based wave function representations closer to thestate-of-the-art in machine learning.",http://arxiv.org/abs/1803.09780v3,,
1404,The Structure Transfer Machine Theory and Applications,"Representation learning is a fundamental but challenging problem, especiallywhen the distribution of data is unknown. We propose a new representationlearning method, termed Structure Transfer Machine (STM), which enables featurelearning process to converge at the representation expectation in aprobabilistic way. We theoretically show that such an expected value of therepresentation (mean) is achievable if the manifold structure can betransferred from the data space to the feature space. The resulting structureregularization term, named manifold loss, is incorporated into the lossfunction of the typical deep learning pipeline. The STM architecture isconstructed to enforce the learned deep representation to satisfy the intrinsicmanifold structure from the data, which results in robust features that suitvarious application scenarios, such as digit recognition, image classificationand object tracking. Compared to state-of-the-art CNN architectures, we achievethe better results on several commonly used benchmarks\footnote{The source codeis available. https://github.com/stmstmstm/stm }.",http://arxiv.org/abs/1804.00243v1,,
1405,Hyperbolic Entailment Cones for Learning Hierarchical Embeddings,"Learning graph representations via low-dimensional embeddings that preserverelevant network properties is an important class of problems in machinelearning. We here present a novel method to embed directed acyclic graphs.Following prior work, we first advocate for using hyperbolic spaces whichprovably model tree-like structures better than Euclidean geometry. Second, weview hierarchical relations as partial orders defined using a family of nestedgeodesically convex cones. We prove that these entailment cones admit anoptimal shape with a closed form expression both in the Euclidean andhyperbolic spaces, and they canonically define the embedding learning process.Experiments show significant improvements of our method over strong recentbaselines both in terms of representational capacity and generalization.",http://arxiv.org/abs/1804.01882v3,,
1406,"What we learn from learning - Understanding capabilities and limitations  of machine learning in botnet attacks","With a growing increase in botnet attacks, computer networks are constantlyunder threat from attacks that cripple cyber-infrastructure. Detecting theseattacks in real-time proves to be a difficult and resource intensive task. Oneof the pertinent methods to detect such attacks is signature based detectionusing machine learning models. This paper explores the efficacy of these modelsat detecting botnet attacks, using data captured from large-scale networkattacks. Our study provides a comprehensive overview of performancecharacteristics two machine learning models --- Random Forest and Multi-LayerPerceptron (Deep Learning) in such attack scenarios. Using Big Data analytics,the study explores the advantages, limitations, model/feature parameters, andoverall performance of using machine learning in botnet attacks /communication. With insights gained from the analysis, this work recommendsalgorithms/models for specific attacks of botnets instead of a generalizedmodel.",http://arxiv.org/abs/1805.01333v1,,
1407,"Domain Adaptive Generation of Aircraft on Satellite Imagery via  Simulated and Unsupervised Learning","Object detection and classification for aircraft are the most important tasksin the satellite image analysis. The success of modern detection andclassification methods has been based on machine learning and deep learning.One of the key requirements for those learning processes is huge data to train.However, there is an insufficient portion of aircraft since the targets are onmilitary action and oper- ation. Considering the characteristics of satelliteimagery, this paper attempts to provide a framework of the simulated andunsupervised methodology without any additional su- pervision or physicalassumptions. Finally, the qualitative and quantitative analysis revealed apotential to replenish insufficient data for machine learning platform forsatellite image analysis.",http://arxiv.org/abs/1806.03002v1,,
1408,Auto-Meta: Automated Gradient Based Meta Learner Search,"Fully automating machine learning pipelines is one of the key challenges ofcurrent artificial intelligence research, since practical machine learningoften requires costly and time-consuming human-powered processes such as modeldesign, algorithm development, and hyperparameter tuning. In this paper, weverify that automated architecture search synergizes with the effect ofgradient-based meta learning. We adopt the progressive neural architecturesearch \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimalarchitectures for meta-learners. The gradient based meta-learner whosearchitecture was automatically found achieved state-of-the-art results on the5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy,which is $11.54\%$ improvement over the result obtained by the firstgradient-based meta-learner called MAML\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work isthe first successful neural architecture search implementation in the contextof meta learning.",http://arxiv.org/abs/1806.06927v2,,
1409,Deep Learning and its Application to LHC Physics,"Machine learning has played an important role in the analysis of high-energyphysics data for decades. The emergence of deep learning in 2012 allowed formachine learning tools which could adeptly handle higher-dimensional and morecomplex problems than previously feasible. This review is aimed at the readerwho is familiar with high energy physics but not machine learning. Theconnections between machine learning and high energy physics data analysis areexplored, followed by an introduction to the core concepts of neural networks,examples of the key results demonstrating the power of deep learning foranalysis of LHC data, and discussion of future prospects and concerns.",http://arxiv.org/abs/1806.11484v1,,
1410,"YouTube for Patient Education: A Deep Learning Approach for  Understanding Medical Knowledge from User-Generated Videos","YouTube presents an unprecedented opportunity to explore how machine learningmethods can improve healthcare information dissemination. We propose aninterdisciplinary lens that synthesizes machine learning methods withhealthcare informatics themes to address the critical issue of developing ascalable algorithmic solution to evaluate videos from a health literacy andpatient education perspective. We develop a deep learning method to understandthe level of medical knowledge encoded in YouTube videos. Preliminary resultssuggest that we can extract medical knowledge from YouTube videos and classifyvideos according to the embedded knowledge with satisfying performance. Deeplearning methods show great promise in knowledge extraction, natural languageunderstanding, and image classification, especially in an era ofpatient-centric care and precision medicine.",http://arxiv.org/abs/1807.03179v1,,
1411,"Uncertainty Modelling in Deep Networks: Forecasting Short and Noisy  Series","Deep Learning is a consolidated, state-of-the-art Machine Learning tool tofit a function when provided with large data sets of examples. However, inregression tasks, the straightforward application of Deep Learning modelsprovides a point estimate of the target. In addition, the model does not takeinto account the uncertainty of a prediction. This represents a greatlimitation for tasks where communicating an erroneous prediction carries arisk. In this paper we tackle a real-world problem of forecasting impendingfinancial expenses and incomings of customers, while displaying predictablemonetary amounts on a mobile app. In this context, we investigate if we wouldobtain an advantage by applying Deep Learning models with a Heteroscedasticmodel of the variance of a network's output. Experimentally, we achieve ahigher accuracy than non-trivial baselines. More importantly, we introduce amechanism to discard low-confidence predictions, which means that they will notbe visible to users. This should help enhance the user experience of ourproduct.",http://arxiv.org/abs/1807.09011v1,,
1412,"Development of deep learning algorithms to categorize free-text notes  pertaining to diabetes: convolution neural networks achieve higher accuracy  than support vector machines","Health professionals can use natural language processing (NLP) technologieswhen reviewing electronic health records (EHR). Machine learning free-textclassifiers can help them identify problems and make critical decisions. We aimto develop deep learning neural network algorithms that identify EHR progressnotes pertaining to diabetes and validate the algorithms at two institutions.The data used are 2,000 EHR progress notes retrieved from patients withdiabetes and all notes were annotated manually as diabetic or non-diabetic.Several deep learning classifiers were developed, and their performances wereevaluated with the area under the ROC curve (AUC). The convolutional neuralnetwork (CNN) model with a separable convolution layer accurately identifieddiabetes-related notes in the Brigham and Womens Hospital testing set with thehighest AUC of 0.975. Deep learning classifiers can be used to identify EHRprogress notes pertaining to diabetes. In particular, the CNN-based classifiercan achieve a higher AUC than an SVM-based classifier.",http://arxiv.org/abs/1809.05814v1,,
1413,"Local Explanation Methods for Deep Neural Networks Lack Sensitivity to  Parameter Values","Explaining the output of a complicated machine learning model like a deepneural network (DNN) is a central challenge in machine learning. Severalproposed local explanation methods address this issue by identifying whatdimensions of a single input are most responsible for a DNN's output. The goalof this work is to assess the sensitivity of local explanations to DNNparameter values. Somewhat surprisingly, we find that DNNs withrandomly-initialized weights produce explanations that are both visually andquantitatively similar to those produced by DNNs with learned weights. Ourconjecture is that this phenomenon occurs because these explanations aredominated by the lower level features of a DNN, and that a DNN's architectureprovides a strong prior which significantly affects the representations learnedat these lower layers. NOTE: This work is now subsumed by our recentmanuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where weexpand on findings and address concerns raised in Sundararajan et. al. (2018).",http://arxiv.org/abs/1810.03307v1,,
1414,Effective Parallelisation for Machine Learning,"We present a novel parallelisation scheme that simplifies the adaptation oflearning algorithms to growing amounts of data as well as growing needs foraccurate and confident predictions in critical applications. In contrast toother parallelisation techniques, it can be applied to a broad class oflearning algorithms without further mathematical derivations and withoutwriting dedicated code, while at the same time maintaining theoreticalperformance guarantees. Moreover, our parallelisation scheme is able to reducethe runtime of many learning algorithms to polylogarithmic time onquasi-polynomially many processing units. This is a significant step towards ageneral answer to an open question on the efficient parallelisation of machinelearning algorithms in the sense of Nick's Class (NC). The cost of thisparallelisation is in the form of a larger sample complexity. Our empiricalstudy confirms the potential of our parallelisation scheme with fixed numbersof processors and instances in realistic application scenarios.",http://arxiv.org/abs/1810.03530v1,,
1415,"Reinforcement Learning based Dynamic Model Selection for Short-Term Load  Forecasting","With the growing prevalence of smart grid technology, short-term loadforecasting (STLF) becomes particularly important in power system operations.There is a large collection of methods developed for STLF, but selecting asuitable method under varying conditions is still challenging. This paperdevelops a novel reinforcement learning based dynamic model selection (DMS)method for STLF. A forecasting model pool is first built, including tenstate-of-the-art machine learning based forecasting models. Then a Q-learningagent learns the optimal policy of selecting the best forecasting model for thenext time step, based on the model performance. The optimal DMS policy isapplied to select the best model at each time step with a moving window.Numerical simulations on two-year load and weather data show that theQ-learning algorithm converges fast, resulting in effective and efficient DMS.The developed STLF model with Q-learning based DMS improves the forecastingaccuracy by approximately 50%, compared to the state-of-the-art machinelearning based STLF models.",http://arxiv.org/abs/1811.01846v1,,
1416,LEAF: A Benchmark for Federated Settings,"Modern federated networks, such as those comprised of wearable devices,mobile phones, or autonomous vehicles, generate massive amounts of data eachday. This wealth of data can help to learn models that can improve the userexperience on each device. However, learning in federated settings presents newchallenges at all stages of the machine learning pipeline. As the machinelearning community begins to tackle these challenges, we are at a critical timeto ensure that developments made in this area are grounded in real-worldassumptions. To this end, we propose LEAF, a modular benchmarking framework forlearning in federated settings. LEAF includes a suite of open-source federateddatasets, a rigorous evaluation framework, and a set of referenceimplementations, all geared toward capturing the obstacles and intricacies ofpractical federated environments.",http://arxiv.org/abs/1812.01097v2,,
1417,Active Learning Methods based on Statistical Leverage Scores,"In many real-world machine learning applications, unlabeled data are abundantwhereas class labels are expensive and scarce. An active learner aims to obtaina model of high accuracy with as few labeled instances as possible byeffectively selecting useful examples for labeling. We propose a new selectioncriterion that is based on statistical leverage scores and present two novelactive learning methods based on this criterion: ALEVS for querying singleexample at each iteration and DBALEVS for querying a batch of examples. Toassess the representativeness of the examples in the pool, ALEVS and DBALEVSuse the statistical leverage scores of the kernel matrices computed on theexamples of each class. Additionally, DBALEVS selects a diverse a set ofexamples that are highly representative but are dissimilar to already labeledexamples through maximizing a submodular set function defined with thestatistical leverage scores and the kernel matrix computed on the pool of theexamples. The submodularity property of the set scoring function let usidentify batches with a constant factor approximate to the optimal batch in anefficient manner. Our experiments on diverse datasets show that querying basedon leverage scores is a powerful strategy for active learning.",http://arxiv.org/abs/1812.02497v1,,
1418,"Using Machine Learning for Handover Optimization in Vehicular Fog  Computing","Smart mobility management would be an important prerequisite for future fogcomputing systems. In this research, we propose a learning-based handoveroptimization for the Internet of Vehicles that would assist the smoothtransition of device connections and offloaded tasks between fog nodes. Toaccomplish this, we make use of machine learning algorithms to learn fromvehicle interactions with fog nodes. Our approach uses a three-layerfeed-forward neural network to predict the correct fog node at a given locationand time with 99.2 % accuracy on a test set. We also implement a dual stackedrecurrent neural network (RNN) with long short-term memory (LSTM) cells capableof learning the latency, or cost, associated with these service requests. Wecreate a simulation in JAMScript using a dataset of real-world vehiclemovements to create a dataset to train these networks. We further propose theuse of this predictive system in a smarter request routing mechanism tominimize the service interruption during handovers between fog nodes and toanticipate areas of low coverage through a series of experiments and test themodels' performance on a test set.",http://arxiv.org/abs/1812.11652v1,,
1419,Learning Interpretable Models with Causal Guarantees,"Machine learning has shown much promise in helping improve the quality ofmedical, legal, and economic decision-making. In these applications, machinelearning models must satisfy two important criteria: (i) they must be causal,since the goal is typically to predict individual treatment effects, and (ii)they must be interpretable, so that human decision makers can validate andtrust the model predictions. There has recently been much progress along eachdirection independently, yet the state-of-the-art approaches are fundamentallyincompatible. We propose a framework for learning causal interpretablemodels---from observational data---that can be used to predict individualtreatment effects. Our framework can be used with any algorithm for learninginterpretable models. Furthermore, we prove an error bound on the treatmenteffects predicted by our model. Finally, in an experiment on real-world data,we show that the models trained using our framework significantly outperform anumber of baselines.",http://arxiv.org/abs/1901.08576v1,,
1420,Improved Adversarial Learning for Fair Classification,"Motivated by concerns that machine learning algorithms may introducesignificant bias in classification models, developing fair classifiers hasbecome an important problem in machine learning research. One importantparadigm towards this has been providing algorithms for adversarially learningfair classifiers (Zhang et al., 2018; Madras et al., 2018). We formulate theadversarial learning problem as a multi-objective optimization problem and findthe fair model using gradient descent-ascent algorithm with a modified gradientupdate step, inspired by the approach of Zhang et al., 2018. We providetheoretical insight and guarantees that formalize the heuristic argumentspresented previously towards taking such an approach. We test our approachempirically on the Adult dataset and synthetic datasets and compare againststate of the art algorithms (Celis et al., 2018; Zhang et al., 2018; Zafar etal., 2017). The results show that our models and algorithms have comparable orbetter accuracy than other algorithms while performing better in terms offairness, as measured using statistical rate or false discovery rate.",http://arxiv.org/abs/1901.10443v1,,
1421,Measuring Compositionality in Representation Learning,"Many machine learning algorithms represent input data with vector embeddingsor discrete codes. When inputs exhibit compositional structure (e.g. objectsbuilt from parts or procedures from subroutines), it is natural to ask whetherthis compositional structure is reflected in the the inputs' learnedrepresentations. While the assessment of compositionality in languages hasreceived significant attention in linguistics and adjacent fields, the machinelearning literature lacks general-purpose tools for producing gradedmeasurements of compositional structure in more general (e.g. vector-valued)representation spaces. We describe a procedure for evaluating compositionalityby measuring how well the true representation-producing model can beapproximated by a model that explicitly composes a collection of inferredrepresentational primitives. We use the procedure to provide formal andempirical characterizations of compositional structure in a variety ofsettings, exploring the relationship between compositionality and learningdynamics, human judgments, representational similarity, and generalization.",http://arxiv.org/abs/1902.07181v2,,
1422,Financial series prediction using Attention LSTM,"Financial time series prediction, especially with machine learningtechniques, is an extensive field of study. In recent times, deep learningmethods (especially time series analysis) have performed outstandingly forvarious industrial problems, with better prediction than machine learningmethods. Moreover, many researchers have used deep learning methods to predictfinancial time series with various models in recent years. In this paper, wewill compare various deep learning models, such as multilayer perceptron (MLP),one-dimensional convolutional neural networks (1D CNN), stacked long short-termmemory (stacked LSTM), attention networks, and weighted attention networks forfinancial time series prediction. In particular, attention LSTM is not onlyused for prediction, but also for visualizing intermediate outputs to analyzethe reason of prediction; therefore, we will show an example for understandingthe model prediction intuitively with attention vectors. In addition, we focuson time and factors, which lead to an easy understanding of why certain trendsare predicted when accessing a given time series table. We also modify the lossfunctions of the attention models with weighted categorical cross entropy; ourproposed model produces a 0.76 hit ratio, which is superior to those of othermethods for predicting the trends of the KOSPI 200.",http://arxiv.org/abs/1902.10877v1,,
1423,"Exploiting Synthetically Generated Data with Semi-Supervised Learning  for Small and Imbalanced Datasets","Data augmentation is rapidly gaining attention in machine learning. Syntheticdata can be generated by simple transformations or through the datadistribution. In the latter case, the main challenge is to estimate the labelassociated to new synthetic patterns. This paper studies the effect ofgenerating synthetic data by convex combination of patterns and the use ofthese as unsupervised information in a semi-supervised learning framework withsupport vector machines, avoiding thus the need to label synthetic examples. Weperform experiments on a total of 53 binary classification datasets. Ourresults show that this type of data over-sampling supports the well-knowncluster assumption in semi-supervised learning, showing outstanding results forsmall high-dimensional datasets and imbalanced learning problems.",http://arxiv.org/abs/1903.10022v1,,
1424,Unveiling phase transitions with machine learning,"The classification of phase transitions is a central and challenging task incondensed matter physics. Typically, it relies on the identification of orderparameters and the analysis of singularities in the free energy and itsderivatives. Here, we propose an alternative framework to identify quantumphase transitions, employing both unsupervised and supervised machine learningtechniques. Using the axial next-nearest neighbor Ising (ANNNI) model as abenchmark, we show how unsupervised learning can detect three phases(ferromagnetic, paramagnetic, and a cluster of the antiphase with the floatingphase) as well as two distinct regions within the paramagnetic phase. Employingsupervised learning we show that transfer learning becomes possible: a machinetrained only with nearest-neighbour interactions can learn to identify a newtype of phase occurring when next-nearest-neighbour interactions areintroduced. All our results rely on few and low dimensional input data (up totwelve lattice sites), thus providing a computational friendly and generalframework for the study of phase transitions in many-body systems.",http://arxiv.org/abs/1904.01486v1,,
1425,Learning to learn with backpropagation of Hebbian plasticity,"Hebbian plasticity is a powerful principle that allows biological brains tolearn from their lifetime experience. By contrast, artificial neural networkstrained with backpropagation generally have fixed connection weights that donot change once training is complete. While recent methods can endow neuralnetworks with long-term memories, Hebbian plasticity is currently not amenableto gradient descent. Here we derive analytical expressions for activitygradients in neural networks with Hebbian plastic connections. Using theseexpressions, we can use backpropagation to train not just the baseline weightsof the connections, but also their plasticity. As a result, the networks ""learnhow to learn"" in order to solve the problem at hand: the trained networksautomatically perform fast learning of unpredictable environmental featuresduring their lifetime, expanding the range of solvable problems. We test thealgorithm on various on-line learning tasks, including pattern completion,one-shot learning, and reversal learning. The algorithm successfully learns howto learn the relevant associations from one-shot instruction, and fine-tunesthe temporal dynamics of plasticity to allow for continual learning in responseto changing environmental parameters. We conclude that backpropagation ofHebbian plasticity offers a powerful model for lifelong learning.",http://arxiv.org/abs/1609.02228v2,,
1426,Learning to Learn: Meta-Critic Networks for Sample Efficient Learning,"We propose a novel and flexible approach to meta-learning forlearning-to-learn from only a few examples. Our framework is motivated byactor-critic reinforcement learning, but can be applied to both reinforcementand supervised learning. The key idea is to learn a meta-critic: anaction-value function neural network that learns to criticise any actor tryingto solve any specified task. For supervised learning, this corresponds to thenovel idea of a trainable task-parametrised loss generator. This meta-criticapproach provides a route to knowledge transfer that can flexibly deal withfew-shot and semi-supervised conditions for both reinforcement and supervisedlearning. Promising results are shown on both reinforcement and supervisedlearning problems.",http://arxiv.org/abs/1706.09529v1,,
1427,"Meta-Learning and Universality: Deep Representations and Gradient  Descent can Approximate any Learning Algorithm","Learning to learn is a powerful paradigm for enabling models to learn fromdata more effectively and efficiently. A popular approach to meta-learning isto train a recurrent model to read in a training dataset as input and outputthe parameters of a learned model, or output predictions for new test inputs.Alternatively, a more recent approach to meta-learning aims to acquire deeprepresentations that can be effectively fine-tuned, via standard gradientdescent, to new tasks. In this paper, we consider the meta-learning problemfrom the perspective of universality, formalizing the notion of learningalgorithm approximation and comparing the expressive power of theaforementioned recurrent models to the more recent approaches that embedgradient descent into the meta-learner. In particular, we seek to answer thefollowing question: does deep representation combined with standard gradientdescent have sufficient capacity to approximate any learning algorithm? We findthat this is indeed true, and further find, in our experiments, thatgradient-based meta-learning consistently leads to learning strategies thatgeneralize more widely compared to those represented by recurrent models.",http://arxiv.org/abs/1710.11622v3,,
1428,Socially Guided Intrinsic Motivation for Robot Learning of Motor Skills,"This paper presents a technical approach to robot learning of motor skillswhich combines active intrinsically motivated learning with imitation learning.Our architecture, called SGIM-D, allows efficient learning of high-dimensionalcontinuous sensorimotor inverse models in robots, and in particular learnsdistributions of parameterised motor policies that solve a correspondingdistribution of parameterised goals/tasks. This is made possible by thetechnical integration of imitation learning techniques within an algorithm forlearning inverse models that relies on active goal babbling. After reviewingsocial learning and intrinsic motivation approaches to action learning, wedescribe the general framework of our algorithm, before detailing itsarchitecture. In an experiment where a robot arm has to learn to use a flexiblefishing line , we illustrate that SGIM-D efficiently combines the advantages ofsocial learning and intrinsic motivation and benefits from human demonstrationproperties to learn how to produce varied outcomes in the environment, whiledeveloping more precise control policies in large spaces.",http://arxiv.org/abs/1804.07269v1,,
1429,Semi-supervised Eigenvectors for Large-scale Locally-biased Learning,"In many applications, one has side information, e.g., labels that areprovided in a semi-supervised manner, about a specific target region of a largedata set, and one wants to perform machine learning and data analysis tasks""nearby"" that prespecified target region. For example, one might be interestedin the clustering structure of a data graph near a prespecified ""seed set"" ofnodes, or one might be interested in finding partitions in an image that arenear a prespecified ""ground truth"" set of pixels. Locally-biased problems ofthis sort are particularly challenging for popular eigenvector-based machinelearning and data analysis tools. At root, the reason is that eigenvectors areinherently global quantities, thus limiting the applicability ofeigenvector-based methods in situations where one is interested in very localproperties of the data.  In this paper, we address this issue by providing a methodology to constructsemi-supervised eigenvectors of a graph Laplacian, and we illustrate how theselocally-biased eigenvectors can be used to perform locally-biased machinelearning. These semi-supervised eigenvectors capturesuccessively-orthogonalized directions of maximum variance, conditioned onbeing well-correlated with an input seed set of nodes that is assumed to beprovided in a semi-supervised manner. We show that these semi-supervisedeigenvectors can be computed quickly as the solution to a system of linearequations; and we also describe several variants of our basic method that haveimproved scaling properties. We provide several empirical examplesdemonstrating how these semi-supervised eigenvectors can be used to performlocally-biased learning; and we discuss the relationship between our resultsand recent machine learning algorithms that use global eigenvectors of thegraph Laplacian.",http://arxiv.org/abs/1304.7528v1,,
1430,Towards Geo-Distributed Machine Learning,"Latency to end-users and regulatory requirements push large companies tobuild data centers all around the world. The resulting data is ""born""geographically distributed. On the other hand, many machine learningapplications require a global view of such data in order to achieve the bestresults. These types of applications form a new class of learning problems,which we call Geo-Distributed Machine Learning (GDML). Such applications needto cope with: 1) scarce and expensive cross-data center bandwidth, and 2)growing privacy concerns that are pushing for stricter data sovereigntyregulations. Current solutions to learning from geo-distributed data sourcesrevolve around the idea of first centralizing the data in one data center, andthen training locally. As machine learning algorithms arecommunication-intensive, the cost of centralizing the data is thought to beoffset by the lower cost of intra-data center communication during training. Inthis work, we show that the current centralized practice can be far fromoptimal, and propose a system for doing geo-distributed training. Furthermore,we argue that the geo-distributed approach is structurally more amenable todealing with regulatory constraints, as raw data never leaves the source datacenter. Our empirical evaluation on three real datasets confirms the generalvalidity of our approach, and shows that GDML is not only possible but alsoadvisable in many scenarios.",http://arxiv.org/abs/1603.09035v1,,
1431,Online Machine Learning in Big Data Streams,"The area of online machine learning in big data streams covers algorithmsthat are (1) distributed and (2) work from data streams with only a limitedpossibility to store past data. The first requirement mostly concerns softwarearchitectures and efficient algorithms. The second one also imposes nontrivialtheoretical restrictions on the modeling methods: In the data stream model,older data is no longer available to revise earlier suboptimal modelingdecisions as the fresh data arrives.  In this article, we provide an overview of distributed software architecturesand libraries as well as machine learning models for online learning. Wehighlight the most important ideas for classification, regression,recommendation, and unsupervised modeling from streaming data, and we show howthey are implemented in various distributed data stream processing systems.  This article is a reference material and not a survey. We do not attempt tobe comprehensive in describing all existing methods and solutions; rather, wegive pointers to the most important resources in the field. All relatedsub-fields, online algorithms, online learning, and distributed data processingare hugely dominant in current research and development with conceptually newresearch results and software components emerging at the time of writing. Inthis article, we refer to several survey results, both for distributed dataprocessing and for online machine learning. Compared to past surveys, ourarticle is different because we discuss recommender systems in extended detail.",http://arxiv.org/abs/1802.05872v1,,
1432,Mitigation of Adversarial Attacks through Embedded Feature Selection,"Machine learning has become one of the main components for task automation inmany application domains. Despite the advancements and impressive achievementsof machine learning, it has been shown that learning algorithms can becompromised by attackers both at training and test time. Machine learningsystems are especially vulnerable to adversarial examples where smallperturbations added to the original data points can produce incorrect orunexpected outputs in the learning algorithms at test time. Mitigation of theseattacks is hard as adversarial examples are difficult to detect. Existingrelated work states that the security of machine learning systems againstadversarial examples can be weakened when feature selection is applied toreduce the systems' complexity. In this paper, we empirically disprove thisidea, showing that the relative distortion that the attacker has to introduceto succeed in the attack is greater when the target is using a reduced set offeatures. We also show that the minimal adversarial examples differstatistically more strongly from genuine examples with a lower number offeatures. However, reducing the feature count can negatively impact thesystem's performance. We illustrate the trade-off between security and accuracywith specific examples. We propose a design methodology to evaluate thesecurity of machine learning classifiers with embedded feature selectionagainst adversarial examples crafted using different attack strategies.",http://arxiv.org/abs/1808.05705v1,,
1433,Racial categories in machine learning,"Controversies around race and machine learning have sparked debate amongcomputer scientists over how to design machine learning systems that guaranteefairness. These debates rarely engage with how racial identity is embedded inour social experience, making for sociological and psychological complexity.This complexity challenges the paradigm of considering fairness to be a formalproperty of supervised learning with respect to protected personal attributes.Racial identity is not simply a personal subjective quality. For people labeled""Black"" it is an ascribed political category that has consequences for socialdifferentiation embedded in systemic patterns of social inequality achievedthrough both social and spatial segregation. In the United States, racialclassification can best be understood as a system of inherently unequal statuscategories that places whites as the most privileged category while signifyingthe Negro/black category as stigmatized. Social stigma is reinforced throughthe unequal distribution of societal rewards and goods along racial lines thatis reinforced by state, corporate, and civic institutions and practices. Thiscreates a dilemma for society and designers: be blind to racial groupdisparities and thereby reify racialized social inequality by no longermeasuring systemic inequality, or be conscious of racial categories in a waythat itself reifies race. We propose a third option. By preceding groupfairness interventions with unsupervised learning to dynamically detectpatterns of segregation, machine learning systems can mitigate the root causeof social disparities, social segregation and stratification, without furtheranchoring status categories of disadvantage.",http://arxiv.org/abs/1811.11668v1,,
1434,"Netherlands Dataset: A New Public Dataset for Machine Learning in  Seismic Interpretation","Machine learning and, more specifically, deep learning algorithms have seenremarkable growth in their popularity and usefulness in the last years. This isarguably due to three main factors: powerful computers, new techniques to traindeeper networks and larger datasets. Although the first two are readilyavailable in modern computers and ML libraries, the last one remains achallenge for many domains. It is a fact that big data is a reality in almostall fields nowadays, and geosciences are not an exception. However, to achievethe success of general-purpose applications such as ImageNet - for which thereare +14 million labeled images for 1000 target classes - we not only need moredata, we need more high-quality labeled data. When it comes to the Oil&Gasindustry, confidentiality issues hamper even more the sharing of datasets. Inthis work, we present the Netherlands interpretation dataset, a contribution tothe development of machine learning in seismic interpretation. The NetherlandsF3 dataset acquisition was carried out in the North Sea, Netherlands offshore.The data is publicly available and contains pos-stack data, 8 horizons and welllogs of 4 wells. For the purposes of our machine learning tasks, the originaldataset was reinterpreted, generating 9 horizons separating different seismicfacies intervals. The interpreted horizons were used to generate approximatelly190,000 labeled images for inlines and crosslines. Finally, we present two deeplearning applications in which the proposed dataset was employed and producedcompelling results.",http://arxiv.org/abs/1904.00770v1,,
1435,Efficient Representation Learning Using Random Walks for Dynamic Graphs,"An important part of many machine learning workflows on graphs is vertexrepresentation learning, i.e., learning a low-dimensional vector representationfor each vertex in the graph. Recently, several powerful techniques forunsupervised representation learning have been demonstrated to give thestate-of-the-art performance in downstream tasks such as vertex classificationand edge prediction. These techniques rely on random walks performed on thegraph in order to capture its structural properties. These structuralproperties are then encoded in the vector representation space.  However, most contemporary representation learning methods only apply tostatic graphs while real-world graphs are often dynamic and change over time.Static representation learning methods are not able to update the vectorrepresentations when the graph changes; therefore, they must re-generate thevector representations on an updated static snapshot of the graph regardless ofthe extent of the change in the graph. In this work, we propose computationallyefficient algorithms for vertex representation learning that extend random walkbased methods to dynamic graphs. The computation complexity of our algorithmsdepends upon the extent and rate of changes (the number of edges changed perupdate) and on the density of the graph. We empirically evaluate our algorithmson real world datasets for downstream machine learning tasks of multi-class andmulti-label vertex classification. The results show that our algorithms canachieve competitive results to the state-of-the-art methods while beingcomputationally efficient.",http://arxiv.org/abs/1901.01346v2,,
1436,"Distance metric learning based on structural neighborhoods for  dimensionality reduction and classification performance improvement","Distance metric learning can be viewed as one of the fundamental interests inpattern recognition and machine learning, which plays a pivotal role in theperformance of many learning methods. One of the effective methods in learningsuch a metric is to learn it from a set of labeled training samples. The issueof data imbalance is the most important challenge of recent methods. Thisresearch tries not only to preserve the local structures but also covers theissue of imbalanced datasets. To do this, the proposed method first tries toextract a low dimensional manifold from the input data. Then, it learns thelocal neighborhood structures and the relationship of the data points in theambient space based on the adjacencies of the same data points on the embeddedlow dimensional manifold. Using the local neighborhood relationships extractedfrom the manifold space, the proposed method learns the distance metric in away which minimizes the distance between similar data and maximizes theirdistance from the dissimilar data points. The evaluations of the proposedmethod on numerous datasets from the UCI repository of machine learning, andalso the KDDCup98 dataset as the most imbalance dataset, justify the supremacyof the proposed approach in comparison with other approaches especially whenthe imbalance factor is high.",http://arxiv.org/abs/1902.03453v1,,
1437,"Activation Analysis of a Byte-Based Deep Neural Network for Malware  Classification","Feature engineering is one of the most costly aspects of developing effectivemachine learning models, and that cost is even greater in specialized problemdomains, like malware classification, where expert skills are necessary toidentify useful features. Recent work, however, has shown that deep learningmodels can be used to automatically learn feature representations directly fromthe raw, unstructured bytes of the binaries themselves. In this paper, weexplore what these models are learning about malware. To do so, we examine thelearned features at multiple levels of resolution, from individual byteembeddings to end-to-end analysis of the model. At each step, we connect thesebyte-oriented activations to their original semantics through parsing anddisassembly of the binary to arrive at human-understandable features. Throughour results, we identify several interesting features learned by the model andtheir connection to manually-derived features typically used by traditionalmachine learning models. Additionally, we explore the impact of training datavolume and regularization on the quality of the learned features and theefficacy of the classifiers, revealing the somewhat paradoxical insight thatbetter generalization does not necessarily result in better performance forbyte-based malware classifiers.",http://arxiv.org/abs/1903.04717v2,,
1438,A survey on independence-based Markov networks learning,"This work reports the most relevant technical aspects in the problem oflearning the \emph{Markov network structure} from data. Such problem has becomeincreasingly important in machine learning, and many other application fieldsof machine learning. Markov networks, together with Bayesian networks, areprobabilistic graphical models, a widely used formalism for handlingprobability distributions in intelligent systems. Learning graphical modelsfrom data have been extensively applied for the case of Bayesian networks, butfor Markov networks learning it is not tractable in practice. However, thissituation is changing with time, given the exponential growth of computerscapacity, the plethora of available digital data, and the researching on newlearning technologies. This work stresses on a technology calledindependence-based learning, which allows the learning of the independencestructure of those networks from data in an efficient and sound manner,whenever the dataset is sufficiently large, and data is a representativesampling of the target distribution. In the analysis of such technology, thiswork surveys the current state-of-the-art algorithms for learning Markovnetworks structure, discussing its current limitations, and proposing a seriesof open problems where future works may produce some advances in the area interms of quality and efficiency. The paper concludes by opening a discussionabout how to develop a general formalism for improving the quality of thestructures learned, when data is scarce.",http://arxiv.org/abs/1108.2283v2,,
1439,"Empirical Evaluations of Active Learning Strategies in Legal Document  Review","One type of machine learning, text classification, is now regularly appliedin the legal matters involving voluminous document populations because it canreduce the time and expense associated with the review of those documents. Oneform of machine learning - Active Learning - has drawn attention from the legalcommunity because it offers the potential to make the machine learning processeven more effective. Active Learning, applied to legal documents, is considereda new technology in the legal domain and is continuously applied to alldocuments in a legal matter until an insignificant number of relevant documentsare left for review. This implementation is slightly different than traditionalimplementations of Active Learning where the process stops once achievingacceptable model performance. The purpose of this paper is twofold: (i) toquestion whether Active Learning actually is a superior learning methodologyand (ii) to highlight the ways that Active Learning can be most effectivelyapplied to real legal industry data. Unlike other studies, our experiments wereperformed against large data sets taken from recent, real-world legal matterscovering a variety of areas. We conclude that, although these experiments showthe Active Learning strategy popularly used in legal document review canquickly identify informative training documents, it becomes less effective overtime. In particular, our findings suggest this most popular form of ActiveLearning in the legal arena, where the highest-scoring documents are selectedas training examples, is in fact not the most efficient approach in mostinstances. Ultimately, a different Active Learning strategy may be best suitedto initiate the predictive modeling process but not to continue through theentire document review.",http://arxiv.org/abs/1904.01719v1,,
1440,"Transferring Knowledge Fragments for Learning Distance Metric from A  Heterogeneous Domain","The goal of transfer learning is to improve the performance of targetlearning task by leveraging information (or transferring knowledge) from otherrelated tasks. In this paper, we examine the problem of transfer distancemetric learning (DML), which usually aims to mitigate the label informationdeficiency issue in the target DML. Most of the current Transfer DML (TDML)methods are not applicable to the scenario where data are drawn fromheterogeneous domains. Some existing heterogeneous transfer learning (HTL)approaches can learn target distance metric by usually transforming the samplesof source and target domain into a common subspace. However, these approacheslack flexibility in real-world applications, and the learned transformationsare often restricted to be linear. This motivates us to develop a generalflexible heterogeneous TDML (HTDML) framework. In particular, any(linear/nonlinear) DML algorithms can be employed to learn the source metricbeforehand. Then the pre-learned source metric is represented as a set ofknowledge fragments to help target metric learning. We show how generalizationerror in the target domain could be reduced using the proposed transferstrategy, and develop novel algorithm to learn either linear or nonlineartarget metric. Extensive experiments on various applications demonstrate theeffectiveness of the proposed method.",http://arxiv.org/abs/1904.04061v1,,
1441,"eXpose: A Character-Level Convolutional Neural Network with Embeddings  For Detecting Malicious URLs, File Paths and Registry Keys","For years security machine learning research has promised to obviate the needfor signature based detection by automatically learning to detect indicators ofattack. Unfortunately, this vision hasn't come to fruition: in fact, developingand maintaining today's security machine learning systems can requireengineering resources that are comparable to that of signature-based detectionsystems, due in part to the need to develop and continuously tune the""features"" these machine learning systems look at as attacks evolve. Deeplearning, a subfield of machine learning, promises to change this by operatingon raw input signals and automating the process of feature design andextraction. In this paper we propose the eXpose neural network, which uses adeep learning approach we have developed to take generic, raw short characterstrings as input (a common case for security inputs, which include artifactslike potentially malicious URLs, file paths, named pipes, named mutexes, andregistry keys), and learns to simultaneously extract features and classifyusing character-level embeddings and convolutional neural network. In additionto completely automating the feature design and extraction process, eXposeoutperforms manual feature extraction based baselines on all of the intrusiondetection problems we tested it on, yielding a 5%-10% detection rate gain at0.1% false positive rate compared to these baselines.",http://arxiv.org/abs/1702.08568v1,,
1442,Emotion in Reinforcement Learning Agents and Robots: A Survey,"This article provides the first survey of computational models of emotion inreinforcement learning (RL) agents. The survey focuses on agent/robot emotions,and mostly ignores human user emotions. Emotions are recognized as functionalin decision-making by influencing motivation and action selection. Therefore,computational emotion models are usually grounded in the agent's decisionmaking architecture, of which RL is an important subclass. Studying emotions inRL-based agents is useful for three research fields. For machine learning (ML)researchers, emotion models may improve learning efficiency. For theinteractive ML and human-robot interaction (HRI) community, emotions cancommunicate state and enhance user investment. Lastly, it allows affectivemodelling (AM) researchers to investigate their emotion theories in asuccessful AI agent class. This survey provides background on emotion theoryand RL. It systematically addresses 1) from what underlying dimensions (e.g.,homeostasis, appraisal) emotions can be derived and how these can be modelledin RL-agents, 2) what types of emotions have been derived from thesedimensions, and 3) how these emotions may either influence the learningefficiency of the agent or be useful as social signals. We also systematicallycompare evaluation criteria, and draw connections to important RL sub-domainslike (intrinsic) motivation and model-based RL. In short, this survey providesboth a practical overview for engineers wanting to implement emotions in theirRL agents, and identifies challenges and directions for future emotion-RLresearch.",http://arxiv.org/abs/1705.05172v1,,
1443,Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning,"Learning-based pattern classifiers, including deep networks, have shownimpressive performance in several application domains, ranging from computervision to cybersecurity. However, it has also been shown that adversarial inputperturbations carefully crafted either at training or at test time can easilysubvert their predictions. The vulnerability of machine learning to such wildpatterns (also referred to as adversarial examples), along with the design ofsuitable countermeasures, have been investigated in the research field ofadversarial machine learning. In this work, we provide a thorough overview ofthe evolution of this research area over the last ten years and beyond,starting from pioneering, earlier work on the security of non-deep learningalgorithms up to more recent work aimed to understand the security propertiesof deep learning algorithms, in the context of computer vision andcybersecurity tasks. We report interesting connections between theseapparently-different lines of work, highlighting common misconceptions relatedto the security evaluation of machine-learning algorithms. We review the mainthreat models and attacks defined to this end, and discuss the main limitationsof current work, along with the corresponding future challenges towards thedesign of more secure learning algorithms.",http://arxiv.org/abs/1712.03141v2,,
1444,Crowd ideation of supervised learning problems,"Crowdsourcing is an important avenue for collecting machine learning data,but crowdsourcing can go beyond simple data collection by employing thecreativity and wisdom of crowd workers. Yet crowd participants are unlikely tobe experts in statistics or predictive modeling, and it is not clear how wellnon-experts can contribute creatively to the process of machine learning. Herewe study an end-to-end crowdsourcing algorithm where groups of non-expertworkers propose supervised learning problems, rank and categorize thoseproblems, and then provide data to train predictive models on those problems.Problem proposal includes and extends feature engineering because workerspropose the entire problem, not only the input features but also the targetvariable. We show that workers without machine learning experience cancollectively construct useful datasets and that predictive models can belearned on these datasets. In our experiments, the problems proposed by workerscovered a broad range of topics, from politics and current events to problemscapturing health behavior, demographics, and more. Workers also favoredquestions showing positively correlated relationships, which has interestingimplications given many supervised learning methods perform as well with strongnegative correlations. Proper instructions are crucial for non-experts, so wealso conducted a randomized trial to understand how different instructions mayinfluence the types of problems proposed by workers. In general, shifting thefocus of machine learning tasks from designing and training individualpredictive models to problem proposal allows crowdsourcers to designrequirements for problems of interest and then guide workers towardscontributing to the most suitable problems.",http://arxiv.org/abs/1802.05101v1,,
1445,"Scalable and Interpretable One-class SVMs with Deep Learning and Random  Fourier features","One-class support vector machine (OC-SVM) for a long time has been one of themost effective anomaly detection methods and extensively adopted in bothresearch as well as industrial applications. The biggest issue for OC-SVM isyet the capability to operate with large and high-dimensional datasets due tooptimization complexity. Those problems might be mitigated via dimensionalityreduction techniques such as manifold learning or autoencoder. However,previous work often treats representation learning and anomaly predictionseparately. In this paper, we propose autoencoder based one-class supportvector machine (AE-1SVM) that brings OC-SVM, with the aid of random Fourierfeatures to approximate the radial basis kernel, into deep learning context bycombining it with a representation learning architecture and jointly exploitstochastic gradient descent to obtain end-to-end training. Interestingly, thisalso opens up the possible use of gradient-based attribution methods to explainthe decision making for anomaly detection, which has ever been challenging as aresult of the implicit mappings between the input space and the kernel space.To the best of our knowledge, this is the first work to study theinterpretability of deep learning in anomaly detection. We evaluate our methodon a wide range of unsupervised anomaly detection tasks in which our end-to-endtraining architecture achieves a performance significantly better than theprevious work using separate training.",http://arxiv.org/abs/1804.04888v2,,
1446,SmartChoices: Hybridizing Programming and Machine Learning,"We present SmartChoices, an approach to making machine learning (ML) a firstclass citizen in programming languages. There is a growing divide in approachesto building systems: on the one hand, programming leverages human experts todefine a system while on the other hand behavior is learned from data inmachine learning. We propose to hybridize these two by leveraging the existingconcept of a variable and creating a new type called SmartChoice. SmartChoicesare akin to native variables with one important distinction: they determinetheir value using ML when evaluated. We describe the SmartChoices-interface,how it can be used in programming with minimal code changes, and demonstratethat it is an easy to use but still powerful tool by demonstrating improvementsover not using ML at all on three algorithmic problems: binary search,QuickSort, and caches. In these three examples, we replace the commonly usedheuristics with an ML model entirely encapsulated within a SmartChoice and thusrequiring minimal code changes. As opposed to previous work applying ML toalgorithmic problems, our proposed approach does not require to drop existingimplementations but seamlessly integrates into the standard softwaredevelopment workflow and gives full control to the software developer over howML methods are applied. Our implementation currently relies on standardReinforcement Learning (RL) methods. To learn faster, we use the heuristicfunction, which they are replacing, as an initial function. We show how thisinitial function can be used to speed up and stabilize learning while providinga safety net that prevents performance to become substantially worse --allowing for a safe deployment in critical applications.",http://arxiv.org/abs/1810.00619v2,,
1447,A Comprehensive Survey on Graph Neural Networks,"Deep learning has revolutionized many machine learning tasks in recent years,ranging from image classification and video processing to speech recognitionand natural language understanding. The data in these tasks are typicallyrepresented in the Euclidean space. However, there is an increasing number ofapplications where data are generated from non-Euclidean domains and arerepresented as graphs with complex relationships and interdependency betweenobjects. The complexity of graph data has imposed significant challenges onexisting machine learning algorithms. Recently, many studies on extending deeplearning approaches for graph data have emerged. In this survey, we provide acomprehensive overview of graph neural networks (GNNs) in data mining andmachine learning fields. We propose a new taxonomy to divide thestate-of-the-art graph neural networks into different categories. With a focuson graph convolutional networks, we review alternative architectures that haverecently been developed; these learning paradigms include graph attentionnetworks, graph autoencoders, graph generative networks, and graphspatial-temporal networks. We further discuss the applications of graph neuralnetworks across various domains and summarize the open source codes andbenchmarks of the existing algorithms on different learning tasks. Finally, wepropose potential research directions in this fast-growing field.",http://arxiv.org/abs/1901.00596v2,,
1448,Distributed Learning with Sublinear Communication,"In distributed statistical learning, $N$ samples are split across $m$machines and a learner wishes to use minimal communication to learn as well asif the examples were on a single machine. This model has received substantialinterest in machine learning due to its scalability and potential for parallelspeedup. However, in high-dimensional settings, where the number examples issmaller than the number of features (""dimension""), the speedup afforded bydistributed learning may be overshadowed by the cost of communicating a singleexample. This paper investigates the following question: When is it possible tolearn a $d$-dimensional model in the distributed setting with totalcommunication sublinear in $d$?  Starting with a negative result, we show that for learning $\ell_1$-boundedor sparse linear models, no algorithm can obtain optimal error untilcommunication is linear in dimension. Our main result is that that by slightlyrelaxing the standard boundedness assumptions for linear models, we can obtaindistributed algorithms that enjoy optimal error with communication logarithmicin dimension. This result is based on a family of algorithms that combinemirror descent with randomized sparsification/quantization of iterates, andextends to the general stochastic convex optimization model.",http://arxiv.org/abs/1902.11259v2,,
1449,Schema Independent Relational Learning,"Learning novel concepts and relations from relational databases is animportant problem with many applications in database systems and machinelearning. Relational learning algorithms learn the definition of a new relationin terms of existing relations in the database. Nevertheless, the same data setmay be represented under different schemas for various reasons, such asefficiency, data quality, and usability. Unfortunately, the output of currentrelational learning algorithms tends to vary quite substantially over thechoice of schema, both in terms of learning accuracy and efficiency. Thisvariation complicates their off-the-shelf application. In this paper, weintroduce and formalize the property of schema independence of relationallearning algorithms, and study both the theoretical and empirical dependence ofexisting algorithms on the common class of (de) composition schematransformations. We study both sample-based learning algorithms, which learnfrom sets of labeled examples, and query-based algorithms, which learn byasking queries to an oracle. We prove that current relational learningalgorithms are generally not schema independent. For query-based learningalgorithms we show that the (de) composition transformations influence theirquery complexity. We propose Castor, a sample-based relational learningalgorithm that achieves schema independence by leveraging data dependencies. Wesupport the theoretical results with an empirical study that demonstrates theschema dependence/independence of several algorithms on existing benchmark andreal-world datasets under (de) compositions.",http://arxiv.org/abs/1508.03846v2,,
1450,Unsupervised Learning via Meta-Learning,"A central goal of unsupervised learning is to acquire representations fromunlabeled data or experience that can be used for more effective learning ofdownstream tasks from modest amounts of labeled data. Many prior unsupervisedlearning works aim to do so by developing proxy objectives based onreconstruction, disentanglement, prediction, and other metrics. Instead, wedevelop an unsupervised meta-learning method that explicitly optimizes for theability to learn a variety of tasks from small amounts of data. To do so, weconstruct tasks from unlabeled data in an automatic way and run meta-learningover the constructed tasks. Surprisingly, we find that, when integrated withmeta-learning, relatively simple task construction mechanisms, such asclustering embeddings, lead to good performance on a variety of downstream,human-specified tasks. Our experiments across four image datasets indicate thatour unsupervised meta-learning approach acquires a learning algorithm withoutany labeled data that is applicable to a wide range of downstreamclassification tasks, improving upon the embedding learned by four priorunsupervised learning methods.",http://arxiv.org/abs/1810.02334v6,,
1451,"An Integrated Transfer Learning and Multitask Learning Approach for  Pharmacokinetic Parameter Prediction","Background: Pharmacokinetic evaluation is one of the key processes in drugdiscovery and development. However, current absorption, distribution,metabolism, excretion prediction models still have limited accuracy. Aim: Thisstudy aims to construct an integrated transfer learning and multitask learningapproach for developing quantitative structure-activity relationship models topredict four human pharmacokinetic parameters. Methods: A pharmacokineticdataset included 1104 U.S. FDA approved small molecule drugs. The datasetincluded four human pharmacokinetic parameter subsets (oral bioavailability,plasma protein binding rate, apparent volume of distribution at steady-stateand elimination half-life). The pre-trained model was trained on over 30million bioactivity data. An integrated transfer learning and multitasklearning approach was established to enhance the model generalization. Results:The pharmacokinetic dataset was split into three parts (60:20:20) for training,validation and test by the improved Maximum Dissimilarity algorithm with therepresentative initial set selection algorithm and the weighted distancefunction. The multitask learning techniques enhanced the model predictiveability. The integrated transfer learning and multitask learning modeldemonstrated the best accuracies, because deep neural networks have the generalfeature extraction ability, transfer learning and multitask learning improvedthe model generalization. Conclusions: The integrated transfer learning andmultitask learning approach with the improved dataset splitting algorithm wasfirstly introduced to predict the pharmacokinetic parameters. This method canbe further employed in drug discovery and development.",http://arxiv.org/abs/1812.09073v1,,
1452,Understanding Exhaustive Pattern Learning,"Pattern learning in an important problem in Natural Language Processing(NLP). Some exhaustive pattern learning (EPL) methods (Bod, 1992) were provedto be flawed (Johnson, 2002), while similar algorithms (Och and Ney, 2004)showed great advantages on other tasks, such as machine translation. In thisarticle, we first formalize EPL, and then show that the probability given by anEPL model is constant-factor approximation of the probability given by anensemble method that integrates exponential number of models obtained withvarious segmentations of the training data. This work for the first timeprovides theoretical justification for the widely used EPL algorithm in NLP,which was previously viewed as a flawed heuristic method. Better understandingof EPL may lead to improved pattern learning algorithms in future.",http://arxiv.org/abs/1104.3929v1,,
1453,Semi-supervised Learning with Density Based Distances,"We present a simple, yet effective, approach to Semi-Supervised Learning. Ourapproach is based on estimating density-based distances (DBD) using a shortestpath calculation on a graph. These Graph-DBD estimates can then be used in anydistance-based supervised learning method, such as Nearest Neighbor methods andSVMs with RBF kernels. In order to apply the method to very large data sets, wealso present a novel algorithm which integrates nearest neighbor computationsinto the shortest path search and can find exact shortest paths even inextremely large dense graphs. Significant runtime improvement over the commonlyused Laplacian regularization method is then shown on a large scale dataset.",http://arxiv.org/abs/1202.3702v1,,
1454,Robust learning Bayesian networks for prior belief,"Recent reports have described that learning Bayesian networks are highlysensitive to the chosen equivalent sample size (ESS) in the Bayesian Dirichletequivalence uniform (BDeu). This sensitivity often engenders some unstable orundesirable results. This paper describes some asymptotic analyses of BDeu toexplain the reasons for the sensitivity and its effects. Furthermore, thispaper presents a proposal for a robust learning score for ESS by eliminatingthe sensitive factors from the approximation of log-BDeu.",http://arxiv.org/abs/1202.3766v1,,
1455,Soft Rule Ensembles for Statistical Learning,"In this article supervised learning problems are solved using soft ruleensembles. We first review the importance sampling learning ensembles (ISLE)approach that is useful for generating hard rules. The soft rules are thenobtained with logistic regression from the corresponding hard rules. In orderto deal with the perfect separation problem related to the logistic regression,Firth's bias corrected likelihood is used. Various examples and simulationresults show that soft rule ensembles can improve predictive performance overhard rule ensembles.",http://arxiv.org/abs/1205.4476v3,,
1456,Efficient Parametric Projection Pursuit Density Estimation,"Product models of low dimensional experts are a powerful way to avoid thecurse of dimensionality. We present the ``under-complete product of experts'(UPoE), where each expert models a one dimensional projection of the data. TheUPoE is fully tractable and may be interpreted as a parametric probabilisticmodel for projection pursuit. Its ML learning rules are identical to theapproximate learning rules proposed before for under-complete ICA. We alsoderive an efficient sequential learning algorithm and discuss its relationshipto projection pursuit density estimation and feature induction algorithms foradditive random field models.",http://arxiv.org/abs/1212.2513v1,,
1457,On learning parametric-output HMMs,"We present a novel approach for learning an HMM whose outputs are distributedaccording to a parametric family. This is done by {\em decoupling} the learningtask into two steps: first estimating the output parameters, and thenestimating the hidden states transition probabilities. The first step isaccomplished by fitting a mixture model to the output stationary distribution.Given the parameters of this mixture model, the second step is formulated asthe solution of an easily solvable convex quadratic program. We provide anerror analysis for the estimated transition probabilities and show they arerobust to small perturbations in the estimates of the mixture parameters.Finally, we support our analysis with some encouraging empirical results.",http://arxiv.org/abs/1302.6009v1,,
1458,Logistic Tensor Factorization for Multi-Relational Data,"Tensor factorizations have become increasingly popular approaches for variouslearning tasks on structured data. In this work, we extend the RESCAL tensorfactorization, which has shown state-of-the-art results for multi-relationallearning, to account for the binary nature of adjacency tensors. We study theimprovements that can be gained via this approach on various benchmark datasetsand show that the logistic extension can improve the prediction resultssignificantly.",http://arxiv.org/abs/1306.2084v1,,
1459,"Horizontal and Vertical Ensemble with Deep Representation for  Classification","Representation learning, especially which by using deep learning, has beenwidely applied in classification. However, how to use limited size of labeleddata to achieve good classification performance with deep neural network, andhow can the learned features further improve classification remain indefinite.In this paper, we propose Horizontal Voting Vertical Voting and HorizontalStacked Ensemble methods to improve the classification performance of deepneural networks. In the ICML 2013 Black Box Challenge, via using these methodsindependently, Bing Xu achieved 3rd in public leaderboard, and 7th in privateleaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th inprivate leaderboard.",http://arxiv.org/abs/1306.2759v1,,
1460,Deep Learning by Scattering,"We introduce general scattering transforms as mathematical models of deepneural networks with l2 pooling. Scattering networks iteratively apply complexvalued unitary operators, and the pooling is performed by a complex modulus. Anexpected scattering defines a contractive representation of a high-dimensionalprobability distribution, which preserves its mean-square norm. We show thatunsupervised learning can be casted as an optimization of the space contractionto preserve the volume occupied by unlabeled examples, at each layer of thenetwork. Supervised learning and classification are performed with an averagedscattering, which provides scattering estimations for multiple classes.",http://arxiv.org/abs/1306.5532v2,,
1461,When does Active Learning Work?,"Active Learning (AL) methods seek to improve classifier performance whenlabels are expensive or scarce. We consider two central questions: Where doesAL work? How much does it help? To address these questions, a comprehensiveexperimental simulation study of Active Learning is presented. We consider avariety of tasks, classifiers and other AL factors, to present a broadexploration of AL performance in various settings. A precise way to quantifyperformance is needed in order to know when AL works. Thus we also present adetailed methodology for tackling the complexities of assessing AL performancein the context of this experimental study.",http://arxiv.org/abs/1408.1319v1,,
1462,Learning networks determined by the ratio of prior and data,"Recent reports have described that the equivalent sample size (ESS) in aDirichlet prior plays an important role in learning Bayesian networks. Thispaper provides an asymptotic analysis of the marginal likelihood score for aBayesian network. Results show that the ratio of the ESS and sample sizedetermine the penalty of adding arcs in learning Bayesian networks. The numberof arcs increases monotonically as the ESS increases; the number of arcsmonotonically decreases as the ESS decreases. Furthermore, the marginallikelihood score provides a unified expression of various score metrics bychanging prior knowledge.",http://arxiv.org/abs/1203.3521v1,,
1463,Unsupervised spectral learning,"In spectral clustering and spectral image segmentation, the data is partionedstarting from a given matrix of pairwise similarities S. the matrix S isconstructed by hand, or learned on a separate training set. In this paper weshow how to achieve spectral clustering in unsupervised mode. Our algorithmstarts with a set of observed pairwise features, which are possible componentsof an unknown, parametric similarity function. This function is learnediteratively, at the same time as the clustering of the data. The algorithmshows promosing results on synthetic and real data.",http://arxiv.org/abs/1207.1358v1,,
1464,Unsupervised Active Learning in Large Domains,"Active learning is a powerful approach to analyzing data effectively. We showthat the feasibility of active learning depends crucially on the choice ofmeasure with respect to which the query is being optimized. The standardinformation gain, for example, does not permit an accurate evaluation with asmall committee, a representative subset of the model space. We propose asurrogate measure requiring only a small committee and discuss the propertiesof this new measure. We devise, in addition, a bootstrap approach for committeeselection. The advantages of this approach are illustrated in the context ofrecovering (regulatory) network models.",http://arxiv.org/abs/1301.0602v1,,
1465,A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks,"This paper extends the work in [Suzuki, 1996] and presents an efficientdepth-first branch-and-bound algorithm for learning Bayesian networkstructures, based on the minimum description length (MDL) principle, for agiven (consistent) variable ordering. The algorithm exhaustively searchesthrough all network structures and guarantees to find the network with the bestMDL score. Preliminary experiments show that the algorithm is efficient, andthat the time complexity grows slowly with the sample size. The algorithm isuseful for empirically studying both the performance of suboptimal heuristicsearch algorithms and the adequacy of the MDL principle in learning Bayesiannetworks.",http://arxiv.org/abs/1301.3897v1,,
1466,Evaluation of a Supervised Learning Approach for Stock Market Operations,"Data mining methods have been widely applied in financial markets, with thepurpose of providing suitable tools for prices forecasting and automatictrading. Particularly, learning methods aim to identify patterns in time seriesand, based on such patterns, to recommend buy/sell operations. The objective ofthis work is to evaluate the performance of Random Forests, a supervisedlearning method based on ensembles of decision trees, for decision support instock markets. Preliminary results indicate good rates of successful operationsand good rates of return per operation, providing a strong motivation forfurther research in this topic.",http://arxiv.org/abs/1301.4944v1,,
1467,Semi-supervised Ranking Pursuit,"We propose a novel sparse preference learning/ranking algorithm. Ouralgorithm approximates the true utility function by a weighted sum of basisfunctions using the squared loss on pairs of data points, and is ageneralization of the kernel matching pursuit method. It can operate both in asupervised and a semi-supervised setting and allows efficient search formultiple, near-optimal solutions. Furthermore, we describe the extension of thealgorithm suitable for combined ranking and regression tasks. In ourexperiments we demonstrate that the proposed algorithm outperforms severalstate-of-the-art learning methods when taking into account unlabeled data andperforms comparably in a supervised learning scenario, while providing sparsersolutions.",http://arxiv.org/abs/1307.0846v1,,
1468,"In Search of the Real Inductive Bias: On the Role of Implicit  Regularization in Deep Learning","We present experiments demonstrating that some other form of capacitycontrol, different from network size, plays a central role in learningmultilayer feed-forward networks. We argue, partially through analogy to matrixfactorization, that this is an inductive bias that can help shed light on deeplearning.",http://arxiv.org/abs/1412.6614v4,,
1469,A Bayesian encourages dropout,"Dropout is one of the key techniques to prevent the learning fromoverfitting. It is explained that dropout works as a kind of modified L2regularization. Here, we shed light on the dropout from Bayesian standpoint.Bayesian interpretation enables us to optimize the dropout rate, which isbeneficial for learning of weight parameters and prediction after learning. Theexperiment result also encourages the optimization of the dropout.",http://arxiv.org/abs/1412.7003v3,,
1470,Model-based Reinforcement Learning and the Eluder Dimension,"We consider the problem of learning to optimize an unknown Markov decisionprocess (MDP). We show that, if the MDP can be parameterized within some knownfunction class, we can obtain regret bounds that scale with the dimensionality,rather than cardinality, of the system. We characterize this dependenceexplicitly as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ isthe Kolmogorov dimension and $d_E$ is the \emph{eluder dimension}. Theserepresent the first unified regret bounds for model-based reinforcementlearning and provide state of the art guarantees in several important settings.Moreover, we present a simple and computationally efficient algorithm\emph{posterior sampling for reinforcement learning} (PSRL) that satisfiesthese bounds.",http://arxiv.org/abs/1406.1853v2,,
1471,Learning Word Representations with Hierarchical Sparse Coding,"We propose a new method for learning word representations using hierarchicalregularization in sparse coding inspired by the linguistic study of wordmeanings. We show an efficient learning algorithm based on stochastic proximalmethods that is significantly faster than previous approaches, making itpossible to perform hierarchical sparse coding on a corpus of billions of wordtokens. Experiments on various benchmark tasks---word similarity ranking,analogies, sentence completion, and sentiment analysis---demonstrate that themethod outperforms or is competitive with state-of-the-art methods. Our wordrepresentations are available at\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.",http://arxiv.org/abs/1406.2035v2,,
1472,Compressed Online Dictionary Learning for Fast fMRI Decomposition,"We present a method for fast resting-state fMRI spatial decomposi-tions ofvery large datasets, based on the reduction of the temporal dimension beforeapplying dictionary learning on concatenated individual records from groups ofsubjects. Introducing a measure of correspondence between spatialdecompositions of rest fMRI, we demonstrates that time-reduced dictionarylearning produces result as reliable as non-reduced decompositions. We alsoshow that this reduction significantly improves computational scalability.",http://arxiv.org/abs/1602.02701v1,,
1473,Learning to Abstain from Binary Prediction,"A binary classifier capable of abstaining from making a label prediction hastwo goals in tension: minimizing errors, and avoiding abstaining unnecessarilyoften. In this work, we exactly characterize the best achievable tradeoffbetween these two goals in a general semi-supervised setting, given an ensembleof predictors of varying competence as well as unlabeled data on which we wishto predict or abstain. We give an algorithm for learning a classifier in thissetting which trades off its errors with abstentions in a minimax optimalmanner, is as efficient as linear learning and prediction, and is demonstrablypractical. Our analysis extends to a large class of loss functions and otherscenarios, including ensembles comprised of specialists that can themselvesabstain.",http://arxiv.org/abs/1602.08151v2,,
1474,"Learning with the Weighted Trace-norm under Arbitrary Sampling  Distributions","We provide rigorous guarantees on learning with the weighted trace-norm underarbitrary sampling distributions. We show that the standard weighted trace-normmight fail when the sampling distribution is not a product distribution (i.e.when row and column indexes are not selected independently), present acorrected variant for which we establish strong learning guarantees, anddemonstrate that it works better in practice. We provide guarantees whenweighting by either the true or empirical sampling distribution, and suggestthat even if the true distribution is known (or is uniform), weighting by theempirical distribution may be beneficial.",http://arxiv.org/abs/1106.4251v1,,
1475,"On Discarding, Caching, and Recalling Samples in Active Learning","We address challenges of active learning under scarce informational resourcesin non-stationary environments. In real-world settings, data labeled andintegrated into a predictive model may become invalid over time. However, thedata can become informative again with switches in context and such changes mayindicate unmodeled cyclic or other temporal dynamics. We explore principles fordiscarding, caching, and recalling labeled data points in active learning basedon computations of value of information. We review key concepts and study thevalue of the methods via investigations of predictive performance and costs ofacquiring data for simulated and real-world data sets.",http://arxiv.org/abs/1206.5274v1,,
1476,"Learning Mixed Membership Community Models in Social Tagging Networks  through Tensor Methods","Community detection in graphs has been extensively studied both in theory andin applications. However, detecting communities in hypergraphs is morechallenging. In this paper, we propose a tensor decomposition approach forguaranteed learning of communities in a special class of hypergraphs modelingsocial tagging systems or folksonomies. A folksonomy is a tripartite 3-uniformhypergraph consisting of (user, tag, resource) hyperedges. We posit aprobabilistic mixed membership community model, and prove that the tensormethod consistently learns the communities under efficient sample complexityand separation requirements.",http://arxiv.org/abs/1503.04567v2,,
1477,Iterative Regularization for Learning with Convex Loss Functions,"We consider the problem of supervised learning with convex loss functions andpropose a new form of iterative regularization based on the subgradient method.Unlike other regularization approaches, in iterative regularization noconstraint or penalization is considered, and generalization is achieved by(early) stopping an empirical iteration. We consider a nonparametric setting,in the framework of reproducing kernel Hilbert spaces, and prove finite samplebounds on the excess risk under general regularity conditions. Our studyprovides a new class of efficient regularized learning algorithms and givesinsights on the interplay between statistics and optimization in machinelearning.",http://arxiv.org/abs/1503.08985v2,,
1478,Anchored Discrete Factor Analysis,"We present a semi-supervised learning algorithm for learning discrete factoranalysis models with arbitrary structure on the latent variables. Our algorithmassumes that every latent variable has an ""anchor"", an observed variable withonly that latent variable as its parent. Given such anchors, we show that it ispossible to consistently recover moments of the latent variables and use thesemoments to learn complete models. We also introduce a new technique forimproving the robustness of method-of-moment algorithms by optimizing over themarginal polytope or its relaxations. We evaluate our algorithm using tworeal-world tasks, tag prediction on questions from the Stack Overflow websiteand medical diagnosis in an emergency department.",http://arxiv.org/abs/1511.03299v1,,
1479,Incremental Semiparametric Inverse Dynamics Learning,"This paper presents a novel approach for incremental semiparametric inversedynamics learning. In particular, we consider the mixture of two approaches:Parametric modeling based on rigid body dynamics equations and nonparametricmodeling based on incremental kernel methods, with no prior information on themechanical properties of the system. This yields to an incrementalsemiparametric approach, leveraging the advantages of both the parametric andnonparametric models. We validate the proposed technique learning the dynamicsof one arm of the iCub humanoid robot.",http://arxiv.org/abs/1601.04549v1,,
1480,Cover Tree Bayesian Reinforcement Learning,"This paper proposes an online tree-based Bayesian approach for reinforcementlearning. For inference, we employ a generalised context tree model. Thisdefines a distribution on multivariate Gaussian piecewise-linear models, whichcan be updated in closed form. The tree structure itself is constructed usingthe cover tree method, which remains efficient in high dimensional spaces. Wecombine the model with Thompson sampling and approximate dynamic programming toobtain effective exploration policies in unknown environments. The flexibilityand computational simplicity of the model render it suitable for manyreinforcement learning problems in continuous state spaces. We demonstrate thisin an experimental comparison with least squares policy iteration.",http://arxiv.org/abs/1305.1809v2,,
1481,The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited,"Hypergraphs allow one to encode higher-order relationships in data and arethus a very flexible modeling tool. Current learning methods are either basedon approximations of the hypergraphs via graphs or on tensor methods which areonly applicable under special conditions. In this paper, we present a newlearning framework on hypergraphs which fully uses the hypergraph structure.The key element is a family of regularization functionals based on the totalvariation on hypergraphs.",http://arxiv.org/abs/1312.5179v1,,
1482,Learning Transformations for Classification Forests,"This work introduces a transformation-based learner model for classificationforests. The weak learner at each split node plays a crucial role in aclassification tree. We propose to optimize the splitting objective by learninga linear transformation on subspaces using nuclear norm as the optimizationcriteria. The learned linear transformation restores a low-rank structure fordata from the same class, and, at the same time, maximizes the separationbetween different classes, thereby improving the performance of the splitfunction. Theoretical and experimental results support the proposed framework.",http://arxiv.org/abs/1312.5604v2,,
1483,Dual-to-kernel learning with ideals,"In this paper, we propose a theory which unifies kernel learning and symbolicalgebraic methods. We show that both worlds are inherently dual to each other,and we use this duality to combine the structure-awareness of algebraic methodswith the efficiency and generality of kernels. The main idea lies in relatingpolynomial rings to feature space, and ideals to manifolds, then exploitingthis generative-discriminative duality on kernel matrices. We illustrate thisby proposing two algorithms, IPCA and AVICA, for simultaneous manifold andfeature learning, and test their accuracy on synthetic and real world data.",http://arxiv.org/abs/1402.0099v1,,
1484,Subspace Learning with Partial Information,"The goal of subspace learning is to find a $k$-dimensional subspace of$\mathbb{R}^d$, such that the expected squared distance between instancevectors and the subspace is as small as possible. In this paper we studysubspace learning in a partial information setting, in which the learner canonly observe $r \le d$ attributes from each instance vector. We propose severalefficient algorithms for this task, and analyze their sample complexity",http://arxiv.org/abs/1402.4844v2,,
1485,Learning Multi-Relational Semantics Using Neural-Embedding Models,"In this paper we present a unified framework for modeling multi-relationalrepresentations, scoring, and learning, and conduct an empirical study ofseveral recent multi-relational embedding models under the framework. Weinvestigate the different choices of relation operators based on linear andbilinear transformations, and also the effects of entity representations byincorporating unsupervised vectors pre-trained on extra textual resources. Ourresults show several interesting findings, enabling the design of a simpleembedding model that achieves the new state-of-the-art performance on a popularknowledge base completion task evaluated on Freebase.",http://arxiv.org/abs/1411.4072v1,,
1486,Passing Expectation Propagation Messages with Kernel Methods,"We propose to learn a kernel-based message operator which takes as input allexpectation propagation (EP) incoming messages to a factor node and produces anoutgoing message. In ordinary EP, computing an outgoing message involvesestimating a multivariate integral which may not have an analytic expression.Learning such an operator allows one to bypass the expensive computation of theintegral during inference by directly mapping all incoming messages into anoutgoing message. The operator can be learned from training data (examples ofinput and output messages) which allows automated inference to be made on anykind of factor that can be sampled.",http://arxiv.org/abs/1501.00375v1,,
1487,Bayesian Learning for Low-Rank matrix reconstruction,"We develop latent variable models for Bayesian learning based low-rank matrixcompletion and reconstruction from linear measurements. For under-determinedsystems, the developed methods are shown to reconstruct low-rank matrices whenneither the rank nor the noise power is known a-priori. We derive relationsbetween the latent variable models and several low-rank promoting penaltyfunctions. The relations justify the use of Kronecker structured covariancematrices in a Gaussian based prior. In the methods, we use evidenceapproximation and expectation-maximization to learn the model parameters. Theperformance of the methods is evaluated through extensive numericalsimulations.",http://arxiv.org/abs/1501.05740v1,,
1488,Semi-Supervised Learning with Ladder Networks,"We combine supervised learning with unsupervised learning in deep neuralnetworks. The proposed model is trained to simultaneously minimize the sum ofsupervised and unsupervised cost functions by backpropagation, avoiding theneed for layer-wise pre-training. Our work builds on the Ladder networkproposed by Valpola (2015), which we extend by combining the model withsupervision. We show that the resulting model reaches state-of-the-artperformance in semi-supervised MNIST and CIFAR-10 classification, in additionto permutation-invariant MNIST classification with all labels.",http://arxiv.org/abs/1507.02672v2,,
1489,Churn analysis using deep convolutional neural networks and autoencoders,"Customer temporal behavioral data was represented as images in order toperform churn prediction by leveraging deep learning architectures prominent inimage classification. Supervised learning was performed on labeled data of over6 million customers using deep convolutional neural networks, which achieved anAUC of 0.743 on the test dataset using no more than 12 temporal features foreach customer. Unsupervised learning was conducted using autoencoders to betterunderstand the reasons for customer churn. Images that maximally activate thehidden units of an autoencoder trained with churned customers reveal ampleopportunities for action to be taken to prevent churn among strong data, novoice users.",http://arxiv.org/abs/1604.05377v1,,
1490,Scale Normalization,"One of the difficulties of training deep neural networks is caused byimproper scaling between layers. Scaling issues introduce exploding / gradientproblems, and have typically been addressed by careful scale-preservinginitialization. We investigate the value of preserving scale, or isometry,beyond the initial weights. We propose two methods of maintaing isometry, oneexact and one stochastic. Preliminary experiments show that for bothdeterminant and scale-normalization effectively speeds up learning. Resultssuggest that isometry is important in the beginning of learning, andmaintaining it leads to faster learning.",http://arxiv.org/abs/1604.07796v1,,
1491,Decentralized Dynamic Discriminative Dictionary Learning,"We consider discriminative dictionary learning in a distributed onlinesetting, where a network of agents aims to learn a common set of dictionaryelements of a feature space and model parameters while sequentially receivingobservations. We formulate this problem as a distributed stochastic programwith a non-convex objective and present a block variant of the Arrow-Hurwiczsaddle point algorithm to solve it. Using Lagrange multipliers to penalize thediscrepancy between them, only neighboring nodes exchange model information. Weshow that decisions made with this saddle point algorithm asymptoticallyachieve a first-order stationarity condition on average.",http://arxiv.org/abs/1605.01107v1,,
1492,"Learning and Policy Search in Stochastic Dynamical Systems with Bayesian  Neural Networks","We present an algorithm for model-based reinforcement learning that combinesBayesian neural networks (BNNs) with random roll-outs and stochasticoptimization for policy learning. The BNNs are trained by minimizing$\alpha$-divergences, allowing us to capture complicated statistical patternsin the transition dynamics, e.g. multi-modality and heteroskedasticity, whichare usually missed by other common modeling approaches. We illustrate theperformance of our method by solving a challenging benchmark where model-basedapproaches usually fail and by obtaining promising results in a real-worldscenario for controlling a gas turbine.",http://arxiv.org/abs/1605.07127v3,,
1493,"Adaptive Learning Rate via Covariance Matrix Based Preconditioning for  Deep Neural Networks","Adaptive learning rate algorithms such as RMSProp are widely used fortraining deep neural networks. RMSProp offers efficient training since it usesfirst order gradients to approximate Hessian-based preconditioning. However,since the first order gradients include noise caused by stochasticoptimization, the approximation may be inaccurate. In this paper, we propose anovel adaptive learning rate algorithm called SDProp. Its key idea is effectivehandling of the noise by preconditioning based on covariance matrix. Forvarious neural networks, our approach is more efficient and effective thanRMSProp and its variant.",http://arxiv.org/abs/1605.09593v2,,
1494,"Why is Posterior Sampling Better than Optimism for Reinforcement  Learning?","Computational results demonstrate that posterior sampling for reinforcementlearning (PSRL) dramatically outperforms algorithms driven by optimism, such asUCRL2. We provide insight into the extent of this performance boost and thephenomenon that drives it. We leverage this insight to establish an$\tilde{O}(H\sqrt{SAT})$ Bayesian expected regret bound for PSRL infinite-horizon episodic Markov decision processes, where $H$ is the horizon,$S$ is the number of states, $A$ is the number of actions and $T$ is the timeelapsed. This improves upon the best previous bound of $\tilde{O}(H S\sqrt{AT})$ for any reinforcement learning algorithm.",http://arxiv.org/abs/1607.00215v3,,
1495,Learning from Multiway Data: Simple and Efficient Tensor Regression,"Tensor regression has shown to be advantageous in learning tasks withmulti-directional relatedness. Given massive multiway data, traditional methodsare often too slow to operate on or suffer from memory bottleneck. In thispaper, we introduce subsampled tensor projected gradient to solve the problem.Our algorithm is impressively simple and efficient. It is built upon projectedgradient method with fast tensor power iterations, leveraging randomizedsketching for further acceleration. Theoretical analysis shows that ouralgorithm converges to the correct solution in fixed number of iterations. Thememory requirement grows linearly with the size of the problem. We demonstratesuperior empirical performance on both multi-linear multi-task learning andspatio-temporal applications.",http://arxiv.org/abs/1607.02535v1,,
1496,Imitation Learning with Recurrent Neural Networks,"We present a novel view that unifies two frameworks that aim to solvesequential prediction problems: learning to search (L2S) and recurrent neuralnetworks (RNN). We point out equivalences between elements of the twoframeworks. By complementing what is missing from one framework comparing tothe other, we introduce a more advanced imitation learning framework that, onone hand, augments L2S s notion of search space and, on the other hand,enhances RNNs training procedure to be more robust to compounding errorsarising from training on highly correlated examples.",http://arxiv.org/abs/1607.05241v1,,
1497,Self-Sustaining Iterated Learning,"An important result from psycholinguistics (Griffiths & Kalish, 2005) statesthat no language can be learned iteratively by rational agents in aself-sustaining manner. We show how to modify the learning process slightly inorder to achieve self-sustainability. Our work is in two parts. First, wecharacterize iterated learnability in geometric terms and show how a slight,steady increase in the lengths of the training sessions ensuresself-sustainability for any discrete language class. In the second part, wetackle the nondiscrete case and investigate self-sustainability for iteratedlinear regression. We discuss the implications of our findings to issues ofnon-equilibrium dynamics in natural algorithms.",http://arxiv.org/abs/1609.03960v1,,
1498,A Tour of TensorFlow,"Deep learning is a branch of artificial intelligence employing deep neuralnetwork architectures that has significantly advanced the state-of-the-art incomputer vision, speech recognition, natural language processing and otherdomains. In November 2015, Google released $\textit{TensorFlow}$, an opensource deep learning software library for defining, training and deployingmachine learning models. In this paper, we review TensorFlow and put it incontext of modern deep learning concepts and software. We discuss its basiccomputational paradigms and distributed execution model, its programminginterface as well as accompanying visualization toolkits. We then compareTensorFlow to alternative libraries such as Theano, Torch or Caffe on aqualitative as well as quantitative basis and finally comment on observeduse-cases of TensorFlow in academia and industry.",http://arxiv.org/abs/1610.01178v1,,
1499,"Utilization of Deep Reinforcement Learning for saccadic-based object  visual search","The paper focuses on the problem of learning saccades enabling visual objectsearch. The developed system combines reinforcement learning with a neuralnetwork for learning to predict the possible outcomes of its actions. Wevalidated the solution in three types of environment consisting of(pseudo)-randomly generated matrices of digits. The experimental verificationis followed by the discussion regarding elements required by systems mimickingthe fovea movement and possible further research directions.",http://arxiv.org/abs/1610.06492v1,,
1500,Optimal Binary Autoencoding with Pairwise Correlations,"We formulate learning of a binary autoencoder as a biconvex optimizationproblem which learns from the pairwise correlations between encoded and decodedbits. Among all possible algorithms that use this information, ours finds theautoencoder that reconstructs its inputs with worst-case optimal loss. Theoptimal decoder is a single layer of artificial neurons, emerging entirely fromthe minimax loss minimization, and with weights learned by convex optimization.All this is reflected in competitive experimental results, demonstrating thatbinary autoencoding can be done efficiently by conveying information inpairwise correlations in an optimal fashion.",http://arxiv.org/abs/1611.02268v1,,
1501,Spikes as regularizers,"We present a confidence-based single-layer feed-forward learning algorithmSPIRAL (Spike Regularized Adaptive Learning) relying on an encoding ofactivation spikes. We adaptively update a weight vector relying on confidenceestimates and activation offsets relative to previous activity. We regularizeupdates proportionally to item-level confidence and weight-specific support,loosely inspired by the observation from neurophysiology that high spike ratesare sometimes accompanied by low temporal precision. Our experiments suggestthat the new learning algorithm SPIRAL is more robust and less prone tooverfitting than both the averaged perceptron and AROW.",http://arxiv.org/abs/1611.06245v1,,
1502,Predicting Process Behaviour using Deep Learning,"Predicting business process behaviour is an important aspect of businessprocess management. Motivated by research in natural language processing, thispaper describes an application of deep learning with recurrent neural networksto the problem of predicting the next event in a business process. This is botha novel method in process prediction, which has largely relied on explicitprocess models, and also a novel application of deep learning methods. Theapproach is evaluated on two real datasets and our results surpass thestate-of-the-art in prediction precision.",http://arxiv.org/abs/1612.04600v2,,
1503,Sample-efficient Deep Reinforcement Learning for Dialog Control,"Representing a dialog policy as a recurrent neural network (RNN) isattractive because it handles partial observability, infers a latentrepresentation of state, and can be optimized with supervised learning (SL) orreinforcement learning (RL). For RL, a policy gradient approach is natural, butis sample inefficient. In this paper, we present 3 methods for reducing thenumber of dialogs required to optimize an RNN-based dialog policy with RL. Thekey idea is to maintain a second RNN which predicts the value of the currentpolicy, and to apply experience replay to both networks. On two tasks, thesemethods reduce the number of dialogs/episodes required by about a third, vs.standard policy gradient methods.",http://arxiv.org/abs/1612.06000v1,,
1504,Dictionary Learning from Incomplete Data,"This paper extends the recently proposed and theoretically justifiediterative thresholding and $K$ residual means algorithm ITKrM to learningdicionaries from incomplete/masked training data (ITKrMM). It further adaptsthe algorithm to the presence of a low rank component in the data and providesa strategy for recovering this low rank component again from incomplete data.Several synthetic experiments show the advantages of incorporating informationabout the corruption into the algorithm. Finally, image inpainting isconsidered as application example, which demonstrates the superior performanceof ITKrMM in terms of speed at similar or better reconstruction qualitycompared to its closest dictionary learning counterpart.",http://arxiv.org/abs/1701.03655v2,,
1505,Multi-view Regularized Gaussian Processes,"Gaussian processes (GPs) have been proven to be powerful tools in variousareas of machine learning. However, there are very few applications of GPs inthe scenario of multi-view learning. In this paper, we present a new GP modelfor multi-view learning. Unlike existing methods, it combines multiple views byregularizing marginal likelihood with the consistency among the posteriordistributions of latent functions from different views. Moreover, we give ageneral point selection scheme for multi-view learning and improve the proposedmodel by this criterion. Experimental results on multiple real world data setshave verified the effectiveness of the proposed model and witnessed theperformance improvement through employing this novel point selection scheme.",http://arxiv.org/abs/1701.04532v1,,
1506,Gaussian-Dirichlet Posterior Dominance in Sequential Learning,"We consider the problem of sequential learning from categorical observationsbounded in [0,1]. We establish an ordering between the Dirichlet posterior overcategorical outcomes and a Gaussian posterior under observations with N(0,1)noise. We establish that, conditioned upon identical data with at least twoobservations, the posterior mean of the categorical distribution will alwayssecond-order stochastically dominate the posterior mean of the Gaussiandistribution. These results provide a useful tool for the analysis ofsequential learning under categorical outcomes.",http://arxiv.org/abs/1702.04126v3,,
1507,Online Learning for Distribution-Free Prediction,"We develop an online learning method for prediction, which is important inproblems with large and/or streaming data sets. We formulate the learningapproach using a covariance-fitting methodology, and show that the resultingpredictor has desirable computational and distribution-free properties: It isimplemented online with a runtime that scales linearly in the number ofsamples; has a constant memory requirement; avoids local minima problems; andprunes away redundant feature dimensions without relying on restrictiveassumptions on the data distribution. In conjunction with the split conformalapproach, it also produces distribution-free prediction confidence intervals ina computationally efficient manner. The method is demonstrated on both real andsynthetic datasets.",http://arxiv.org/abs/1703.05060v1,,
1508,Diversification-Based Learning in Computing and Optimization,"Diversification-Based Learning (DBL) derives from a collection of principlesand methods introduced in the field of metaheuristics that have broadapplications in computing and optimization. We show that the DBL framework goessignificantly beyond that of the more recent Opposition-based learning (OBL)framework introduced in Tizhoosh (2005), which has become the focus of numerousresearch initiatives in machine learning and metaheuristic optimization. Weunify and extend earlier proposals in metaheuristic search (Glover, 1997,Glover and Laguna, 1997) to give a collection of approaches that are moreflexible and comprehensive than OBL for creating intensification anddiversification strategies in metaheuristic search. We also describe potentialapplications of DBL to various subfields of machine learning and optimization.",http://arxiv.org/abs/1703.07929v1,,
1509,Failures of Gradient-Based Deep Learning,"In recent years, Deep Learning has become the go-to solution for a broadrange of applications, often outperforming state-of-the-art. However, it isimportant, for both theoreticians and practitioners, to gain a deeperunderstanding of the difficulties and limitations associated with commonapproaches and algorithms. We describe four types of simple problems, for whichthe gradient-based algorithms commonly used in deep learning either fail orsuffer from significant difficulties. We illustrate the failures throughpractical experiments, and provide theoretical insights explaining theirsource, and how they might be remedied.",http://arxiv.org/abs/1703.07950v2,,
1510,GPU Activity Prediction using Representation Learning,"GPU activity prediction is an important and complex problem. This is due tothe high level of contention among thousands of parallel threads. This problemwas mostly addressed using heuristics. We propose a representation learningapproach to address this problem. We model any performance metric as a temporalfunction of the executed instructions with the intuition that the flow ofinstructions can be identified as distinct activities of the code. Ourexperiments show high accuracy and non-trivial predictive power ofrepresentation learning on a benchmark.",http://arxiv.org/abs/1703.09146v1,,
1511,Marginal likelihood based model comparison in Fuzzy Bayesian Learning,"In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL)paradigm where expert opinions can be encoded in the form of fuzzy rule basesand the hyper-parameters of the fuzzy sets can be learned from data using aBayesian approach. The present paper extends this work for selecting the mostappropriate rule base among a set of competing alternatives, which bestexplains the data, by calculating the model evidence or marginal likelihood. Weexplain why this is an attractive alternative over simply minimizing a meansquared error metric of prediction and show the validity of the propositionusing synthetic examples and a real world case study in the financial servicessector.",http://arxiv.org/abs/1703.09956v1,,
1512,Sentence Simplification with Deep Reinforcement Learning,"Sentence simplification aims to make sentences easier to read and understand.Most recent approaches draw on insights from machine translation to learnsimplification rewrites from monolingual corpora of complex and simplesentences. We address the simplification problem with an encoder-decoder modelcoupled with a deep reinforcement learning framework. Our model, which we call{\sc Dress} (as shorthand for {\bf D}eep {\bf RE}inforcement {\bf S}entence{\bf S}implification), explores the space of possible simplifications whilelearning to optimize a reward function that encourages outputs which aresimple, fluent, and preserve the meaning of the input. Experiments on threedatasets demonstrate that our model outperforms competitive simplificationsystems.",http://arxiv.org/abs/1703.10931v2,,
1513,Summarized Network Behavior Prediction,"This work studies the entity-wise topical behavior from massive network logs.Both the temporal and the spatial relationships of the behavior are exploredwith the learning architectures combing the recurrent neural network (RNN) andthe convolutional neural network (CNN). To make the behavioral data appropriatefor the spatial learning in CNN, several reduction steps are taken to form thetopical metrics and place them homogeneously like pixels in the images. Theexperimental result shows both the temporal- and the spatial- gains whencompared to a multilayer perceptron (MLP) network. A new learning frameworkcalled spatially connected convolutional networks (SCCN) is introduced to moreefficiently predict the behavior.",http://arxiv.org/abs/1705.01143v1,,
1514,"Deep Episodic Value Iteration for Model-based Meta-Reinforcement  Learning","We present a new deep meta reinforcement learner, which we call Deep EpisodicValue Iteration (DEVI). DEVI uses a deep neural network to learn a similaritymetric for a non-parametric model-based reinforcement learning algorithm. Ourmodel is trained end-to-end via back-propagation. Despite being trained usingthe model-free Q-learning objective, we show that DEVI's model-based internalstructure provides `one-shot' transfer to changes in reward and transitionstructure, even for tasks with very high-dimensional state spaces.",http://arxiv.org/abs/1705.03562v1,,
1515,Delving into adversarial attacks on deep policies,"Adversarial examples have been shown to exist for a variety of deep learningarchitectures. Deep reinforcement learning has shown promising results ontraining agent policies directly on raw inputs such as image pixels. In thispaper we present a novel study into adversarial attacks on deep reinforcementlearning polices. We compare the effectiveness of the attacks using adversarialexamples vs. random noise. We present a novel method for reducing the number oftimes adversarial examples need to be injected for a successful attack, basedon the value function. We further explore how re-training on random noise andFGSM perturbations affects the resilience against adversarial examples.",http://arxiv.org/abs/1705.06452v1,,
1516,"Information-theoretic analysis of generalization capability of learning  algorithms","We derive upper bounds on the generalization error of a learning algorithm interms of the mutual information between its input and output. The boundsprovide an information-theoretic understanding of generalization in learningproblems, and give theoretical guidelines for striking the right balancebetween data fit and generalization by controlling the input-output mutualinformation. We propose a number of methods for this purpose, among which arealgorithms that regularize the ERM algorithm with relative entropy or withrandom noise. Our work extends and leads to nontrivial improvements on therecent results of Russo and Zou.",http://arxiv.org/abs/1705.07809v2,,
1517,Unsupervised Learning of Disentangled Representations from Video,"We present a new model DrNET that learns disentangled image representationsfrom video. Our approach leverages the temporal coherence of video and a noveladversarial loss to learn a representation that factorizes each frame into astationary part and a temporally varying component. The disentangledrepresentation can be used for a range of tasks. For example, applying astandard LSTM to the time-vary components enables prediction of future frames.We evaluate our approach on a range of synthetic and real videos, demonstratingthe ability to coherently generate hundreds of steps into the future.",http://arxiv.org/abs/1705.10915v1,,
1518,Bayesian Conditional Generative Adverserial Networks,"Traditional GANs use a deterministic generator function (typically a neuralnetwork) to transform a random noise input $z$ to a sample $\mathbf{x}$ thatthe discriminator seeks to distinguish. We propose a new GAN called BayesianConditional Generative Adversarial Networks (BC-GANs) that use a randomgenerator function to transform a deterministic input $y'$ to a sample$\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, andnaturally handle unsupervised learning, supervised learning, andsemi-supervised learning problems. Experiments show that the proposed BC-GANsoutperforms the state-of-the-arts.",http://arxiv.org/abs/1706.05477v1,,
1519,Deep Learning in (and of) Agent-Based Models: A Prospectus,"A very timely issue for economic agent-based models (ABMs) is their empiricalestimation. This paper describes a line of research that could resolve theissue by using machine learning techniques, using multi-layer artificial neuralnetworks (ANNs), or so called Deep Nets. The seminal contribution by Hinton etal. (2006) introduced a fast and efficient training algorithm called DeepLearning, and there have been major breakthroughs in machine learning eversince. Economics has not yet benefited from these developments, and thereforewe believe that now is the right time to apply Deep Learning and multi-layeredneural networks to agent-based models in economics.",http://arxiv.org/abs/1706.06302v1,,
1520,"Energy-Based Sequence GANs for Recommendation and Their Connection to  Imitation Learning","Recommender systems aim to find an accurate and efficient mapping fromhistoric data of user-preferred items to a new item that is to be liked by auser. Towards this goal, energy-based sequence generative adversarial nets(EB-SeqGANs) are adopted for recommendation by learning a generative model forthe time series of user-preferred items. By recasting the energy function asthe feature function, the proposed EB-SeqGANs is interpreted as an instance ofmaximum-entropy imitation learning.",http://arxiv.org/abs/1706.09200v1,,
1521,Probabilistic Active Learning of Functions in Structural Causal Models,"We consider the problem of learning the functions computing children fromparents in a Structural Causal Model once the underlying causal graph has beenidentified. This is in some sense the second step after causal discovery.Taking a probabilistic approach to estimating these functions, we derive anatural myopic active learning scheme that identifies the intervention which isoptimally informative about all of the unknown functions jointly, givenpreviously observed data. We test the derived algorithms on simple examples, todemonstrate that they produce a structured exploration policy thatsignificantly improves on unstructured base-lines.",http://arxiv.org/abs/1706.10234v1,,
1522,"Leveraging Sparse and Dense Feature Combinations for Sentiment  Classification","Neural networks are one of the most popular approaches for many naturallanguage processing tasks such as sentiment analysis. They often outperformtraditional machine learning models and achieve the state-of-art results onmost tasks. However, many existing deep learning models are complex, difficultto train and provide a limited improvement over simpler methods. We propose asimple, robust and powerful model for sentiment classification. This modeloutperforms many deep learning models and achieves comparable results to otherdeep learning models with complex architectures on sentiment analysis datasets.We publish the code online.",http://arxiv.org/abs/1708.03940v1,,
1523,Active Learning amidst Logical Knowledge,"Structured prediction is ubiquitous in applications of machine learning suchas knowledge extraction and natural language processing. Structure often can beformulated in terms of logical constraints. We consider the question of how toperform efficient active learning in the presence of logical constraints amongvariables inferred by different classifiers. We propose several methods andprovide theoretical results that demonstrate the inappropriateness of employinguncertainty guided sampling, a commonly used active learning method.Furthermore, experiments on ten different datasets demonstrate that the methodssignificantly outperform alternatives in practice. The results are of practicalsignificance in situations where labeled data is scarce.",http://arxiv.org/abs/1709.08850v1,,
1524,KeyVec: Key-semantics Preserving Document Representations,"Previous studies have demonstrated the empirical success of word embeddingsin various applications. In this paper, we investigate the problem of learningdistributed representations for text documents which many machine learningalgorithms take as input for a number of NLP tasks.  We propose a neural network model, KeyVec, which learns documentrepresentations with the goal of preserving key semantics of the input text. Itenables the learned low-dimensional vectors to retain the topics and importantinformation from the documents that will flow to downstream tasks. Ourempirical evaluations show the superior quality of KeyVec representations intwo different document understanding tasks.",http://arxiv.org/abs/1709.09749v1,,
1525,"The Deep Ritz method: A deep learning-based numerical algorithm for  solving variational problems","We propose a deep learning based method, the Deep Ritz Method, fornumerically solving variational problems, particularly the ones that arise frompartial differential equations. The Deep Ritz method is naturally nonlinear,naturally adaptive and has the potential to work in rather high dimensions. Theframework is quite simple and fits well with the stochastic gradient descentmethod used in deep learning. We illustrate the method on several problemsincluding some eigenvalue problems.",http://arxiv.org/abs/1710.00211v1,,
1526,Deep Learning in Multiple Multistep Time Series Prediction,"The project aims to research on combining deep learning specificallyLong-Short Memory (LSTM) and basic statistics in multiple multistep time seriesprediction. LSTM can dive into all the pages and learn the general trends ofvariation in a large scope, while the well selected medians for each page cankeep the special seasonality of different pages so that the future trend willnot fluctuate too much from the reality. A recent Kaggle competition on 145KWeb Traffic Time Series Forecasting [1] is used to thoroughly illustrate andtest this idea.",http://arxiv.org/abs/1710.04373v1,,
1527,Generalization in Deep Learning,"Throughout this chapter, we provide theoretical insights into why and howdeep learning can generalize well, despite its large capacity, complexity,possible algorithmic instability, nonrobustness, and sharp minima, respondingto an open question in the literature. We also propose new open problems anddiscuss the limitations of our results.",http://arxiv.org/abs/1710.05468v4,,
1528,Similarity-based Multi-label Learning,"Multi-label classification is an important learning problem with manyapplications. In this work, we propose a principled similarity-based approachfor multi-label learning called SML. We also introduce a similarity-basedapproach for predicting the label set size. The experimental resultsdemonstrate the effectiveness of SML for multi-label classification where it isshown to compare favorably with a wide variety of existing algorithms across arange of evaluation criterion.",http://arxiv.org/abs/1710.10335v1,,
1529,Learning Linear Dynamical Systems via Spectral Filtering,"We present an efficient and practical algorithm for the online prediction ofdiscrete-time linear dynamical systems with a symmetric transition matrix. Wecircumvent the non-convex optimization problem using improper learning:carefully overparameterize the class of LDSs by a polylogarithmic factor, inexchange for convexity of the loss functions. From this arises apolynomial-time algorithm with a near-optimal regret guarantee, with ananalogous sample complexity bound for agnostic learning. Our algorithm is basedon a novel filtering technique, which may be of independent interest: weconvolve the time series with the eigenvectors of a certain Hankel matrix.",http://arxiv.org/abs/1711.00946v1,,
1530,Genetic Algorithms for Evolving Deep Neural Networks,"In recent years, deep learning methods applying unsupervised learning totrain deep layers of neural networks have achieved remarkable results innumerous fields. In the past, many genetic algorithms based methods have beensuccessfully applied to training neural networks. In this paper, we extendprevious work and propose a GA-assisted method for deep learning. Ourexperimental results indicate that this GA-assisted approach improves theperformance of a deep autoencoder, producing a sparser neural network.",http://arxiv.org/abs/1711.07655v1,,
1531,Transferring Agent Behaviors from Videos via Motion GANs,"A major bottleneck for developing general reinforcement learning agents isdetermining rewards that will yield desirable behaviors under variouscircumstances. We introduce a general mechanism for automatically specifyingmeaningful behaviors from raw pixels. In particular, we train a generativeadversarial network to produce short sub-goals represented through motiontemplates. We demonstrate that this approach generates visually meaningfulbehaviors in unknown environments with novel agents and describe how thesemotions can be used to train reinforcement learning agents.",http://arxiv.org/abs/1711.07676v1,,
1532,"Feature Selection Facilitates Learning Mixtures of Discrete Product  Distributions","Feature selection can facilitate the learning of mixtures of discrete randomvariables as they arise, e.g. in crowdsourcing tasks. Intuitively, not allworkers are equally reliable but, if the less reliable ones could beeliminated, then learning should be more robust. By analogy with Gaussianmixture models, we seek a low-order statistical approach, and here introduce analgorithm based on the (pairwise) mutual information. This induces an orderover workers that is well structured for the `one coin' model. More generally,it is justified by a goodness-of-fit measure and is validated empirically.Improvement in real data sets can be substantial.",http://arxiv.org/abs/1711.09195v1,,
1533,An Adaptive Strategy for Active Learning with Smooth Decision Boundary,"We present the first adaptive strategy for active learning in the setting ofclassification with smooth decision boundary. The problem of adaptivity (tounknown distributional parameters) has remained opened since the seminal workof Castro and Nowak (2007), which first established (active learning) rates forthis setting. While some recent advances on this problem establish adaptiverates in the case of univariate data, adaptivity in the more practical settingof multivariate data has so far remained elusive. Combining insights fromvarious recent works, we show that, for the multivariate case, a carefulreduction to univariate-adaptive strategies yield near-optimal rates withoutprior knowledge of distributional parameters.",http://arxiv.org/abs/1711.09294v1,,
1534,Trading the Twitter Sentiment with Reinforcement Learning,"This paper is to explore the possibility to use alternative data andartificial intelligence techniques to trade stocks. The efficacy of the dailyTwitter sentiment on predicting the stock return is examined using machinelearning methods. Reinforcement learning(Q-learning) is applied to generate theoptimal trading policy based on the sentiment signal. The predicting power ofthe sentiment signal is more significant if the stock price is driven by theexpectation of the company growth and when the company has a major event thatdraws the public attention. The optimal trading strategy based on reinforcementlearning outperforms the trading strategy based on the machine learningprediction.",http://arxiv.org/abs/1801.02243v1,,
1535,Deep Canonically Correlated LSTMs,"We examine Deep Canonically Correlated LSTMs as a way to learn nonlineartransformations of variable length sequences and embed them into a correlated,fixed dimensional space. We use LSTMs to transform multi-view time-series datanon-linearly while learning temporal relationships within the data. We thenperform correlation analysis on the outputs of these neural networks to find acorrelated subspace through which we get our final representation viaprojection. This work follows from previous work done on Deep CanonicalCorrelation (DCCA), in which deep feed-forward neural networks were used tolearn nonlinear transformations of data while maximizing correlation.",http://arxiv.org/abs/1801.05407v1,,
1536,What Does a TextCNN Learn?,"TextCNN, the convolutional neural network for text, is a useful deep learningalgorithm for sentence classification tasks such as sentiment analysis andquestion classification. However, neural networks have long been known as blackboxes because interpreting them is a challenging task. Researchers havedeveloped several tools to understand a CNN for image classification by deepvisualization, but research about deep TextCNNs is still insufficient. In thispaper, we are trying to understand what a TextCNN learns on two classical NLPdatasets. Our work focuses on functions of different convolutional kernels andcorrelations between convolutional kernels.",http://arxiv.org/abs/1801.06287v1,,
1537,Coordinated Exploration in Concurrent Reinforcement Learning,"We consider a team of reinforcement learning agents that concurrently learnto operate in a common environment. We identify three properties - adaptivity,commitment, and diversity - which are necessary for efficient coordinatedexploration and demonstrate that straightforward extensions to single-agentoptimistic and posterior sampling approaches fail to satisfy them. As analternative, we propose seed sampling, which extends posterior sampling in amanner that meets these requirements. Simulation results investigate howper-agent regret decreases as the number of agents grows, establishingsubstantial advantages of seed sampling over alternative exploration schemes.",http://arxiv.org/abs/1802.01282v1,,
1538,Shamap: Shape-based Manifold Learning,"For manifold learning, it is assumed that high-dimensional sample/data pointsare on an embedded low-dimensional manifold. Usually, distances among samplesare computed to represent the underlying data structure, for a specifieddistance measure such as the Euclidean distance or geodesic distance. Formanifold learning, here we propose a metric according to the angular changealong a geodesic line, thereby reflecting the underlying shape-orientedinformation or the similarity between high- and low-dimensional representationsof a data cloud. Our numerical results are described to demonstrate thefeasibility and merits of the proposed dimensionality reduction scheme",http://arxiv.org/abs/1802.05386v1,,
1539,Direct Learning to Rank and Rerank,"Learning-to-rank techniques have proven to be extremely useful forprioritization problems, where we rank items in order of their estimatedprobabilities, and dedicate our limited resources to the top-ranked items. Thiswork exposes a serious problem with the state of learning-to-rank algorithms,which is that they are based on convex proxies that lead to poorapproximations. We then discuss the possibility of ""exact"" reranking algorithmsbased on mathematical programming. We prove that a relaxed version of the""exact"" problem has the same optimal solution, and provide an empiricalanalysis.",http://arxiv.org/abs/1802.07400v1,,
1540,"Average performance analysis of the stochastic gradient method for  online PCA","This paper studies the complexity of the stochastic gradient algorithm forPCA when the data are observed in a streaming setting. We also propose anonline approach for selecting the learning rate. Simulation experiments confirmthe practical relevance of the plain stochastic gradient approach and thatdrastic improvements can be achieved by learning the learning rate.",http://arxiv.org/abs/1804.01071v1,,
1541,Online Improper Learning with an Approximation Oracle,"We revisit the question of reducing online learning to approximateoptimization of the offline problem. In this setting, we give two algorithmswith near-optimal performance in the full information setting: they guaranteeoptimal regret and require only poly-logarithmically many calls to theapproximation oracle per iteration. Furthermore, these algorithms apply to themore general improper learning problems. In the bandit setting, our algorithmalso significantly improves the best previously known oracle complexity whilemaintaining the same regret.",http://arxiv.org/abs/1804.07837v1,,
1542,"Semi-Supervised Learning with Declaratively Specified Entropy  Constraints","We propose a technique for declaratively specifying strategies forsemi-supervised learning (SSL). The proposed method can be used to specifyensembles of semi-supervised learning, as well as agreement constraints andentropic regularization constraints between these learners, and can be used tomodel both well-known heuristics such as co-training and novel domain-specificheuristics. In addition to representing individual SSL heuristics, we show thatmultiple heuristics can also be automatically combined using Bayesianoptimization methods. We show consistent improvements on a suite ofwell-studied SSL benchmarks, including a new state-of-the-art result on adifficult relation extraction task.",http://arxiv.org/abs/1804.09238v2,,
1543,Power Law in Sparsified Deep Neural Networks,"The power law has been observed in the degree distributions of manybiological neural networks. Sparse deep neural networks, which learn aneconomical representation from the data, resemble biological neural networks inmany ways. In this paper, we study if these artificial networks also exhibitproperties of the power law. Experimental results on two popular deep learningmodels, namely, multilayer perceptrons and convolutional neural networks, areaffirmative. The power law is also naturally related to preferentialattachment. To study the dynamical properties of deep networks in continuallearning, we propose an internal preferential attachment model to explain howthe network topology evolves. Experimental results show that with the arrivalof a new task, the new connections made follow this preferential attachmentprocess.",http://arxiv.org/abs/1805.01891v1,,
1544,GAN Q-learning,"Distributional reinforcement learning (distributional RL) has seen empiricalsuccess in complex Markov Decision Processes (MDPs) in the setting of nonlinearfunction approximation. However, there are many different ways in which one canleverage the distributional approach to reinforcement learning. In this paper,we propose GAN Q-learning, a novel distributional RL method based on generativeadversarial networks (GANs) and analyze its performance in simple tabularenvironments, as well as OpenAI Gym. We empirically show that our algorithmleverages the flexibility and blackbox approach of deep learning models whileproviding a viable alternative to traditional methods.",http://arxiv.org/abs/1805.04874v3,,
1545,Invariant Representations from Adversarially Censored Autoencoders,"We combine conditional variational autoencoders (VAE) with adversarialcensoring in order to learn invariant representations that are disentangledfrom nuisance/sensitive variations. In this method, an adversarial networkattempts to recover the nuisance variable from the representation, which theVAE is trained to prevent. Conditioning the decoder on the nuisance variableenables clean separation of the representation, since they are recombined formodel learning and data reconstruction. We show this natural approach istheoretically well-founded with information-theoretic arguments. Experimentsdemonstrate that this method achieves invariance while preserving modellearning performance, and results in visually improved performance for styletransfer and generative sampling tasks.",http://arxiv.org/abs/1805.08097v1,,
1546,A New Lower Bound for Agnostic Learning with Sample Compression Schemes,"We establish a tight characterization of the worst-case rates for the excessrisk of agnostic learning with sample compression schemes and for uniformconvergence for agnostic sample compression schemes. In particular, we findthat the optimal rates of convergence for size-$k$ agnostic sample compressionschemes are of the form $\sqrt{\frac{k \log(n/k)}{n}}$, which contrasts withagnostic learning with classes of VC dimension $k$, where the optimal rates areof the form $\sqrt{\frac{k}{n}}$.",http://arxiv.org/abs/1805.08140v1,,
1547,"Learning Maximum-A-Posteriori Perturbation Models for Structured  Prediction in Polynomial Time","MAP perturbation models have emerged as a powerful framework for inference instructured prediction. Such models provide a way to efficiently sample from theGibbs distribution and facilitate predictions that are robust to random noise.In this paper, we propose a provably polynomial time randomized algorithm forlearning the parameters of perturbed MAP predictors. Our approach is based onminimizing a novel Rademacher-based generalization bound on the expected lossof a perturbed MAP predictor, which can be computed in polynomial time. Weobtain conditions under which our randomized learning algorithm can guaranteegeneralization to unseen examples.",http://arxiv.org/abs/1805.08196v1,,
1548,"""Why Should I Trust Interactive Learners?"" Explaining Interactive  Queries of Classifiers to Users","Although interactive learning puts the user into the loop, the learnerremains mostly a black box for the user. Understanding the reasons behindqueries and predictions is important when assessing how the learner works and,in turn, trust. Consequently, we propose the novel framework of explanatoryinteractive learning: in each step, the learner explains its interactive queryto the user, and she queries of any active classifier for visualizingexplanations of the corresponding predictions. We demonstrate that this canboost the predictive and explanatory powers of and the trust into the learnedmodel, using text (e.g. SVMs) and image classification (e.g. neural networks)experiments as well as a user study.",http://arxiv.org/abs/1805.08578v1,,
1549,Adversarial Label Learning,"We consider the task of training classifiers without labels. We propose aweakly supervised method---adversarial label learning---that trains classifiersto perform well against an adversary that chooses labels for training data. Theweak supervision constrains what labels the adversary can choose. The methodtherefore minimizes an upper bound of the classifier's error rate usingprojected primal-dual subgradient descent. Minimizing this bound protectsagainst bias and dependencies in the weak supervision. Experiments on threereal datasets show that our method can train without labels and outperformsother approaches for weakly supervised learning.",http://arxiv.org/abs/1805.08877v3,,
1550,"Toward a Thinking Microscope: Deep Learning in Optical Microscopy and  Image Reconstruction","We discuss recently emerging applications of the state-of-art deep learningmethods on optical microscopy and microscopic image reconstruction, whichenable new transformations among different modes and modalities of microscopicimaging, driven entirely by image data. We believe that deep learning willfundamentally change both the hardware and image reconstruction methods used inoptical microscopy in a holistic manner.",http://arxiv.org/abs/1805.08970v1,,
1551,Learning Temporal Structures of Random Patterns,"A cornerstone of human statistical learning is the ability to extracttemporal regularities / patterns from random sequences. Here we present amethod of computing pattern time statistics with generating functions forfirst-order Markov trials and independent Bernoulli trials. We show that thepattern time statistics cover a wide range of measurements commonly used inexisting studies of both human and machine learning of stochastic processes,including probability of alternation, temporal correlation between patternevents, and related variance / risk measures. Moreover, we show that recurrentprocessing and event segmentation by pattern overlap may provide a coherentexplanation for the sensitivity of the human brain to the rich statistics andthe latent structures in the learning environment.",http://arxiv.org/abs/1805.10827v1,,
1552,Depth and nonlinearity induce implicit exploration for RL,"The question of how to explore, i.e., take actions with uncertain outcomes tolearn about possible future rewards, is a key question in reinforcementlearning (RL). Here, we show a surprising result: We show that Q-learning withnonlinear Q-function and no explicit exploration (i.e., a purely greedy policy)can learn several standard benchmark tasks, including mountain car, equallywell as, or better than, the most commonly-used $\epsilon$-greedy exploration.We carefully examine this result and show that both the depth of the Q-networkand the type of nonlinearity are important to induce such deterministicexploration.",http://arxiv.org/abs/1805.11711v1,,
1553,"Accelerated Randomized Coordinate Descent Algorithms for Stochastic  Optimization and Online Learning","We propose accelerated randomized coordinate descent algorithms forstochastic optimization and online learning. Our algorithms have significantlyless per-iteration complexity than the known accelerated gradient algorithms.The proposed algorithms for online learning have better regret performance thanthe known randomized online coordinate descent algorithms. Furthermore, theproposed algorithms for stochastic optimization exhibit as good convergencerates as the best known randomized coordinate descent algorithms. We also showsimulation results to demonstrate performance of the proposed algorithms.",http://arxiv.org/abs/1806.01600v2,,
1554,Self-Imitation Learning,"This paper proposes Self-Imitation Learning (SIL), a simple off-policyactor-critic algorithm that learns to reproduce the agent's past gooddecisions. This algorithm is designed to verify our hypothesis that exploitingpast good experiences can indirectly drive deep exploration. Our empiricalresults show that SIL significantly improves advantage actor-critic (A2C) onseveral hard exploration Atari games and is competitive to the state-of-the-artcount-based exploration methods. We also show that SIL improves proximal policyoptimization (PPO) on MuJoCo tasks.",http://arxiv.org/abs/1806.05635v1,,
1555,"Structured Variational Learning of Bayesian Neural Networks with  Horseshoe Priors","Bayesian Neural Networks (BNNs) have recently received increasing attentionfor their ability to provide well-calibrated posterior uncertainties. However,model selection---even choosing the number of nodes---remains an open question.Recent work has proposed the use of a horseshoe prior over node pre-activationsof a Bayesian neural network, which effectively turns off nodes that do nothelp explain the data. In this work, we propose several modeling and inferenceadvances that consistently improve the compactness of the model learned whilemaintaining predictive performance, especially in smaller-sample settingsincluding reinforcement learning.",http://arxiv.org/abs/1806.05975v2,,
1556,"On the Relationship between Data Efficiency and Error for Uncertainty  Sampling","While active learning offers potential cost savings, the actual dataefficiency---the reduction in amount of labeled data needed to obtain the sameerror rate---observed in practice is mixed. This paper poses a basic question:when is active learning actually helpful? We provide an answer for logisticregression with the popular active learning algorithm, uncertainty sampling.Empirically, on 21 datasets from OpenML, we find a strong inverse correlationbetween data efficiency and the error rate of the final classifier.Theoretically, we show that for a variant of uncertainty sampling, theasymptotic data efficiency is within a constant factor of the inverse errorrate of the limiting classifier.",http://arxiv.org/abs/1806.06123v1,,
1557,Reinforcement Learning using Augmented Neural Networks,"Neural networks allow Q-learning reinforcement learning agents such as deepQ-networks (DQN) to approximate complex mappings from state spaces to valuefunctions. However, this also brings drawbacks when compared to other functionapproximators such as tile coding or their generalisations, radial basisfunctions (RBF) because they introduce instability due to the side effect ofglobalised updates present in neural networks. This instability does not evenvanish in neural networks that do not have any hidden layers. In this paper, weshow that simple modifications to the structure of the neural network canimprove stability of DQN learning when a multi-layer perceptron is used forfunction approximation.",http://arxiv.org/abs/1806.07692v1,,
1558,DLOPT: Deep Learning Optimization Library,"Deep learning hyper-parameter optimization is a tough task. Finding anappropriate network configuration is a key to success, however most of thetimes this labor is roughly done. In this work we introduce a novel library totackle this problem, the Deep Learning Optimization Library: DLOPT. We brieflydescribe its architecture and present a set of use examples. This is an opensource project developed under the GNU GPL v3 license and it is freelyavailable at https://github.com/acamero/dlopt",http://arxiv.org/abs/1807.03523v1,,
1559,Scikit-Multiflow: A Multi-output Streaming Framework,"Scikit-multiflow is a multi-output/multi-label and stream data miningframework for the Python programming language. Conceived to serve as a platformto encourage democratization of stream learning research, it provides multiplestate of the art methods for stream learning, stream generators and evaluators.scikit-multiflow builds upon popular open source frameworks includingscikit-learn, MOA and MEKA. Development follows the FOSS principles and qualityis enforced by complying with PEP8 guidelines and using continuous integrationand automatic testing. The source code is publicly available athttps://github.com/scikit-multiflow/scikit-multiflow.",http://arxiv.org/abs/1807.04662v1,,
1560,"DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate  Dependency Structures","We present a non-parametric Bayesian latent variable model capable oflearning dependency structures across dimensions in a multivariate setting. Ourapproach is based on flexible Gaussian process priors for the generativemappings and interchangeable Dirichlet process priors to learn the structure.The introduction of the Dirichlet process as a specific structural prior allowsour model to circumvent issues associated with previous Gaussian process latentvariable models. Inference is performed by deriving an efficient variationalbound on the marginal log-likelihood on the model.",http://arxiv.org/abs/1807.04833v1,,
1561,DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging,"DELIMIT is a framework extension for deep learning in diffusion imaging,which extends the basic framework PyTorch towards spherical signals. Based onseveral novel layers, deep learning can be applied to spherical diffusionimaging data in a very convenient way. First, two spherical harmonicinterpolation layers are added to the extension, which allow to transform thesignal from spherical surface space into the spherical harmonic space, and viceversa. In addition, a local spherical convolution layer is introduced that addsthe possibility to include gradient neighborhood information within thenetwork. Furthermore, these extensions can also be utilized for thepreprocessing of diffusion signals.",http://arxiv.org/abs/1808.01517v1,,
1562,Visual Sensor Network Reconfiguration with Deep Reinforcement Learning,"We present an approach for reconfiguration of dynamic visual sensor networkswith deep reinforcement learning (RL). Our RL agent uses a modifiedasynchronous advantage actor-critic framework and the recently proposedRelational Network module at the foundation of its network architecture. Toaddress the issue of sample inefficiency in current approaches to model-freereinforcement learning, we train our system in an abstract simulationenvironment that represents inputs from a dynamic scene. Our system isvalidated using inputs from a real-world scenario and preexisting objectdetection and tracking algorithms.",http://arxiv.org/abs/1808.04287v1,,
1563,Transfer Learning for Estimating Causal Effects using Neural Networks,"We develop new algorithms for estimating heterogeneous treatment effects,combining recent developments in transfer learning for neural networks withinsights from the causal inference literature. By taking advantage of transferlearning, we are able to efficiently use different data sources that arerelated to the same underlying causal mechanisms. We compare our algorithmswith those in the extant literature using extensive simulation studies based onlarge-scale voter persuasion experiments and the MNIST database. Our methodscan perform an order of magnitude better than existing benchmarks while using afraction of the data.",http://arxiv.org/abs/1808.07804v1,,
1564,Multitask Learning for Fundamental Frequency Estimation in Music,"Fundamental frequency (f0) estimation from polyphonic music includes thetasks of multiple-f0, melody, vocal, and bass line estimation. Historicallythese problems have been approached separately, and only recently, usinglearning-based approaches. We present a multitask deep learning architecturethat jointly estimates outputs for various tasks including multiple-f0, melody,vocal and bass line estimation, and is trained using a large,semi-automatically annotated dataset. We show that the multitask modeloutperforms its single-task counterparts, and explore the effect of variousdesign decisions in our approach, and show that it performs better or at leastcompetitively when compared against strong baseline methods.",http://arxiv.org/abs/1809.00381v1,,
1565,"Beyond the Selected Completely At Random Assumption for Learning from  Positive and Unlabeled Data","Most positive and unlabeled data is subject to selection biases. The labeledexamples can, for example, be selected from the positive set because they areeasier to obtain or more obviously positive. This paper investigates howlearning can be enabled in this setting. We propose and theoretically analyzean empirical-risk-based method for incorporating the labeling mechanism.Additionally, we investigate under which assumptions learning is possible whenthe labeling mechanism is not fully understood and propose a practical methodto enable this. Our empirical analysis supports the theoretical results andshows that taking into account the possibility of a selection bias, even whenthe labeling mechanism is unknown, improves the trained classifiers.",http://arxiv.org/abs/1809.03207v1,,
1566,Torchbearer: A Model Fitting Library for PyTorch,"We introduce torchbearer, a model fitting library for pytorch aimed atresearchers working on deep learning or differentiable programming. Thetorchbearer library provides a high level metric and callback API that can beused for a wide range of applications. We also include a series of built incallbacks that can be used for: model persistence, learning rate decay,logging, data visualization and more. The extensive documentation includes anexample library for deep learning and dynamic programming problems and can befound at http://torchbearer.readthedocs.io. The code is licensed under the MITLicense and available at https://github.com/ecs-vlc/torchbearer.",http://arxiv.org/abs/1809.03363v1,,
1567,Minimax Learning of Ergodic Markov Chains,"We compute the finite-sample minimax (modulo logarithmic factors) samplecomplexity of learning the parameters of a finite Markov chain from a singlelong sequence of states. Our error metric is a natural variant of totalvariation. The sample complexity necessarily depends on the spectral gap andminimal stationary probability of the unknown chain, for which there are knownfinite-sample estimators with fully empirical confidence intervals. To ourknowledge, this is the first PAC-type result with nearly matching (up tologarithmic factors) upper and lower bounds for learning, in any metric, in thecontext of Markov chains.",http://arxiv.org/abs/1809.05014v2,,
1568,EpiRL: A Reinforcement Learning Agent to Facilitate Epistasis Detection,"Epistasis (gene-gene interaction) is crucial to predicting genetic disease.Our work tackles the computational challenges faced by previous works inepistasis detection by modeling it as a one-step Markov Decision Process wherethe state is genome data, the actions are the interacted genes, and the rewardis an interaction measurement for the selected actions. A reinforcementlearning agent using policy gradient method then learns to discover a set ofhighly interacted genes.",http://arxiv.org/abs/1809.09143v1,,
1569,"Comparison of Reinforcement Learning algorithms applied to the Cart Pole  problem","Designing optimal controllers continues to be challenging as systems arebecoming complex and are inherently nonlinear. The principal advantage ofreinforcement learning (RL) is its ability to learn from the interaction withthe environment and provide optimal control strategy. In this paper, RL isexplored in the context of control of the benchmark cartpole dynamical systemwith no prior knowledge of the dynamics. RL algorithms such astemporal-difference, policy gradient actor-critic, and value functionapproximation are compared in this context with the standard LQR solution.Further, we propose a novel approach to integrate RL and swing-up controllers.",http://arxiv.org/abs/1810.01940v1,,
1570,Deep learning with differential Gaussian process flows,"We propose a novel deep learning paradigm of differential flows that learn astochastic differential equation transformations of inputs prior to a standardclassification or regression function. The key property of differentialGaussian processes is the warping of inputs through infinitely deep, butinfinitesimal, differential fields, that generalise discrete layers into adynamical system. We demonstrate state-of-the-art results that exceed theperformance of deep Gaussian processes and neural networks",http://arxiv.org/abs/1810.04066v2,,
1571,Mode Normalization,"Normalization methods are a central building block in the deep learningtoolbox. They accelerate and stabilize training, while decreasing thedependence on manually tuned learning rate schedules. When learning frommulti-modal distributions, the effectiveness of batch normalization (BN),arguably the most prominent normalization method, is reduced. As a remedy, wepropose a more flexible approach: by extending the normalization to more than asingle mean and variance, we detect modes of data on-the-fly, jointlynormalizing samples that share common features. We demonstrate that our methodoutperforms BN and other widely used normalization techniques in severalexperiments, including single and multi-task datasets.",http://arxiv.org/abs/1810.05466v1,,
1572,Deep Neural Maps,"We introduce a new unsupervised representation learning and visualizationusing deep convolutional networks and self organizing maps called Deep NeuralMaps (DNM). DNM jointly learns an embedding of the input data and a mappingfrom the embedding space to a two-dimensional lattice. We comparevisualizations of DNM with those of t-SNE and LLE on the MNIST and COIL-20 datasets. Our experiments show that the DNM can learn efficient representations ofthe input data, which reflects characteristics of each class. This is shown viaback-projecting the neurons of the map on the data space.",http://arxiv.org/abs/1810.07291v1,,
1573,Deep Learning with the Random Neural Network and its Applications,"The random neural network (RNN) is a mathematical model for an ""integrate andfire"" spiking network that closely resembles the stochastic behaviour ofneurons in mammalian brains. Since its proposal in 1989, there have beennumerous investigations into the RNN's applications and learning algorithms.Deep learning (DL) has achieved great success in machine learning. Recently,the properties of the RNN for DL have been investigated, in order to combinetheir power. Recent results demonstrate that the gap between RNNs and DL can bebridged and the DL tools based on the RNN are faster and can potentially beused with less energy expenditure than existing methods.",http://arxiv.org/abs/1810.08653v1,,
1574,"A neuro-inspired architecture for unsupervised continual learning based  on online clustering and hierarchical predictive coding","We propose that the Continual Learning desiderata can be achieved through aneuro-inspired architecture, grounded on Mountcastle's cortical columnhypothesis. The proposed architecture involves a single module, calledSelf-Taught Associative Memory (STAM), which models the function of a corticalcolumn. STAMs are repeated in multi-level hierarchies involving feedforward,lateral and feedback connections. STAM networks learn in an unsupervisedmanner, based on a combination of online clustering and hierarchical predictivecoding. This short paper only presents the architecture and its connectionswith neuroscience. A mathematical formulation and experimental results will bepresented in an extended version of this paper.",http://arxiv.org/abs/1810.09391v1,,
1575,Subgradient Descent Learns Orthogonal Dictionaries,"This paper concerns dictionary learning, i.e., sparse coding, a fundamentalrepresentation learning problem. We show that a subgradient descent algorithm,with random initialization, can provably recover orthogonal dictionaries on anatural nonsmooth, nonconvex $\ell_1$ minimization formulation of the problem,under mild statistical assumptions on the data. This is in contrast to previousprovable methods that require either expensive computation or delicateinitialization schemes. Our analysis develops several tools for characterizinglandscapes of nonsmooth functions, which might be of independent interest forprovable training of deep networks with nonsmooth activations (e.g., ReLU),among numerous other applications. Preliminary experiments corroborate ouranalysis and show that our algorithm works well empirically in recoveringorthogonal dictionaries.",http://arxiv.org/abs/1810.10702v1,,
1576,Resampled Priors for Variational Autoencoders,"We propose Learned Accept/Reject Sampling (LARS), a method for constructingricher priors using rejection sampling with a learned acceptance function. Thiswork is motivated by recent analyses of the VAE objective, which pointed outthat commonly used simple priors can lead to underfitting. As the distributioninduced by LARS involves an intractable normalizing constant, we show how toestimate it and its gradients efficiently. We demonstrate that LARS priorsimprove VAE performance on several standard datasets both when they are learnedjointly with the rest of the model and when they are fitted to a pretrainedmodel. Finally, we show that LARS can be combined with existing methods fordefining flexible priors for an additional boost in performance.",http://arxiv.org/abs/1810.11428v1,,
1577,Stochastic Normalizations as Bayesian Learning,"In this work we investigate the reasons why Batch Normalization (BN) improvesthe generalization performance of deep networks. We argue that one majorreason, distinguishing it from data-independent normalization methods, israndomness of batch statistics. This randomness appears in the parametersrather than in activations and admits an interpretation as a practical Bayesianlearning. We apply this idea to other (deterministic) normalization techniquesthat are oblivious to the batch size. We show that their generalizationperformance can be improved significantly by Bayesian learning of the sameform. We obtain test performance comparable to BN and, at the same time, bettervalidation losses suitable for subsequent output uncertainty estimation throughapproximate Bayesian posterior.",http://arxiv.org/abs/1811.00639v1,,
1578,Learning sparse mixtures of rankings from noisy information,"We study the problem of learning an unknown mixture of $k$ rankings over $n$elements, given access to noisy samples drawn from the unknown mixture. Weconsider a range of different noise models, including natural variants of the""heat kernel"" noise framework and the Mallows model. For each of these noisemodels we give an algorithm which, under mild assumptions, learns the unknownmixture to high accuracy and runs in $n^{O(\log k)}$ time. The best previousalgorithms for closely related problems have running times which areexponential in $k$.",http://arxiv.org/abs/1811.01216v1,,
1579,"Deep Probabilistic Ensembles: Approximate Variational Inference through  KL Regularization","In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalabletechnique that uses a regularized ensemble to approximate a deep BayesianNeural Network (BNN). We do so by incorporating a KL divergence penalty terminto the training objective of an ensemble, derived from the evidence lowerbound used in variational inference. We evaluate the uncertainty estimatesobtained from our models for active learning on visual classification. Ourapproach steadily improves upon active learning baselines as the annotationbudget is increased.",http://arxiv.org/abs/1811.02640v2,,
1580,"Adversarial Learning of Label Dependency: A Novel Framework for  Multi-class Classification","Recent work has shown that exploiting relations between labels improves theperformance of multi-label classification. We propose a novel framework basedon generative adversarial networks (GANs) to model label dependency. Thediscriminator learns to model label dependency by discriminating real andgenerated label sets. To fool the discriminator, the classifier, or generator,learns to generate label sets with dependencies close to real data. Extensiveexperiments and comparisons on two large-scale image classification benchmarkdatasets (MS-COCO and NUS-WIDE) show that the discriminator improvesgeneralization ability for different kinds of models",http://arxiv.org/abs/1811.04689v1,,
1581,$HS^2$: Active Learning over Hypergraphs,"We propose a hypergraph-based active learning scheme which we term $HS^2$,$HS^2$ generalizes the previously reported algorithm $S^2$ originally proposedfor graph-based active learning with pointwise queries [Dasarathy et al., COLT2015]. Our $HS^2$ method can accommodate hypergraph structures and allows oneto ask both pointwise queries and pairwise queries. Based on a novel parametricsystem particularly designed for hypergraphs, we derive theoretical results onthe query complexity of $HS^2$ for the above described generalized settings.Both the theoretical and empirical results show that $HS^2$ requires asignificantly fewer number of queries than $S^2$ when one uses $S^2$ over agraph obtained from the corresponding hypergraph via clique expansion.",http://arxiv.org/abs/1811.11549v1,,
1582,Online Abstraction with MDP Homomorphisms for Deep Learning,"Abstraction of Markov Decision Processes is a useful tool for solving complexproblems, as it can ignore unimportant aspects of an environment, simplifyingthe process of learning an optimal policy. In this paper, we propose a newalgorithm for finding abstract MDPs in environments with continuous statespaces. It is based on MDP homomorphisms, a structure-preserving mappingbetween MDPs. We demonstrate our algorithm's ability to learn abstractions fromcollected experience and show how to reuse the abstractions to guideexploration in new tasks the agent encounters. Our novel task transfer methodoutperforms baselines based on a deep Q-network in the majority of ourexperiments. The source code is at https://github.com/ondrejba/aamas_19.",http://arxiv.org/abs/1811.12929v2,,
1583,Bach2Bach: Generating Music Using A Deep Reinforcement Learning Approach,"A model of music needs to have the ability to recall past details and have aclear, coherent understanding of musical structure. Detailed in the paper is adeep reinforcement learning architecture that predicts and generates polyphonicmusic aligned with musical rules. The probabilistic model presented is aBi-axial LSTM trained with a pseudo-kernel reminiscent of a convolutionalkernel. To encourage exploration and impose greater global coherence on thegenerated music, a deep reinforcement learning approach DQN is adopted. Whenanalyzed quantitatively and qualitatively, this approach performs well incomposing polyphonic music.",http://arxiv.org/abs/1812.01060v1,,
1584,Learning Graph Representation via Formal Concept Analysis,"We present a novel method that can learn a graph representation frommultivariate data. In our representation, each node represents a cluster ofdata points and each edge represents the subset-superset relationship betweenclusters, which can be mutually overlapped. The key to our method is to useformal concept analysis (FCA), which can extract hierarchical relationshipsbetween clusters based on the algebraic closedness property. We empiricallyshow that our method can effectively extract hierarchical structures ofclusters compared to the baseline method.",http://arxiv.org/abs/1812.03395v1,,
1585,Learning Multiplication-free Linear Transformations,"In this paper, we propose several dictionary learning algorithms for sparserepresentations that also impose specific structures on the learneddictionaries such that they are numerically efficient to use: reduced number ofaddition/multiplications and even avoiding multiplications altogether. We baseour work on factorizations of the dictionary in highly structured basicbuilding blocks (binary orthonormal, scaling and shear transformations) forwhich we can write closed-form solutions to the optimization problems that weconsider. We show the effectiveness of our methods on image data where we cancompare against well-known numerically efficient transforms such as the fastFourier and the fast discrete cosine transforms.",http://arxiv.org/abs/1812.03412v1,,
1586,"KF-LAX: Kronecker-factored curvature estimation for control variate  optimization in reinforcement learning","A key challenge for gradient based optimization methods in model-freereinforcement learning is to develop an approach that is sample efficient andhas low variance. In this work, we apply Kronecker-factored curvatureestimation technique (KFAC) to a recently proposed gradient estimator forcontrol variate optimization, RELAX, to increase the sample efficiency of usingthis gradient estimation method in reinforcement learning. The performance ofthe proposed method is demonstrated on a synthetic problem and a set of threediscrete control task Atari games.",http://arxiv.org/abs/1812.04181v1,,
1587,Weakly Supervised Active Learning with Cluster Annotation,"In this work, we introduce a novel framework that employs cluster annotationto boost active learning by reducing the number of human interactions requiredto train deep neural networks. Instead of annotating single samplesindividually, humans can also label clusters, producing a higher number ofannotated samples with the cost of a small label error. Our experiments showthat the proposed framework requires 82% and 87% less human interactions forCIFAR-10 and EuroSAT datasets respectively when compared with thefully-supervised training while maintaining similar performance on the testset.",http://arxiv.org/abs/1812.11780v2,,
1588,"Warm-starting Contextual Bandits: Robustly Combining Supervised and  Bandit Feedback","We investigate the feasibility of learning from both fully-labeled superviseddata and contextual bandit data. We specifically consider settings in which theunderlying learning signal may be different between these two data sources.Theoretically, we state and prove no-regret algorithms for learning that isrobust to divergences between the two sources. Empirically, we evaluate some ofthese algorithms on a large selection of datasets, showing that our approachesare feasible, and helpful in practice.",http://arxiv.org/abs/1901.00301v1,,
1589,Bayesian Learning of Neural Network Architectures,"In this paper we propose a Bayesian method for estimating architecturalparameters of neural networks, namely layer size and network depth. We do thisby learning concrete distributions over these parameters. Our results show thatregular networks with a learnt structure can generalise better on smalldatasets, while fully stochastic networks can be more robust to parameterinitialisation. The proposed method relies on standard neural variationallearning and, unlike randomised architecture search, does not require aretraining of the model, thus keeping the computational overhead at minimum.",http://arxiv.org/abs/1901.04436v2,,
1590,A Short Survey on Probabilistic Reinforcement Learning,"A reinforcement learning agent tries to maximize its cumulative payoff byinteracting in an unknown environment. It is important for the agent to exploresuboptimal actions as well as to pick actions with highest known rewards. Yet,in sensitive domains, collecting more data with exploration is not alwayspossible, but it is important to find a policy with a certain performanceguaranty. In this paper, we present a brief survey of methods available in theliterature for balancing exploration-exploitation trade off and computingrobust solutions from fixed samples in reinforcement learning.",http://arxiv.org/abs/1901.07010v1,,
1591,On the Statistical Efficiency of Optimal Kernel Sum Classifiers,"We propose a novel combination of optimization tools with learning theorybounds in order to analyze the sample complexity of optimal kernel sumclassifiers. This contrasts the typical learning theoretic results which holdfor all (potentially suboptimal) classifiers. Our work also justifiesassumptions made in prior work on multiple kernel learning. As a byproduct ofour analysis, we also provide a new form of Rademacher complexity forhypothesis classes containing only optimal classifiers.",http://arxiv.org/abs/1901.09087v1,,
1592,Active Automata Learning with Adaptive Distinguishing Sequences,"This document investigates the integration of adaptive distinguishingsequences into the process of active automata learning (AAL). A novel AALalgorithm ""ADT"" (adaptive discrimination tree) is developed and presented.Since the submission of the original thesis, the presented algorithm has beenintegrated into LearnLib - an open-source library for active automata learning- and has been successfully used in related fields of research.",http://arxiv.org/abs/1902.01139v1,,
1593,Neural network models and deep learning - a primer for biologists,"Originally inspired by neurobiology, deep neural network models have become apowerful tool of machine learning and artificial intelligence, where they areused to approximate functions and dynamics by learning from examples. Here wegive a brief introduction to neural network models and deep learning forbiologists. We introduce feedforward and recurrent networks and explain theexpressive power of this modeling framework and the backpropagation algorithmfor setting the parameters. Finally, we consider how deep neural networks mighthelp us understand the brain's computations.",http://arxiv.org/abs/1902.04704v2,,
1594,Learning Ising Models with Independent Failures,"We give the first efficient algorithm for learning the structure of an Isingmodel that tolerates independent failures; that is, each entry of the observedsample is missing with some unknown probability p. Our algorithm matches theessentially optimal runtime and sample complexity bounds of recent work forlearning Ising models due to Klivans and Meka (2017).  We devise a novel unbiased estimator for the gradient of the InteractionScreening Objective (ISO) due to Vuffray et al. (2016) and apply a stochasticmultiplicative gradient descent algorithm to minimize this objective. Solutionsto this minimization recover the neighborhood information of the underlyingIsing model on a node by node basis.",http://arxiv.org/abs/1902.04728v1,,
1595,"On the Expressive Power of Kernel Methods and the Efficiency of Kernel  Learning by Association Schemes","We study the expressive power of kernel methods and the algorithmicfeasibility of multiple kernel learning for a special rich class of kernels.  Specifically, we define \emph{Euclidean kernels}, a diverse class thatincludes most, if not all, families of kernels studied in literature such aspolynomial kernels and radial basis functions. We then describe the geometricand spectral structure of this family of kernels over the hypercube (and tosome extent for any compact domain). Our structural results allow us to provemeaningful limitations on the expressive power of the class as well as deriveseveral efficient algorithms for learning kernels over different domains.",http://arxiv.org/abs/1902.04782v1,,
1596,"Classifying textual data: shallow, deep and ensemble methods","This paper focuses on a comparative evaluation of the most common and modernmethods for text classification, including the recent deep learning strategiesand ensemble methods. The study is motivated by a challenging real dataproblem, characterized by high-dimensional and extremely sparse data, derivingfrom incoming calls to the customer care of an Italian phone company. We willshow that deep learning outperforms many classical (shallow) strategies but thecombination of shallow and deep learning methods in a unique ensembleclassifier may improve the robustness and the accuracy of ""single""classification methods.",http://arxiv.org/abs/1902.07068v1,,
1597,"MisGAN: Learning from Incomplete Data with Generative Adversarial  Networks","Generative adversarial networks (GANs) have been shown to provide aneffective way to model complex distributions and have obtained impressiveresults on various challenging tasks. However, typical GANs requirefully-observed data during training. In this paper, we present a GAN-basedframework for learning from complex, high-dimensional incomplete data. Theproposed framework learns a complete data generator along with a mask generatorthat models the missing data distribution. We further demonstrate how to imputemissing data by equipping our framework with an adversarially trained imputer.We evaluate the proposed framework using a series of experiments with severaltypes of missing data processes under the missing completely at randomassumption.",http://arxiv.org/abs/1902.09599v1,,
1598,Interpreting Active Learning Methods Through Information Losses,"We propose a new way of interpreting active learning methods by analyzing theinformation `lost' upon sampling a random variable. We use some recentanalytical developments of these losses to formally prove that facilitylocation methods reduce these losses under mild assumptions, and to derive anew data dependent bound on information losses that can be used to evaluateother active learning methods. We show that this new bound is extremely tightto experiment, and further show that the bound has a decent predictive powerfor classification accuracy.",http://arxiv.org/abs/1902.09602v1,,
1599,"Stochastic Prediction of Multi-Agent Interactions from Partial  Observations","We present a method that learns to integrate temporal information, from alearned dynamics model, with ambiguous visual information, from a learnedvision model, in the context of interacting agents. Our method is based on agraph-structured variational recurrent neural network (Graph-VRNN), which istrained end-to-end to infer the current state of the (partially observed)world, as well as to forecast future states. We show that our methodoutperforms various baselines on two sports datasets, one based on realbasketball trajectories, and one generated by a soccer game engine.",http://arxiv.org/abs/1902.09641v1,,
1600,Online Data Poisoning Attack,"We study data poisoning attacks in the online learning setting where thetraining items stream in one at a time, and the adversary perturbs the currenttraining item to manipulate present and future learning. In contrast, priorwork on data poisoning attacks has focused on either batch learners in theoffline setting, or online learners but with full knowledge of the wholetraining sequence. We show that online poisoning attack can be formulated asstochastic optimal control, and provide several practical attack algorithmsbased on control and deep reinforcement learning. Extensive experimentsdemonstrate the effectiveness of the attacks.",http://arxiv.org/abs/1903.01666v1,,
1601,Fast Graph Representation Learning with PyTorch Geometric,"We introduce PyTorch Geometric, a library for deep learning on irregularlystructured input data such as graphs, point clouds and manifolds, built uponPyTorch. In addition to general graph data structures and processing methods,it contains a variety of recently published methods from the domains ofrelational learning and 3D data processing. PyTorch Geometric achieves highdata throughput by leveraging sparse GPU acceleration, by providing dedicatedCUDA kernels and by introducing efficient mini-batch handling for inputexamples of different size. In this work, we present the library in detail andperform a comprehensive comparative study of the implemented methods inhomogeneous evaluation scenarios.",http://arxiv.org/abs/1903.02428v2,,
1602,"Everything old is new again: A multi-view learning approach to learning  using privileged information and distillation","We adopt a multi-view approach for analyzing two knowledge transfersettings---learning using privileged information (LUPI) and distillation---in acommon framework. Under reasonable assumptions about the complexities ofhypothesis spaces, and being optimistic about the expected loss achievable bythe student (in distillation) and a transformed teacher predictor (in LUPI), weshow that encouraging agreement between the teacher and the student leads toreduced search space. As a result, improved convergence rate can be obtainedwith regularized empirical risk minimization.",http://arxiv.org/abs/1903.03694v1,,
1603,Learning to Modulate for Non-coherent MIMO,"The deep learning trend has recently impacted a variety of fields, includingcommunication systems, where various approaches have explored the applicationof neural networks in place of traditional designs. Neural networks flexiblyallow for data/simulation-driven optimization, but are often employed as blackboxes detached from direct application of domain knowledge. Our work considerslearning-based approaches addressing modulation and signal detection design forthe non-coherent MIMO channel. We demonstrate that simulation-drivenoptimization can be performed while entirely avoiding neural networks, yetstill perform comparably. Additionally, we show the feasibility of MIMOcommunications over extremely short coherence windows (i.e., channelcoefficient stability period), with as few as two time slots.",http://arxiv.org/abs/1903.03711v1,,
1604,"A semi-supervised deep learning algorithm for abnormal EEG  identification","Systems that can automatically analyze EEG signals can aid neurologists byreducing heavy workload and delays. However, such systems need to be firsttrained using a labeled dataset. While large corpuses of EEG data exist, afraction of them are labeled. Hand-labeling data increases workload for thevery neurologists we try to aid. This paper proposes a semi-supervised learningalgorithm that can not only extract meaningful information from large unlabeledEEG datasets but also perform task-specific learning on labeled datasets assmall as 5 examples.",http://arxiv.org/abs/1903.07822v1,,
1605,Localized Linear Regression in Networked Data,"The network Lasso (nLasso) has been proposed recently as an efficientlearning algorithm for massive networked data sets (big data over networks). Itextends the well-known least least absolute shrinkage and selection operator(Lasso) from learning sparse (generalized) linear models to network models.Efficient implementations of the nLasso have been obtained using convexoptimization methods. These implementations naturally lend to highly scalablemessage passing methods. In this paper, we analyze the performance of nLassowhen applied to localized linear regression problems involving networked data.Our main result is a sufficient conditions on the network structure andavailable label information such that nLasso accurately learns a localizedlinear regression model from few labeled data points.",http://arxiv.org/abs/1903.11178v1,,
1606,"Distributed Statistical Machine Learning in Adversarial Settings:  Byzantine Gradient Descent","We consider the problem of distributed statistical machine learning inadversarial settings, where some unknown and time-varying subset of workingmachines may be compromised and behave arbitrarily to prevent an accurate modelfrom being learned. This setting captures the potential adversarial attacksfaced by Federated Learning -- a modern machine learning paradigm that isproposed by Google researchers and has been intensively studied for ensuringuser privacy. Formally, we focus on a distributed system consisting of aparameter server and $m$ working machines. Each working machine keeps $N/m$data samples, where $N$ is the total number of samples. The goal is tocollectively learn the underlying true model parameter of dimension $d$.  In classical batch gradient descent methods, the gradients reported to theserver by the working machines are aggregated via simple averaging, which isvulnerable to a single Byzantine failure. In this paper, we propose a Byzantinegradient descent method based on the geometric median of means of thegradients. We show that our method can tolerate $q \le (m-1)/2$ Byzantinefailures, and the parameter estimate converges in $O(\log N)$ rounds with anestimation error of $\sqrt{d(2q+1)/N}$, hence approaching the optimal errorrate $\sqrt{d/N}$ in the centralized and failure-free setting. The totalcomputational complexity of our algorithm is of $O((Nd/m) \log N)$ at eachworking machine and $O(md + kd \log^3 N)$ at the central server, and the totalcommunication cost is of $O(m d \log N)$. We further provide an application ofour general results to the linear regression problem.  A key challenge arises in the above problem is that Byzantine failures createarbitrary and unspecified dependency among the iterations and the aggregatedgradients. We prove that the aggregated gradient converges uniformly to thetrue gradient function.",http://arxiv.org/abs/1705.05491v2,,
1607,A metric learning perspective of SVM: on the relation of SVM and LMNN,"Support Vector Machines, SVMs, and the Large Margin Nearest Neighboralgorithm, LMNN, are two very popular learning algorithms with quite differentlearning biases. In this paper we bring them into a unified view and show thatthey have a much stronger relation than what is commonly thought. We analyzeSVMs from a metric learning perspective and cast them as a metric learningproblem, a view which helps us uncover the relations of the two algorithms. Weshow that LMNN can be seen as learning a set of local SVM-like models in aquadratic space. Along the way and inspired by the metric-based interpretationof SVM s we derive a novel variant of SVMs, epsilon-SVM, to which LMNN is evenmore similar. We give a unified view of LMNN and the different SVM variants.Finally we provide some preliminary experiments on a number of benchmarkdatasets in which show that epsilon-SVM compares favorably both with respect toLMNN and SVM.",http://arxiv.org/abs/1201.4714v1,,
1608,Learning Onto-Relational Rules with Inductive Logic Programming,"Rules complement and extend ontologies on the Semantic Web. We refer to theserules as onto-relational since they combine DL-based ontology languages andKnowledge Representation formalisms supporting the relational data model withinthe tradition of Logic Programming and Deductive Databases. Rule authoring is avery demanding Knowledge Engineering task which can be automated thoughpartially by applying Machine Learning algorithms. In this chapter we show howInductive Logic Programming (ILP), born at the intersection of Machine Learningand Logic Programming and considered as a major approach to RelationalLearning, can be adapted to Onto-Relational Learning. For the sake ofillustration, we provide details of a specific Onto-Relational Learningsolution to the problem of learning rule-based definitions of DL concepts androles with ILP.",http://arxiv.org/abs/1210.2984v2,,
1609,Learning Sparse Low-Threshold Linear Classifiers,"We consider the problem of learning a non-negative linear classifier with a$1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. Thisproblem generalizes the problem of learning a $k$-monotone disjunction. Weprove that we can learn efficiently in this setting, at a rate which is linearin both $k$ and the size of the threshold, and that this is the best possiblerate. We provide an efficient online learning algorithm that achieves theoptimal rate, and show that in the batch case, empirical risk minimizationachieves this rate as well. The rates we show are tighter than the uniformconvergence rate, which grows with $k^2$.",http://arxiv.org/abs/1212.3276v3,,
1610,"Regularization in Relevance Learning Vector Quantization Using l one  Norms","We propose in this contribution a method for l one regularization inprototype based relevance learning vector quantization (LVQ) for sparserelevance profiles. Sparse relevance profiles in hyperspectral data analysisfade down those spectral bands which are not necessary for classification. Inparticular, we consider the sparsity in the relevance profile enforced by LASSOoptimization. The latter one is obtained by a gradient learning scheme using adifferentiable parametrized approximation of the $l_{1}$-norm, which has anupper error bound. We extend this regularization idea also to the matrixlearning variant of LVQ as the natural generalization of relevance learning.",http://arxiv.org/abs/1310.5095v1,,
1611,Online Learning with Pairwise Loss Functions,"Efficient online learning with pairwise loss functions is a crucial componentin building large-scale learning system that maximizes the area under theReceiver Operator Characteristic (ROC) curve. In this paper we investigate thegeneralization performance of online learning algorithms with pairwise lossfunctions. We show that the existing proof techniques for generalization boundsof online algorithms with a univariate loss can not be directly applied topairwise losses. In this paper, we derive the first result providingdata-dependent bounds for the average risk of the sequence of hypothesesgenerated by an arbitrary online learner in terms of an easily computablestatistic, and show how to extract a low risk hypothesis from the sequence. Wedemonstrate the generality of our results by applying it to two importantproblems in machine learning. First, we analyze two online algorithms forbipartite ranking; one being a natural extension of the perceptron algorithmand the other using online convex optimization. Secondly, we provide ananalysis for the risk bound for an online algorithm for supervised metriclearning.",http://arxiv.org/abs/1301.5332v1,,
1612,"Supervised Learning and Anti-learning of Colorectal Cancer Classes and  Survival Rates from Cellular Biology Parameters","In this paper, we describe a dataset relating to cellular and physicalconditions of patients who are operated upon to remove colorectal tumours. Thisdata provides a unique insight into immunological status at the point of tumourremoval, tumour classification and post-operative survival. Attempts are madeto learn relationships between attributes (physical and immunological) and theresulting tumour stage and survival. Results for conventional machine learningapproaches can be considered poor, especially for predicting tumour stages forthe most important types of cancer. This poor performance is furtherinvestigated and compared with a synthetic, dataset based on the logicalexclusive-OR function and it is shown that there is a significant level of'anti-learning' present in all supervised methods used and this can beexplained by the highly dimensional, complex and sparsely representativedataset. For predicting the stage of cancer from the immunological attributes,anti-learning approaches outperform a range of popular algorithms.",http://arxiv.org/abs/1307.1599v1,,
1613,MCMC Learning,"The theory of learning under the uniform distribution is rich and deep, withconnections to cryptography, computational complexity, and the analysis ofboolean functions to name a few areas. This theory however is very limited dueto the fact that the uniform distribution and the corresponding Fourier basisare rarely encountered as a statistical model.  A family of distributions that vastly generalizes the uniform distribution onthe Boolean cube is that of distributions represented by Markov Random Fields(MRF). Markov Random Fields are one of the main tools for modeling highdimensional data in many areas of statistics and machine learning.  In this paper we initiate the investigation of extending central ideas,methods and algorithms from the theory of learning under the uniformdistribution to the setup of learning concepts given examples from MRFdistributions. In particular, our results establish a novel connection betweenproperties of MCMC sampling of MRFs and learning under the MRF distribution.",http://arxiv.org/abs/1307.3617v2,,
1614,Visual Causal Feature Learning,"We provide a rigorous definition of the visual cause of a behavior that isbroadly applicable to the visually driven behavior in humans, animals, neurons,robots and other perceiving systems. Our framework generalizes standardaccounts of causal learning to settings in which the causal variables need tobe constructed from micro-variables. We prove the Causal Coarsening Theorem,which allows us to gain causal knowledge from observational data with minimalexperimental effort. The theorem provides a connection to standard inferencetechniques in machine learning that identify features of an image thatcorrelate with, but may not cause, the target behavior. Finally, we propose anactive learning scheme to learn a manipulator function that performs optimalmanipulations on the image to automatically identify the visual cause of atarget behavior. We illustrate our inference and learning algorithms inexperiments based on both synthetic and real data.",http://arxiv.org/abs/1412.2309v2,,
1615,A Probabilistic Theory of Deep Learning,"A grand challenge in machine learning is the development of computationalalgorithms that match or outperform humans in perceptual inference tasks thatare complicated by nuisance variation. For instance, visual object recognitioninvolves the unknown object position, orientation, and scale in objectrecognition while speech recognition involves the unknown voice pronunciation,pitch, and speed. Recently, a new breed of deep learning algorithms haveemerged for high-nuisance inference tasks that routinely yield patternrecognition systems with near- or super-human capabilities. But a fundamentalquestion remains: Why do they work? Intuitions abound, but a coherent frameworkfor understanding, analyzing, and synthesizing deep learning architectures hasremained elusive. We answer this question by developing a new probabilisticframework for deep learning based on the Deep Rendering Model: a generativeprobabilistic model that explicitly captures latent nuisance variation. Byrelaxing the generative model to a discriminative one, we can recover two ofthe current leading deep learning systems, deep convolutional neural networksand random decision forests, providing insights into their successes andshortcomings, as well as a principled route to their improvement.",http://arxiv.org/abs/1504.00641v1,,
1616,"Secure Multi-Party Computation Based Privacy Preserving Extreme Learning  Machine Algorithm Over Vertically Distributed Data","Especially in the Big Data era, the usage of different classification methodsis increasing day by day. The success of these classification methods dependson the effectiveness of learning methods. Extreme learning machine (ELM)classification algorithm is a relatively new learning method built onfeed-forward neural-network. ELM classification algorithm is a simple and fastmethod that can create a model from high-dimensional data sets. Traditional ELMlearning algorithm implicitly assumes complete access to whole data set. Thisis a major privacy concern in most of cases. Sharing of private data (i.e.medical records) is prevented because of security concerns. In this research,we propose an efficient and secure privacy-preserving learning algorithm forELM classification over data that is vertically partitioned among severalparties. The new learning method preserves the privacy on numerical attributes,builds a classification model without sharing private data without disclosingthe data of each party to others.",http://arxiv.org/abs/1602.02899v1,,
1617,DCM Bandits: Learning to Rank with Multiple Clicks,"A search engine recommends to the user a list of web pages. The user examinesthis list, from the first page to the last, and clicks on all attractive pagesuntil the user is satisfied. This behavior of the user can be described by thedependent click model (DCM). We propose DCM bandits, an online learning variantof the DCM where the goal is to maximize the probability of recommendingsatisfactory items, such as web pages. The main challenge of our learningproblem is that we do not observe which attractive item is satisfactory. Wepropose a computationally-efficient learning algorithm for solving our problem,dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonableassumptions; and also prove a matching lower bound up to logarithmic factors.We evaluate our algorithm on synthetic and real-world problems, and show thatit performs well even when our model is misspecified. This work presents thefirst practical and regret-optimal online algorithm for learning to rank withmultiple clicks in a cascade-like click model.",http://arxiv.org/abs/1602.03146v2,,
1618,Cross-Domain Multitask Learning with Latent Probit Models,"Learning multiple tasks across heterogeneous domains is a challenging problemsince the feature space may not be the same for different tasks. We assume thedata in multiple tasks are generated from a latent common domain via sparsedomain transforms and propose a latent probit model (LPM) to jointly learn thedomain transforms, and the shared probit classifier in the common domain. Tolearn meaningful task relatedness and avoid over-fitting in classification, weintroduce sparsity in the domain transforms matrices, as well as in the commonclassifier. We derive theoretical bounds for the estimation error of theclassifier in terms of the sparsity of domain transforms. Anexpectation-maximization algorithm is derived for learning the LPM. Theeffectiveness of the approach is demonstrated on several real datasets.",http://arxiv.org/abs/1206.6419v1,,
1619,Structured Learning from Partial Annotations,"Structured learning is appropriate when predicting structured outputs such astrees, graphs, or sequences. Most prior work requires the training set toconsist of complete trees, graphs or sequences. Specifying such detailed groundtruth can be tedious or infeasible for large outputs. Our main contribution isa large margin formulation that makes structured learning from only partiallyannotated data possible. The resulting optimization problem is non-convex, yetcan be efficiently solve by concave-convex procedure (CCCP) with novel speedupstrategies. We apply our method to a challenging tracking-by-assignment problemof a variable number of divisible objects. On this benchmark, using only 25% ofa full annotation we achieve a performance comparable to a model learned with afull annotation. Finally, we offer a unifying perspective of previous workusing the hinge, ramp, or max loss for structured learning, followed by anempirical comparison on their practical performance.",http://arxiv.org/abs/1206.6421v1,,
1620,Flexible Modeling of Latent Task Structures in Multitask Learning,"Multitask learning algorithms are typically designed assuming some fixed, apriori known latent structure shared by all the tasks. However, it is usuallyunclear what type of latent task structure is the most appropriate for a givenmultitask learning problem. Ideally, the ""right"" latent task structure shouldbe learned in a data-driven manner. We present a flexible, nonparametricBayesian model that posits a mixture of factor analyzers structure on thetasks. The nonparametric aspect makes the model expressive enough to subsumemany existing models of latent task structures (e.g, mean-regularized tasks,clustered tasks, low-rank or linear/non-linear subspace assumption on tasks,etc.). Moreover, it can also learn more general task structures, addressing theshortcomings of such models. We present a variational inference algorithm forour model. Experimental results on synthetic and real-world datasets, on bothregression and classification problems, demonstrate the effectiveness of theproposed method.",http://arxiv.org/abs/1206.6486v1,,
1621,Deep Unsupervised Learning using Nonequilibrium Thermodynamics,"A central problem in machine learning involves modeling complex data-setsusing highly flexible families of probability distributions in which learning,sampling, inference, and evaluation are still analytically or computationallytractable. Here, we develop an approach that simultaneously achieves bothflexibility and tractability. The essential idea, inspired by non-equilibriumstatistical physics, is to systematically and slowly destroy structure in adata distribution through an iterative forward diffusion process. We then learna reverse diffusion process that restores structure in data, yielding a highlyflexible and tractable generative model of the data. This approach allows us torapidly learn, sample from, and evaluate probabilities in deep generativemodels with thousands of layers or time steps, as well as to computeconditional and posterior probabilities under the learned model. Weadditionally release an open source reference implementation of the algorithm.",http://arxiv.org/abs/1503.03585v8,,
1622,"Confidence-Constrained Maximum Entropy Framework for Learning from  Multi-Instance Data","Multi-instance data, in which each object (bag) contains a collection ofinstances, are widespread in machine learning, computer vision, bioinformatics,signal processing, and social sciences. We present a maximum entropy (ME)framework for learning from multi-instance data. In this approach each bag isrepresented as a distribution using the principle of ME. We introduce theconcept of confidence-constrained ME (CME) to simultaneously learn thestructure of distribution space and infer each distribution. The sharedstructure underlying each density is used to learn from instances inside eachbag. The proposed CME is free of tuning parameters. We devise a fastoptimization algorithm capable of handling large scale multi-instance data. Inthe experimental section, we evaluate the performance of the proposed approachin terms of exact rank recovery in the space of distributions and compare itwith the regularized ME approach. Moreover, we compare the performance of CMEwith Multi-Instance Learning (MIL) state-of-the-art algorithms and show acomparable performance in terms of accuracy with reduced computationalcomplexity.",http://arxiv.org/abs/1603.01901v1,,
1623,Time-varying Learning and Content Analytics via Sparse Factor Analysis,"We propose SPARFA-Trace, a new machine learning-based framework fortime-varying learning and content analytics for education applications. Wedevelop a novel message passing-based, blind, approximate Kalman filter forsparse factor analysis (SPARFA), that jointly (i) traces learner conceptknowledge over time, (ii) analyzes learner concept knowledge state transitions(induced by interacting with learning resources, such as textbook sections,lecture videos, etc, or the forgetting effect), and (iii) estimates the contentorganization and intrinsic difficulty of the assessment questions. Thesequantities are estimated solely from binary-valued (correct/incorrect) gradedlearner response data and a summary of the specific actions each learnerperforms (e.g., answering a question or studying a learning resource) at eachtime instance. Experimental results on two online course datasets demonstratethat SPARFA-Trace is capable of tracing each learner's concept knowledgeevolution over time, as well as analyzing the quality and content organizationof learning resources, the question-concept associations, and the questionintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparableor better performance in predicting unobserved learner responses than existingcollaborative filtering and knowledge tracing approaches for personalizededucation.",http://arxiv.org/abs/1312.5734v1,,
1624,From neural PCA to deep unsupervised learning,"A network supporting deep unsupervised learning is presented. The network isan autoencoder with lateral shortcut connections from the encoder to decoder ateach level of the hierarchy. The lateral shortcut connections allow the higherlevels of the hierarchy to focus on abstract invariant features. While standardautoencoders are analogous to latent variable models with a single layer ofstochastic variables, the proposed network is analogous to hierarchical latentvariables models. Learning combines denoising autoencoder and denoising sourcesseparation frameworks. Each layer of the network contributes to the costfunction a term which measures the distance of the representations produced bythe encoder and the decoder. Since training signals originate from all levelsof the network, all layers can learn efficiently even in deep networks. Thespeedup offered by cost terms from higher levels of the hierarchy and theability to learn invariant features are demonstrated in experiments.",http://arxiv.org/abs/1411.7783v2,,
1625,Online Learning to Sample,"Stochastic Gradient Descent (SGD) is one of the most widely used techniquesfor online optimization in machine learning. In this work, we accelerate SGD byadaptively learning how to sample the most useful training examples at eachtime step. First, we show that SGD can be used to learn the best possiblesampling distribution of an importance sampling estimator. Second, we show thatthe sampling distribution of an SGD algorithm can be estimated online byincrementally minimizing the variance of the gradient. The resulting algorithm- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters tooptimize, as well as a set of parameters to sample learning examples. We showthat AWSGD yields faster convergence in three different applications: (i) imageclassification with deep features, where the sampling of images depends ontheir labels, (ii) matrix factorization, where rows and columns are not sampleduniformly, and (iii) reinforcement learning, where the optimized andexploration policies are estimated at the same time, where our approachcorresponds to an off-policy gradient algorithm.",http://arxiv.org/abs/1506.09016v2,,
1626,Learning Combinatorial Functions from Pairwise Comparisons,"A large body of work in machine learning has focused on the problem oflearning a close approximation to an underlying combinatorial function, given asmall set of labeled examples. However, for real-valued functions, cardinallabels might not be accessible, or it may be difficult for an expert toconsistently assign real-valued labels over the entire set of examples. Forinstance, it is notoriously hard for consumers to reliably assign values tobundles of merchandise. Instead, it might be much easier for a consumer toreport which of two bundles she likes better. With this motivation in mind, weconsider an alternative learning model, wherein the algorithm must learn theunderlying function up to pairwise comparisons, from pairwise comparisons. Inthis model, we present a series of novel algorithms that learn over a widevariety of combinatorial function classes. These range from graph functions tobroad classes of valuation functions that are fundamentally important inmicroeconomic theory, the analysis of social networks, and machine learning,such as coverage, submodular, XOS, and subadditive functions, as well asfunctions with sparse Fourier support.",http://arxiv.org/abs/1605.09227v1,,
1627,Learning Protein Dynamics with Metastable Switching Systems,"We introduce a machine learning approach for extracting fine-grainedrepresentations of protein evolution from molecular dynamics datasets.Metastable switching linear dynamical systems extend standard switching modelswith a physically-inspired stability constraint. This constraint enables thelearning of nuanced representations of protein dynamics that closely matchphysical reality. We derive an EM algorithm for learning, where the E-stepextends the forward-backward algorithm for HMMs and the M-step requires thesolution of large biconvex optimization problems. We construct an approximatesemidefinite program solver based on the Frank-Wolfe algorithm and use it tosolve the M-step. We apply our EM algorithm to learn accurate dynamics fromlarge simulation datasets for the opioid peptide met-enkephalin and theproto-oncogene Src-kinase. Our learned models demonstrate significantimprovements in temporal coherence over HMMs and standard switching models formet-enkephalin, and sample transition paths (possibly useful in rational drugdesign) for Src-kinase.",http://arxiv.org/abs/1610.01642v1,,
1628,"Learning what to look in chest X-rays with a recurrent visual attention  model","X-rays are commonly performed imaging tests that use small amounts ofradiation to produce pictures of the organs, tissues, and bones of the body.X-rays of the chest are used to detect abnormalities or diseases of theairways, blood vessels, bones, heart, and lungs. In this work we present astochastic attention-based model that is capable of learning what regionswithin a chest X-ray scan should be visually explored in order to conclude thatthe scan contains a specific radiological abnormality. The proposed model is arecurrent neural network (RNN) that learns to sequentially sample the entireX-ray and focus only on informative areas that are likely to contain therelevant information. We report on experiments carried out with more than$100,000$ X-rays containing enlarged hearts or medical devices. The model hasbeen trained using reinforcement learning methods to learn task-specificpolicies.",http://arxiv.org/abs/1701.06452v1,,
1629,"Learning Discrete Representations via Information Maximizing  Self-Augmented Training","Learning discrete representations of data is a central machine learning taskbecause of the compactness of the representations and ease of interpretation.The task includes clustering and hash learning as special cases. Deep neuralnetworks are promising to be used because they can model the non-linearity ofdata and scale to large datasets. However, their model complexity is huge, andtherefore, we need to carefully regularize the networks in order to learnuseful representations that exhibit intended invariance for applications ofinterest. To this end, we propose a method called Information MaximizingSelf-Augmented Training (IMSAT). In IMSAT, we use data augmentation to imposethe invariance on discrete representations. More specifically, we encourage thepredicted representations of augmented data points to be close to those of theoriginal data points in an end-to-end fashion. At the same time, we maximizethe information-theoretic dependency between data and their predicted discreterepresentations. Extensive experiments on benchmark datasets show that IMSATproduces state-of-the-art results for both clustering and unsupervised hashlearning.",http://arxiv.org/abs/1702.08720v3,,
1630,Self-Paced Multitask Learning with Shared Knowledge,"This paper introduces self-paced task selection to multitask learning, whereinstances from more closely related tasks are selected in a progression ofeasier-to-harder tasks, to emulate an effective human education strategy, butapplied to multitask machine learning. We develop the mathematical foundationfor the approach based on iterative selection of the most appropriate task,learning the task parameters, and updating the shared knowledge, optimizing anew bi-convex loss function. This proposed method applies quite generally,including to multitask feature learning, multitask learning with alternatingstructure optimization, etc. Results show that in each of the aboveformulations self-paced (easier-to-harder) task selection outperforms thebaseline version of these methods in all the experiments.",http://arxiv.org/abs/1703.00977v2,,
1631,Demystifying Relational Latent Representations,"Latent features learned by deep learning approaches have proven to be apowerful tool for machine learning. They serve as a data abstraction that makeslearning easier by capturing regularities in data explicitly. Their benefitsmotivated their adaptation to relational learning context. In our previouswork, we introduce an approach that learns relational latent features by meansof clustering instances and their relations. The major drawback of latentrepresentations is that they are often black-box and difficult to interpret.This work addresses these issues and shows that (1) latent features created byclustering are interpretable and capture interesting properties of data; (2)they identify local regions of instances that match well with the label, whichpartially explains their benefit; and (3) although the number of latentfeatures generated by this approach is large, often many of them are highlyredundant and can be removed without hurting performance much.",http://arxiv.org/abs/1705.05785v3,,
1632,"Feature learning in feature-sample networks using multi-objective  optimization","Data and knowledge representation are fundamental concepts in machinelearning. The quality of the representation impacts the performance of thelearning model directly. Feature learning transforms or enhances raw data tostructures that are effectively exploited by those models. In recent years,several works have been using complex networks for data representation andanalysis. However, no feature learning method has been proposed for suchcategory of techniques. Here, we present an unsupervised feature learningmechanism that works on datasets with binary features. First, the dataset ismapped into a feature--sample network. Then, a multi-objective optimizationprocess selects a set of new vertices to produce an enhanced version of thenetwork. The new features depend on a nonlinear function of a combination ofpreexisting features. Effectively, the process projects the input data into ahigher-dimensional space. To solve the optimization problem, we design twometaheuristics based on the lexicographic genetic algorithm and the improvedstrength Pareto evolutionary algorithm (SPEA2). We show that the enhancednetwork contains more information and can be exploited to improve theperformance of machine learning methods. The advantages and disadvantages ofeach optimization strategy are discussed.",http://arxiv.org/abs/1710.09300v1,,
1633,"Applications of Deep Learning and Reinforcement Learning to Biological  Data","Rapid advances of hardware-based technologies during the past decades haveopened up new possibilities for Life scientists to gather multimodal data invarious application domains (e.g., Omics, Bioimaging, Medical Imaging, and[Brain/Body]-Machine Interfaces), thus generating novel opportunities fordevelopment of dedicated data intensive machine learning techniques. Overall,recent research in Deep learning (DL), Reinforcement learning (RL), and theircombination (Deep RL) promise to revolutionize Artificial Intelligence. Thegrowth in computational power accompanied by faster and increased data storageand declining computing costs have already allowed scientists in various fieldsto apply these techniques on datasets that were previously intractable fortheir size and complexity. This review article provides a comprehensive surveyon the application of DL, RL, and Deep RL techniques in mining Biological data.In addition, we compare performances of DL techniques when applied to differentdatasets across various application domains. Finally, we outline open issues inthis challenging research area and discuss future development perspectives.",http://arxiv.org/abs/1711.03985v2,,
1634,Learning Independent Causal Mechanisms,"Statistical learning relies upon data sampled from a distribution, and weusually do not care what actually generated it in the first place. From thepoint of view of causal modeling, the structure of each distribution is inducedby physical mechanisms that give rise to dependences between observables.Mechanisms, however, can be meaningful autonomous modules of generative modelsthat make sense beyond a particular entailed data distribution, lendingthemselves to transfer between problems. We develop an algorithm to recover aset of independent (inverse) mechanisms from a set of transformed data points.The approach is unsupervised and based on a set of experts that compete fordata generated by the mechanisms, driving specialization. We analyze theproposed method in a series of experiments on image data. Each expert learns tomap a subset of the transformed data back to a reference distribution. Thelearned mechanisms generalize to novel domains. We discuss implications fortransfer learning and links to recent trends in generative modeling.",http://arxiv.org/abs/1712.00961v5,,
1635,Multimodal Generative Models for Scalable Weakly-Supervised Learning,"Multiple modalities often co-occur when describing natural phenomena.Learning a joint representation of these modalities should yield deeper andmore useful representations. Previous generative approaches to multi-modalinput either do not learn a joint distribution or require additionalcomputation to handle missing data. Here, we introduce a multimodal variationalautoencoder (MVAE) that uses a product-of-experts inference network and asub-sampled training paradigm to solve the multi-modal inference problem.Notably, our model shares parameters to efficiently learn under any combinationof missing modalities. We apply the MVAE on four datasets and matchstate-of-the-art performance using many fewer parameters. In addition, we showthat the MVAE is directly applicable to weakly-supervised learning, and isrobust to incomplete supervision. We then consider two case studies, one oflearning image transformations---edge detection, colorization,segmentation---as a set of modalities, followed by one of machine translationbetween two languages. We find appealing results across this range of tasks.",http://arxiv.org/abs/1802.05335v3,,
1636,Learning Determinantal Point Processes by Corrective Negative Sampling,"Determinantal Point Processes (DPPs) have attracted significant interest fromthe machine-learning community due to their ability to elegantly and tractablymodel the delicate balance between quality and diversity of sets. DPPs arecommonly learned from data using maximum likelihood estimation (MLE). Whilefitting observed sets well, MLE for DPPs may also assign high likelihoods tounobserved sets that are far from the true generative distribution of the data.To address this issue, which reduces the quality of the learned model, weintroduce a novel optimization problem, Contrastive Estimation (CE), whichencodes information about ""negative"" samples into the basic learning model. CEis grounded in the successful use of negative information in machine-vision andlanguage modeling. Depending on the chosen negative distribution (which may bestatic or evolve during optimization), CE assumes two different forms, which weanalyze theoretically and experimentally. We evaluate our new model onreal-world datasets; on a challenging dataset, CE learning delivers aconsiderable improvement in predictive performance over a DPP learned withoutusing contrastive information.",http://arxiv.org/abs/1802.05649v4,,
1637,"Improving a Neural Semantic Parser by Counterfactual Learning from Human  Bandit Feedback","Counterfactual learning from human bandit feedback describes a scenario whereuser feedback on the quality of outputs of a historic system is logged and usedto improve a target system. We show how to apply this learning framework toneural semantic parsing. From a machine learning perspective, the key challengelies in a proper reweighting of the estimator so as to avoid known degeneraciesin counterfactual learning, while still being applicable to stochastic gradientoptimization. To conduct experiments with human users, we devise an easy-to-useinterface to collect human feedback on semantic parses. Our work is the firstto show that semantic parsers can be improved significantly by counterfactuallearning from logged human feedback data.",http://arxiv.org/abs/1805.01252v2,,
1638,Distribution Aware Active Learning,"Discriminative learning machines often need a large set of labeled samplesfor training. Active learning (AL) settings assume that the learner has thefreedom to ask an oracle to label its desired samples. Traditional ALalgorithms heuristically choose query samples about which the current learneris uncertain. This strategy does not make good use of the structure of thedataset at hand and is prone to be misguided by outliers. To alleviate thisproblem, we propose to distill the structural information into a probabilisticgenerative model which acts as a \emph{teacher} in our model. The active\emph{learner} uses this information effectively at each cycle of activelearning. The proposed method is generic and does not depend on the type oflearner and teacher. We then suggest a query criterion for active learning thatis aware of distribution of data and is more robust against outliers. Ourmethod can be combined readily with several other query criteria for activelearning. We provide the formulation and empirically show our idea via toy andreal examples.",http://arxiv.org/abs/1805.08916v1,,
1639,Security and Privacy Issues in Deep Learning,"With the development of machine learning, expectations for artificialintelligence (AI) technology are increasing day by day. In particular, deeplearning has shown enriched performance results in a variety of fields. Thereare many applications that are closely related to our daily life, such asmaking significant decisions in application area based on predictions orclassifications, in which a deep learning (DL) model could be relevant. Hence,if a DL model causes mispredictions or misclassifications due to maliciousexternal influences, it can cause very large difficulties in real life.Moreover, training deep learning models involves relying on an enormous amountof data and the training data often includes sensitive information. Therefore,deep learning models should not expose the privacy of such data. In this paper,we reviewed the threats and developed defense methods on the security of themodels and the data privacy under the notion of SPAI: Secure and Private AI. Wealso discuss current challenges and open issues.",http://arxiv.org/abs/1807.11655v2,,
1640,Active Learning for Wireless IoT Intrusion Detection,"Internet of Things (IoT) is becoming truly ubiquitous in our everyday life,but it also faces unique security challenges. Intrusion detection is criticalfor the security and safety of a wireless IoT network. This paper discusses thehuman-in-the-loop active learning approach for wireless intrusion detection. Wefirst present the fundamental challenges against the design of a successfulIntrusion Detection System (IDS) for wireless IoT network. We then brieflyreview the rudimentary concepts of active learning and propose its employmentin the diverse applications of wireless intrusion detection. Experimentalexample is also presented to show the significant performance improvement ofthe active learning method over traditional supervised learning approach. Whilemachine learning techniques have been widely employed for intrusion detection,the application of human-in-the-loop machine learning that leverages bothmachine and human intelligence to intrusion detection of IoT is still in itsinfancy. We hope this article can assist the readers in understanding the keyconcepts of active learning and spur further research in this area.",http://arxiv.org/abs/1808.01412v1,,
1641,"Semi-unsupervised Learning of Human Activity using Deep Generative  Models","We introduce 'semi-unsupervised learning', a problem regime related totransfer learning and zero-shot learning where, in the training data, someclasses are sparsely labelled and others entirely unlabelled. Models able tolearn from training data of this type are potentially of great use as manyreal-world datasets are like this. Here we demonstrate a new deep generativemodel for classification in this regime. Our model, a Gaussian mixture deepgenerative model, demonstrates superior semi-unsupervised classificationperformance on MNIST to model M2 from Kingma and Welling (2014). We apply themodel to human accelerometer data, performing activity classification andstructure discovery on windows of time series data.",http://arxiv.org/abs/1810.12176v2,,
1642,Improving Robustness of Attention Models on Graphs,"Machine learning models that can exploit the inherent structure in data havegained prominence. In particular, there is a surge in deep learning solutionsfor graph-structured data, due to its wide-spread applicability in severalfields. Graph attention networks (GAT), a recent addition to the broad class offeature learning models in graphs, utilizes the attention mechanism toefficiently learn continuous vector representations for semi-supervisedlearning problems. In this paper, we perform a detailed analysis of GAT models,and present interesting insights into their behavior. In particular, we showthat the models are vulnerable to adversaries (rogue nodes) and hence proposenovel regularization strategies to improve the robustness of GAT models. Usingbenchmark datasets, we demonstrate performance improvements on semi-supervisedlearning, using the proposed robust variant of GAT.",http://arxiv.org/abs/1811.00181v1,,
1643,"Un-normalized hypergraph p-Laplacian based semi-supervised learning  methods","Most network-based machine learning methods assume that the labels of twoadjacent samples in the network are likely to be the same. However, assumingthe pairwise relationship between samples is not complete. The information agroup of samples that shows very similar pattern and tends to have similarlabels is missed. The natural way overcoming the information loss of the aboveassumption is to represent the feature dataset of samples as the hypergraph.Thus, in this paper, we will present the un-normalized hypergraph p-Laplaciansemi-supervised learning methods. These methods will be applied to the zoodataset and the tiny version of 20 newsgroups dataset. Experiment results showthat the accuracy performance measures of these un-normalized hypergraphp-Laplacian based semi-supervised learning methods are significantly greaterthan the accuracy performance measure of the un-normalized hypergraph Laplacianbased semi-supervised learning method (the current state of the art methodhypergraph Laplacian based semi-supervised learning method for classificationproblem with p=2).",http://arxiv.org/abs/1811.02986v2,,
1644,"An Adaptive Oversampling Learning Method for Class-Imbalanced Fault  Diagnostics and Prognostics","Data-driven fault diagnostics and prognostics suffers from class-imbalanceproblem in industrial systems and it raises challenges to common machinelearning algorithms as it becomes difficult to learn the features of theminority class samples. Synthetic oversampling methods are commonly used totackle these problems by generating the minority class samples to balance thedistributions between majority and minority classes. However, many ofoversampling methods are inappropriate that they cannot generate effective anduseful minority class samples according to different distributions of data,which further complicate the process of learning samples. Thus, this paperproposes a novel adaptive oversampling technique: EM-based Weighted MinorityOversampling TEchnique (EWMOTE) for industrial fault diagnostics andprognostics. The methods comprises a weighted minority sampling strategy toidentify hard-to-learn informative minority fault samples and ExpectationMaximization (EM) based imputation algorithm to generate fault samples. Tovalidate the performance of the proposed methods, experiments are conducted intwo real datasets. The results show that the method could achieve betterperformance on not only binary class, but multi-class imbalance learning taskin different imbalance ratios than other oversampling-based baseline models.",http://arxiv.org/abs/1811.07674v1,,
1645,"What Should I Learn First: Introducing LectureBank for NLP Education and  Prerequisite Chain Learning","Recent years have witnessed the rising popularity of Natural LanguageProcessing (NLP) and related fields such as Artificial Intelligence (AI) andMachine Learning (ML). Many online courses and resources are available even forthose without a strong background in the field. Often the student is curiousabout a specific topic but does not quite know where to begin studying. Toanswer the question of ""what should one learn first,"" we apply anembedding-based method to learn prerequisite relations for course concepts inthe domain of NLP. We introduce LectureBank, a dataset containing 1,352 Englishlecture files collected from university courses which are each classifiedaccording to an existing taxonomy as well as 208 manually-labeled prerequisiterelation topics, which is publicly available. The dataset will be useful foreducational purposes such as lecture preparation and organization as well asapplications such as reading list generation. Additionally, we experiment withneural graph-based networks and non-neural classifiers to learn theseprerequisite relations from our dataset.",http://arxiv.org/abs/1811.12181v1,,
1646,"Theory of Curriculum Learning, with Convex Loss Functions","Curriculum Learning - the idea of teaching by gradually exposing the learnerto examples in a meaningful order, from easy to hard, has been investigated inthe context of machine learning long ago. Although methods based on thisconcept have been empirically shown to improve performance of several learningalgorithms, no theoretical analysis has been provided even for simple cases. Toaddress this shortfall, we start by formulating an ideal definition ofdifficulty score - the loss of the optimal hypothesis at a given datapoint. Weanalyze the possible contribution of curriculum learning based on this score intwo convex problems - linear regression, and binary classification by hingeloss minimization. We show that in both cases, the expected convergence ratedecreases monotonically with the ideal difficulty score, in accordance withearlier empirical results. We also prove that when the ideal difficulty scoreis fixed, the convergence rate is monotonically increasing with respect to theloss of the current hypothesis at each point. We discuss how these resultsbring to term two apparently contradicting heuristics: curriculum learning onthe one hand, and hard data mining on the other.",http://arxiv.org/abs/1812.03472v1,,
1647,Transfer learning to model inertial confinement fusion experiments,"Inertial confinement fusion (ICF) experiments are designed using computersimulations that are approximations of reality, and therefore must becalibrated to accurately predict experimental observations. In this work, wepropose a novel nonlinear technique for calibrating from simulations toexperiments, or from low fidelity simulations to high fidelity simulations, via""transfer learning"". Transfer learning is a commonly used technique in themachine learning community, in which models trained on one task are partiallyretrained to solve a separate, but related task, for which there is a limitedquantity of data. We introduce the idea of hierarchical transfer learning, inwhich neural networks trained on low fidelity models are calibrated to highfidelity models, then to experimental data. This technique essentiallybootstraps the calibration process, enabling the creation of models whichpredict high fidelity simulations or experiments with minimal computationalcost. We apply this technique to a database of ICF simulations and experimentscarried out at the Omega laser facility. Transfer learning with deep neuralnetworks enables the creation of models that are more predictive of Omegaexperiments than simulations alone. The calibrated models accurately predictfuture Omega experiments, and are used to search for new, optimal implosiondesigns.",http://arxiv.org/abs/1812.06055v1,,
1648,"Interpretable preference learning: a game theoretic framework for large  margin on-line feature and rule learning","A large body of research is currently investigating on the connection betweenmachine learning and game theory. In this work, game theory notions areinjected into a preference learning framework. Specifically, a preferencelearning problem is seen as a two-players zero-sum game. An algorithm isproposed to incrementally include new useful features into the hypothesis. Thiscan be particularly important when dealing with a very large number ofpotential features like, for instance, in relational learning and ruleextraction. A game theoretical analysis is used to demonstrate the convergenceof the algorithm. Furthermore, leveraging on the natural analogy betweenfeatures and rules, the resulting models can be easily interpreted by humans.An extensive set of experiments on classification tasks shows the effectivenessof the proposed method in terms of interpretability and feature selectionquality, with accuracy at the state-of-the-art.",http://arxiv.org/abs/1812.07895v1,,
1649,Differential Temporal Difference Learning,"Value functions derived from Markov decision processes arise as a centralcomponent of algorithms as well as performance metrics in many statistics andengineering applications of machine learning techniques. Computation of thesolution to the associated Bellman equations is challenging in most practicalcases of interest. A popular class of approximation techniques, known asTemporal Difference (TD) learning algorithms, are an important sub-class ofgeneral reinforcement learning methods. The algorithms introduced in this paperare intended to resolve two well-known difficulties of TD-learning approaches:Their slow convergence due to very high variance, and the fact that, for theproblem of computing the relative value function, consistent algorithms existonly in special cases. First we show that the gradients of these valuefunctions admit a representation that lends itself to algorithm design. Basedon this result, a new class of differential TD-learning algorithms isintroduced. For Markovian models on Euclidean space with smooth dynamics, thealgorithms are shown to be consistent under general conditions. Numericalresults show dramatic variance reduction when compared to standard methods.",http://arxiv.org/abs/1812.11137v1,,
1650,Quantized Epoch-SGD for Communication-Efficient Distributed Learning,"Due to its efficiency and ease to implement, stochastic gradient descent(SGD) has been widely used in machine learning. In particular, SGD is one ofthe most popular optimization methods for distributed learning. Recently,quantized SGD (QSGD), which adopts quantization to reduce the communicationcost in SGD-based distributed learning, has attracted much attention. Althoughseveral QSGD methods have been proposed, some of them are heuristic withouttheoretical guarantee, and others have high quantization variance which makesthe convergence become slow. In this paper, we propose a new method, calledQuantized Epoch-SGD (QESGD), for communication-efficient distributed learning.QESGD compresses (quantizes) the parameter with variance reduction, so that itcan get almost the same performance as that of SGD with less communicationcost. QESGD is implemented on the Parameter Server framework, and empiricalresults on distributed deep learning show that QESGD can outperform otherstate-of-the-art quantization methods to achieve the best performance.",http://arxiv.org/abs/1901.03040v1,,
1651,"Active Learning for One-Class Classification Using Two One-Class  Classifiers","This paper introduces a novel, generic active learning method for one-classclassification. Active learning methods play an important role to reduce theefforts of manual labeling in the field of machine learning. Although manyactive learning approaches have been proposed during the last years, most ofthem are restricted on binary or multi-class problems. One-class classifiersuse samples from only one class, the so-called target class, during trainingand hence require special active learning strategies. The few strategiesproposed for one-class classification either suffer from their limitation onspecific one-class classifiers or their performance depends on particularassumptions about datasets like imbalance. Our proposed method bases on usingtwo one-class classifiers, one for the desired target class and one for theso-called outlier class. It allows to invent new query strategies, to usebinary query strategies and to define simple stopping criteria. Based on thenew method, two query strategies are proposed. The provided experiments comparethe proposed approach with known strategies on various datasets and showimproved results in almost all situations.",http://arxiv.org/abs/1901.03124v1,,
1652,Human few-shot learning of compositional instructions,"People learn in fast and flexible ways that have not been emulated bymachines. Once a person learns a new verb ""dax,"" he or she can effortlesslyunderstand how to ""dax twice,"" ""walk and dax,"" or ""dax vigorously."" There havebeen striking recent improvements in machine learning for natural languageprocessing, yet the best algorithms require vast amounts of experience andstruggle to generalize new concepts in compositional ways. To better understandthese distinctively human abilities, we study the compositional skills ofpeople through language-like instruction learning tasks. Our results show thatpeople can learn and use novel functional concepts from very few examples(few-shot learning), successfully applying familiar functions to novel inputs.People can also compose concepts in complex ways that go beyond the provideddemonstrations. Two additional experiments examined the assumptions andinductive biases that people make when solving these tasks, revealing threebiases: mutual exclusivity, one-to-one mappings, and iconic concatenation. Wediscuss the implications for cognitive modeling and the potential for buildingmachines with more human-like language learning capabilities.",http://arxiv.org/abs/1901.04587v1,,
1653,"Comparing two deep learning sequence-based models for protein-protein  interaction prediction","Biological data are extremely diverse, complex but also quite sparse. Therecent developments in deep learning methods are offering new possibilities forthe analysis of complex data. However, it is easy to be get a deep learningmodel that seems to have good results but is in fact either overfitting thetraining data or the validation data. In particular, the fact to overfit thevalidation data, called ""information leak"", is almost never treated in papersproposing deep learning models to predict protein-protein interactions (PPI).In this work, we compare two carefully designed deep learning models and showpitfalls to avoid while predicting PPIs through machine learning methods. Ourbest model predicts accurately more than 78% of human PPI, in very strictconditions both for training and testing. The methodology we propose here allowus to have strong confidences about the ability of a model to scale up onlarger datasets. This would allow sharper models when larger datasets would beavailable, rather than current models prone to information leaks. Our solidmethodological foundations shall be applicable to more organisms and wholeproteome networks predictions.",http://arxiv.org/abs/1901.06268v1,,
1654,Peer-to-peer Federated Learning on Graphs,"We consider the problem of training a machine learning model over a networkof nodes in a fully decentralized framework. The nodes take a Bayesian-likeapproach via the introduction of a belief over the model parameter space. Wepropose a distributed learning algorithm in which nodes update their belief byaggregate information from their one-hop neighbors to learn a model that bestfits the observations over the entire network. In addition, we also obtainsufficient conditions to ensure that the probability of error is small forevery node in the network. We discuss approximations required for applying thisalgorithm to train Deep Neural Networks (DNNs). Experiments on training linearregression model and on training a DNN show that the proposed learning rulealgorithm provides a significant improvement in the accuracy compared to thecase where nodes learn without cooperation.",http://arxiv.org/abs/1901.11173v1,,
1655,Online Budgeted Learning for Classifier Induction,"In real-world machine learning applications, there is a cost associated withsampling of different features. Budgeted learning can be used to select whichfeature-values to acquire from each instance in a dataset, such that the bestmodel is induced under a given constraint. However, this approach is notpossible in the domain of online learning since one may not retroactivelyacquire feature-values from past instances. In online learning, the challengeis to find the optimum set of features to be acquired from each instance uponarrival from a data stream. In this paper we introduce the issue of onlinebudgeted learning and describe a general framework for addressing thischallenge. We propose two types of feature value acquisition policies based onthe multi-armed bandit problem: random and adaptive. Adaptive policies performonline adjustments according to new information coming from a data stream,while random policies are not sensitive to the information that arrives fromthe data stream. Our comparative study on five real-world datasets indicatesthat adaptive policies outperform random policies for most budget limitationsand datasets. Furthermore, we found that in some cases adaptive policiesachieve near-optimal results.",http://arxiv.org/abs/1903.05382v1,,
1656,"Learning Two layer Networks with Multinomial Activation and High  Thresholds","Giving provable guarantees for learning neural networks is a core challengeof machine learning theory. Most prior work gives parameter recovery guaranteesfor one hidden layer networks. In this work we study a two layer network wherethe top node instead of a sum (one layer) is a well-behaved multivariatepolynomial in all its inputs. We show that if the thresholds (biases) of thefirst layer neurons are higher than $\Omega(\sqrt{\log d})$ for $d$ being theinput dimension, then the weights are learnable under the gaussian input.Furthermore even for lower thresholds, we can learn the lowest layer usingpolynomial sample complexity although exponential time. As an application ofour results, we give a polynomial time algorithm for learning an intersectionof halfspaces that are $\Omega(\sqrt{\log d})$ far from the origin for gaussianinput distribution. Finally for deep networks with depth larger than two,assuming the layers two onwards can be expressed as a polynomial by simplyusing the taylor series, we can learn the lowest layer under the conditionsrequired by our assumptions.",http://arxiv.org/abs/1903.09231v1,,
1657,"Rallying Adversarial Techniques against Deep Learning for Network  Security","Recent advances in artificial intelligence and the increasing need forpowerful defensive measures in the domain of network security, have led to theadoption of deep learning approaches for use in network intrusion detectionsystems. These methods have achieved superior performance against conventionalnetwork attacks, which enable the deployment of practical security systems tounique and dynamic sectors. Adversarial machine learning, unfortunately, hasrecently shown that deep learning models are inherently vulnerable toadversarial modifications on their input data. Because of this susceptibility,the deep learning models deployed to power a network defense could in fact bethe weakest entry point for compromising a network system. In this paper, weshow that by modifying on average as little as 1.38 of the input features, anadversary can generate malicious inputs which effectively fool a deep learningbased NIDS. Therefore, when designing such systems, it is crucial to considerthe performance from not only the conventional network security perspective butalso the adversarial machine learning domain.",http://arxiv.org/abs/1903.11688v1,,
1658,Building Automated Survey Coders via Interactive Machine Learning,"Software systems trained via machine learning to automatically classifyopen-ended answers (a.k.a. verbatims) are by now a reality. Still, theiradoption in the survey coding industry has been less widespread than it mighthave been. Among the factors that have hindered a more massive takeup of thistechnology are the effort involved in manually coding a sufficient amount oftraining data, the fact that small studies do not seem to justify this effort,and the fact that the process needs to be repeated anew when brand new codingtasks arise. In this paper we will argue for an approach to building verbatimclassifiers that we will call ""Interactive Learning"", and that addresses allthe above problems. We will show that, for the same amount of training effort,interactive learning delivers much better coding accuracy than standard""non-interactive"" learning. This is especially true when the amount of data weare willing to manually code is small, which makes this approach attractivealso for small-scale studies. Interactive learning also lends itself to reusingpreviously trained classifiers for dealing with new (albeit related) codingtasks. Interactive learning also integrates better in the daily workflow of thesurvey specialist, and delivers a better user experience overall.",http://arxiv.org/abs/1903.12110v1,,
1659,Meta-Learning Acquisition Functions for Bayesian Optimization,"Many practical applications of machine learning require data-efficientblack-box function optimization, e.g., to identify hyperparameters or processsettings. However, readily available algorithms are typically designed to beuniversal optimizers and are, thus, often suboptimal for specific tasks. Wetherefore propose a method to learn optimizers which are automatically adaptedto a given class of objective functions, e.g., in the context of sim-to-realapplications. Instead of learning optimization from scratch, the proposedapproach is firmly based within the famous Bayesian optimization framework.Only the acquisition function (AF) is replaced by a learned neural network andtherefore the resulting algorithm is still able to exploit the provengeneralization capabilities of Gaussian processes. We present experiments onseveral simulated as well as on a sim-to-real transfer task. The results showthat the learned optimizers (1) consistently perform better than or on-par withknown AFs on general function classes and (2) can automatically identifystructural properties of a function class using cheap simulations and transferthis knowledge to adapt rapidly to real hardware tasks, thereby significantlyoutperforming existing problem-agnostic AFs.",http://arxiv.org/abs/1904.02642v2,,
1660,"Beyond Volume: The Impact of Complex Healthcare Data on the Machine  Learning Pipeline","From medical charts to national census, healthcare has traditionally operatedunder a paper-based paradigm. However, the past decade has marked a long andarduous transformation bringing healthcare into the digital age. Ranging fromelectronic health records, to digitized imaging and laboratory reports, topublic health datasets, today, healthcare now generates an incredible amount ofdigital information. Such a wealth of data presents an exciting opportunity forintegrated machine learning solutions to address problems across multiplefacets of healthcare practice and administration. Unfortunately, the ability toderive accurate and informative insights requires more than the ability toexecute machine learning models. Rather, a deeper understanding of the data onwhich the models are run is imperative for their success. While a significanteffort has been undertaken to develop models able to process the volume of dataobtained during the analysis of millions of digitalized patient records, it isimportant to remember that volume represents only one aspect of the data. Infact, drawing on data from an increasingly diverse set of sources, healthcaredata presents an incredibly complex set of attributes that must be accountedfor throughout the machine learning pipeline. This chapter focuses onhighlighting such challenges, and is broken down into three distinctcomponents, each representing a phase of the pipeline. We begin with attributesof the data accounted for during preprocessing, then move to considerationsduring model building, and end with challenges to the interpretation of modeloutput. For each component, we present a discussion around data as it relatesto the healthcare domain and offer insight into the challenges each may imposeon the efficiency of machine learning techniques.",http://arxiv.org/abs/1706.01513v2,,
1661,"Software Engineers vs. Machine Learning Algorithms: An Empirical Study  Assessing Performance and Reuse Tasks","Several papers have recently contained reports on applying machine learning(ML) to the automation of software engineering (SE) tasks, such as projectmanagement, modeling and development. However, there appear to be no approachescomparing how software engineers fare against machine-learning algorithms asapplied to specific software development tasks. Such a comparison is essentialto gain insight into which tasks are better performed by humans and which bymachine learning and how cooperative work or human-in-the-loop processes can beimplemented more effectively. In this paper, we present an empirical study thatcompares how software engineers and machine-learning algorithms perform andreuse tasks. The empirical study involves the synthesis of the controlstructure of an autonomous streetlight application. Our approach consists offour steps. First, we solved the problem using machine learning to determinespecific performance and reuse tasks. Second, we asked software engineers withdifferent domain knowledge levels to provide a solution to the same tasks.Third, we compared how software engineers fare against machine-learningalgorithms when accomplishing the performance and reuse tasks based on criteriasuch as energy consumption and safety. Finally, we analyzed the results tounderstand which tasks are better performed by either humans or algorithms sothat they can work together more effectively. Such an understanding and theresulting human-in-the-loop approaches, which take into account the strengthsand weaknesses of humans and machine-learning algorithms, are fundamental notonly to provide a basis for cooperative work in support of softwareengineering, but also, in other areas.",http://arxiv.org/abs/1802.01096v2,,
1662,Efficient and Robust Machine Learning for Real-World Systems,"While machine learning is traditionally a resource intensive task, embeddedsystems, autonomous navigation and the vision of the Internet-of-Things fuelthe interest in resource efficient approaches. These approaches require acarefully chosen trade-off between performance and resource consumption interms of computation and energy. On top of this, it is crucial to treatuncertainty in a consistent manner in all but the simplest applications ofmachine learning systems. In particular, a desideratum for any real-worldsystem is to be robust in the presence of outliers and corrupted data, as wellas being `aware' of its limits, i.e.\ the system should maintain and provide anuncertainty estimate over its own predictions. These complex demands are amongthe major challenges in current machine learning research and key to ensure asmooth transition of machine learning technology into every day's applications.In this article, we provide an overview of the current state of the art ofmachine learning techniques facilitating these real-world requirements. Firstwe provide a comprehensive review of resource-efficiency in deep neuralnetworks with focus on techniques for model size reduction, compression andreduced precision. These techniques can be applied during training or aspost-processing and are widely used to reduce both computational complexity andmemory footprint. As most (practical) neural networks are limited in their waysto treat uncertainty, we contrast them with probabilistic graphical models,which readily serve these desiderata by means of probabilistic inference. Inthat way, we provide an extensive overview of the current state-of-the-art ofrobust and efficient machine learning for real-world systems.",http://arxiv.org/abs/1812.02240v1,,
1663,"Support vector machines/relevance vector machine for remote sensing  classification: A review","Kernel-based machine learning algorithms are based on mapping data from theoriginal input feature space to a kernel feature space of higher dimensionalityto solve a linear problem in that space. Over the last decade, kernel basedclassification and regression approaches such as support vector machines havewidely been used in remote sensing as well as in various civil engineeringapplications. In spite of their better performance with different datasets,support vector machines still suffer from shortcomings such asvisualization/interpretation of model, choice of kernel and kernel specificparameter as well as the regularization parameter. Relevance vector machinesare another kernel based approach being explored for classification andregression with in last few years. The advantages of the relevance vectormachines over the support vector machines is the availability of probabilisticpredictions, using arbitrary kernel functions and not requiring setting of theregularization parameter. This paper presents a state-of-the-art review of SVMand RVM in remote sensing and provides some details of their use in other civilengineering application also.",http://arxiv.org/abs/1101.2987v1,,
1664,Distributed Nonparametric Regression under Communication Constraints,"This paper studies the problem of nonparametric estimation of a smoothfunction with data distributed across multiple machines. We assume anindependent sample from a white noise model is collected at each machine, andan estimator of the underlying true function needs to be constructed at acentral machine. We place limits on the number of bits that each machine canuse to transmit information to the central machine. Our results give bothasymptotic lower bounds and matching upper bounds on the statistical risk undervarious settings. We identify three regimes, depending on the relationshipamong the number of machines, the size of the data available at each machine,and the communication budget. When the communication budget is small, thestatistical risk depends solely on this communication bottleneck, regardless ofthe sample size. In the regime where the communication budget is large, theclassic minimax risk in the non-distributed estimation setting is recovered. Inan intermediate regime, the statistical risk depends on both the sample sizeand the communication budget.",http://arxiv.org/abs/1803.01302v2,,
1665,Barrier Certificates for Assured Machine Teaching,"Machine teaching has received significant attention in the past few years asa paradigm shift from machine learning. While machine learning is oftenconcerned with improving the performance of learners, machine teaching pertainsto the efficiency of teachers. For example, machine teaching seeks to find theoptimal (minimum) number of data samples needed for teaching a targethypothesis to a learner. Hence, it is natural to raise the question of how canwe provide assurances for teaching given a machine teaching algorithm. In thispaper, we address this question by borrowing notions from control theory. Webegin by proposing a model based on partially observable Markov decisionprocesses (POMDPs) for a class of machine teaching problems. We then show thatthe POMDP formulation can be cast as a special hybrid system, i.e., adiscrete-time switched system. Subsequently, we use barrier certificates toverify properties of this special hybrid system. We show how the computation ofthe barrier certificate can be decomposed and numerically implemented as thesolution to a sum-of-squares (SOS) program. For illustration, we show how theproposed framework based on control theory can be used to verify the teachingperformance of two well-known machine teaching methods.",http://arxiv.org/abs/1810.00093v1,,
1666,Minimax Analysis of Active Learning,"This work establishes distribution-free upper and lower bounds on the minimaxlabel complexity of active learning with general hypothesis classes, undervarious noise models. The results reveal a number of surprising facts. Inparticular, under the noise model of Tsybakov (2004), the minimax labelcomplexity of active learning with a VC class is always asymptotically smallerthan that of passive learning, and is typically significantly smaller than thebest previously-published upper bounds in the active learning literature. Inhigh-noise regimes, it turns out that all active learning problems of a givenVC dimension have roughly the same minimax label complexity, which contrastswith well-known results for bounded noise. In low-noise regimes, we find thatthe label complexity is well-characterized by a simple combinatorial complexitymeasure we call the star number. Interestingly, we find that almost all of thecomplexity measures previously explored in the active learning literature haveworst-case values exactly equal to the star number. We also propose new activelearning strategies that nearly achieve these minimax label complexities.",http://arxiv.org/abs/1410.0996v1,,
1667,Deep metric learning using Triplet network,"Deep learning has proven itself as a successful set of models for learninguseful semantic representations of data. These, however, are mostly implicitlylearned as part of a classification task. In this paper we propose the tripletnetwork model, which aims to learn useful representations by distancecomparisons. A similar model was defined by Wang et al. (2014), tailor made forlearning a ranking for image information retrieval. Here we demonstrate usingvarious datasets that our model learns a better representation than that of itsimmediate competitor, the Siamese network. We also discuss future possibleusage as a framework for unsupervised learning.",http://arxiv.org/abs/1412.6622v4,,
1668,Reinforcement and Imitation Learning via Interactive No-Regret Learning,"Recent work has demonstrated that problems-- particularly imitation learningand structured prediction-- where a learner's predictions influence theinput-distribution it is tested on can be naturally addressed by an interactiveapproach and analyzed using no-regret online learning. These approaches toimitation learning, however, neither require nor benefit from information aboutthe cost of actions. We extend existing results in two directions: first, wedevelop an interactive imitation learning approach that leverages costinformation; second, we extend the technique to address reinforcement learning.The results provide theoretical support to the commonly observed successes ofonline approximate policy iteration. Our approach suggests a broad new familyof algorithms and provides a unifying view of existing techniques for imitationand reinforcement learning.",http://arxiv.org/abs/1406.5979v1,,
1669,Unsupervised Learning of Predictors from Unpaired Input-Output Samples,"Unsupervised learning is the most challenging problem in machine learning andespecially in deep learning. Among many scenarios, we study an unsupervisedlearning problem of high economic value --- learning to predict without costlypairing of input data and corresponding labels. Part of the difficulty in thisproblem is a lack of solid evaluation measures. In this paper, we take apractical approach to grounding unsupervised learning by using the same successcriterion as for supervised learning in prediction tasks but we do not requirethe presence of paired input-output training data. In particular, we propose anobjective function that aims to make the predicted outputs fit well thestructure of the output while preserving the correlation between the input andthe predicted output. We experiment with a synthetic structural predictionproblem and show that even with simple linear classifiers, the objectivefunction is already highly non-convex. We further demonstrate the nature ofthis non-convex optimization problem as well as potential solutions. Inparticular, we show that with regularization via a generative model, learningwith the proposed unsupervised objective function converges to an optimalsolution.",http://arxiv.org/abs/1606.04646v1,,
1670,"On the Number of Samples Needed to Learn the Correct Structure of a  Bayesian Network","Bayesian Networks (BNs) are useful tools giving a natural and compactrepresentation of joint probability distributions. In many applications oneneeds to learn a Bayesian Network (BN) from data. In this context, it isimportant to understand the number of samples needed in order to guarantee asuccessful learning. Previous work have studied BNs sample complexity, yet itmainly focused on the requirement that the learned distribution will be closeto the original distribution which generated the data. In this work, we study adifferent aspect of the learning, namely the number of samples needed in orderto learn the correct structure of the network. We give both asymptotic results,valid in the large sample limit, and experimental results, demonstrating thelearning behavior for feasible sample sizes. We show that structure learning isa more difficult task, compared to approximating the correct distribution, inthe sense that it requires a much larger number of samples, regardless of thecomputational power available for the learner.",http://arxiv.org/abs/1206.6862v1,,
1671,Active Player Modelling,"We argue for the use of active learning methods for player modelling. Inactive learning, the learning algorithm chooses where to sample the searchspace so as to optimise learning progress. We hypothesise that player modellingbased on active learning could result in vastly more efficient learning, butwill require big changes in how data is collected. Some example active playermodelling scenarios are described. A particular form of active learning is alsoequivalent to an influential formalisation of (human and machine) curiosity,and games with active learning could therefore be seen as being curious aboutthe player. We further hypothesise that this form of curiosity is symmetric,and therefore that games that explore their players based on the principles ofactive learning will turn out to select game configurations that areinteresting to the player that is being explored.",http://arxiv.org/abs/1312.2936v1,,
1672,Online Pairwise Learning Algorithms with Kernels,"Pairwise learning usually refers to a learning task which involves a lossfunction depending on pairs of examples, among which most notable ones includeranking, metric learning and AUC maximization. In this paper, we study anonline algorithm for pairwise learning with a least-square loss function in anunconstrained setting of a reproducing kernel Hilbert space (RKHS), which werefer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast toexisting works \cite{Kar,Wang} which require that the iterates are restrictedto a bounded domain or the loss function is strongly-convex, OPERA isassociated with a non-strongly convex objective function and learns the targetfunction in an unconstrained RKHS. Specifically, we establish a general theoremwhich guarantees the almost surely convergence for the last iterate of OPERAwithout any assumptions on the underlying distribution. Explicit convergencerates are derived under the condition of polynomially decaying step sizes. Wealso establish an interesting property for a family of widely-used kernels inthe setting of pairwise learning and illustrate the above convergence resultsusing such kernels. Our methodology mainly depends on the characterization ofRKHSs using its associated integral operators and probability inequalities forrandom variables with values in a Hilbert space.",http://arxiv.org/abs/1502.07229v1,,
1673,"Asymptotic Analysis of Objectives based on Fisher Information in Active  Learning","Obtaining labels can be costly and time-consuming. Active learning allows alearning algorithm to intelligently query samples to be labeled for efficientlearning. Fisher information ratio (FIR) has been used as an objective forselecting queries in active learning. However, little is known about the theorybehind the use of FIR for active learning. There is a gap between theunderlying theory and the motivation of its usage in practice. In this paper,we attempt to fill this gap and provide a rigorous framework for analyzingexisting FIR-based active learning methods. In particular, we show that FIR canbe asymptotically viewed as an upper bound of the expected variance of thelog-likelihood ratio. Additionally, our analysis suggests a unifying frameworkthat not only enables us to make theoretical comparisons among the existingquerying methods based on FIR, but also allows us to give insight into thedevelopment of new active learning approaches based on this objective.",http://arxiv.org/abs/1605.08798v2,,
1674,Probabilistic Dimensionality Reduction via Structure Learning,"We propose a novel probabilistic dimensionality reduction framework that cannaturally integrate the generative model and the locality information of data.Based on this framework, we present a new model, which is able to learn asmooth skeleton of embedding points in a low-dimensional space fromhigh-dimensional noisy data. The formulation of the new model can beequivalently interpreted as two coupled learning problem, i.e., structurelearning and the learning of projection matrix. This interpretation motivatesthe learning of the embedding points that can directly form an explicit graphstructure. We develop a new method to learn the embedding points that form aspanning tree, which is further extended to obtain a discriminative and compactfeature representation for clustering problems. Unlike traditional clusteringmethods, we assume that centers of clusters should be close to each other ifthey are connected in a learned graph, and other cluster centers should bedistant. This can greatly facilitate data visualization and scientificdiscovery in downstream analysis. Extensive experiments are performed thatdemonstrate that the proposed framework is able to obtain discriminativefeature representations, and correctly recover the intrinsic structures ofvarious real-world datasets.",http://arxiv.org/abs/1610.04929v1,,
1675,"Causal Network Learning from Multiple Interventions of Unknown  Manipulated Targets","In this paper, we discuss structure learning of causal networks from multipledata sets obtained by external intervention experiments where we do not knowwhat variables are manipulated. For example, the conditions in theseexperiments are changed by changing temperature or using drugs, but we do notknow what target variables are manipulated by the external interventions. Fromsuch data sets, the structure learning becomes more difficult. For this case,we first discuss the identifiability of causal structures. Next we present agraph-merging method for learning causal networks for the case that the samplesizes are large for these interventions. Then for the case that the samplesizes of these interventions are relatively small, we propose a data-poolingmethod for learning causal networks in which we pool all data sets of theseinterventions together for the learning. Further we propose a re-samplingapproach to evaluate the edges of the causal network learned by thedata-pooling method. Finally we illustrate the proposed learning methods bysimulations.",http://arxiv.org/abs/1610.08611v1,,
1676,Learning to Optimize Neural Nets,"Learning to Optimize is a recently proposed framework for learningoptimization algorithms using reinforcement learning. In this paper, we explorelearning an optimization algorithm for training shallow neural nets. Suchhigh-dimensional stochastic optimization problems present interestingchallenges for existing reinforcement learning algorithms. We develop anextension that is suited to learning optimization algorithms in this settingand demonstrate that the learned optimization algorithm consistentlyoutperforms other known optimization algorithms even on unseen tasks and isrobust to changes in stochasticity of gradients and the neural netarchitecture. More specifically, we show that an optimization algorithm trainedwith the proposed method on the problem of training a neural net on MNISTgeneralizes to the problems of training neural nets on the Toronto FacesDataset, CIFAR-10 and CIFAR-100.",http://arxiv.org/abs/1703.00441v2,,
1677,Deep Asymmetric Multi-task Feature Learning,"We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which canlearn deep representations shared across multiple tasks while effectivelypreventing negative transfer that may happen in the feature sharing process.Specifically, we introduce an asymmetric autoencoder term that allows reliablepredictors for the easy tasks to have high contribution to the feature learningwhile suppressing the influences of unreliable predictors for more difficulttasks. This allows the learning of less noisy representations, and enablesunreliable predictors to exploit knowledge from the reliable predictors via theshared latent features. Such asymmetric knowledge transfer through sharedfeatures is also more scalable and efficient than inter-task asymmetrictransfer. We validate our Deep-AMTFL model on multiple benchmark datasets formultitask learning and image classification, on which it significantlyoutperforms existing symmetric and asymmetric multitask learning models, byeffectively preventing negative transfer in deep feature learning.",http://arxiv.org/abs/1708.00260v3,,
1678,Stochastic Gradient Descent: Going As Fast As Possible But Not Faster,"When applied to training deep neural networks, stochastic gradient descent(SGD) often incurs steady progression phases, interrupted by catastrophicepisodes in which loss and gradient norm explode. A possible mitigation of suchevents is to slow down the learning process. This paper presents a novelapproach to control the SGD learning rate, that uses two statistical tests. Thefirst one, aimed at fast learning, compares the momentum of the normalizedgradient vectors to that of random unit vectors and accordingly gracefullyincreases or decreases the learning rate. The second one is a change pointdetection test, aimed at the detection of catastrophic learning episodes; uponits triggering the learning rate is instantly halved. Both abilities ofspeeding up and slowing down the learning rate allows the proposed approach,called SALeRA, to learn as fast as possible but not faster. Experiments onstandard benchmarks show that SALeRA performs well in practice, and comparesfavorably to the state of the art.",http://arxiv.org/abs/1709.01427v1,,
1679,"Representation and Reinforcement Learning for Personalized Glycemic  Control in Septic Patients","Glycemic control is essential for critical care. However, it is a challengingtask because there has been no study on personalized optimal strategies forglycemic control. This work aims to learn personalized optimal glycemictrajectories for severely ill septic patients by learning data-driven policiesto identify optimal targeted blood glucose levels as a reference forclinicians. We encoded patient states using a sparse autoencoder and adopted areinforcement learning paradigm using policy iteration to learn the optimalpolicy from data. We also estimated the expected return following the policylearned from the recorded glycemic trajectories, which yielded a functionindicating the relationship between real blood glucose values and 90-daymortality rates. This suggests that the learned optimal policy could reduce thepatients' estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. Theresult demonstrates that reinforcement learning with appropriate patient stateencoding can potentially provide optimal glycemic trajectories and allowclinicians to design a personalized strategy for glycemic control in septicpatients.",http://arxiv.org/abs/1712.00654v1,,
1680,"Random Feature-based Online Multi-kernel Learning in Environments with  Unknown Dynamics","Kernel-based methods exhibit well-documented performance in various nonlinearlearning tasks. Most of them rely on a preselected kernel, whose prudent choicepresumes task-specific prior information. Especially when the latter is notavailable, multi-kernel learning has gained popularity thanks to itsflexibility in choosing kernels from a prescribed kernel dictionary. Leveragingthe random feature approximation and its recent orthogonality-promotingvariant, the present contribution develops a scalable multi-kernel learningscheme (termed Raker) to obtain the sought nonlinear learning function `on thefly,' first for static environments. To further boost performance in dynamicenvironments, an adaptive multi-kernel learning scheme (termed AdaRaker) isdeveloped. AdaRaker accounts not only for data-driven learning of kernelcombination, but also for the unknown dynamics. Performance is analyzed interms of both static and dynamic regrets. AdaRaker is uniquely capable oftracking nonlinear learning functions in environments with unknown dynamics,and with with analytic performance guarantees. Tests with synthetic and realdatasets are carried out to showcase the effectiveness of the novel algorithms.",http://arxiv.org/abs/1712.09983v3,,
1681,"Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With  Expert Demonstrations","Pretraining with expert demonstrations have been found useful in speeding upthe training process of deep reinforcement learning algorithms since lessonline simulation data is required. Some people use supervised learning tospeed up the process of feature learning, others pretrain the policies byimitating expert demonstrations. However, these methods are unstable and notsuitable for actor-critic reinforcement learning algorithms. Also, someexisting methods rely on the global optimum assumption, which is not true inmost scenarios. In this paper, we employ expert demonstrations in aactor-critic reinforcement learning framework, and meanwhile ensure that theperformance is not affected by the fact that expert demonstrations are notglobal optimal. We theoretically derive a method for computing policy gradientsand value estimators with only expert demonstrations. Our method istheoretically plausible for actor-critic reinforcement learning algorithms thatpretrains both policy and value functions. We apply our method to two of thetypical actor-critic reinforcement learning algorithms, DDPG and ACER, anddemonstrate with experiments that our method not only outperforms the RLalgorithms without pretraining process, but also is more simulation efficient.",http://arxiv.org/abs/1801.10459v2,,
1682,DiGrad: Multi-Task Reinforcement Learning with Shared Actions,"Most reinforcement learning algorithms are inefficient for learning multipletasks in complex robotic systems, where different tasks share a set of actions.In such environments a compound policy may be learnt with shared neural networkparameters, which performs multiple tasks concurrently. However such compoundpolicy may get biased towards a task or the gradients from different tasksnegate each other, making the learning unstable and sometimes less dataefficient. In this paper, we propose a new approach for simultaneous trainingof multiple tasks sharing a set of common actions in continuous action spaces,which we call as DiGrad (Differential Policy Gradient). The proposed frameworkis based on differential policy gradients and can accommodate multi-tasklearning in a single actor-critic network. We also propose a simple heuristicin the differential policy gradient update to further improve the learning. Theproposed architecture was tested on 8 link planar manipulator and 27 degrees offreedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2end effectors respectively. We show that our approach supports efficientmulti-task learning in complex robotic systems, outperforming related methodsin continuous action spaces.",http://arxiv.org/abs/1802.10463v1,,
1683,"Pseudo-task Augmentation: From Deep Multitask Learning to Intratask  Sharing---and Back","Deep multitask learning boosts performance by sharing learned structureacross related tasks. This paper adapts ideas from deep multitask learning tothe setting where only a single task is available. The method is formalized aspseudo-task augmentation, in which models are trained with multiple decodersfor each task. Pseudo-tasks simulate the effect of training towardsclosely-related tasks drawn from the same universe. In a suite of experiments,pseudo-task augmentation is shown to improve performance on single-tasklearning problems. When combined with multitask learning, further improvementsare achieved, including state-of-the-art performance on the CelebA dataset,showing that pseudo-task augmentation and multitask learning have complementaryvalue. All in all, pseudo-task augmentation is a broadly applicable andefficient way to boost performance in deep learning systems.",http://arxiv.org/abs/1803.04062v2,,
1684,Meta Reinforcement Learning with Latent Variable Gaussian Processes,"Learning from small data sets is critical in many practical applicationswhere data collection is time consuming or expensive, e.g., robotics, animalexperiments or drug design. Meta learning is one way to increase the dataefficiency of learning algorithms by generalizing learned concepts from a setof training tasks to unseen, but related, tasks. Often, this relationshipbetween tasks is hard coded or relies in some other way on human expertise. Inthis paper, we frame meta learning as a hierarchical latent variable model andinfer the relationship between tasks automatically from data. We apply ourframework in a model-based reinforcement learning setting and show that ourmeta-learning model effectively generalizes to novel tasks by identifying hownew tasks relate to prior ones from minimal data. This results in up to a 60%reduction in the average interaction time needed to solve tasks compared tostrong baselines.",http://arxiv.org/abs/1803.07551v2,,
1685,Learning Recommendations While Influencing Interests,"Personalized recommendation systems (RS) are extensively used in manyservices. Many of these are based on learning algorithms where the RS uses therecommendation history and the user response to learn an optimal strategy.Further, these algorithms are based on the assumption that the user interestsare rigid. Specifically, they do not account for the effect of learningstrategy on the evolution of the user interests. In this paper we developinfluence models for a learning algorithm that is used to optimally recommendwebsites to web users. We adapt the model of \cite{Ioannidis10} to include anitem-dependent reward to the RS from the suggestions that are accepted by theuser. For this we first develop a static optimisation scheme when all theparameters are known. Next we develop a stochastic approximation based learningscheme for the RS to learn the optimal strategy when the user profiles are notknown. Finally, we describe several user-influence models for the learningalgorithm and analyze their effect on the steady user interests and on thesteady state optimal strategy as compared to that when the users are notinfluenced.",http://arxiv.org/abs/1803.08651v1,,
1686,"Learning Pretopological Spaces to Model Complex Propagation Phenomena: A  Multiple Instance Learning Approach Based on a Logical Modeling","This paper addresses the problem of learning the concept of ""propagation"" inthe pretopology theoretical formalism. Our proposal is first to define thepseudo-closure operator (modeling the propagation concept) as a logicalcombination of neighborhoods. We show that learning such an operator lapsesinto the Multiple Instance (MI) framework, where the learning process isperformed on bags of instances instead of individual instances. Though thisframework is well suited for this task, its use for learning a pretopologicalspace leads to a set of bags exponential in size. To overcome this issue wethus propose a learning method based on a low estimation of the bags covered bya concept under construction. As an experiment, percolation processes (forestfires typically) are simulated and the corresponding propagation models arelearned based on a subset of observations. It reveals that the proposed MIapproach is significantly more efficient on the task of propagation modelrecognition than existing methods.",http://arxiv.org/abs/1805.01278v1,,
1687,Progress & Compress: A scalable framework for continual learning,"We introduce a conceptually simple and scalable framework for continuallearning domains where tasks are learned sequentially. Our method is constantin the number of parameters and is designed to preserve performance onpreviously encountered tasks while accelerating learning progress on subsequentproblems. This is achieved by training a network with two components: Aknowledge base, capable of solving previously encountered problems, which isconnected to an active column that is employed to efficiently learn the currenttask. After learning a new task, the active column is distilled into theknowledge base, taking care to protect any previously acquired skills. Thiscycle of active learning (progression) followed by consolidation (compression)requires no architecture growth, no access to or storing of previous data ortasks, and no task-specific parameters. We demonstrate the progress & compressapproach on sequential classification of handwritten alphabets as well as tworeinforcement learning domains: Atari games and 3D maze navigation.",http://arxiv.org/abs/1805.06370v2,,
1688,"Deep Loopy Neural Network Model for Graph Structured Data Representation  Learning","Existing deep learning models may encounter great challenges in handlinggraph structured data. In this paper, we introduce a new deep learning modelfor graph data specifically, namely the deep loopy neural network.Significantly different from the previous deep models, inside the deep loopyneural network, there exist a large number of loops created by the extensiveconnections among nodes in the input graph data, which makes model learning aninfeasible task. To resolve such a problem, in this paper, we will introduce anew learning algorithm for the deep loopy neural network specifically. Insteadof learning the model variables based on the original model, in the proposedlearning algorithm, errors will be back-propagated through the edges in a groupof extracted spanning trees. Extensive numerical experiments have been done onseveral real-world graph datasets, and the experimental results demonstrate theeffectiveness of both the proposed model and the learning algorithm in handlinggraph data.",http://arxiv.org/abs/1805.07504v1,,
1689,Task-Agnostic Meta-Learning for Few-shot Learning,"Meta-learning approaches have been proposed to tackle the few-shot learningproblem.Typically, a meta-learner is trained on a variety of tasks in the hopesof being generalizable to new tasks. However, the generalizability on new tasksof a meta-learner could be fragile when it is over-trained on existing tasksduring meta-training phase. In other words, the initial model of a meta-learnercould be too biased towards existing tasks to adapt to new tasks, especiallywhen only very few examples are available to update the model. To avoid abiased meta-learner and improve its generalizability, we propose a novelparadigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, wepresent an entropy-based approach that meta-learns an unbiased initial modelwith the largest uncertainty over the output labels by preventing it fromover-performing in classification tasks. Alternatively, a more generalinequality-minimization TAML is presented for more ubiquitous scenarios bydirectly minimizing the inequality of initial losses beyond the classificationtasks wherever a suitable loss can be defined.Experiments on benchmarkeddatasets demonstrate that the proposed approaches outperform comparedmeta-learning algorithms in both few-shot classification and reinforcementlearning tasks.",http://arxiv.org/abs/1805.07722v1,,
1690,Network Learning with Local Propagation,"This paper presents a locally decoupled network parameter learning with localpropagation. Three elements are taken into account: (i) sets of nonlineartransforms that describe the representations at all nodes, (ii) a localobjective at each node related to the corresponding local representation goal,and (iii) a local propagation model that relates the nonlinear error vectors ateach node with the goal error vectors from the directly connected nodes. Themodeling concepts (i), (ii) and (iii) offer several advantages, including (a) aunified learning principle for any network that is represented as a graph, (b)understanding and interpretation of the local and the global learning dynamics,(c) decoupled and parallel parameter learning, (d) a possibility for learningin infinitely long, multi-path and multi-goal networks. Numerical experimentsvalidate the potential of the learning principle. The preliminary results showadvantages in comparison to the state-of-the-art methods, w.r.t. the learningtime and the network size while having comparable recognition accuracy.",http://arxiv.org/abs/1805.07802v1,,
1691,Adversarial Meta-Learning,"Meta-learning enables a model to learn from very limited data to undertake anew task. In this paper, we study the general meta-learning with adversarialsamples. We present a meta-learning algorithm, ADML (ADversarial Meta-Learner),which leverages clean and adversarial samples to optimize the initialization ofa learning model in an adversarial manner. ADML leads to the followingdesirable properties: 1) it turns out to be very effective even in the caseswith only clean samples; 2) it is model-agnostic, i.e., it is compatible withany learning model that can be trained with gradient descent; and mostimportantly, 3) it is robust to adversarial samples, i.e., unlike othermeta-learning methods, it only leads to a minor performance degradation whenthere are adversarial samples. We show via extensive experiments that ADMLdelivers the state-of-the-art performance on two widely-used image datasets,MiniImageNet and CIFAR100, in terms of both accuracy and robustness.",http://arxiv.org/abs/1806.03316v1,,
1692,Bayesian Model-Agnostic Meta-Learning,"Learning to infer Bayesian posterior from a few-shot dataset is an importantstep towards robust meta-learning due to the model uncertainty inherent in theproblem. In this paper, we propose a novel Bayesian model-agnosticmeta-learning method. The proposed method combines scalable gradient-basedmeta-learning with nonparametric variational inference in a principledprobabilistic framework. During fast adaptation, the method is capable oflearning complex uncertainty structure beyond a point estimate or a simpleGaussian approximation. In addition, a robust Bayesian meta-update mechanismwith a new meta-loss prevents overfitting during meta-update. Remaining anefficient gradient-based meta-learner, the method is also model-agnostic andsimple to implement. Experiment results show the accuracy and robustness of theproposed method in various tasks: sinusoidal regression, image classification,active learning, and reinforcement learning.",http://arxiv.org/abs/1806.03836v4,,
1693,Bilevel Programming for Hyperparameter Optimization and Meta-Learning,"We introduce a framework based on bilevel programming that unifiesgradient-based hyperparameter optimization and meta-learning. We show that anapproximate version of the bilevel problem can be solved by taking intoexplicit account the optimization dynamics for the inner objective. Dependingon the specific setting, the outer variables take either the meaning ofhyperparameters in a supervised learning problem or parameters of ameta-learner. We provide sufficient conditions under which solutions of theapproximate problem converge to those of the exact problem. We instantiate ourapproach for meta-learning in the case of deep learning where representationlayers are treated as hyperparameters shared across a set of training episodes.In experiments, we confirm our theoretical findings, present encouragingresults for few-shot learning and contrast the bilevel approach againstclassical approaches for learning-to-learn.",http://arxiv.org/abs/1806.04910v2,,
1694,Constructing Deep Neural Networks by Bayesian Network Structure Learning,"We introduce a principled approach for unsupervised structure learning ofdeep neural networks. We propose a new interpretation for depth and inter-layerconnectivity where conditional independencies in the input distribution areencoded hierarchically in the network structure. Thus, the depth of the networkis determined inherently. The proposed method casts the problem of neuralnetwork structure learning as a problem of Bayesian network structure learning.Then, instead of directly learning the discriminative structure, it learns agenerative graph, constructs its stochastic inverse, and then constructs adiscriminative graph. We prove that conditional-dependency relations among thelatent variables in the generative graph are preserved in the class-conditionaldiscriminative graph. We demonstrate on image classification benchmarks thatthe deepest layers (convolutional and dense) of common networks can be replacedby significantly smaller learned structures, while maintaining classificationaccuracy---state-of-the-art on tested benchmarks. Our structure learningalgorithm requires a small computational cost and runs efficiently on astandard desktop CPU.",http://arxiv.org/abs/1806.09141v3,,
1695,"Towards Mixed Optimization for Reinforcement Learning with Program  Synthesis","Deep reinforcement learning has led to several recent breakthroughs, thoughthe learned policies are often based on black-box neural networks. This makesthem difficult to interpret and to impose desired specification constraintsduring learning. We present an iterative framework, MORL, for improving thelearned policies using program synthesis. Concretely, we propose to usesynthesis techniques to obtain a symbolic representation of the learned policy,which can then be debugged manually or automatically using program repair.After the repair step, we use behavior cloning to obtain the policycorresponding to the repaired program, which is then further improved usinggradient descent. This process continues until the learned policy satisfiesdesired constraints. We instantiate MORL for the simple CartPole problem andshow that the programmatic representation allows for high-level modificationsthat in turn lead to improved learning of the policies.",http://arxiv.org/abs/1807.00403v2,,
1696,Scalable Structure Learning for Probabilistic Soft Logic,"Statistical relational frameworks such as Markov logic networks andprobabilistic soft logic (PSL) encode model structure with weighted first-orderlogical clauses. Learning these clauses from data is referred to as structurelearning. Structure learning alleviates the manual cost of specifying models.However, this benefit comes with high computational costs; structure learningtypically requires an expensive search over the space of clauses which involvesrepeated optimization of clause weights. In this paper, we propose the firsttwo approaches to structure learning for PSL. We introduce a greedysearch-based algorithm and a novel optimization method that trade-offscalability and approximations to the structure learning problem in varyingways. The highly scalable optimization method combines data-driven generationof clauses with a piecewise pseudolikelihood (PPLL) objective that learns modelstructure by optimizing clause weights only once. We compare both methodsacross five real-world tasks, showing that PPLL achieves an order of magnituderuntime speedup and AUC gains up to 15% over greedy search.",http://arxiv.org/abs/1807.00973v1,,
1697,Online Robust Policy Learning in the Presence of Unknown Adversaries,"The growing prospect of deep reinforcement learning (DRL) being used incyber-physical systems has raised concerns around safety and robustness ofautonomous agents. Recent work on generating adversarial attacks have shownthat it is computationally feasible for a bad actor to fool a DRL policy intobehaving sub optimally. Although certain adversarial attacks with specificattack models have been addressed, most studies are only interested in off-lineoptimization in the data space (e.g., example fitting, distillation). Thispaper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that isattack model-agnostic and more suited to reinforcement learning, via handlingthe attacks in the decision space (as opposed to data space) and directlymitigating learned bias introduced by the adversary. In MLAH, we learn separatesub-policies (nominal and adversarial) in an online manner, as guided by asupervisory master agent that detects the presence of the adversary byleveraging the advantage function for the sub-policies. We demonstrate that theproposed algorithm enables policy learning with significantly lower bias ascompared to the state-of-the-art policy learning approaches even in thepresence of heavy state information attacks. We present algorithm analysis andsimulation results using popular OpenAI Gym environments.",http://arxiv.org/abs/1807.06064v1,,
1698,Are Efficient Deep Representations Learnable?,"Many theories of deep learning have shown that a deep network can requiredramatically fewer resources to represent a given function compared to ashallow network. But a question remains: can these efficient representations belearned using current deep learning techniques? In this work, we test whetherstandard deep learning methods can in fact find the efficient representationsposited by several theories of deep representation. Specifically, we train deepneural networks to learn two simple functions with known efficient solutions:the parity function and the fast Fourier transform. We find that usinggradient-based optimization, a deep network does not learn the parity function,unless initialized very close to a hand-coded exact solution. We also find thata deep linear neural network does not learn the fast Fourier transform, even inthe best-case scenario of infinite training data, unless the weights areinitialized very close to the exact hand-coded solution. Our results suggestthat not every element of the class of compositional functions can be learnedefficiently by a deep network, and further restrictions are necessary tounderstand what functions are both efficiently representable and learnable.",http://arxiv.org/abs/1807.06399v1,,
1699,"LIFT: Reinforcement Learning in Computer Systems by Learning From  Demonstrations","Reinforcement learning approaches have long appealed to the data managementcommunity due to their ability to learn to control dynamic behavior from rawsystem performance. Recent successes in combining deep neural networks withreinforcement learning have sparked significant new interest in this domain.However, practical solutions remain elusive due to large training datarequirements, algorithmic instability, and lack of standard tools. In thiswork, we introduce LIFT, an end-to-end software stack for applying deepreinforcement learning to data management tasks. While prior work hasfrequently explored applications in simulations, LIFT centers on utilizinghuman expertise to learn from demonstrations, thus lowering online trainingtimes. We further introduce TensorForce, a TensorFlow library for applied deepreinforcement learning exposing a unified declarative interface to common RLalgorithms, thus providing a backend to LIFT. We demonstrate the utility ofLIFT in two case studies in database compound indexing and resource managementin stream processing. Results show LIFT controllers initialized fromdemonstrations can outperform human baselines and heuristics across latencymetrics and space usage by up to 70%.",http://arxiv.org/abs/1808.07903v1,,
1700,"Generalization Properties of hyper-RKHS and its Application to  Out-of-Sample Extensions","Hyper-kernels endowed by hyper-Reproducing Kernel Hilbert Space (hyper-RKHS)formulate the kernel learning task as learning on the space of kernels itself,which provides significant model flexibility for kernel learning withoutstanding performance in real-world applications. However, the convergencebehavior of these learning algorithms in hyper-RKHS has not been investigatedin learning theory. In this paper, we conduct approximation analysis of kernelridge regression (KRR) and support vector regression (SVR) in this space. Tothe best of our knowledge, this is the first work to study the approximationperformance of regression in hyper-RKHS. For applications, we propose a generalkernel learning framework conducted by the introduced two regression models todeal with the out-of-sample extensions problem, i.e., to learn a underlyinggeneral kernel from the pre-given kernel/similarity matrix in hyper-RKHS.Experimental results on several benchmark datasets suggest that our methods areable to learn a general kernel function from an arbitrary given kernel matrix.",http://arxiv.org/abs/1809.09910v1,,
1701,"Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented  Demonstrations using Directed Information","The use of imitation learning to learn a single policy for a complex taskthat has multiple modes or hierarchical structure can be challenging. In fact,previous work has shown that when the modes are known, learning separatepolicies for each mode or sub-task can greatly improve the performance ofimitation learning. In this work, we discover the interaction between sub-tasksfrom their resulting state-action trajectory sequences using a directedgraphical model. We propose a new algorithm based on the generative adversarialimitation learning framework which automatically learns sub-task policies fromunsegmented demonstrations. Our approach maximizes the directed informationflow in the graphical model between sub-task latent variables and theirgenerated trajectories. We also show how our approach connects with theexisting Options framework, which is commonly used to learn hierarchicalpolicies.",http://arxiv.org/abs/1810.01266v2,,
1702,Learning with Random Learning Rates,"Hyperparameter tuning is a bothersome step in the training of deep learningmodels. One of the most sensitive hyperparameters is the learning rate of thegradient descent. We present the 'All Learning Rates At Once' (Alrao)optimization method for neural networks: each unit or feature in the networkgets its own learning rate sampled from a random distribution spanning severalorders of magnitude. This comes at practically no computational cost. Perhapssurprisingly, stochastic gradient descent (SGD) with Alrao performs close toSGD with an optimally tuned learning rate, for various architectures andproblems. Alrao could save time when testing deep learning models: a range ofmodels could be quickly assessed with Alrao, and the most promising modelscould then be trained more extensively. This text comes with a PyTorchimplementation of the method, which can be plugged on an existing PyTorchmodel: https://github.com/leonardblier/alrao .",http://arxiv.org/abs/1810.01322v3,,
1703,"Using Deep Reinforcement Learning for the Continuous Control of Robotic  Arms","Deep reinforcement learning enables algorithms to learn complex behavior,deal with continuous action spaces and find good strategies in environmentswith high dimensional state spaces. With deep reinforcement learning being anactive area of research and many concurrent inventions, we decided to focus ona relatively simple robotic task to evaluate a set of ideas that might help tosolve recent reinforcement learning problems. We test a newly createdcombination of two commonly used reinforcement learning methods, whether it isable to learn more effectively than a baseline. We also compare different ideasto preprocess information before it is fed to the reinforcement learningalgorithm. The goal of this strategy is to reduce training time and eventuallyhelp the algorithm to converge. The concluding evaluation proves the generalapplicability of the described concepts by testing them using a simulatedenvironment. These concepts might be reused for future experiments.",http://arxiv.org/abs/1810.06746v1,,
1704,"Adaptivity of deep ReLU network for learning in Besov and mixed smooth  Besov spaces: optimal rate and curse of dimensionality","Deep learning has shown high performances in various types of tasks fromvisual recognition to natural language processing, which indicates superiorflexibility and adaptivity of deep learning. To understand this phenomenontheoretically, we develop a new approximation and estimation error analysis ofdeep learning with the ReLU activation for functions in a Besov space and itsvariant with mixed smoothness. The Besov space is a considerably generalfunction space including the Holder space and Sobolev space, and especially cancapture spatial inhomogeneity of smoothness. Through the analysis in the Besovspace, it is shown that deep learning can achieve the minimax optimal rate andoutperform any non-adaptive (linear) estimator such as kernel ridge regression,which shows that deep learning has higher adaptivity to the spatialinhomogeneity of the target function than other estimators such as linear ones.In addition to this, it is shown that deep learning can avoid the curse ofdimensionality if the target function is in a mixed smooth Besov space. We alsoshow that the dependency of the convergence rate on the dimensionality is tightdue to its minimax optimality. These results support high adaptivity of deeplearning and its superior ability as a feature extractor.",http://arxiv.org/abs/1810.08033v1,,
1705,"K for the Price of 1: Parameter-efficient Multi-task and Transfer  Learning","We introduce a novel method that enables parameter-efficient transfer andmulti-task learning with deep neural networks. The basic approach is to learn amodel patch - a small set of parameters - that will specialize to each task,instead of fine-tuning the last layer or the entire network. For instance, weshow that learning a set of scales and biases is sufficient to convert apretrained network to perform well on qualitatively different problems (e.g.converting a Single Shot MultiBox Detection (SSD) model into a 1000-class imageclassification model while reusing 98% of parameters of the SSD featureextractor). Similarly, we show that re-learning existing low-parameter layers(such as depth-wise convolutions) while keeping the rest of the network frozenalso improves transfer-learning accuracy significantly. Our approach allowsboth simultaneous (multi-task) as well as sequential transfer learning. Inseveral multi-task learning problems, despite using much fewer parameters thantraditional logits-only fine-tuning, we match single-task performance.",http://arxiv.org/abs/1810.10703v2,,
1706,Learning to Defense by Learning to Attack,"Adversarial training provides a principled approach for training robustneural networks. From an optimization perspective, the adversarial training isessentially solving a minmax robust optimization problem. The outerminimization is trying to learn a robust classifier, while the innermaximization is trying to generate adversarial samples. Unfortunately, such aminmax problem is very difficult to solve due to the lack of convex-concavestructure. This work proposes a new adversarial training method based on ageneral learning-to-learn framework. Specifically, instead of applying theexisting hand-design algorithms for the inner problem, we learn an optimizer,which is parametrized as a convolutional neural network. At the same time, arobust classifier is learned to defense the adversarial attack generated by thelearned optimizer. Our experiments demonstrate that our proposed methodsignificantly outperforms existing adversarial training methods on CIFAR-10 andCIFAR-100 datasets.",http://arxiv.org/abs/1811.01213v1,,
1707,"Towards Governing Agent's Efficacy: Action-Conditional $_$-VAE for  Deep Transparent Reinforcement Learning","We tackle the blackbox issue of deep neural networks in the settings ofreinforcement learning (RL) where neural agents learn towards maximizing rewardgains in an uncontrollable way. Such learning approach is risky when theinteracting environment includes an expanse of state space because it is thenalmost impossible to foresee all unwanted outcomes and penalize them withnegative rewards beforehand. Unlike reverse analysis of learned neural featuresfrom previous works, our proposed method \nj{tackles the blackbox issue byencouraging} an RL policy network to learn interpretable latent featuresthrough an implementation of a disentangled representation learning method.Toward this end, our method allows an RL agent to understand self-efficacy bydistinguishing its influences from uncontrollable environmental factors, whichclosely resembles the way humans understand their scenes. Our experimentalresults show that the learned latent factors not only are interpretable, butalso enable modeling the distribution of entire visited state space with aspecific action condition. We have experimented that this characteristic of theproposed structure can lead to ex post facto governance for desired behaviorsof RL agents.",http://arxiv.org/abs/1811.04350v1,,
1708,An Algorithmic Perspective on Imitation Learning,"As robots and other intelligent agents move from simple environments andproblems to more complex, unstructured settings, manually programming theirbehavior has become increasingly challenging and expensive. Often, it is easierfor a teacher to demonstrate a desired behavior rather than attempt to manuallyengineer it. This process of learning from demonstrations, and the study ofalgorithms to do so, is called imitation learning. This work provides anintroduction to imitation learning. It covers the underlying assumptions,approaches, and how they relate; the rich set of algorithms developed to tacklethe problem; and advice on effective tools and implementation.  We intend this paper to serve two audiences. First, we want to familiarizemachine learning experts with the challenges of imitation learning,particularly those arising in robotics, and the interesting theoretical andpractical distinctions between it and more familiar frameworks like statisticalsupervised learning theory and reinforcement learning. Second, we want to giveroboticists and experts in applied artificial intelligence a broaderappreciation for the frameworks and tools available for imitation learning.",http://arxiv.org/abs/1811.06711v1,,
1709,Learning Curriculum Policies for Reinforcement Learning,"Curriculum learning in reinforcement learning is a training methodology thatseeks to speed up learning of a difficult target task, by first training on aseries of simpler tasks and transferring the knowledge acquired to the targettask. Automatically choosing a sequence of such tasks (i.e. a curriculum) is anopen problem that has been the subject of much recent work in this area. Inthis paper, we build upon a recent method for curriculum design, whichformulates the curriculum sequencing problem as a Markov Decision Process. Weextend this model to handle multiple transfer learning algorithms, and show forthe first time that a curriculum policy over this MDP can be learned fromexperience. We explore various representations that make this possible, andevaluate our approach by learning curriculum policies for multiple agents intwo different domains. The results show that our method produces curricula thatcan train agents to perform on a target task as fast or faster than existingmethods.",http://arxiv.org/abs/1812.00285v1,,
1710,The effects of negative adaptation in Model-Agnostic Meta-Learning,"The capacity of meta-learning algorithms to quickly adapt to a variety oftasks, including ones they did not experience during meta-training, has been akey factor in the recent success of these methods on few-shot learningproblems. This particular advantage of using meta-learning over standardsupervised or reinforcement learning is only well founded under the assumptionthat the adaptation phase does improve the performance of our model on the taskof interest. However, in the classical framework of meta-learning, thisconstraint is only mildly enforced, if not at all, and we only see animprovement on average over a distribution of tasks. In this paper, we showthat the adaptation in an algorithm like MAML can significantly decrease theperformance of an agent in a meta-reinforcement learning setting, even on arange of meta-training tasks.",http://arxiv.org/abs/1812.02159v1,,
1711,Off-Policy Deep Reinforcement Learning without Exploration,"Many practical applications of reinforcement learning constrain agents tolearn from a fixed batch of data which has already been gathered, withoutoffering further possibility for data collection. In this paper, we demonstratethat due to errors introduced by extrapolation, standard off-policy deepreinforcement learning algorithms, such as DQN and DDPG, are incapable oflearning with data uncorrelated to the distribution under the current policy,making them ineffective for this fixed batch setting. We introduce a novelclass of off-policy algorithms, batch-constrained reinforcement learning, whichrestricts the action space in order to force the agent towards behaving closeto on-policy with respect to a subset of the given data. We present the firstcontinuous control deep reinforcement learning algorithm which can learneffectively from arbitrary, fixed batch data, and empirically demonstrate thequality of its behavior in several tasks.",http://arxiv.org/abs/1812.02900v2,,
1712,Neuromodulated Learning in Deep Neural Networks,"In the brain, learning signals change over time and synaptic location, andare applied based on the learning history at the synapse, in the complexprocess of neuromodulation. Learning in artificial neural networks, on theother hand, is shaped by hyper-parameters set before learning starts, whichremain static throughout learning, and which are uniform for the entirenetwork. In this work, we propose a method of deep artificial neuromodulationwhich applies the concepts of biological neuromodulation to stochastic gradientdescent. Evolved neuromodulatory dynamics modify learning parameters at eachlayer in a deep neural network over the course of the network's training. Weshow that the same neuromodulatory dynamics can be applied to different modelsand can scale to new problems not encountered during evolution. Finally, weexamine the evolved neuromodulation, showing that evolution found dynamic,location-specific learning strategies.",http://arxiv.org/abs/1812.03365v1,,
1713,Task-Free Continual Learning,"Methods proposed in the literature towards continual deep learning typicallyoperate in a task-based sequential learning setup. A sequence of tasks islearned, one at a time, with all data of current task available but not ofprevious or future tasks. Task boundaries and identities are known at alltimes. This setup, however, is rarely encountered in practical applications.Therefore we investigate how to transform continual learning to an onlinesetup. We develop a system that keeps on learning over time in a streamingfashion, with data distributions gradually changing and without the notion ofseparate tasks. To this end, we build on the work on Memory Aware Synapses, andshow how this method can be made online by providing a protocol to decide i)when to update the importance weights, ii) which data to use to update them,and iii) how to accumulate the importance weights at each update step.Experimental results show the validity of the approach in the context of twoapplications: (self-)supervised learning of a face recognition model bywatching soap series and learning a robot to avoid collisions.",http://arxiv.org/abs/1812.03596v1,,
1714,"Hierarchical Reinforcement Learning via Advantage-Weighted Information  Maximization","Real-world tasks are often highly structured. Hierarchical reinforcementlearning (HRL) has attracted research interest as an approach for leveragingthe hierarchical structure of a given task in reinforcement learning (RL).However, identifying the hierarchical policy structure that enhances theperformance of RL is not a trivial task. In this paper, we propose an HRLmethod that learns a latent variable of a hierarchical policy using mutualinformation maximization. Our approach can be interpreted as a way to learn adiscrete and latent representation of the state-action space. To learn optionpolicies that correspond to modes of the advantage function, we introduceadvantage-weighted importance sampling. In our HRL method, the gating policylearns to select option policies based on an option-value function, and theseoption policies are optimized based on the deterministic policy gradientmethod. This framework is derived by leveraging the analogy between amonolithic policy in standard RL and a hierarchical policy in HRL by using adeterministic option policy. Experimental results indicate that our HRLapproach can learn a diversity of options and that it can enhance theperformance of RL in continuous control tasks.",http://arxiv.org/abs/1901.01365v2,,
1715,Risk-Aware Active Inverse Reinforcement Learning,"Active learning from demonstration allows a robot to query a human forspecific types of input to achieve efficient learning. Existing work hasexplored a variety of active query strategies; however, to our knowledge, noneof these strategies directly minimize the performance risk of the policy therobot is learning. Utilizing recent advances in performance bounds for inversereinforcement learning, we propose a risk-aware active inverse reinforcementlearning algorithm that focuses active queries on areas of the state space withthe potential for large generalization error. We show that risk-aware activelearning outperforms standard active IRL approaches on gridworld, simulateddriving, and table setting tasks, while also providing a performance-basedstopping criterion that allows a robot to know when it has received enoughdemonstrations to safely perform a task.",http://arxiv.org/abs/1901.02161v1,,
1716,Motion Perception in Reinforcement Learning with Dynamic Objects,"In dynamic environments, learned controllers are supposed to take motion intoaccount when selecting the action to be taken. However, in existingreinforcement learning works motion is rarely treated explicitly; it is ratherassumed that the controller learns the necessary motion representation fromtemporal stacks of frames implicitly. In this paper, we show that forcontinuous control tasks learning an explicit representation of motion improvesthe quality of the learned controller in dynamic scenarios. We demonstrate thison common benchmark tasks (Walker, Swimmer, Hopper), on target reaching andball catching tasks with simulated robotic arms, and on a dynamic single balljuggling task. Moreover, we find that when equipped with an appropriate networkarchitecture, the agent can, on some tasks, learn motion features also withpure reinforcement learning, without additional supervision. Further we findthat using an image difference between the current and the previous frame as anadditional input leads to better results than a temporal stack of frames.",http://arxiv.org/abs/1901.03162v2,,
1717,Meta-Learning for Contextual Bandit Exploration,"We describe MELEE, a meta-learning algorithm for learning a good explorationpolicy in the interactive contextual bandit setting. Here, an algorithm musttake actions based on contexts, and learn based only on a reward signal fromthe action taken, thereby generating an exploration/exploitation trade-off.MELEE addresses this trade-off by learning a good exploration strategy foroffline tasks based on synthetic data, on which it can simulate the contextualbandit setting. Based on these simulations, MELEE uses an imitation learningstrategy to learn a good exploration policy that can then be applied to truecontextual bandit tasks at test time. We compare MELEE to seven strong baselinecontextual bandit algorithms on a set of three hundred real-world datasets, onwhich it outperforms alternatives in most settings, especially when differencesin rewards are large. Finally, we demonstrate the importance of having a richfeature representation for learning how to explore.",http://arxiv.org/abs/1901.08159v1,,
1718,"Human-centric Transfer Learning Explanation via Knowledge Graph  [Extended Abstract]","Transfer learning which aims at utilizing knowledge learned from one problem(source domain) to solve another different but related problem (target domain)has attracted wide research attentions. However, the current transfer learningmethods are mostly uninterpretable, especially to people without ML expertise.In this extended abstract, we brief introduce two knowledge graph (KG) basedframeworks towards human understandable transfer learning explanation. Thefirst one explains the transferability of features learned by ConvolutionalNeural Network (CNN) from one domain to another through pre-training andfine-tuning, while the second justifies the model of a target domain predictedby models from multiple source domains in zero-shot learning (ZSL). Bothmethods utilize KG and its reasoning capability to provide rich and humanunderstandable explanations to the transfer procedure.",http://arxiv.org/abs/1901.08547v1,,
1719,Agnostic Federated Learning,"A key learning scenario in large-scale applications is that of federatedlearning, where a centralized model is trained based on data originating from alarge number of clients. We argue that, with the existing training andinference, federated models can be biased towards different clients. Instead,we propose a new framework of agnostic federated learning, where thecentralized model is optimized for any target distribution formed by a mixtureof the client distributions. We further show that this framework naturallyyields a notion of fairness. We present data-dependent Rademacher complexityguarantees for learning with this objective, which guide the definition of analgorithm for agnostic federated learning. We also give a fast stochasticoptimization algorithm for solving the corresponding optimization problem, forwhich we prove convergence bounds, assuming a convex loss function andhypothesis set. We further empirically demonstrate the benefits of our approachin several datasets. Beyond federated learning, our framework and algorithm canbe of interest to other learning scenarios such as cloud computing, domainadaptation, drifting, and other contexts where the training and testdistributions do not coincide.",http://arxiv.org/abs/1902.00146v1,,
1720,Latent Space Reinforcement Learning for Steering Angle Prediction,"Model-free reinforcement learning has recently been shown to successfullylearn navigation policies from raw sensor data. In this work, we address theproblem of learning driving policies for an autonomous agent in a high-fidelitysimulator. Building upon recent research that applies deep reinforcementlearning to navigation problems, we present a modular deep reinforcementlearning approach to predict the steering angle of the car from raw images. Thefirst module extracts a low-dimensional latent semantic representation of theimage. The control module trained with reinforcement learning takes the latentvector as input to predict the correct steering angle. The experimental resultshave showed that our method is capable of learning to maneuver the car withoutany human control signals.",http://arxiv.org/abs/1902.03765v1,,
1721,Online Meta-Learning,"A central capability of intelligent systems is the ability to continuouslybuild upon previous experiences to speed up and enhance learning of new tasks.Two distinct research paradigms have studied this question. Meta-learning viewsthis problem as learning a prior over model parameters that is amenable forfast adaptation on a new task, but typically assumes the set of tasks areavailable together as a batch. In contrast, online (regret based) learningconsiders a sequential setting in which problems are revealed one after theother, but conventionally train only a single model without any task-specificadaptation. This work introduces an online meta-learning setting, which mergesideas from both the aforementioned paradigms to better capture the spirit andpractice of continual lifelong learning. We propose the follow the meta leaderalgorithm which extends the MAML algorithm to this setting. Theoretically, thiswork provides an $\mathcal{O}(\log T)$ regret guarantee with only oneadditional higher order smoothness assumption in comparison to the standardonline setting. Our experimental evaluation on three different large-scaletasks suggest that the proposed algorithm significantly outperformsalternatives based on traditional online learning approaches.",http://arxiv.org/abs/1902.08438v1,,
1722,"S-TRIGGER: Continual State Representation Learning via Self-Triggered  Generative Replay","We consider the problem of building a state representation model for control,in a continual learning setting. As the environment changes, the aim is toefficiently compress the sensory state's information without losing pastknowledge, and then use Reinforcement Learning on the resulting features forefficient policy learning. To this end, we propose S-TRIGGER, a general methodfor Continual State Representation Learning applicable to VariationalAuto-Encoders and its many variants. The method is based on Generative Replay,i.e. the use of generated samples to maintain past knowledge. It comes alongwith a statistically sound method for environment change detection, whichself-triggers the Generative Replay. Our experiments on VAEs show thatS-TRIGGER learns state representations that allows fast and high-performingReinforcement Learning, while avoiding catastrophic forgetting. The resultingsystem is capable of autonomously learning new information without using pastdata and with a bounded system size. Code for our experiments is attached inAppendix.",http://arxiv.org/abs/1902.09434v1,,
1723,"Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via  Random Labels and Data Augmentation","The field of few-shot learning has been laboriously explored in thesupervised setting, where per-class labels are available. On the other hand,the unsupervised few-shot learning setting, where no labels of any kind arerequired, has seen little investigation. We propose a method, named Assume,Augment and Learn or AAL, for generating few-shot tasks using unlabeled data.We randomly label a random subset of images from an unlabeled dataset togenerate a support set. Then by applying data augmentation on the support set'simages, and reusing the support set's labels, we obtain a target set. Theresulting few-shot tasks can be used to train any standard meta-learningframework. Once trained, such a model, can be directly applied on smallreal-labeled datasets without any changes or fine-tuning required. In ourexperiments, the learned models achieve good generalization performance in avariety of established few-shot learning tasks on Omniglot and Mini-Imagenet.",http://arxiv.org/abs/1902.09884v3,,
1724,Active Transfer Learning for Persian Offline Signature Verification,"Offline Signature Verification (OSV) remains a challenging patternrecognition task, especially in the presence of skilled forgeries that are notavailable during the training. This challenge is aggravated when there aresmall labeled training data available but with large intra-personal variations.In this study, we address this issue by employing an active learning approach,which selects the most informative instances to label and therefore reduces thehuman labeling effort significantly. Our proposed OSV includes three steps:feature learning, active learning, and final verification. We benefit fromtransfer learning using a pre-trained CNN for feature learning. We also proposeSVM-based active learning for each user to separate his genuine signatures fromthe random forgeries. We finally used the SVMs to verify the authenticity ofthe questioned signature. We examined our proposed active transfer learningmethod on UTSig: A Persian offline signature dataset. We achieved near 13%improvement compared to the random selection of instances. Our results alsoshowed 1% improvement over the state-of-the-art method in which a fullysupervised setting with five more labeled instances per user was used.",http://arxiv.org/abs/1903.06255v1,,
1725,"Communication-Efficient Federated Deep Learning with Asynchronous Model  Update and Temporally Weighted Aggregation","Federated learning obtains a central model on the server by aggregatingmodels trained locally on clients. As a result, federated learning does notrequire clients to upload their data to the server, thereby preserving the dataprivacy of the clients. One challenge in federated learning is to reduce theclient-server communication since the end devices typically have very limitedcommunication bandwidth. This paper presents an enhanced federated learningtechnique by proposing a synchronous learning strategy on the clients and atemporally weighted aggregation of the local models on the server. In theasynchronous learning strategy, different layers of the deep neural networksare categorized into shallow and deeps layers and the parameters of the deeplayers are updated less frequently than those of the shallow layers.Furthermore, a temporally weighted aggregation strategy is introduced on theserver to make use of the previously trained local models, thereby enhancingthe accuracy and convergence of the central model. The proposed algorithm isempirically on two datasets with different deep neural networks. Our resultsdemonstrate that the proposed asynchronous federated deep learning outperformsthe baseline algorithm both in terms of communication cost and model accuracy.",http://arxiv.org/abs/1903.07424v1,,
1726,Deep Reinforcement Learning with Decorrelation,"Learning an effective representation for high-dimensional data is achallenging problem in reinforcement learning (RL). Deep reinforcement learning(DRL) such as Deep Q networks (DQN) achieves remarkable success in computergames by learning deeply encoded representation from convolution networks. Inthis paper, we propose a simple yet very effective method for representationlearning with DRL algorithms. Our key insight is that features learned by DRLalgorithms are highly correlated, which interferes with learning. By adding aregularized loss that penalizes correlation in latent features (with onlyslight computation), we decorrelate features represented by deep neuralnetworks incrementally. On 49 Atari games, with the same regularization factor,our decorrelation algorithms perform $70\%$ in terms of human-normalizedscores, which is $40\%$ better than DQN. In particular, ours performs betterthan DQN on 39 games with 4 close ties and lost only slightly on $6$ games.Empirical results also show that the decorrelation method applies to QuantileRegression DQN (QR-DQN) and significantly boosts performance. Furtherexperiments on the losing games show that our decorelation algorithms can winover DQN and QR-DQN with a fined tuned regularization factor.",http://arxiv.org/abs/1903.07765v1,,
1727,Meta-Learning surrogate models for sequential decision making,"Meta-learning methods leverage past experience to learn data-driven inductivebiases from related problems, increasing learning efficiency on new tasks. Thisability renders them particularly suitable for sequential decision making withlimited experience. Within this problem family, we argue for the use of suchapproaches in the study of model-based approaches to Bayesian Optimisation,contextual bandits and Reinforcement Learning. We approach the problem bylearning distributions over functions using Neural Processes (NPs), a recentlyintroduced probabilistic meta-learning method. This allows the treatment ofmodel uncertainty to tackle the exploration/exploitation dilemma. We show thatNPs are suitable for sequential decision making on a diverse set of domains,including adversarial task search, recommender systems and model-basedreinforcement learning.",http://arxiv.org/abs/1903.11907v1,,
1728,Learning Relational Representations with Auto-encoding Logic Programs,"Deep learning methods capable of handling relational data have proliferatedover the last years. In contrast to traditional relational learning methodsthat leverage first-order logic for representing such data, these deep learningmethods aim at re-representing symbolic relational data in Euclidean spaces.They offer better scalability, but can only numerically approximate relationalstructures and are less flexible in terms of reasoning tasks supported. Thispaper introduces a novel framework for relational representation learning thatcombines the best of both worlds. This framework, inspired by the auto-encodingprinciple, uses first-order logic as a data representation language, and themapping between the original and latent representation is done by means oflogic programs instead of neural networks. We show how learning can be cast asa constraint optimisation problem for which existing solvers can be used. Theuse of logic as a representation language makes the proposed framework moreaccurate (as the representation is exact, rather than approximate), moreflexible, and more interpretable than deep learning methods. We experimentallyshow that these latent representations are indeed beneficial in relationallearning tasks.",http://arxiv.org/abs/1903.12577v1,,
1729,Reinforced Imitation in Heterogeneous Action Space,"Imitation learning is an effective alternative approach to learn a policywhen the reward function is sparse. In this paper, we consider a challengingsetting where an agent and an expert use different actions from each other. Weassume that the agent has access to a sparse reward function and state-onlyexpert observations. We propose a method which gradually balances between theimitation learning cost and the reinforcement learning objective. In addition,this method adapts the agent's policy based on either mimicking expert behavioror maximizing sparse reward. We show, through navigation scenarios, that (i) anagent is able to efficiently leverage sparse rewards to outperform standardstate-only imitation learning, (ii) it can learn a policy even when its actionsare different from the expert, and (iii) the performance of the agent is notbounded by that of the expert, due to the optimized usage of sparse rewards.",http://arxiv.org/abs/1904.03438v1,,
1730,Learning Neighborhoods for Metric Learning,"Metric learning methods have been shown to perform well on different learningtasks. Many of them rely on target neighborhood relationships that are computedin the original feature space and remain fixed throughout learning. As aresult, the learned metric reflects the original neighborhood relations. Wepropose a novel formulation of the metric learning problem in which, inaddition to the metric, the target neighborhood relations are also learned in atwo-step iterative approach. The new formulation can be seen as ageneralization of many existing metric learning methods. The formulationincludes a target neighbor assignment rule that assigns different numbers ofneighbors to instances according to their quality; `high quality' instances getmore neighbors. We experiment with two of its instantiations that correspond tothe metric learning algorithms LMNN and MCML and compare it to other metriclearning methods on a number of datasets. The experimental results showstate-of-the-art performance and provide evidence that learning theneighborhood relations does improve predictive performance.",http://arxiv.org/abs/1206.6883v1,,
1731,A Self-Paced Regularization Framework for Multi-Label Learning,"In this paper, we propose a novel multi-label learning framework, calledMulti-Label Self-Paced Learning (MLSPL), in an attempt to incorporate theself-paced learning strategy into multi-label learning regime. In light of thebenefits of adopting the easy-to-hard strategy proposed by self-paced learning,the devised MLSPL aims to learn multiple labels jointly by gradually includinglabel learning tasks and instances into model training from the easy to thehard. We first introduce a self-paced function as a regularizer in themulti-label learning formulation, so as to simultaneously rank priorities ofthe label learning tasks and the instances in each learning iteration.Considering that different multi-label learning scenarios often need differentself-paced schemes during optimization, we thus propose a general way to findthe desired self-paced functions. Experimental results on three benchmarkdatasets suggest the state-of-the-art performance of our approach.",http://arxiv.org/abs/1603.06708v2,,
1732,A First Look at Deep Learning Apps on Smartphones,"We are in the dawn of deep learning explosion for smartphones. To bridge thegap between research and practice, we present the first empirical study on16,500 the most popular Android apps, demystifying how smartphone apps exploitdeep learning in the wild. To this end, we build a new static tool thatdissects apps and analyzes their deep learning functions. Our study answersthreefold questions: what are the early adopter apps of deep learning, what dothey use deep learning for, and how do their deep learning models look like.Our study has strong implications for app developers, smartphone vendors, anddeep learning R\&D. On one hand, our findings paint a promising picture of deeplearning for smartphones, showing the prosperity of mobile deep learningframeworks as well as the prosperity of apps building their cores atop deeplearning. On the other hand, our findings urge optimizations on deep learningmodels deployed on smartphones, the protection of these models, and validationof research ideas on these models.",http://arxiv.org/abs/1812.05448v2,,
1733,"Learning Position Evaluation Functions Used in Monte Carlo Softmax  Search","This paper makes two proposals for Monte Carlo Softmax Search, which is arecently proposed method that is classified as a selective search like theMonte Carlo Tree Search. The first proposal separately defines thenode-selection and backup policies to allow researchers to freely design anode-selection policy based on their searching strategies and confirms theprincipal variation produced by the Monte Carlo Softmax Search to that producedby a minimax search. The second proposal modifies commonly used learningmethods for positional evaluation functions. In our new proposals, evaluationfunctions are learned by Monte Carlo sampling, which is performed with thebackup policy in the search tree produced by Monte Carlo Softmax Search. Thelearning methods under consideration include supervised learning, reinforcementlearning, regression learning, and search bootstrapping. Our sampling-basedlearning not only uses current positions and principal variations but also theinternal nodes and important variations of a search tree. This step reduces thenumber of games necessary for learning. New learning rules are derived forsampling-based learning based on the Monte Carlo Softmax Search andcombinations of the modified learning methods are also proposed in this paper.",http://arxiv.org/abs/1901.10706v1,,
1734,The Feature Importance Ranking Measure,"Most accurate predictions are typically obtained by learning machines withcomplex feature spaces (as e.g. induced by kernels). Unfortunately, suchdecision rules are hardly accessible to humans and cannot easily be used togain insights about the application domain. Therefore, one often resorts tolinear models in combination with variable selection, thereby sacrificing somepredictive power for presumptive interpretability. Here, we introduce theFeature Importance Ranking Measure (FIRM), which by retrospective analysis ofarbitrary learning machines allows to achieve both excellent predictiveperformance and superior interpretation. In contrast to standard raw featureweighting, FIRM takes the underlying correlation structure of the features intoaccount. Thereby, it is able to discover the most relevant features, even iftheir appearance in the training data is entirely prevented by noise. Thedesirable properties of FIRM are investigated analytically and illustrated insimulations.",http://arxiv.org/abs/0906.4258v1,,
1735,Machine Learning for Bioclimatic Modelling,"Many machine learning (ML) approaches are widely used to generate bioclimaticmodels for prediction of geographic range of organism as a function of climate.Applications such as prediction of range shift in organism, range of invasivespecies influenced by climate change are important parameters in understandingthe impact of climate change. However, success of machine learning-basedapproaches depends on a number of factors. While it can be safely said that noparticular ML technique can be effective in all applications and success of atechnique is predominantly dependent on the application or the type of theproblem, it is useful to understand their behavior to ensure informed choice oftechniques. This paper presents a comprehensive review of machinelearning-based bioclimatic model generation and analyses the factorsinfluencing success of such models. Considering the wide use of statisticaltechniques, in our discussion we also include conventional statisticaltechniques used in bioclimatic modelling.",http://arxiv.org/abs/1303.2739v1,,
1736,"A quantum speedup in machine learning: Finding a N-bit Boolean function  for a classification","We compare quantum and classical machines designed for learning an N-bitBoolean function in order to address how a quantum system improves the machinelearning behavior. The machines of the two types consist of the same number ofoperations and control parameters, but only the quantum machines utilize thequantum coherence naturally induced by unitary operators. We show that quantumsuperposition enables quantum learning that is faster than classical learningby expanding the approximate solution regions, i.e., the acceptable regions.This is also demonstrated by means of numerical simulations with a standardfeedback model, namely random search, and a practical model, namelydifferential evolution.",http://arxiv.org/abs/1303.6055v4,,
1737,Thurstonian Boltzmann Machines: Learning from Multiple Inequalities,"We introduce Thurstonian Boltzmann Machines (TBM), a unified architecturethat can naturally incorporate a wide range of data inputs at the same time.Our motivation rests in the Thurstonian view that many discrete data types canbe considered as being generated from a subset of underlying latent continuousvariables, and in the observation that each realisation of a discrete typeimposes certain inequalities on those variables. Thus learning and inference inTBM reduce to making sense of a set of inequalities. Our proposed TBM naturallysupports the following types: Gaussian, intervals, censored, binary,categorical, muticategorical, ordinal, (in)-complete rank with and withoutties. We demonstrate the versatility and capacity of the proposed model onthree applications of very different natures; namely handwritten digitrecognition, collaborative filtering and complex social survey analysis.",http://arxiv.org/abs/1408.0055v1,,
1738,"LARSEN-ELM: Selective Ensemble of Extreme Learning Machines using LARS  for Blended Data","Extreme learning machine (ELM) as a neural network algorithm has shown itsgood performance, such as fast speed, simple structure etc, but also, weakrobustness is an unavoidable defect in original ELM for blended data. Wepresent a new machine learning framework called LARSEN-ELM for overcoming thisproblem. In our paper, we would like to show two key steps in LARSEN-ELM. Inthe first step, preprocessing, we select the input variables highly related tothe output using least angle regression (LARS). In the second step, training,we employ Genetic Algorithm (GA) based selective ensemble and original ELM. Inthe experiments, we apply a sum of two sines and four datasets from UCIrepository to verify the robustness of our approach. The experimental resultsshow that compared with original ELM and other methods such as OP-ELM,GASEN-ELM and LSBoost, LARSEN-ELM significantly improve robustness performancewhile keeping a relatively high speed.",http://arxiv.org/abs/1408.2003v2,,
1739,Determinantal point processes for machine learning,"Determinantal point processes (DPPs) are elegant probabilistic models ofrepulsion that arise in quantum physics and random matrix theory. In contrastto traditional structured models like Markov random fields, which becomeintractable and hard to approximate in the presence of negative correlations,DPPs offer efficient and exact algorithms for sampling, marginalization,conditioning, and other inference tasks. We provide a gentle introduction toDPPs, focusing on the intuitions, algorithms, and extensions that are mostrelevant to the machine learning community, and show how DPPs can be applied toreal-world applications like finding diverse sets of high-quality searchresults, building informative summaries by selecting diverse sentences fromdocuments, modeling non-overlapping human poses in images or video, andautomatically building timelines of important news stories.",http://arxiv.org/abs/1207.6083v4,,
1740,"Supervised Laplacian Eigenmaps with Applications in Clinical Diagnostics  for Pediatric Cardiology","Electronic health records contain rich textual data which possess criticalpredictive information for machine-learning based diagnostic aids. However manytraditional machine learning methods fail to simultaneously integrate bothvector space data and text. We present a supervised method using Laplacianeigenmaps to augment existing machine-learning methods with low-dimensionalrepresentations of textual predictors which preserve the local similarities.The proposed implementation performs alternating optimization using gradientdescent. For the evaluation we applied our method to over 2,000 patient recordsfrom a large single-center pediatric cardiology practice to predict if patientswere diagnosed with cardiac disease. Our method was compared with latentsemantic indexing, latent Dirichlet allocation, and local Fisher discriminantanalysis. The results were assessed using AUC, MCC, specificity, andsensitivity. Results indicate supervised Laplacian eigenmaps was the highestperforming method in our study, achieving 0.782 and 0.374 for AUC and MCCrespectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over thebaseline which excluded textual data and a 2.69% and 5.35% increase in AUC andMCC respectively over unsupervised Laplacian eigenmaps. This method allows manyexisting machine learning predictors to effectively and efficiently utilize thepotential of textual predictors.",http://arxiv.org/abs/1207.7035v1,,
1741,Ensemble Committees for Stock Return Classification and Prediction,"This paper considers a portfolio trading strategy formulated by algorithms inthe field of machine learning. The profitability of the strategy is measured bythe algorithm's capability to consistently and accurately identify stockindices with positive or negative returns, and to generate a preferredportfolio allocation on the basis of a learned model. Stocks are characterizedby time series data sets consisting of technical variables that reflect marketconditions in a previous time interval, which are utilized produce binaryclassification decisions in subsequent intervals. The learned model isconstructed as a committee of random forest classifiers, a non-linear supportvector machine classifier, a relevance vector machine classifier, and aconstituent ensemble of k-nearest neighbors classifiers. The Global IndustryClassification Standard (GICS) is used to explore the ensemble model's efficacywithin the context of various fields of investment including Energy, Materials,Financials, and Information Technology. Data from 2006 to 2012, inclusive, areconsidered, which are chosen for providing a range of market circumstances forevaluating the model. The model is observed to achieve an accuracy ofapproximately 70% when predicting stock price returns three months in advance.",http://arxiv.org/abs/1404.1492v1,,
1742,"Applying machine learning to the problem of choosing a heuristic to  select the variable ordering for cylindrical algebraic decomposition","Cylindrical algebraic decomposition(CAD) is a key tool in computationalalgebraic geometry, particularly for quantifier elimination over real-closedfields. When using CAD, there is often a choice for the ordering placed on thevariables. This can be important, with some problems infeasible with onevariable ordering but easy with another. Machine learning is the process offitting a computer model to a complex function based on properties learned frommeasured data. In this paper we use machine learning (specifically a supportvector machine) to select between heuristics for choosing a variable ordering,outperforming each of the separate heuristics.",http://arxiv.org/abs/1404.6369v1,,
1743,Convex Optimization: Algorithms and Complexity,"This monograph presents the main complexity theorems in convex optimizationand their corresponding algorithms. Starting from the fundamental theory ofblack-box optimization, the material progresses towards recent advances instructural optimization and stochastic optimization. Our presentation ofblack-box optimization, strongly influenced by Nesterov's seminal book andNemirovski's lecture notes, includes the analysis of cutting plane methods, aswell as (accelerated) gradient descent schemes. We also pay special attentionto non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirrordescent, and dual averaging) and discuss their relevance in machine learning.We provide a gentle introduction to structural optimization with FISTA (tooptimize a sum of a smooth and a simple non-smooth term), saddle-point mirrorprox (Nemirovski's alternative to Nesterov's smoothing), and a concisedescription of interior point methods. In stochastic optimization we discussstochastic gradient descent, mini-batches, random coordinate descent, andsublinear algorithms. We also briefly touch upon convex relaxation ofcombinatorial problems and the use of randomness to round solutions, as well asrandom walks based methods.",http://arxiv.org/abs/1405.4980v2,,
1744,"Combining human and machine learning for morphological analysis of  galaxy images","The increasing importance of digital sky surveys collecting many millions ofgalaxy images has reinforced the need for robust methods that can performmorphological analysis of large galaxy image databases. Citizen scienceinitiatives such as Galaxy Zoo showed that large datasets of galaxy images canbe analyzed effectively by non-scientist volunteers, but since databasesgenerated by robotic telescopes grow much faster than the processing power ofany group of citizen scientists, it is clear that computer analysis isrequired. Here we propose to use citizen science data for training machinelearning systems, and show experimental results demonstrating that machinelearning systems can be trained with citizen science data. Our findings showthat the performance of machine learning depends on the quality of the data,which can be improved by using samples that have a high degree of agreementbetween the citizen scientists. The source code of the method is publiclyavailable.",http://arxiv.org/abs/1409.7935v1,,
1745,Detection of cheating by decimation algorithm,"We expand the item response theory to study the case of ""cheating students""for a set of exams, trying to detect them by applying a greedy algorithm ofinference. This extended model is closely related to the Boltzmann machinelearning. In this paper we aim to infer the correct biases and interactions ofour model by considering a relatively small number of sets of training data.Nevertheless, the greedy algorithm that we employed in the present studyexhibits good performance with a few number of training data. The key point isthe sparseness of the interactions in our problem in the context of theBoltzmann machine learning: the existence of cheating students is expected tobe very rare (possibly even in real world). We compare a standard approach toinfer the sparse interactions in the Boltzmann machine learning to our greedyalgorithm and we find the latter to be superior in several aspects.",http://arxiv.org/abs/1410.3596v2,,
1746,"Learning interpretable models of phenotypes from whole genome sequences  with the Set Covering Machine","The increased affordability of whole genome sequencing has motivated its usefor phenotypic studies. We address the problem of learning interpretable modelsfor discrete phenotypes from whole genomes. We propose a general approach thatrelies on the Set Covering Machine and a k-mer representation of the genomes.We show results for the problem of predicting the resistance of PseudomonasAeruginosa, an important human pathogen, against 4 antibiotics. Our resultsdemonstrate that extremely sparse models which are biologically relevant can belearnt using this approach.",http://arxiv.org/abs/1412.1074v1,,
1747,Becoming the Expert - Interactive Multi-Class Machine Teaching,"Compared to machines, humans are extremely good at classifying images intocategories, especially when they possess prior knowledge of the categories athand. If this prior information is not available, supervision in the form ofteaching images is required. To learn categories more quickly, people shouldsee important and representative images first, followed by less importantimages later - or not at all. However, image-importance is individual-specific,i.e. a teaching image is important to a student if it changes their overallability to discriminate between classes. Further, students keep learning, sowhile image-importance depends on their current knowledge, it also varies withtime.  In this work we propose an Interactive Machine Teaching algorithm thatenables a computer to teach challenging visual concepts to a human. Ouradaptive algorithm chooses, online, which labeled images from a teaching setshould be shown to the student as they learn. We show that a teaching strategythat probabilistically models the student's ability and progress, based ontheir correct and incorrect answers, produces better 'experts'. We presentresults using real human participants across several varied and challengingreal-world datasets.",http://arxiv.org/abs/1504.07575v1,,
1748,"Lens depth function and k-relative neighborhood graph: versatile tools  for ordinal data analysis","In recent years it has become popular to study machine learning problems in asetting of ordinal distance information rather than numerical distancemeasurements. By ordinal distance information we refer to binary answers todistance comparisons such as $d(A,B)<d(C,D)$. For many problems in machinelearning and statistics it is unclear how to solve them in such a scenario. Upto now, the main approach is to explicitly construct an ordinal embedding ofthe data points in the Euclidean space, an approach that has a number ofdrawbacks. In this paper, we propose algorithms for the problems of medoidestimation, outlier identification, classification, and clustering when givenonly ordinal data. They are based on estimating the lens depth function and the$k$-relative neighborhood graph on a data set. Our algorithms are simple, aremuch faster than an ordinal embedding approach and avoid some of its drawbacks,and can easily be parallelized.",http://arxiv.org/abs/1602.07194v2,,
1749,"Time for a change: a tutorial for comparing multiple classifiers through  Bayesian analysis","The machine learning community adopted the use of null hypothesissignificance testing (NHST) in order to ensure the statistical validity ofresults. Many scientific fields however realized the shortcomings offrequentist reasoning and in the most radical cases even banned its use inpublications. We should do the same: just as we have embraced the Bayesianparadigm in the development of new machine learning methods, so we should alsouse it in the analysis of our own results. We argue for abandonment of NHST byexposing its fallacies and, more importantly, offer better - more sound anduseful - alternatives for it.",http://arxiv.org/abs/1606.04316v3,,
1750,A Unified Robust Classification Model,"A wide variety of machine learning algorithms such as support vector machine(SVM), minimax probability machine (MPM), and Fisher discriminant analysis(FDA), exist for binary classification. The purpose of this paper is to providea unified classification model that includes the above models through a robustoptimization approach. This unified model has several benefits. One is that theextensions and improvements intended for SVM become applicable to MPM and FDA,and vice versa. Another benefit is to provide theoretical results to abovelearning methods at once by dealing with the unified model. We give astatistical interpretation of the unified classification model and propose anon-convex optimization algorithm that can be applied to non-convex variants ofexisting learning methods.",http://arxiv.org/abs/1206.4599v1,,
1751,"Neutralized Empirical Risk Minimization with Generalization Neutrality  Bound","Currently, machine learning plays an important role in the lives andindividual activities of numerous people. Accordingly, it has become necessaryto design machine learning algorithms to ensure that discrimination, biasedviews, or unfair treatment do not result from decision making or predictionsmade via machine learning. In this work, we introduce a novel empirical riskminimization (ERM) framework for supervised learning, neutralized ERM (NERM)that ensures that any classifiers obtained can be guaranteed to be neutral withrespect to a viewpoint hypothesis. More specifically, given a viewpointhypothesis, NERM works to find a target hypothesis that minimizes the empiricalrisk while simultaneously identifying a target hypothesis that is neutral tothe viewpoint hypothesis. Within the NERM framework, we derive a theoreticalbound on empirical and generalization neutrality risks. Furthermore, as arealization of NERM with linear classification, we derive a max-marginalgorithm, neutral support vector machine (SVM). Experimental results show thatour neutral SVM shows improved classification performance in real datasetswithout sacrificing the neutrality guarantee.",http://arxiv.org/abs/1511.01987v1,,
1752,"Efficient Construction of Local Parametric Reduced Order Models Using  Machine Learning Techniques","Reduced order models are computationally inexpensive approximations thatcapture the important dynamical characteristics of large, high-fidelitycomputer models of physical systems. This paper applies machine learningtechniques to improve the design of parametric reduced order models.Specifically, machine learning is used to develop feasible regions in theparameter space where the admissible target accuracy is achieved with apredefined reduced order basis, to construct parametric maps, to chose the besttwo already existing bases for a new parameter configuration from accuracypoint of view and to pre-select the optimal dimension of the reduced basis suchas to meet the desired accuracy. By combining available information using basesconcatenation and interpolation as well as high-fidelity solutionsinterpolation we are able to build accurate reduced order models associatedwith new parameter settings. Promising numerical results with a viscous Burgersmodel illustrate the potential of machine learning approaches to help designbetter reduced order models.",http://arxiv.org/abs/1511.02909v1,,
1753,"Automating biomedical data science through tree-based pipeline  optimization","Over the past decade, data science and machine learning has grown from amysterious art form to a staple tool across a variety of fields in academia,business, and government. In this paper, we introduce the concept of tree-basedpipeline optimization for automating one of the most tedious parts of machinelearning---pipeline design. We implement a Tree-based Pipeline OptimizationTool (TPOT) and demonstrate its effectiveness on a series of simulated andreal-world genetic data sets. In particular, we show that TPOT can buildmachine learning pipelines that achieve competitive classification accuracy anddiscover novel pipeline operators---such as synthetic featureconstructors---that significantly improve classification accuracy on these datasets. We also highlight the current challenges to pipeline optimization, suchas the tendency to produce pipelines that overfit the data, and suggest futureresearch paths to overcome these challenges. As such, this work represents anearly step toward fully automating machine learning pipeline design.",http://arxiv.org/abs/1601.07925v1,,
1754,Quantum machine learning over infinite dimensions,"Machine learning is a fascinating and exciting field within computer science.Recently, this excitement has been transferred to the quantum informationrealm. Currently, all proposals for the quantum version of machine learningutilize the finite-dimensional substrate of discrete variables. Here wegeneralize quantum machine learning to the more complex, but still remarkablypractical, infinite-dimensional systems. We present the critical subroutines ofquantum machine learning algorithms for an all-photonic continuous-variablequantum computer that achieve an exponential speedup compared to theirequivalent classical counterparts. Finally, we also map out an experimentalimplementation which can be used as a blueprint for future photonicdemonstrations.",http://arxiv.org/abs/1603.06222v2,,
1755,Photonic Delay Systems as Machine Learning Implementations,"Nonlinear photonic delay systems present interesting implementation platformsfor machine learning models. They can be extremely fast, offer great degrees ofparallelism and potentially consume far less power than digital processors. Sofar they have been successfully employed for signal processing using theReservoir Computing paradigm. In this paper we show that their range ofapplicability can be greatly extended if we use gradient descent withbackpropagation through time on a model of the system to optimize the inputencoding of such systems. We perform physical experiments that demonstrate thatthe obtained input encodings work well in reality, and we show that optimizedsystems perform significantly better than the common Reservoir Computingapproach. The results presented here demonstrate that common gradient descenttechniques from machine learning may well be applicable on physicalneuro-inspired analog computers.",http://arxiv.org/abs/1501.02592v1,,
1756,"Stochastic Local Interaction (SLI) Model: Interfacing Machine Learning  and Geostatistics","Machine learning and geostatistics are powerful mathematical frameworks formodeling spatial data. Both approaches, however, suffer from poor scaling ofthe required computational resources for large data applications. We presentthe Stochastic Local Interaction (SLI) model, which employs a localrepresentation to improve computational efficiency. SLI combines geostatisticsand machine learning with ideas from statistical physics and computationalgeometry. It is based on a joint probability density function defined by anenergy functional which involves local interactions implemented by means ofkernel functions with adaptive local kernel bandwidths. SLI is expressed interms of an explicit, typically sparse, precision (inverse covariance) matrix.This representation leads to a semi-analytical expression for interpolation(prediction), which is valid in any number of dimensions and avoids thecomputationally costly covariance matrix inversion.",http://arxiv.org/abs/1501.04053v2,,
1757,"Multi-Objective Optimization for Self-Adjusting Weighted Gradient in  Machine Learning Tasks","Much of the focus in machine learning research is placed in creating newarchitectures and optimization methods, but the overall loss function is seldomquestioned. This paper interprets machine learning from a multi-objectiveoptimization perspective, showing the limitations of the default linearcombination of loss functions over a data set and introducing the hypervolumeindicator as an alternative. It is shown that the gradient of the hypervolumeis defined by a self-adjusting weighted mean of the individual loss gradients,making it similar to the gradient of a weighted mean loss but without requiringthe weights to be defined a priori. This enables an inner boosting-likebehavior, where the current model is used to automatically place higher weightson samples with higher losses but without requiring the use of multiple models.Results on a denoising autoencoder show that the new formulation is able toachieve better mean loss than the direct optimization of the mean loss,providing evidence to the conjecture that self-adjusting the weights creates asmoother loss surface.",http://arxiv.org/abs/1506.01113v2,,
1758,"Machine learning for many-body physics: efficient solution of dynamical  mean-field theory","Machine learning methods for solving the equations of dynamical mean-fieldtheory are developed. The method is demonstrated on the three dimensionalHubbard model. The key technical issues are defining a mapping of an inputfunction to an output function, and distinguishing metallic from insulatingsolutions. Both metallic and Mott insulator solutions can be predicted. Thevalidity of the machine learning scheme is assessed by comparing predictions offull correlation functions, of quasi-particle weight and particle density tovalues directly computed. The results indicate that with modest furtherdevelopment, machine learning approach may be an attractive computationalefficient option for real materials predictions for strongly correlatedsystems.",http://arxiv.org/abs/1506.08858v1,,
1759,Transfer Learning for Low-Resource Neural Machine Translation,"The encoder-decoder framework for neural machine translation (NMT) has beenshown effective in large data scenarios, but is much less effective forlow-resource languages. We present a transfer learning method thatsignificantly improves Bleu scores across a range of low-resource languages.Our key idea is to first train a high-resource language pair (the parentmodel), then transfer some of the learned parameters to the low-resource pair(the child model) to initialize and constrain training. Using our transferlearning method we improve baseline NMT models by an average of 5.6 Bleu onfour low-resource language pairs. Ensembling and unknown word replacement addanother 2 Bleu which brings the NMT performance on low-resource machinetranslation close to a strong syntax based machine translation (SBMT) system,exceeding its performance on one language pair. Additionally, using thetransfer learning model for re-scoring, we can improve the SBMT system by anaverage of 1.3 Bleu, improving the state-of-the-art on low-resource machinetranslation.",http://arxiv.org/abs/1604.02201v1,,
1760,"Optimization for Large-Scale Machine Learning with Distributed Features  and Observations","As the size of modern data sets exceeds the disk and memory capacities of asingle computer, machine learning practitioners have resorted to parallel anddistributed computing. Given that optimization is one of the pillars of machinelearning and predictive modeling, distributed optimization methods haverecently garnered ample attention in the literature. Although previous researchhas mostly focused on settings where either the observations, or features ofthe problem at hand are stored in distributed fashion, the situation where bothare partitioned across the nodes of a computer cluster (doubly distributed) hasbarely been studied. In this work we propose two doubly distributedoptimization algorithms. The first one falls under the umbrella of distributeddual coordinate ascent methods, while the second one belongs to the class ofstochastic gradient/coordinate descent hybrid methods. We conduct numericalexperiments in Spark using real-world and simulated data sets and study thescaling properties of our methods. Our empirical evaluation of the proposedalgorithms demonstrates the out-performance of a block distributed ADMM method,which, to the best of our knowledge is the only other existing doublydistributed optimization algorithm.",http://arxiv.org/abs/1610.10060v2,,
1761,Quantum Machine Learning,"Fuelled by increasing computer power and algorithmic advances, machinelearning techniques have become powerful tools for finding patterns in data.Since quantum systems produce counter-intuitive patterns believed not to beefficiently produced by classical systems, it is reasonable to postulate thatquantum computers may outperform classical computers on machine learning tasks.The field of quantum machine learning explores how to devise and implementconcrete quantum software that offers such advantages. Recent work has madeclear that the hardware and software challenges are still considerable but hasalso opened paths towards solutions.",http://arxiv.org/abs/1611.09347v2,,
1762,A Noise-Filtering Approach for Cancer Drug Sensitivity Prediction,"Accurately predicting drug responses to cancer is an important problemhindering oncologists' efforts to find the most effective drugs to treatcancer, which is a core goal in precision medicine. The scientific communityhas focused on improving this prediction based on genomic, epigenomic, andproteomic datasets measured in human cancer cell lines. Real-world cancer celllines contain noise, which degrades the performance of machine learningalgorithms. This problem is rarely addressed in the existing approaches. Inthis paper, we present a noise-filtering approach that integrates techniquesfrom numerical linear algebra and information retrieval targeted at filteringout noisy cancer cell lines. By filtering out noisy cancer cell lines, we cantrain machine learning algorithms on better quality cancer cell lines. Weevaluate the performance of our approach and compare it with an existingapproach using the Area Under the ROC Curve (AUC) on clinical trial data. Theexperimental results show that our proposed approach is stable and also yieldsthe highest AUC at a statistically significant level.",http://arxiv.org/abs/1612.00525v2,,
1763,Corporate Disruption in the Science of Machine Learning,"This MSc dissertation considers the effects of the current corporate intereston researchers in the field of machine learning. Situated within the field'scyclical history of academic, public and corporate interest, this dissertationinvestigates how current researchers view recent developments and negotiatetheir own research practices within an environment of increased commercialinterest and funding. The original research consists of in-depth interviewswith 12 machine learning researchers working in both academia and industry.Building on theory from science, technology and society studies, thisdissertation problematizes the traditional narratives of the neoliberalizationof academic research by allowing the researchers themselves to discuss howtheir career choices, working environments and interactions with others in thefield have been affected by the reinvigorated corporate interest of recentyears.",http://arxiv.org/abs/1612.04108v1,,
1764,Machine learning and multivariate goodness of fit,"Multivariate goodness-of-fit and two-sample tests are important components ofmany nuclear and particle physics analyses. While a variety of powerful methodsare available if the dimensionality of the feature space is small, such testsrapidly lose power as the dimensionality increases and the data inevitablybecome sparse. Machine learning classifiers are powerful tools capable ofreducing highly multivariate problems into univariate ones, on which commonlyused tests such as $\chi^2$ or Kolmogorov-Smirnov may be applied. We exploreapplying both traditional and machine-learning-based tests to several exampleproblems, and study how the power of each approach depends on thedimensionality. A pedagogical discussion is provided on which types of problemsare best suited to using traditional versus machine-learning-based tests, andon the how to properly employ the machine-learning-based approach.",http://arxiv.org/abs/1612.07186v1,,
1765,An Empirical Analysis of Feature Engineering for Predictive Modeling,"Machine learning models, such as neural networks, decision trees, randomforests and gradient boosting machines accept a feature vector and provide aprediction. These models learn in a supervised fashion where a set of featurevectors with expected output is provided. It is very common practice toengineer new features from the provided feature set. Such engineered featureswill either augment, or replace portions of the existing feature vector. Theseengineered features are essentially calculated fields, based on the values ofthe other features.  Engineering such features is primarily a manual, time-consuming task.Additionally, each type of model will respond differently to different types ofengineered features. This paper reports on empirical research to demonstratewhat types of engineered features are best suited to which machine learningmodel type. This is accomplished by generating several datasets that aredesigned to benefit from a particular type of engineered feature. Theexperiment demonstrates to what degree the machine learning model is capable ofsynthesizing the needed feature on its own. If a model is capable ofsynthesizing an engineered feature, it is not necessary to provide thatfeature. The research demonstrated that the studied models do indeed performdifferently with various types of engineered features.",http://arxiv.org/abs/1701.07852v1,,
1766,"Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial  Domains","While modern day web applications aim to create impact at the civilizationlevel, they have become vulnerable to adversarial activity, where the nextcyber-attack can take any shape and can originate from anywhere. The increasingscale and sophistication of attacks, has prompted the need for a data drivensolution, with machine learning forming the core of many cybersecurity systems.Machine learning was not designed with security in mind, and the essentialassumption of stationarity, requiring that the training and testing data followsimilar distributions, is violated in an adversarial domain. In this paper, anadversary's view point of a classification based system, is presented. Based ona formal adversarial model, the Seed-Explore-Exploit framework is presented,for simulating the generation of data driven and reverse engineering attacks onclassifiers. Experimental evaluation, on 10 real world datasets and using theGoogle Cloud Prediction Platform, demonstrates the innate vulnerability ofclassifiers and the ease with which evasion can be carried out, without anyexplicit information about the classifier type, the training data or theapplication domain. The proposed framework, algorithms and empiricalevaluation, serve as a white hat analysis of the vulnerabilities, and aim tofoster the development of secure machine learning frameworks.",http://arxiv.org/abs/1703.07909v1,,
1767,Smartphone Based Colorimetric Detection via Machine Learning,"We report the application of machine learning to smartphone basedcolorimetric detection of pH values. The strip images were used as the trainingset for Least Squares-Support Vector Machine (LS-SVM) classifier algorithmsthat were able to successfully classify the distinct pH values. The differencein the obtained image formats was found not to significantly affect theperformance of the proposed machine learning approach. Moreover, the influenceof the illumination conditions on the perceived color of pH strips wasinvestigated and further experiments were carried out to study effect of colorchange on the learning model. Test results on JPEG, RAW and RAW-corrected imageformats captured in different lighting conditions lead to perfectclassification accuracy, sensitivity and specificity, which proves that thecolorimetric detection using machine learning based systems is able to adapt tovarious experimental conditions and is a great candidate for smartphone basedsensing in paper-based colorimetric assays.",http://arxiv.org/abs/1703.10217v1,,
1768,CDS Rate Construction Methods by Machine Learning Techniques,"Regulators require financial institutions to estimate counterparty defaultrisks from liquid CDS quotes for the valuation and risk management of OTCderivatives. However, the vast majority of counterparties do not have liquidCDS quotes and need proxy CDS rates. Existing methods cannot account forcounterparty-specific default risks; we propose to construct proxy CDS rates byassociating to illiquid counterparty liquid CDS Proxy based on Machine LearningTechniques. After testing 156 classifiers from 8 most popular classifierfamilies, we found that some classifiers achieve highly satisfactory accuracyrates. Furthermore, we have rank-ordered the performances and investigatedperformance variations amongst and within the 8 classifier families. This paperis, to the best of our knowledge, the first systematic study of CDS Proxyconstruction by Machine Learning techniques, and the first systematicclassifier comparison study based entirely on financial market data. Itsfindings both confirm and contrast existing classifier performance literature.Given the typically highly correlated nature of financial data, we investigatedthe impact of correlation on classifier performance. The techniques used inthis paper should be of interest for financial institutions seeking a CDS Proxymethod, and can serve for proxy construction for other financial variables.Some directions for future research are indicated.",http://arxiv.org/abs/1705.06899v1,,
1769,"Evolving imputation strategies for missing data in classification  problems with TPOT","Missing data has a ubiquitous presence in real-life applications of machinelearning techniques. Imputation methods are algorithms conceived for restoringmissing values in the data, based on other entries in the database. The choiceof the imputation method has an influence on the performance of the machinelearning technique, e.g., it influences the accuracy of the classificationalgorithm applied to the data. Therefore, selecting and applying the rightimputation method is important and usually requires a substantial amount ofhuman intervention. In this paper we propose the use of genetic programmingtechniques to search for the right combination of imputation and classificationalgorithms. We build our work on the recently introduced Python-based TPOTlibrary, and incorporate a heterogeneous set of imputation algorithms as partof the machine learning pipeline search. We show that genetic programming canautomatically find increasingly better pipelines that include the mosteffective combinations of imputation methods, feature pre-processing, andclassifiers for a variety of classification problems with missing data.",http://arxiv.org/abs/1706.01120v2,,
1770,Fair Pipelines,"This work facilitates ensuring fairness of machine learning in the real worldby decoupling fairness considerations in compound decisions. In particular,this work studies how fairness propagates through a compound decision-makingprocesses, which we call a pipeline. Prior work in algorithmic fairness onlyfocuses on fairness with respect to one decision. However, many decision-makingprocesses require more than one decision. For instance, hiring is at least atwo stage model: deciding who to interview from the applicant pool and thendeciding who to hire from the interview pool. Perhaps surprisingly, we showthat the composition of fair components may not guarantee a fair pipeline undera $(1+\varepsilon)$-equal opportunity definition of fair. However, we identifycircumstances that do provide that guarantee. We also propose numerousdirections for future work on more general compound machine learning decisions.",http://arxiv.org/abs/1707.00391v1,,
1771,A Nonlinear Kernel Support Matrix Machine for Matrix Learning,"In many problems of supervised tensor learning (STL), real world data such asface images or MRI scans are naturally represented as matrices, which are alsocalled as second order tensors. Most existing classifiers based on tensorrepresentation, such as support tensor machine (STM) need to solve iterativelywhich occupy much time and may suffer from local minima. In this paper, wepresent a kernel support matrix machine (KSMM) to perform supervised learningwhen data are represented as matrices. KSMM is a general framework for theconstruction of matrix-based hyperplane to exploit structural information. Weanalyze a unifying optimization problem for which we propose an asymptoticallyconvergent algorithm. Theoretical analysis for the generalization bounds isderived based on Rademacher complexity with respect to a probabilitydistribution. We demonstrate the merits of the proposed method by exhaustiveexperiments on both simulation study and a number of real-word datasets from avariety of application domains.",http://arxiv.org/abs/1707.06487v2,,
1772,Machine Learning for Structured Clinical Data,"Research is a tertiary priority in the EHR, where the priorities are patientcare and billing. Because of this, the data is not standardized or formatted ina manner easily adapted to machine learning approaches. Data may be missing fora large variety of reasons ranging from individual input styles to differencesin clinical decision making, for example, which lab tests to issue. Fewpatients are annotated at a research quality, limiting sample size andpresenting a moving gold standard. Patient progression over time is key tounderstanding many diseases but many machine learning algorithms require asnapshot, at a single time point, to create a usable vector form. Furthermore,algorithms that produce black box results do not provide the interpretabilityrequired for clinical adoption. This chapter discusses these challenges andothers in applying machine learning techniques to the structured EHR (i.e.Patient Demographics, Family History, Medication Information, Vital Signs,Laboratory Tests, Genetic Testing). It does not cover feature extraction fromadditional sources such as imaging data or free text patient notes but theapproaches discussed can include features extracted from these sources.",http://arxiv.org/abs/1707.06997v1,,
1773,Copy the dynamics using a learning machine,"Is it possible to generally construct a dynamical system to simulate a blacksystem without recovering the equations of motion of the latter? Here we showthat this goal can be approached by a learning machine. Trained by a set ofinput-output responses or a segment of time series of a black system, alearning machine can be served as a copy system to mimic the dynamics ofvarious black systems. It can not only behave as the black system at theparameter set that the training data are made, but also recur the evolutionhistory of the black system. As a result, the learning machine provides aneffective way for prediction, and enables one to probe the global dynamics of ablack system. These findings have significance for practical systems whoseequations of motion cannot be approached accurately. Examples of copying thedynamics of an artificial neural network, the Lorenz system, and a variablestar are given. Our idea paves a possible way towards copy a living brain.",http://arxiv.org/abs/1707.07637v1,,
1774,"Review of Machine Learning Algorithms in Differential Expression  Analysis","In biological research machine learning algorithms are part of nearly everyanalytical process. They are used to identify new insights into biologicalphenomena, interpret data, provide molecular diagnosis for diseases and developpersonalized medicine that will enable future treatments of diseases. In thispaper we (1) illustrate the importance of machine learning in the analysis oflarge scale sequencing data, (2) present an illustrative standardized workflowof the analysis process, (3) perform a Differential Expression (DE) analysis ofa publicly available RNA sequencing (RNASeq) data set to demonstrate thecapabilities of various algorithms at each step of the workflow, and (4) show amachine learning solution in improving the computing time, storagerequirements, and minimize utilization of computer memory in analyses ofRNA-Seq datasets. The source code of the analysis pipeline and associatedscripts are presented in the paper appendix to allow replication ofexperiments.",http://arxiv.org/abs/1707.09837v1,,
1775,"Beyond the technical challenges for deploying Machine Learning solutions  in a software company","Recently software development companies started to embrace Machine Learning(ML) techniques for introducing a series of advanced functionality in theirproducts such as personalisation of the user experience, improved search,content recommendation and automation. The technical challenges for tacklingthese problems are heavily researched in literature. A less studied area is apragmatic approach to the role of humans in a complex modern industrialenvironment where ML based systems are developed. Key stakeholders affect thesystem from inception and up to operation and maintenance. Product managerswant to embed ""smart"" experiences for their users and drive the decisions onwhat should be built next; software engineers are challenged to build orutilise ML software tools that require skills that are well outside of theircomfort zone; legal and risk departments may influence design choices and dataaccess; operations teams are requested to maintain ML systems which arenon-stationary in their nature and change behaviour over time; and finally MLpractitioners should communicate with all these stakeholders to successfullybuild a reliable system. This paper discusses some of the challenges we facedin Atlassian as we started investing more in the ML space.",http://arxiv.org/abs/1708.02363v1,,
1776,Machine Learning for Survival Analysis: A Survey,"Accurately predicting the time of occurrence of an event of interest is acritical problem in longitudinal data analysis. One of the main challenges inthis context is the presence of instances whose event outcomes becomeunobservable after a certain time point or when some instances do notexperience any event during the monitoring period. Such a phenomenon is calledcensoring which can be effectively handled using survival analysis techniques.Traditionally, statistical approaches have been widely developed in theliterature to overcome this censoring issue. In addition, many machine learningalgorithms are adapted to effectively handle survival data and tackle otherchallenging problems that arise in real-world data. In this survey, we providea comprehensive and structured review of the representative statistical methodsalong with the machine learning techniques used in survival analysis andprovide a detailed taxonomy of the existing methods. We also discuss severaltopics that are closely related to survival analysis and illustrate severalsuccessful applications in various real-world application domains. We hope thatthis paper will provide a more thorough understanding of the recent advances insurvival analysis and offer some guidelines on applying these approaches tosolve new problems that arise in applications with censored data.",http://arxiv.org/abs/1708.04649v1,,
1777,"Data-driven Advice for Applying Machine Learning to Bioinformatics  Problems","As the bioinformatics field grows, it must keep pace not only with new databut with new algorithms. Here we contribute a thorough analysis of 13state-of-the-art, commonly used machine learning algorithms on a set of 165publicly available classification problems in order to provide data-drivenalgorithm recommendations to current researchers. We present a number ofstatistical and visual comparisons of algorithm performance and quantify theeffect of model selection and algorithm tuning for each algorithm and dataset.The analysis culminates in the recommendation of five algorithms withhyperparameters that maximize classifier performance across the testedproblems, as well as general guidelines for applying machine learning tosupervised classification problems.",http://arxiv.org/abs/1708.05070v2,,
1778,The Convergence of Machine Learning and Communications,"The areas of machine learning and communication technology are converging.Today's communications systems generate a huge amount of traffic data, whichcan help to significantly enhance the design and management of networks andcommunication components when combined with advanced machine learning methods.Furthermore, recently developed end-to-end training procedures offer new waysto jointly optimize the components of a communication system. Also in manyemerging application fields of communication technology, e.g., smart cities orinternet of things, machine learning methods are of central importance. Thispaper gives an overview over the use of machine learning in different areas ofcommunications and discusses two exemplar applications in wireless networking.Furthermore, it identifies promising future research topics and discusses theirpotential impact.",http://arxiv.org/abs/1708.08299v1,,
1779,On Formalizing Fairness in Prediction with Machine Learning,"Machine learning algorithms for prediction are increasingly being used incritical decisions affecting human lives. Various fairness formalizations, withno firm consensus yet, are employed to prevent such algorithms fromsystematically discriminating against people based on certain attributesprotected by law. The aim of this article is to survey how fairness isformalized in the machine learning literature for the task of prediction andpresent these formalizations with their corresponding notions of distributivejustice from the social sciences literature. We provide theoretical as well asempirical critiques of these notions from the social sciences literature andexplain how these critiques limit the suitability of the corresponding fairnessformalizations to certain domains. We also suggest two notions of distributivejustice which address some of these critiques and discuss avenues forprospective fairness formalizations.",http://arxiv.org/abs/1710.03184v3,,
1780,Machine Learning Bell Nonlocality in Quantum Many-body Systems,"Machine learning, the core of artificial intelligence and big data science,is one of today's most rapidly growing interdisciplinary fields. Recently, itstools and techniques have been adopted to tackle intricate quantum many-bodyproblems. In this work, we introduce machine learning techniques to thedetection of quantum nonlocality in many-body systems, with a focus on therestricted-Boltzmann-machine (RBM) architecture. Using reinforcement learning,we demonstrate that RBM is capable of finding the maximum quantum violations ofmultipartite Bell inequalities with given measurement settings. Our resultsbuild a novel bridge between computer-science-based machine learning andquantum many-body nonlocality, which will benefit future studies in both areas.",http://arxiv.org/abs/1710.04226v1,,
1781,Two-stage Algorithm for Fairness-aware Machine Learning,"Algorithmic decision making process now affects many aspects of our lives.Standard tools for machine learning, such as classification and regression, aresubject to the bias in data, and thus direct application of such off-the-shelftools could lead to a specific group being unfairly discriminated. Removingsensitive attributes of data does not solve this problem because a\textit{disparate impact} can arise when non-sensitive attributes and sensitiveattributes are correlated. Here, we study a fair machine learning algorithmthat avoids such a disparate impact when making a decision. Inspired by thetwo-stage least squares method that is widely used in the field of economics,we propose a two-stage algorithm that removes bias in the training data. Theproposed algorithm is conceptually simple. Unlike most of existing fairalgorithms that are designed for classification tasks, the proposed method isable to (i) deal with regression tasks, (ii) combine explanatory attributes toremove reverse discrimination, and (iii) deal with numerical sensitiveattributes. The performance and fairness of the proposed algorithm areevaluated in simulations with synthetic and real-world datasets.",http://arxiv.org/abs/1710.04924v1,,
1782,"Themis-ml: A Fairness-aware Machine Learning Interface for End-to-end  Discrimination Discovery and Mitigation","As more industries integrate machine learning into socially sensitivedecision processes like hiring, loan-approval, and parole-granting, we are atrisk of perpetuating historical and contemporary socioeconomic disparities.This is a critical problem because on the one hand, organizations who use butdo not understand the discriminatory potential of such systems will facilitatethe widening of social disparities under the assumption that algorithms arecategorically objective. On the other hand, the responsible use of machinelearning can help us measure, understand, and mitigate the implicit historicalbiases in socially sensitive data by expressing implicit decision-making mentalmodels in terms of explicit statistical models. In this paper we specify,implement, and evaluate a ""fairness-aware"" machine learning interface calledthemis-ml, which is intended for use by individual data scientists andengineers, academic research teams, or larger product teams who use machinelearning in production systems.",http://arxiv.org/abs/1710.06921v1,,
1783,Interpretable Machine Learning for Privacy-Preserving Pervasive Systems,"The presence of pervasive systems in our everyday lives and the interactionof users with connected devices such as smartphones or home appliances generateincreasing amounts of traces that reflect users' behavior. A plethora ofmachine learning techniques enable service providers to process these traces toextract latent information about the users. While most of the existing projectshave focused on the accuracy of these techniques, little work has been done onthe interpretation of the inference and identification algorithms based onthem. In this paper, we propose a machine learning interpretability frameworkfor inference algorithms based on data collected through pervasive systems andwe outline the open challenges in this research area. Our interpretabilityframework enable users to understand how the traces they generate could exposetheir privacy, while allowing for usable and personalized services at the sametime.",http://arxiv.org/abs/1710.08464v5,,
1784,Machine learning out-of-equilibrium phases of matter,"Neural network based machine learning is emerging as a powerful tool forobtaining phase diagrams when traditional regression schemes using localequilibrium order parameters are not available, as in many-body localized ortopological phases. Nevertheless, instances of machine learning offering newinsights have been rare up to now. Here we show that a single feed-forwardneural network can decode the defining structures of two distinct MBL phasesand a thermalizing phase, using entanglement spectra obtained from individualeigenstates. For this, we introduce a simplicial geometry based method forextracting multi-partite phase boundaries. We find that this method outperformsconventional metrics (like the entanglement entropy) for identifying MBL phasetransitions, revealing a sharper phase boundary and shedding new insight intothe topology of the phase diagram. Furthermore, the phase diagram we acquirefrom a single disorder configuration confirms that the machine-learning basedapproach we establish here can enable speedy exploration of large phase spacesthat can assist with the discovery of new MBL phases. To our knowledge thiswork represents the first example of a machine learning approach revealing newinformation beyond conventional knowledge.",http://arxiv.org/abs/1711.00020v1,,
1785,"Cost-sensitive detection with variational autoencoders for environmental  acoustic sensing","Environmental acoustic sensing involves the retrieval and processing of audiosignals to better understand our surroundings. While large-scale acoustic datamake manual analysis infeasible, they provide a suitable playground for machinelearning approaches. Most existing machine learning techniques developed forenvironmental acoustic sensing do not provide flexible control of the trade-offbetween the false positive rate and the false negative rate. This paperpresents a cost-sensitive classification paradigm, in which thehyper-parameters of classifiers and the structure of variational autoencodersare selected in a principled Neyman-Pearson framework. We examine theperformance of the proposed approach using a dataset from the HumBug projectwhich aims to detect the presence of mosquitoes using sound collected by simpleembedded devices.",http://arxiv.org/abs/1712.02488v1,,
1786,Fairness in Machine Learning: Lessons from Political Philosophy,"What does it mean for a machine learning model to be `fair', in terms whichcan be operationalised? Should fairness consist of ensuring everyone has anequal probability of obtaining some benefit, or should we aim instead tominimise the harms to the least advantaged? Can the relevant ideal bedetermined by reference to some alternative state of affairs in which aparticular social pattern of discrimination does not exist? Various definitionsproposed in recent literature make different assumptions about what terms likediscrimination and fairness mean and how they can be defined in mathematicalterms. Questions of discrimination, egalitarianism and justice are ofsignificant interest to moral and political philosophers, who have expendedsignificant efforts in formalising and defending these central concepts. It istherefore unsurprising that attempts to formalise `fairness' in machinelearning contain echoes of these old philosophical debates. This paper draws onexisting work in moral and political philosophy in order to elucidate emergingdebates about fair machine learning.",http://arxiv.org/abs/1712.03586v2,,
1787,Machine Learning for Vehicular Networks,"The emerging vehicular networks are expected to make everyday vehicularoperation safer, greener, and more efficient, and pave the path to autonomousdriving in the advent of the fifth generation (5G) cellular system. Machinelearning, as a major branch of artificial intelligence, has been recentlyapplied to wireless networks to provide a data-driven approach to solvetraditionally challenging problems. In this article, we review recent advancesin applying machine learning in vehicular networks and attempt to bring moreattention to this emerging area. After a brief overview of the major concept ofmachine learning, we present some application examples of machine learning insolving problems arising in vehicular networks. We finally discuss andhighlight several open issues that warrant further research.",http://arxiv.org/abs/1712.07143v2,,
1788,Query-limited Black-box Attacks to Classifiers,"We study black-box attacks on machine learning classifiers where each queryto the model incurs some cost or risk of detection to the adversary. We focusexplicitly on minimizing the number of queries as a major objective.Specifically, we consider the problem of attacking machine learning classifierssubject to a budget of feature modification cost while minimizing the number ofqueries, where each query returns only a class and confidence score. Wedescribe an approach that uses Bayesian optimization to minimize the number ofqueries, and find that the number of queries can be reduced to approximatelyone tenth of the number needed through a random strategy for scenarios wherethe feature modification cost budget is low.",http://arxiv.org/abs/1712.08713v1,,
1789,"General framework for constructing fast and near-optimal  machine-learning-based decoder of the topological stabilizer codes","Quantum error correction is an essential technique for constructing ascalable quantum computer. In order to implement quantum error correction withnear-term quantum devices, a fast and near-optimal decoding method is demanded.A decoder based on machine learning is considered as one of the most viablesolutions for this purpose, since its prediction is fast once training has beendone, and it is applicable to any quantum error correcting code and any noisemodel. So far, various formulations of the decoding problem as the task ofmachine learning have been proposed. Here, we discuss general constructions ofmachine-learning-based decoders. We found several conditions to achievenear-optimal performance, and proposed a criterion which should be optimizedwhen a size of training data set is limited. We also discuss preferableconstructions of neural networks, and proposed a decoder using spatialstructures of topological codes using a convolutional neural network. Wenumerically show that our method can improve the performance ofmachine-learning-based decoders in various topological codes and noise models.",http://arxiv.org/abs/1801.04377v2,,
1790,"Mobile Machine Learning Hardware at ARM: A Systems-on-Chip (SoC)  Perspective","Machine learning is playing an increasingly significant role in emergingmobile application domains such as AR/VR, ADAS, etc. Accordingly, hardwarearchitects have designed customized hardware for machine learning algorithms,especially neural networks, to improve compute efficiency. However, machinelearning is typically just one processing stage in complex end-to-endapplications, involving multiple components in a mobile Systems-on-a-chip(SoC). Focusing only on ML accelerators loses bigger optimization opportunityat the system (SoC) level. This paper argues that hardware architects shouldexpand the optimization scope to the entire SoC. We demonstrate one particularcase-study in the domain of continuous computer vision where camera sensor,image signal processor (ISP), memory, and NN accelerator are synergisticallyco-designed to achieve optimal system-level efficiency.",http://arxiv.org/abs/1801.06274v2,,
1791,"A Game-Theoretic Approach to Design Secure and Resilient Distributed  Support Vector Machines","Distributed Support Vector Machines (DSVM) have been developed to solvelarge-scale classification problems in networked systems with a large number ofsensors and control units. However, the systems become more vulnerable asdetection and defense are increasingly difficult and expensive. This work aimsto develop secure and resilient DSVM algorithms under adversarial environmentsin which an attacker can manipulate the training data to achieve his objective.We establish a game-theoretic framework to capture the conflicting interestsbetween an adversary and a set of distributed data processing units. The Nashequilibrium of the game allows predicting the outcome of learning algorithms inadversarial environments, and enhancing the resilience of the machine learningthrough dynamic distributed learning algorithms. We prove that the convergenceof the distributed algorithm is guaranteed without assumptions on the trainingdata or network topologies. Numerical experiments are conducted to corroboratethe results. We show that network topology plays an important role in thesecurity of DSVM. Networks with fewer nodes and higher average degrees are moresecure. Moreover, a balanced network is found to be less vulnerable to attacks.",http://arxiv.org/abs/1802.02907v1,,
1792,"Classifying surface probe images in strongly correlated electronic  systems via machine learning","Scanning probe experiments such as scanning tunneling microscopy (STM) andatomic force microscopy (AFM) on strongly correlated electronic systems oftenreveal complex pattern formation on multiple length scales. By studying theuniversal scaling in these images, we have shown in several distinct correlatedelectronic systems that the pattern formation is driven by proximity to adisorder-driven critical point, revealing a unification of the patternformation in these materials. As an alternative approach to this imageclassification problem of novel materials, here we report the firstinvestigation of the machine learning method to determine which underlyingphysical model is driving pattern formation in a system. Using a neural networkarchitecture, we are able to achieve 97% accuracy on classifying configurationimages from three models with Ising symmetry. This investigation alsodemonstrates that machine learning can capture the implicit universal behaviorof a physical system. This broadens our understanding of what machine learningcan do, and we expect more synergy between machine learning and condensedmatter physics in the future.",http://arxiv.org/abs/1802.08862v1,,
1793,Delayed Impact of Fair Machine Learning,"Fairness in machine learning has predominantly been studied in staticclassification settings without concern for how decisions change the underlyingpopulation over time. Conventional wisdom suggests that fairness criteriapromote the long-term well-being of those groups they aim to protect.  We study how static fairness criteria interact with temporal indicators ofwell-being, such as long-term improvement, stagnation, and decline in avariable of interest. We demonstrate that even in a one-step feedback model,common fairness criteria in general do not promote improvement over time, andmay in fact cause harm in cases where an unconstrained objective would not.  We completely characterize the delayed impact of three standard criteria,contrasting the regimes in which these exhibit qualitatively differentbehavior. In addition, we find that a natural form of measurement errorbroadens the regime in which fairness criteria perform favorably.  Our results highlight the importance of measurement and temporal modeling inthe evaluation of fairness criteria, suggesting a range of new challenges andtrade-offs.",http://arxiv.org/abs/1803.04383v2,,
1794,Quantum machine learning for data scientists,"This text aims to present and explain quantum machine learning algorithms toa data scientist in an accessible and consistent way. The algorithms andequations presented are not written in rigorous mathematical fashion, instead,the pressure is put on examples and step by step explanation of difficulttopics. This contribution gives an overview of selected quantum machinelearning algorithms, however there is also a method of scores extraction forquantum PCA algorithm proposed as well as a new cost function in feed-forwardquantum neural networks is introduced. The text is divided into four parts: thefirst part explains the basic quantum theory, then quantum computation andquantum computer architecture are explained in section two. The third partpresents quantum algorithms which will be used as subroutines in quantummachine learning algorithms. Finally, the fourth section describes quantummachine learning algorithms with the use of knowledge accumulated in previousparts.",http://arxiv.org/abs/1804.10068v1,,
1795,Residual Unfairness in Fair Machine Learning from Prejudiced Data,"Recent work in fairness in machine learning has proposed adjusting forfairness by equalizing accuracy metrics across groups and has also studied howdatasets affected by historical prejudices may lead to unfair decisionpolicies. We connect these lines of work and study the residual unfairness thatarises when a fairness-adjusted predictor is not actually fair on the targetpopulation due to systematic censoring of training data by existing biasedpolicies. This scenario is particularly common in the same applications wherefairness is a concern. We characterize theoretically the impact of suchcensoring on standard fairness metrics for binary classifiers and providecriteria for when residual unfairness may or may not appear. We prove that,under certain conditions, fairness-adjusted classifiers will in fact induceresidual unfairness that perpetuates the same injustices, against the samegroups, that biased the data to begin with, thus showing that evenstate-of-the-art fair machine learning can have a ""bias in, bias out"" property.When certain benchmark data is available, we show how sample reweighting canestimate and adjust fairness metrics while accounting for censoring. We usethis to study the case of Stop, Question, and Frisk (SQF) and demonstrate thatattempting to adjust for fairness perpetuates the same injustices that thepolicy is infamous for.",http://arxiv.org/abs/1806.02887v1,,
1796,What About Applied Fairness?,"Machine learning practitioners are often ambivalent about the ethical aspectsof their products. We believe anything that gets us from that current state toone in which our systems are achieving some degree of fairness is animprovement that should be welcomed. This is true even when that progress doesnot get us 100% of the way to the goal of ""complete"" fairness or perfectlyalign with our personal belief on which measure of fairness is used. Somemeasure of fairness being built would still put us in a better position thanthe status quo. Impediments to getting fairness and ethical concerns applied inreal applications, whether they are abstruse philosophical debates or technicaloverhead such as the introduction of ever more hyper-parameters, should beavoided. In this paper we further elaborate on our argument for this viewpointand its importance.",http://arxiv.org/abs/1806.05250v1,,
1797,"The committee machine: Computational to statistical gaps in learning a  two-layers neural network","Heuristic tools from statistical physics have been used in the past to locatethe phase transitions and compute the optimal learning and generalizationerrors in the teacher-student scenario in multi-layer neural networks. In thiscontribution, we provide a rigorous justification of these approaches for atwo-layers neural network model called the committee machine. We also introducea version of the approximate message passing (AMP) algorithm for the committeemachine that allows to perform optimal learning in polynomial time for a largeset of parameters. We find that there are regimes in which a low generalizationerror is information-theoretically achievable while the AMP algorithm fails todeliver it, strongly suggesting that no efficient algorithm exists for thosecases, and unveiling a large computational gap.",http://arxiv.org/abs/1806.05451v1,,
1798,"Machine learning for prediction of extreme statistics in modulation  instability","A central area of research in nonlinear science is the study of instabilitiesthat drive the emergence of extreme events. Unfortunately, experimentaltechniques for measuring such phenomena often provide only partialcharacterization. For example, real-time studies of instabilities in nonlinearfibre optics frequently use only spectral data, precluding detailed predictionsabout the associated temporal properties. Here, we show how Machine Learningcan overcome this limitation by predicting statistics for the maximum intensityof temporal peaks in modulation instability based only on spectralmeasurements. Specifically, we train a neural network based Machine Learningmodel to correlate spectral and temporal properties of optical fibre modulationinstability using data from numerical simulations, and we then use this modelto predict the temporal probability distribution based on high-dynamic rangespectral data from experiments. These results open novel perspectives in allsystems exhibiting chaos and instability where direct time-domain observationsare difficult.",http://arxiv.org/abs/1806.06121v1,,
1799,"Interpretable to Whom? A Role-based Model for Analyzing Interpretable  Machine Learning Systems","Several researchers have argued that a machine learning system'sinterpretability should be defined in relation to a specific agent or task: weshould not ask if the system is interpretable, but to whom is it interpretable.We describe a model intended to help answer this question, by identifyingdifferent roles that agents can fulfill in relation to the machine learningsystem. We illustrate the use of our model in a variety of scenarios, exploringhow an agent's role influences its goals, and the implications for defininginterpretability. Finally, we make suggestions for how our model could beuseful to interpretability researchers, system developers, and regulatorybodies auditing machine learning systems.",http://arxiv.org/abs/1806.07552v1,,
1800,Bayesian Optimization of Combinatorial Structures,"The optimization of expensive-to-evaluate black-box functions overcombinatorial structures is an ubiquitous task in machine learning, engineeringand the natural sciences. The combinatorial explosion of the search space andcostly evaluations pose challenges for current techniques in discreteoptimization and machine learning, and critically require new algorithmicideas. This article proposes, to the best of our knowledge, the first algorithmto overcome these challenges, based on an adaptive, scalable model thatidentifies useful combinatorial structure even when data is scarce. Ouracquisition function pioneers the use of semidefinite programming to achieveefficiency and scalability. Experimental evaluations demonstrate that thisalgorithm consistently outperforms other methods from combinatorial andBayesian optimization.",http://arxiv.org/abs/1806.08838v2,,
1801,Automatic Exploration of Machine Learning Experiments on OpenML,"Understanding the influence of hyperparameters on the performance of amachine learning algorithm is an important scientific topic in itself and canhelp to improve automatic hyperparameter tuning procedures. Unfortunately,experimental meta data for this purpose is still rare. This paper presents alarge, free and open dataset addressing this problem, containing results on 38OpenML data sets, six different machine learning algorithms and many differenthyperparameter configurations. Results where generated by an automated randomsampling strategy, termed the OpenML Random Bot. Each algorithm wascross-validated up to 20.000 times per dataset with different hyperparameterssettings, resulting in a meta dataset of around 2.5 million experimentsoverall.",http://arxiv.org/abs/1806.10961v3,,
1802,Measuring the quality of Synthetic data for use in competitions,"Machine learning has the potential to assist many communities in using thelarge datasets that are becoming more and more available. Unfortunately, muchof that potential is not being realized because it would require sharing datain a way that compromises privacy. In order to overcome this hurdle, severalmethods have been proposed that generate synthetic data while preserving theprivacy of the real data. In this paper we consider a key characteristic thatsynthetic data should have in order to be useful for machine learningresearchers - the relative performance of two algorithms (trained and tested)on the synthetic dataset should be the same as their relative performance (whentrained and tested) on the original dataset.",http://arxiv.org/abs/1806.11345v1,,
1803,Machine learning 2.0 : Engineering Data Driven AI Products,"ML 2.0: In this paper, we propose a paradigm shift from the current practiceof creating machine learning models - which requires months-long discovery,exploration and ""feasibility report"" generation, followed by re-engineering fordeployment - in favor of a rapid, 8-week process of development, understanding,validation and deployment that can executed by developers or subject matterexperts (non-ML experts) using reusable APIs. This accomplishes what we call a""minimum viable data-driven model,"" delivering a ready-to-use machine learningmodel for problems that haven't been solved before using machine learning. Weprovide provisions for the refinement and adaptation of the ""model,"" withstrict enforcement and adherence to both the scaffolding/abstractions and theprocess. We imagine that this will bring forth the second phase in machinelearning, in which discovery is subsumed by more targeted goals of delivery andimpact.",http://arxiv.org/abs/1807.00401v1,,
1804,"A Broader View on Bias in Automated Decision-Making: Reflecting on  Epistemology and Dynamics","Machine learning (ML) is increasingly deployed in real world contexts,supplying actionable insights and forming the basis of automateddecision-making systems. While issues resulting from biases pre-existing intraining data have been at the center of the fairness debate, these systems arealso affected by technical and emergent biases, which often arise ascontext-specific artifacts of implementation. This position paper interpretstechnical bias as an epistemological problem and emergent bias as a dynamicalfeedback phenomenon. In order to stimulate debate on how to change machinelearning practice to effectively address these issues, we explore this broaderview on bias, stress the need to reflect on epistemology, and point tovalue-sensitive design methodologies to revisit the design and implementationprocess of automated decision-making systems.",http://arxiv.org/abs/1807.00553v2,,
1805,"Greedy Algorithms for Approximating the Diameter of Machine Learning  Datasets in Multidimensional Euclidean Space","Finding the diameter of a dataset in multidimensional Euclidean space is awell-established problem, with well-known algorithms. However, most of thealgorithms found in the literature do not scale well with large values of datadimension, so the time complexity grows exponentially in most cases, whichmakes these algorithms impractical. Therefore, we implemented 4 simple greedyalgorithms to be used for approximating the diameter of a multidimensionaldataset; these are based on minimum/maximum l2 norms, hill climbing search,Tabu search and Beam search approaches, respectively. The time complexity ofthe implemented algorithms is near-linear, as they scale near-linearly withdata size and its dimensions. The results of the experiments (conducted ondifferent machine learning data sets) prove the efficiency of the implementedalgorithms and can therefore be recommended for finding the diameter to be usedby different machine learning applications when needed.",http://arxiv.org/abs/1808.03566v1,,
1806,"Multi-task multiple kernel machines for personalized pain recognition  from functional near-infrared spectroscopy brain signals","Currently there is no validated objective measure of pain. Recentneuroimaging studies have explored the feasibility of using functionalnear-infrared spectroscopy (fNIRS) to measure alterations in brain function inevoked and ongoing pain. In this study, we applied multi-task machine learningmethods to derive a practical algorithm for pain detection derived from fNIRSsignals in healthy volunteers exposed to a painful stimulus. Especially, weemployed multi-task multiple kernel learning to account for the inter-subjectvariability in pain response. Our results support the use of fNIRS and machinelearning techniques in developing objective pain detection, and also highlightthe importance of adopting personalized analysis in the process.",http://arxiv.org/abs/1808.06774v1,,
1807,"Machine learning for predicting thermal power consumption of the Mars  Express Spacecraft","The thermal subsystem of the Mars Express (MEX) spacecraft keeps the on-boardequipment within its pre-defined operating temperatures range. To plan andoptimize the scientific operations of MEX, its operators need to estimate inadvance, as accurately as possible, the power consumption of the thermalsubsystem. The remaining power can then be allocated for scientific purposes.We present a machine learning pipeline for efficiently constructing accuratepredictive models for predicting the power of the thermal subsystem on boardMEX. In particular, we employ state-of-the-art feature engineering approachesfor transforming raw telemetry data, in turn used for constructing accuratemodels with different state-of-the-art machine learning methods. We show thatthe proposed pipeline considerably improve our previous (competition-winning)work in terms of time efficiency and predictive performance. Moreover, whileachieving superior predictive performance, the constructed models also provideimportant insight into the spacecraft's behavior, allowing for further analysesand optimal planning of MEX's operation.",http://arxiv.org/abs/1809.00542v2,,
1808,"Decentralized Differentially Private Without-Replacement Stochastic  Gradient Descent","While machine learning has achieved remarkable results in a wide variety ofdomains, the training of models often requires large datasets that may need tobe collected from different individuals. As sensitive information may becontained in the individual's dataset, sharing training data may lead to severeprivacy concerns. Therefore, there is a compelling need to developprivacy-aware machine learning methods, for which one effective approach is toleverage the generic framework of differential privacy. Considering thatstochastic gradient descent (SGD) is one of the mostly adopted methods forlarge-scale machine learning problems, two decentralized differentially privateSGD algorithms are proposed in this work. Particularly, we focus on SGD withoutreplacement due to its favorable structure for practical implementation. Inaddition, both privacy and convergence analysis are provided for the proposedalgorithms. Finally, extensive experiments are performed to verify thetheoretical results and demonstrate the effectiveness of the proposedalgorithms.",http://arxiv.org/abs/1809.02727v3,,
1809,"Atomic positions independent descriptor for machine learning of material  properties","The high-throughput screening of periodic inorganic solids using machinelearning methods requires atomic positions to encode structural andcompositional details into appropriate material descriptors. These atomicpositions are not available {\it a priori} for new materials which severelylimits exploration of novel materials. We overcome this limitation by usingonly crystallographic symmetry information in the structural description ofmaterials. We show that for materials with identical structural symmetry,machine learning is trivial and accuracies similar to that of densityfunctional theory calculations can be achieved by using only atomic numbers inthe material description. For machine learning of formation energies of bulkcrystalline solids, this simple material descriptor is able to achieveprediction mean absolute errors of only 0.07 eV/atom on a test datasetconsisting of more than 85,000 diverse materials. This atomic-positionindependent material descriptor presents a new route of materials discoverywherein millions of materials can be screened by training a machine learningmodel over a drastically reduced subspace of materials.",http://arxiv.org/abs/1809.03960v2,,
1810,"Machine Learning for Forecasting Mid Price Movement using Limit Order  Book Data","Forecasting the movements of stock prices is one the most challengingproblems in financial markets analysis. In this paper, we use Machine Learning(ML) algorithms for the prediction of future price movements using limit orderbook data. Two different sets of features are combined and evaluated:handcrafted features based on the raw order book data and features extracted byML algorithms, resulting in feature vectors with highly variantdimensionalities. Three classifiers are evaluated using combinations of thesesets of features on two different evaluation setups and three predictionscenarios. Even though the large scale and high frequency nature of the limitorder book poses several challenges, the scope of the conducted experiments andthe significance of the experimental results indicate that Machine Learninghighly befits this task carving the path towards future research in this field.",http://arxiv.org/abs/1809.07861v2,,
1811,Unrestricted Adversarial Examples,"We introduce a two-player contest for evaluating the safety and robustness ofmachine learning systems, with a large prize pool. Unlike most prior work in MLrobustness, which studies norm-constrained adversaries, we shift our focus tounconstrained adversaries. Defenders submit machine learning models, and try toachieve high accuracy and coverage on non-adversarial data while making noconfident mistakes on adversarial inputs. Attackers try to subvert defenses byfinding arbitrary unambiguous inputs where the model assigns an incorrect labelwith high confidence. We propose a simple unambiguous dataset (""bird-or-bicycle"") to use as part of this contest. We hope this contest will help tomore comprehensively evaluate the worst-case adversarial risk of machinelearning models.",http://arxiv.org/abs/1809.08352v1,,
1812,A Machine Learning Approach to Shipping Box Design,"Having the right assortment of shipping boxes in the fulfillment warehouse topack and ship customer's online orders is an indispensable and integral part ofnowadays eCommerce business, as it will not only help maintain a profitablebusiness but also create great experiences for customers. However, it is anextremely challenging operations task to strategically select the bestcombination of tens of box sizes from thousands of feasible ones to beresponsible for hundreds of thousands of orders daily placed on millions ofinventory products. In this paper, we present a machine learning approach totackle the task by formulating the box design problem prescriptively as ageneralized version of weighted $k$-medoids clustering problem, where theparameters are estimated through a variety of descriptive analytics. We testthis machine learning approach on fulfillment data collected from Walmart U.S.eCommerce, and our approach is shown to be capable of improving the boxutilization rate by more than $10\%$.",http://arxiv.org/abs/1809.10210v3,,
1813,Building a Reproducible Machine Learning Pipeline,"Reproducibility of modeling is a problem that exists for any machine learningpractitioner, whether in industry or academia. The consequences of anirreproducible model can include significant financial costs, lost time, andeven loss of personal reputation (if results prove unable to be replicated).This paper will first discuss the problems we have encountered while building avariety of machine learning models, and subsequently describe the framework webuilt to tackle the problem of model reproducibility. The framework iscomprised of four main components (data, feature, scoring, and evaluationlayers), which are themselves comprised of well defined transformations. Thisenables us to not only exactly replicate a model, but also to reuse thetransformations across different models. As a result, the platform hasdramatically increased the speed of both offline and online experimentationwhile also ensuring model reproducibility.",http://arxiv.org/abs/1810.04570v1,,
1814,"Fast and High-Fidelity Readout of Single Trapped-Ion Qubit via Machine  Learning Methods","In this work, we introduce machine learning methods to implement readout of asingle qubit on $^{171}\mathrm{Yb^{+}}$ trapped-ion system. Different machinelearning methods including convolutional neural networks and fully-connectedneural networks are compared with traditional methods in the tests. The resultsshow that machine learning methods have higher fidelity, more robust readoutresults in relatively short time. To obtain a 99% readout fidelity, neuralnetworks only take half of the detection time needed by traditional thresholdor maximum likelihood methods. Furthermore, we implement the machine learningalgorithms on hardware-based field-programmable gate arrays and an ARMprocessor. An average readout fidelity of 99.5% (with $10^5$ magnitude trials)within 171 $\mu$s is demonstrated on the embedded hardware system for$^{171}\mathrm{Yb^{+}}$ ion trap.",http://arxiv.org/abs/1810.07997v2,,
1815,Multimodal Machine Learning for Automated ICD Coding,"This study presents a multimodal machine learning model to predict ICD-10diagnostic codes. We developed separate machine learning models that can handledata from different modalities, including unstructured text, semi-structuredtext and structured tabular data. We further employed an ensemble method tointegrate all modality-specific models to generate ICD-10 codes. Key evidencewas also extracted to make our prediction more convincing and explainable. Weused the Medical Information Mart for Intensive Care III (MIMIC -III) datasetto validate our approach. For ICD code prediction, our best-performing model(micro-F1 = 0.7633, micro-AUC = 0.9541) significantly outperforms otherbaseline models including TF-IDF (micro-F1 = 0.6721, micro-AUC = 0.7879) andText-CNN model (micro-F1 = 0.6569, micro-AUC = 0.9235). For interpretability,our approach achieves a Jaccard Similarity Coefficient (JSC) of 0.1806 on textdata and 0.3105 on tabular data, where well-trained physicians achieve 0.2780and 0.5002 respectively.",http://arxiv.org/abs/1810.13348v1,,
1816,"Persistent-Homology-based Machine Learning and its Applications -- A  Survey","A suitable feature representation that can both preserve the data intrinsicinformation and reduce data complexity and dimensionality is key to theperformance of machine learning models. Deeply rooted in algebraic topology,persistent homology (PH) provides a delicate balance between datasimplification and intrinsic structure characterization, and has been appliedto various areas successfully. However, the combination of PH and machinelearning has been hindered greatly by three challenges, namely topologicalrepresentation of data, PH-based distance measurements or metrics, and PH-basedfeature representation. With the development of topological data analysis,progresses have been made on all these three problems, but widely scattered indifferent literatures. In this paper, we provide a systematical review of PHand PH-based supervised and unsupervised models from a computationalperspective. Our emphasizes are the recent development of mathematical modelsand tools, including PH softwares and PH-based functions, featurerepresentations, kernels, and similarity models. Essentially, this paper canwork as a roadmap for the practical application of PH-based machine learningtools. Further, we consider different topological feature representations indifferent machine learning models, and investigate their impacts on the proteinsecondary structure classification.",http://arxiv.org/abs/1811.00252v1,,
1817,Automated Multi-Label Classification based on ML-Plan,"Automated machine learning (AutoML) has received increasing attention in therecent past. While the main tools for AutoML, such as Auto-WEKA, TPOT, andauto-sklearn, mainly deal with single-label classification and regression,there is very little work on other types of machine learning tasks. Inparticular, there is almost no work on automating the engineering of machinelearning applications for multi-label classification. This paper makes twocontributions. First, it discusses the usefulness and feasibility of an AutoMLapproach for multi-label classification. Second, we show how the scope ofML-Plan, an AutoML-tool for multi-class classification, can be extended towardsmulti-label classification using MEKA, which is a multi-label extension of thewell-known Java library WEKA. The resulting approach recursively refines MEKA'smulti-label classifiers, which sometimes nest another multi-label classifier,up to the selection of a single-label base learner provided by WEKA. In ourevaluation, we find that the proposed approach yields superb results andperforms significantly better than a set of baselines.",http://arxiv.org/abs/1811.04060v1,,
1818,Machine Learning for Health (ML4H) Workshop at NeurIPS 2018,"This volume represents the accepted submissions from the Machine Learning forHealth (ML4H) workshop at the conference on Neural Information ProcessingSystems (NeurIPS) 2018, held on December 8, 2018 in Montreal, Canada.",http://arxiv.org/abs/1811.07216v2,,
1819,"State of the Art in Fair ML: From Moral Philosophy and Legislation to  Fair Classifiers","Machine learning is becoming an ever present part in our lives as manydecisions, e.g. to lend a credit, are no longer made by humans but by machinelearning algorithms. However those decisions are often unfair anddiscriminating individuals belonging to protected groups based on race orgender. With the recent General Data Protection Regulation (GDPR) coming intoeffect, new awareness has been raised for such issues and with computerscientists having such a large impact on peoples lives it is necessary thatactions are taken to discover and prevent discrimination. This work aims togive an introduction into discrimination, legislative foundations to counter itand strategies to detect and prevent machine learning algorithms from showingsuch behavior.",http://arxiv.org/abs/1811.09539v1,,
1820,"Rethinking clinical prediction: Why machine learning must consider year  of care and feature aggregation","Machine learning for healthcare often trains models on de-identified datasetswith randomly-shifted calendar dates, ignoring the fact that data weregenerated under hospital operation practices that change over time. Thesechanging practices induce definitive changes in observed data which confoundevaluations which do not account for dates and limit the generalisability ofdate-agnostic models. In this work, we establish the magnitude of this problemon MIMIC, a public hospital dataset, and showcase a simple solution. We augmentMIMIC with the year in which care was provided and show that a model trainedusing standard feature representations will significantly degrade in qualityover time. We find a deterioration of 0.3 AUC when evaluating mortalityprediction on data from 10 years later. We find a similar deterioration of 0.15AUC for length-of-stay. In contrast, we demonstrate that clinically-orientedaggregates of raw features significantly mitigate future deterioration. Oursuggested aggregated representations, when retrained yearly, have predictionquality comparable to year-agnostic models.",http://arxiv.org/abs/1811.12583v1,,
1821,"Machine Learning in Cyber-Security - Problems, Challenges and Data Sets","We present cyber-security problems of high importance. We show that in orderto solve these cyber-security problems, one must cope with certain machinelearning challenges. We provide novel data sets representing the problems inorder to enable the academic community to investigate the problems and suggestmethods to cope with the challenges. We also present a method to generatelabels via pivoting, providing a solution to common problems of lack of labelsin cyber-security.",http://arxiv.org/abs/1812.07858v2,,
1822,"MIMO Channel Interpolation via Tucker Decomposed Extreme Learning  Machine","Channel interpolation is an essential technique for providing high-accuracyestimation of the channel state information (CSI) for wireless systems designwhere the frequency-space structural correlations of MIMO channel are typicallyhidden in matrix or tensor forms. In this letter, a modified extreme learningmachine (ELM) that can process tensorial data, or ELM model with tensorialinputs (TELM), is proposed to handle the channel interpolation task. The TELMinherits many good properties from ELMs. Based on the TELM, the Tuckerdecomposed extreme learning machine (TDELM) is proposed for further improvingthe performance. Furthermore, we establish a theoretical argument to measurethe interpolation capability of the proposed learning machines. Experimentalresults verify that our proposed schemes can achieve comparable mean squarederror (MSE) performance against the traditional ELMs but with 15% shorterrunning time, and outperform the other methods for a 20% margin measured in MSEfor channel interpolation.",http://arxiv.org/abs/1812.10506v1,,
1823,"Features and Machine Learning for Correlating and Classifying between  Brain Areas and Dyslexia","We develop a method that is based on processing gathered Event RelatedPotentials (ERP) signals and the use of machine learning technique formultivariate analysis (i.e. classification) that we apply in order to analyzethe differences between Dyslexic and Skilled readers.  No human intervention is needed in the analysis process. This is the state ofthe art results for automatic identification of Dyslexic readers using aLexical Decision Task. We use mathematical and machine learning basedtechniques to automatically discover novel complex features that (i) allow forreliable distinction between Dyslexic and Normal Control Skilled readers and(ii) to validate the assumption that the most of the differences betweenDyslexic and Skilled readers located in the left hemisphere.  Interestingly, these tools also pointed to the fact that High Pass signals(typically considered as ""noise"" during ERP/EEG analyses) in fact containssignificant relevant information. Finally, the proposed scheme can be used foranalysis of any ERP based studies.",http://arxiv.org/abs/1812.10622v2,,
1824,"Machine learning enables polymer cloud-point engineering via inverse  design","Inverse design is an outstanding challenge in disordered systems withmultiple length scales such as polymers, particularly when designing polymerswith desired phase behavior. We demonstrate high-accuracy tuning ofpoly(2-oxazoline) cloud point via machine learning. With a design space of fourrepeating units and a range of molecular masses, we achieve an accuracy of 4{\deg}C root mean squared error (RMSE) in a temperature range of 24-90 {\deg}C,employing gradient boosting with decision trees. The RMSE is >3x better thanlinear and polynomial regression. We perform inverse design via particle-swarmoptimization, predicting and synthesizing 17 polymers with constrained designat 4 target cloud points from 37 to 80 {\deg}C. Our approach challenges thestatus quo in polymer design with a machine learning algorithm, that is capableof fast and systematic discovery of new polymers.",http://arxiv.org/abs/1812.11212v1,,
1825,Optimal Differentially Private ADMM for Distributed Machine Learning,"Due to massive amounts of data distributed across multiple locations,distributed machine learning has attracted a lot of research interests.Alternating Direction Method of Multipliers (ADMM) is a powerful method ofdesigning distributed machine learning algorithm, whereby each agent computesover local datasets and exchanges computation results with its neighbor agentsin an iterative procedure. There exists significant privacy leakage during thisiterative process if the local data is sensitive. In this paper, we propose adifferentially private ADMM algorithm (P-ADMM) to provide dynamiczero-concentrated differential privacy (dynamic zCDP), by inserting Gaussiannoise with linearly decaying variance. We prove that P-ADMM has the sameconvergence rate compared to the non-private counterpart, i.e.,$\mathcal{O}(1/K)$ with $K$ being the number of iterations and linearconvergence for general convex and strongly convex problems while providingdifferentially private guarantee. Moreover, through our experiments performedon real-world datasets, we empirically show that P-ADMM has the best-knownperformance among the existing differentially private ADMM based algorithms.",http://arxiv.org/abs/1901.02094v2,,
1826,"Putting Fairness Principles into Practice: Challenges, Metrics, and  Improvements","As more researchers have become aware of and passionate about algorithmicfairness, there has been an explosion in papers laying out new metrics,suggesting algorithms to address issues, and calling attention to issues inexisting applications of machine learning. This research has greatly expandedour understanding of the concerns and challenges in deploying machine learning,but there has been much less work in seeing how the rubber meets the road.  In this paper we provide a case-study on the application of fairness inmachine learning research to a production classification system, and offer newinsights in how to measure and address algorithmic fairness issues. We discussopen questions in implementing equality of opportunity and describe ourfairness metric, conditional equality, that takes into account distributionaldifferences. Further, we provide a new approach to improve on the fairnessmetric during model training and demonstrate its efficacy in improvingperformance for a real-world product",http://arxiv.org/abs/1901.04562v1,,
1827,"Predicting wind pressures around circular cylinders using machine  learning techniques","Numerous studies have been carried out to measure wind pressures aroundcircular cylinders since the early 20th century due to its engineeringsignificance. Consequently, a large amount of wind pressure data sets haveaccumulated, which presents an excellent opportunity for using machine learning(ML) techniques to train models to predict wind pressures around circularcylinders. Wind pressures around smooth circular cylinders are a function ofmainly the Reynolds number (Re), turbulence intensity (Ti) of the incidentwind, and circumferential angle of the cylinder. Considering these threeparameters as the inputs, this study trained two ML models to predict mean andfluctuating pressures respectively. Three machine learning algorithms includingdecision tree regressor, random forest, and gradient boosting regression trees(GBRT) were tested. The GBRT models exhibited the best performance forpredicting both mean and fluctuating pressures, and they are capable of makingaccurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to15%. It is believed that the GBRT models provide very efficient and economicalalternative to traditional wind tunnel tests and computational fluid dynamicsimulations for determining wind pressures around smooth circular cylinderswithin the studied Re and Ti range.",http://arxiv.org/abs/1901.06752v1,,
1828,An Evaluation of the Human-Interpretability of Explanation,"Recent years have seen a boom in interest in machine learning systems thatcan provide a human-understandable rationale for their predictions ordecisions. However, exactly what kinds of explanation are trulyhuman-interpretable remains poorly understood. This work advances ourunderstanding of what makes explanations interpretable under three specifictasks that users may perform with machine learning systems: simulation of theresponse, verification of a suggested response, and determining whether thecorrectness of a suggested response changes under a change to the inputs.Through carefully controlled human-subject experiments, we identifyregularizers that can be used to optimize for the interpretability of machinelearning systems. Our results show that the type of complexity matters:cognitive chunks (newly defined concepts) affect performance more than variablerepetitions, and these trends are consistent across tasks and domains. Thissuggests that there may exist some common design principles for explanationsystems.",http://arxiv.org/abs/1902.00006v1,,
1829,TF-Replicator: Distributed Machine Learning for Researchers,"We describe TF-Replicator, a framework for distributed machine learningdesigned for DeepMind researchers and implemented as an abstraction overTensorFlow. TF-Replicator simplifies writing data-parallel and model-parallelresearch code. The same models can be effortlessly deployed to differentcluster architectures (i.e. one or many machines containing CPUs, GPUs or TPUaccelerators) using synchronous or asynchronous training regimes. Todemonstrate the generality and scalability of TF-Replicator, we implement andbenchmark three very different models: (1) A ResNet-50 for ImageNetclassification, (2) a SN-GAN for class-conditional ImageNet image generation,and (3) a D4PG reinforcement learning agent for continuous control. Our resultsshow strong scalability performance without demanding any distributed systemsexpertise of the user. The TF-Replicator programming model will be open-sourcedas part of TensorFlow 2.0 (seehttps://github.com/tensorflow/community/pull/25).",http://arxiv.org/abs/1902.00465v1,,
1830,Quantum Speedup in Adaptive Boosting of Binary Classification,"In classical machine learning, a set of weak classifiers can be adaptivelycombined to form a strong classifier for improving the overall performance, atechnique called adaptive boosting (or AdaBoost). However, constructing thestrong classifier for a large data set is typically resource consuming. Here wepropose a quantum extension of AdaBoost, demonstrating a quantum algorithm thatcan output the optimal strong classifier with a quadratic speedup in the numberof queries of the weak classifiers. Our results also include a generalizationof the standard AdaBoost to the cases where the output of each classifier maybe probabilistic even for the same input. We prove that the update rules andthe query complexity of the non-deterministic classifiers are the same as thoseof deterministic classifiers, which may be of independent interest to theclassical machine-learning community. Furthermore, the AdaBoost algorithm canalso be applied to data encoded in the form of quantum states; we show how thetraining set can be simplified by using the tools of t-design. Our approachdescribes a model of quantum machine learning where quantum speedup is achievedin finding the optimal classifier, which can then be applied for classicalmachine-learning applications.",http://arxiv.org/abs/1902.00869v1,,
1831,Machine Learning for Seizure Type Classification: Setting the benchmark,"Accurate classification of seizure types plays a crucial role in thetreatment and disease management of epileptic patients. Epileptic seizure typenot only impacts on the choice of drugs but also on the range of activities apatient can safely engage in. With recent advances being made towardsartificial intelligence enabled automatic seizure detection, the next frontieris the automatic classification of seizure types. On that note, in this paper,we undertake the first study to explore the application of machine learningalgorithms for multi-class seizure type classification. We used the recentlyreleased TUH EEG Seizure Corpus and conducted a thorough search spaceexploration to evaluate the performance of a combination of variouspre-processing techniques, machine learning algorithms, and correspondinghyperparameters on this task. We show that our algorithms can reach a weightedF1 score of up to 0.907 thereby setting the first benchmark for scalp EEG basedmulti-class seizure type classification.",http://arxiv.org/abs/1902.01012v1,,
1832,"Machine-Learning-Assisted Directed Protein Evolution with Combinatorial  Libraries","To reduce experimental effort associated with directed protein evolution andto explore the sequence space encoded by mutating multiple positionssimultaneously, we incorporate machine learning in the directed evolutionworkflow. Combinatorial sequence space can be quite expensive to sampleexperimentally, but machine learning models trained on tested variants providea fast method for testing sequence space computationally. We validate thisapproach on a large published empirical fitness landscape for human GB1 bindingprotein, demonstrating that machine learning-guided directed evolution findsvariants with higher fitness than those found by other directed evolutionapproaches. We then provide an example application in evolving an enzyme toproduce each of the two possible product enantiomers (stereodivergence) of anew-to-nature carbene Si-H insertion reaction. The approach predicted librariesenriched in functional enzymes and fixed seven mutations in two rounds ofevolution to identify variants for selective catalysis with 93% and 79% ee. Bygreatly increasing throughput with in silico modeling, machine learningenhances the quality and diversity of sequence solutions for a proteinengineering problem.",http://arxiv.org/abs/1902.07231v2,,
1833,"Identifying Malicious Web Domains Using Machine Learning Techniques with  Online Credibility and Performance Data","Malicious web domains represent a big threat to web users' privacy andsecurity. With so much freely available data on the Internet about web domains'popularity and performance, this study investigated the performance ofwell-known machine learning techniques used in conjunction with this type ofonline data to identify malicious web domains. Two datasets consisting ofmalware and phishing domains were collected to build and evaluate the machinelearning classifiers. Five single classifiers and four ensemble classifierswere applied to distinguish malicious domains from benign ones. In addition, abinary particle swarm optimisation (BPSO) based feature selection method wasused to improve the performance of single classifiers. Experimental resultsshow that, based on the web domains' popularity and performance data features,the examined machine learning techniques can accurately identify maliciousdomains in different ways. Furthermore, the BPSO-based feature selectionprocedure is shown to be an effective way to improve the performance ofclassifiers.",http://arxiv.org/abs/1902.08792v1,,
1834,"Field-aware Neural Factorization Machine for Click-Through Rate  Prediction","Recommendation systems and computing advertisements have gradually enteredthe field of academic research from the field of commercial applications.Click-through rate prediction is one of the core research issues because theprediction accuracy affects the user experience and the revenue of merchantsand platforms. Feature engineering is very important to improve click-throughrate prediction. Traditional feature engineering heavily relies on people'sexperience, and is difficult to construct a feature combination that candescribe the complex patterns implied in the data. This paper combinestraditional feature combination methods and deep neural networks to automatefeature combinations to improve the accuracy of click-through rate prediction.We propose a mechannism named 'Field-aware Neural Factorization Machine'(FNFM). This model can have strong second order feature interactive learningability like Field-aware Factorization Machine, on this basis, deep neuralnetwork is used for higher-order feature combination learning. Experiments showthat the model has stronger expression ability than current deep learningfeature combination models like the DeepFM, DCN and NFM.",http://arxiv.org/abs/1902.09096v1,,
1835,Revealing quantum chaos with machine learning,"Understanding the properties of quantum matter is an outstanding challenge inscience. In this work, we demonstrate how machine learning methods can besuccessfully applied for the classification of various regimes insingle-particle and many-body systems. We realize neural network algorithmsthat perform a classification between regular and chaotic behavior in quantumbilliard models with remarkably high accuracy. By taking this method further,we show that machine learning techniques allow to pin down the transition fromintegrability to many-body quantum chaos in Heisenberg XXZ spin chains. Ourresults pave the way for exploring the power of machine learning tools forrevealing exotic phenomena in complex quantum many-body systems.",http://arxiv.org/abs/1902.09216v1,,
1836,Grammar Based Directed Testing of Machine Learning Systems,"The massive progress of machine learning has seen its application over avariety of domains in the past decade. But how do we develop a systematic,scalable and modular strategy to validate machine-learning systems? We present,to the best of our knowledge, the first approach, which provides a systematictest framework for machine-learning systems that accepts grammar-based inputs.Our OGMA approach automatically discovers erroneous behaviours in classifiersand leverages these erroneous behaviours to improve the respective models. OGMAleverages inherent robustness properties present in any well trainedmachine-learning model to direct test generation and thus, implementing ascalable test generation methodology. To evaluate our OGMA approach, we havetested it on three real world natural language processing (NLP) classifiers. Wehave found thousands of erroneous behaviours in these systems. We also compareOGMA with a random test generation approach and observe that OGMA is moreeffective than such random test generation by up to 489%.",http://arxiv.org/abs/1902.10027v1,,
1837,"The Challenge of Machine Learning in Space Weather Nowcasting and  Forecasting","The numerous recent breakthroughs in machine learning (ML) make imperative tocarefully ponder how the scientific community can benefit from a technologythat, although not necessarily new, is today living its golden age. This GrandChallenge review paper is focused on the present and future role of machinelearning in space weather. The purpose is twofold. On one hand, we will discussprevious works that use ML for space weather forecasting, focusing inparticular on the few areas that have seen most activity: the forecasting ofgeomagnetic indices, of relativistic electrons at geosynchronous orbits, ofsolar flares occurrence, of coronal mass ejection propagation time, and ofsolar wind speed. On the other hand, this paper serves as a gentle introductionto the field of machine learning tailored to the space weather community and asa pointer to a number of open challenges that we believe the community shouldundertake in the next decade. The recurring themes throughout the review arethe need to shift our forecasting paradigm to a probabilistic approach focusedon the reliable assessment of uncertainties, and the combination ofphysics-based and machine learning approaches, known as gray-box.",http://arxiv.org/abs/1903.05192v2,,
1838,"Generative Tensor Network Classification Model for Supervised Machine  Learning","Tensor network (TN) has recently triggered extensive interests in developingmachine-learning models in quantum many-body Hilbert space. Here we purpose agenerative TN classification (GTNC) approach for supervised learning. Thestrategy is to train the generative TN for each class of the samples toconstruct the classifiers. The classification is implemented by comparing thedistance in the many-body Hilbert space. The numerical experiments by GTNC showimpressive performance on the MNIST and Fashion-MNIST dataset. The testingaccuracy is competitive to the state-of-the-art convolutional neural networkwhile higher than the naive Bayes classifier (a generative classifier) andsupport vector machine. Moreover, GTNC is more efficient than the existing TNmodels that are in general discriminative. By investigating the distances inthe many-body Hilbert space, we find that (a) the samples are naturallyclustering in such a space; and (b) bounding the bond dimensions of the TN's tofinite values corresponds to removing redundant information in the imagerecognition. These two characters make GTNC an adaptive and universal model ofexcellent performance.",http://arxiv.org/abs/1903.10742v1,,
1839,"SysML'19 demo: customizable and reusable Collective Knowledge pipelines  to automate and reproduce machine learning experiments","Reproducing, comparing and reusing results from machine learning and systemspapers is a very tedious, ad hoc and time-consuming process. I will demonstratehow to automate this process using open-source, portable, customizable andCLI-based Collective Knowledge workflows and pipelines developed by thecommunity. I will help participants run several real-world non-virtualized CKworkflows from the SysML'19 conference, companies (General Motors, Arm) andMLPerf benchmark to automate benchmarking and co-design of efficientsoftware/hardware stacks for machine learning workloads. I hope that ourapproach will help authors reduce their effort when sharing reusable andextensible research artifacts while enabling artifact evaluators toautomatically validate experimental results from published papers in a standardand portable way.",http://arxiv.org/abs/1904.00324v1,,
1840,"A Novel Pixel-Averaging Technique for Extracting Training Data from a  Single Image, Used in ML-Based Image Enlargement","Size of the training dataset is an important factor in the performance of amachine learning algorithms and tools used in medical image processing are notexceptions. Machine learning tools normally require a decent amount of trainingdata before they could efficiently predict a target. For image processing andcomputer vision, the number of images determines the validity and reliabilityof the training set. Medical images in some cases, suffer from poor quality andinadequate quantity required for a suitable training set. The proposedalgorithm in this research obviates the need for large or even small imagedatasets used in machine learning based image enlargement techniques byextracting the required data from a single image. The extracted data was thenintroduced to a decision tree regressor for upscaling greyscale medical imagesat different zoom levels. Results from the algorithm are relatively acceptablecompared to third-party applications and promising for future research. Thistechnique could be tailored to the requirements of other machine learning toolsand the results may be improved by further tweaking of the toolshyperparameters.",http://arxiv.org/abs/1904.00747v1,,
1841,"Machine Learning, Big Data, And Smart Buildings: A Comprehensive Survey","Future buildings will offer new convenience, comfort, and efficiencypossibilities to their residents. Changes will occur to the way people live astechnology involves into people's lives and information processing is fullyintegrated into their daily living activities and objects. The futureexpectation of smart buildings includes making the residents' experience aseasy and comfortable as possible. The massive streaming data generated andcaptured by smart building appliances and devices contains valuable informationthat needs to be mined to facilitate timely actions and better decision making.Machine learning and big data analytics will undoubtedly play a critical roleto enable the delivery of such smart services. In this paper, we survey thearea of smart building with a special focus on the role of techniques frommachine learning and big data analytics. This survey also reviews the currenttrends and challenges faced in the development of smart building services.",http://arxiv.org/abs/1904.01460v1,,
1842,"Quantifying Interpretability of Arbitrary Machine Learning Models  Through Functional Decomposition","To obtain interpretable machine learning models, either interpretable modelsare constructed from the outset - e.g. shallow decision trees, rule lists, orsparse generalized linear models - or post-hoc interpretation methods - e.g.partial dependence or ALE plots - are employed. Both approaches havedisadvantages. While the former can restrict the hypothesis space tooconservatively, leading to potentially suboptimal solutions, the latter canproduce too verbose or misleading results if the resulting model is toocomplex, especially w.r.t. feature interactions. We propose to make thecompromise between predictive power and interpretability explicit byquantifying the complexity / interpretability of machine learning models. Basedon functional decomposition, we propose measures of number of features used,interaction strength and main effect complexity. We show that post-hocinterpretation of models that minimize the three measures becomes more reliableand compact. Furthermore, we demonstrate the application of such measures in amulti-objective optimization approach which considers predictive power andinterpretability at the same time.",http://arxiv.org/abs/1904.03867v1,,
1843,Kernels on Sample Sets via Nonparametric Divergence Estimates,"Most machine learning algorithms, such as classification or regression, treatthe individual data point as the object of interest. Here we consider extendingmachine learning algorithms to operate on groups of data points. We suggesttreating a group of data points as an i.i.d. sample set from an underlyingfeature distribution for that group. Our approach employs kernel machines witha kernel on i.i.d. sample sets of vectors. We define certain kernel functionson pairs of distributions, and then use a nonparametric estimator toconsistently estimate those functions based on sample sets. The projection ofthe estimated Gram matrix to the cone of symmetric positive semi-definitematrices enables us to use kernel machines for classification, regression,anomaly detection, and low-dimensional embedding in the space of distributions.We present several numerical experiments both on real and simulated datasets todemonstrate the advantages of our new approach.",http://arxiv.org/abs/1202.0302v2,,
1844,"Reinforcement Learning for Bandit Neural Machine Translation with  Simulated Human Feedback","Machine translation is a natural candidate problem for reinforcement learningfrom human feedback: users provide quick, dirty ratings on candidatetranslations to guide a system to improve. Yet, current neural machinetranslation training focuses on expensive human-generated referencetranslations. We describe a reinforcement learning algorithm that improvesneural machine translation systems from simulated human feedback. Our algorithmcombines the advantage actor-critic algorithm (Mnih et al., 2016) with theattention-based neural encoder-decoder architecture (Luong et al., 2015). Thisalgorithm (a) is well-designed for problems with a large action space anddelayed rewards, (b) effectively optimizes traditional corpus-level machinetranslation metrics, and (c) is robust to skewed, high-variance, granularfeedback modeled after actual human behaviors.",http://arxiv.org/abs/1707.07402v4,,
1845,"A Machine Learning Driven IoT Solution for Noise Classification in Smart  Cities","We present a machine learning based method for noise classification using alow-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficientsfor audio feature extraction and supervised classification algorithms (that is,support vector machine and k-nearest neighbors) for noise classification. Weevaluate our approach experimentally with a dataset of about 3000 sound samplesgrouped in eight sound classes (such as, car horn, jackhammer, or streetmusic). We explore the parameter space of support vector machine and k-nearestneighbors algorithms to estimate the optimal parameter values forclassification of sound samples in the dataset under study. We achieve a noiseclassification accuracy in the range 85% -- 100%. Training and testing of ourk-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less thana second for a dataset with features of more than 3000 sound samples.",http://arxiv.org/abs/1809.00238v1,,
1846,Accelerating Gradient Boosting Machine,"Gradient Boosting Machine (GBM) is an extremely powerful supervised learningalgorithm that is widely used in practice. GBM routinely features as a leadingalgorithm in machine learning competitions such as Kaggle and the KDDCup. Inthis work, we propose Accelerated Gradient Boosting Machine (AGBM) byincorporating Nesterov's acceleration techniques into the design of GBM. Thedifficulty in accelerating GBM lies in the fact that weak (inexact) learnersare commonly used, and therefore the errors can accumulate in the momentumterm. To overcome it, we design a ""corrected pseudo residual"" and fit best weaklearner to this corrected pseudo residual, in order to perform the z-update.Thus, we are able to derive novel computational guarantees for AGBM. This isthe first GBM type of algorithm with theoretically-justified acceleratedconvergence rate. Finally we demonstrate with a number of numerical experimentsthe effectiveness of AGBM over conventional GBM in obtaining a model with goodtraining and/or testing data fidelity.",http://arxiv.org/abs/1903.08708v1,,
1847,PyTorch-BigGraph: A Large-scale Graph Embedding System,"Graph embedding methods produce unsupervised node features from graphs thatcan then be used for a variety of machine learning tasks. Modern graphs,particularly in industrial applications, contain billions of nodes andtrillions of edges, which exceeds the capability of existing embedding systems.We present PyTorch-BigGraph (PBG), an embedding system that incorporatesseveral modifications to traditional multi-relation embedding systems thatallow it to scale to graphs with billions of nodes and trillions of edges. PBGuses graph partitioning to train arbitrarily large embeddings on either asingle machine or in a distributed environment. We demonstrate comparableperformance with existing embedding systems on common benchmarks, whileallowing for scaling to arbitrarily large graphs and parallelization onmultiple machines. We train and evaluate embeddings on several large socialnetwork graphs as well as the full Freebase dataset, which contains over 100million nodes and 2 billion edges.",http://arxiv.org/abs/1903.12287v3,,
1848,Autoregressive Energy Machines,"Neural density estimators are flexible families of parametric models whichhave seen widespread use in unsupervised machine learning in recent years.Maximum-likelihood training typically dictates that these models be constrainedto specify an explicit density. However, this limitation can be overcome byinstead using a neural network to specify an energy function, or unnormalizeddensity, which can subsequently be normalized to obtain a valid distribution.The challenge with this approach lies in accurately estimating the normalizingconstant of the high-dimensional energy function. We propose the AutoregressiveEnergy Machine, an energy-based model which simultaneously learns anunnormalized density and computes an importance-sampling estimate of thenormalizing constant for each conditional in an autoregressive decomposition.The Autoregressive Energy Machine achieves state-of-the-art performance on asuite of density-estimation tasks.",http://arxiv.org/abs/1904.05626v1,,
1849,Separable Dictionary Learning,"Many techniques in computer vision, machine learning, and statistics rely onthe fact that a signal of interest admits a sparse representation over somedictionary. Dictionaries are either available analytically, or can be learnedfrom a suitable training set. While analytic dictionaries permit to capture theglobal structure of a signal and allow a fast implementation, learneddictionaries often perform better in applications as they are more adapted tothe considered class of signals. In imagery, unfortunately, the numericalburden for (i) learning a dictionary and for (ii) employing the dictionary forreconstruction tasks only allows to deal with relatively small image patchesthat only capture local image information. The approach presented in this paperaims at overcoming these drawbacks by allowing a separable structure on thedictionary throughout the learning process. On the one hand, this permitslarger patch-sizes for the learning phase, on the other hand, the dictionary isapplied efficiently in reconstruction tasks. The learning procedure is based onoptimizing over a product of spheres which updates the dictionary as a whole,thus enforces basic dictionary properties such as mutual coherence explicitlyduring the learning procedure. In the special case where no separable structureis enforced, our method competes with state-of-the-art dictionary learningmethods like K-SVD.",http://arxiv.org/abs/1303.5244v1,,
1850,Interpretable Active Learning,"Active learning has long been a topic of study in machine learning. However,as increasingly complex and opaque models have become standard practice, theprocess of active learning, too, has become more opaque. There has been littleinvestigation into interpreting what specific trends and patterns an activelearning strategy may be exploring. This work expands on the LocalInterpretable Model-agnostic Explanations framework (LIME) to provideexplanations for active learning recommendations. We demonstrate how LIME canbe used to generate locally faithful explanations for an active learningstrategy, and how these explanations can be used to understand how differentmodels and datasets explore a problem space over time. In order to quantify theper-subgroup differences in how an active learning strategy queries spatialregions, we introduce a notion of uncertainty bias (based on disparate impact)to measure the discrepancy in the confidence for a model's predictions betweenone subgroup and another. Using the uncertainty bias measure, we show that ourquery explanations accurately reflect the subgroup focus of the active learningqueries, allowing for an interpretable explanation of what is being learned aspoints with similar sources of uncertainty have their uncertainty biasresolved. We demonstrate that this technique can be applied to trackuncertainty bias over user-defined clusters or automatically generated clustersbased on the source of uncertainty.",http://arxiv.org/abs/1708.00049v2,,
1851,"Ontology-based Fuzzy Markup Language Agent for Student and Robot  Co-Learning","An intelligent robot agent based on domain ontology, machine learningmechanism, and Fuzzy Markup Language (FML) for students and robot co-learningis presented in this paper. The machine-human co-learning model is establishedto help various students learn the mathematical concepts based on theirlearning ability and performance. Meanwhile, the robot acts as a teacher'sassistant to co-learn with children in the class. The FML-based knowledge baseand rule base are embedded in the robot so that the teachers can get feedbackfrom the robot on whether students make progress or not. Next, we inferredstudents' learning performance based on learning content's difficulty andstudents' ability, concentration level, as well as teamwork sprit in the class.Experimental results show that learning with the robot is helpful fordisadvantaged and below-basic children. Moreover, the accuracy of theintelligent FML-based agent for student learning is increased after machinelearning mechanism.",http://arxiv.org/abs/1801.08650v1,,
1852,Continual Lifelong Learning with Neural Networks: A Review,"Humans and animals have the ability to continually acquire, fine-tune, andtransfer knowledge and skills throughout their lifespan. This ability, referredto as lifelong learning, is mediated by a rich set of neurocognitive mechanismsthat together contribute to the development and specialization of oursensorimotor skills as well as to long-term memory consolidation and retrieval.Consequently, lifelong learning capabilities are crucial for autonomous agentsinteracting in the real world and processing continuous streams of information.However, lifelong learning remains a long-standing challenge for machinelearning and neural network models since the continual acquisition ofincrementally available information from non-stationary data distributionsgenerally leads to catastrophic forgetting or interference. This limitationrepresents a major drawback for state-of-the-art deep neural network modelsthat typically learn representations from stationary batches of training data,thus without accounting for situations in which information becomesincrementally available over time. In this review, we critically summarize themain challenges linked to lifelong learning for artificial learning systems andcompare existing neural network approaches that alleviate, to differentextents, catastrophic forgetting. We discuss well-established and emergingresearch motivated by lifelong learning factors in biological systems such asstructural plasticity, memory replay, curriculum and transfer learning,intrinsic motivation, and multisensory integration.",http://arxiv.org/abs/1802.07569v4,,
1853,"Self-learning Local Supervision Encoding Framework to Constrict and  Disperse Feature Distribution for Clustering","To obtain suitable feature distribution is a difficult task in machinelearning, especially for unsupervised learning. In this paper, we propose anovel self-learning local supervision encoding framework based on RBMs, inwhich the self-learning local supervisions from visible layer are integratedinto the contrastive divergence (CD) learning of RBMs to constrict and dispersethe distribution of the hidden layer features for clustering tasks. In theframework, we use sigmoid transformation to obtain hidden layer andreconstructed hidden layer features from visible layer and reconstructedvisible layer units during sampling procedure. The self-learning localsupervisions contain local credible clusters which stem from differentunsupervised learning and unanimous voting strategy. They are fused into hiddenlayer features and reconstructed hidden layer features. For the same localclusters, the hidden features and reconstructed hidden layer features of theframework tend to constrict together. Furthermore, the hidden layer features ofdifferent local clusters tend to disperse in the encoding process. Under suchframework, we present two instantiation models with the reconstruction of twodifferent visible layers. One is self-learning local supervision GRBM (slsGRBM)model with Gaussian linear visible units and binary hidden units using lineartransformation for visible layer reconstruction. The other is self-learninglocal supervision RBM (slsRBM) model with binary visible and hidden units usingsigmoid transformation for visible layer reconstruction.",http://arxiv.org/abs/1812.01967v1,,
1854,"ASTROMLSKIT: A New Statistical Machine Learning Toolkit: A Platform for  Data Analytics in Astronomy","Astroinformatics is a new impact area in the world of astronomy, occasionallycalled the final frontier, where several astrophysicists, statisticians andcomputer scientists work together to tackle various data intensive astronomicalproblems. Exponential growth in the data volume and increased complexity of thedata augments difficult questions to the existing challenges. Classicalproblems in Astronomy are compounded by accumulation of astronomical volume ofcomplex data, rendering the task of classification and interpretationincredibly laborious. The presence of noise in the data makes analysis andinterpretation even more arduous. Machine learning algorithms and data analytictechniques provide the right platform for the challenges posed by theseproblems. A diverse range of open problem like star-galaxy separation,detection and classification of exoplanets, classification of supernovae isdiscussed. The focus of the paper is the applicability and efficacy of variousmachine learning algorithms like K Nearest Neighbor (KNN), random forest (RF),decision tree (DT), Support Vector Machine (SVM), Na\""ive Bayes and LinearDiscriminant Analysis (LDA) in analysis and inference of the decision theoreticproblems in Astronomy. The machine learning algorithms, integrated intoASTROMLSKIT, a toolkit developed in the course of the work, have been used toanalyze HabCat data and supernovae data. Accuracy has been found to beappreciably good.",http://arxiv.org/abs/1504.07865v1,,
1855,"Toward a general, scaleable framework for Bayesian teaching with  applications to topic models","Machines, not humans, are the world's dominant knowledge accumulators buthumans remain the dominant decision makers. Interpreting and disseminating theknowledge accumulated by machines requires expertise, time, and is prone tofailure. The problem of how best to convey accumulated knowledge from computersto humans is a critical bottleneck in the broader application of machinelearning. We propose an approach based on human teaching where the problem isformalized as selecting a small subset of the data that will, with highprobability, lead the human user to the correct inference. This approach,though successful for modeling human learning in simple laboratory experiments,has failed to achieve broader relevance due to challenges in formulatinggeneral and scalable algorithms. We propose general-purpose teaching viapseudo-marginal sampling and demonstrate the algorithm by teaching topicmodels. Simulation results show our sampling-based approach: effectivelyapproximates the probability where ground-truth is possible via enumeration,results in data that are markedly different from those expected by randomsampling, and speeds learning especially for small amounts of data. Applicationto movie synopsis data illustrates differences between teaching and randomsampling for teaching distributions and specific topics, and demonstrates gainsin scalability and applicability to real-world problems.",http://arxiv.org/abs/1605.07999v1,,
1856,From Dependence to Causation,"Machine learning is the science of discovering statistical dependencies indata, and the use of those dependencies to perform predictions. During the lastdecade, machine learning has made spectacular progress, surpassing humanperformance in complex tasks such as object recognition, car driving, andcomputer gaming. However, the central role of prediction in machine learningavoids progress towards general-purpose artificial intelligence. As one wayforward, we argue that causal inference is a fundamental component of humanintelligence, yet ignored by learning algorithms.  Causal inference is the problem of uncovering the cause-effect relationshipsbetween the variables of a data generating system. Causal structures provideunderstanding about how these systems behave under changing, unseenenvironments. In turn, knowledge about these causal dynamics allows to answer""what if"" questions, describing the potential responses of the system underhypothetical manipulations and interventions. Thus, understanding cause andeffect is one step from machine learning towards machine reasoning and machineintelligence. But, currently available causal inference algorithms operate inspecific regimes, and rely on assumptions that are difficult to verify inpractice.  This thesis advances the art of causal inference in three different ways.First, we develop a framework for the study of statistical dependence based oncopulas and random features. Second, we build on this framework to interpretthe problem of causal inference as the task of distribution classification,yielding a family of novel causal inference algorithms. Third, we discovercausal structures in convolutional neural network features using ouralgorithms. The algorithms presented in this thesis are scalable, exhibitstrong theoretical guarantees, and achieve state-of-the-art performance in avariety of real-world benchmarks.",http://arxiv.org/abs/1607.03300v1,,
1857,"Machine learning of neuroimaging to diagnose cognitive impairment and  dementia: a systematic review and comparative analysis","INTRODUCTION: Advanced machine learning methods might help to identifydementia risk from neuroimaging, but their accuracy to date is unclear.  METHODS: We systematically reviewed the literature, 2006 to late 2016, formachine learning studies differentiating healthy ageing through to dementia ofvarious types, assessing study quality, and comparing accuracy at differentdisease boundaries.  RESULTS: Of 111 relevant studies, most assessed Alzheimer's disease (AD) vshealthy controls, used ADNI data, support vector machines and only T1-weightedsequences. Accuracy was highest for differentiating AD from healthy controls,and poor for differentiating healthy controls vs MCI vs AD, or MCI convertersvs non-converters. Accuracy increased using combined data types, but not bydata source, sample size or machine learning method.  DISCUSSION: Machine learning does not differentiate clinically-relevantdisease categories yet. More diverse datasets, combinations of different typesof data, and close clinical integration of machine learning would help toadvance the field.",http://arxiv.org/abs/1804.01961v2,,
1858,"Examining the Use of Neural Networks for Feature Extraction: A  Comparative Analysis using Deep Learning, Support Vector Machines, and  K-Nearest Neighbor Classifiers","Neural networks in many varieties are touted as very powerful machinelearning tools because of their ability to distill large amounts of informationfrom different forms of data, extracting complex features and enabling powerfulclassification abilities. In this study, we use neural networks to extractfeatures from both images and numeric data and use these extracted features asinputs for other machine learning models, namely support vector machines (SVMs)and k-nearest neighbor classifiers (KNNs), in order to see ifneural-network-extracted features enhance the capabilities of these models. Wetested 7 different neural network architectures in this manner, 4 for imagesand 3 for numeric data, training each for varying lengths of time and thencomparing the results of the neural network independently to those of an SVMand KNN on the data, and finally comparing these results to models of SVM andKNN trained using features extracted via the neural network architecture. Thisprocess was repeated on 3 different image datasets and 2 different numericdatasets. The results show that, in many cases, the features extracted usingthe neural network significantly improve the capabilities of SVMs and KNNscompared to running these algorithms on the raw features, and in some casesalso surpass the performance of the neural network alone. This in turn suggeststhat it may be a reasonable practice to use neural networks as a means toextract features for classification by other machine learning models for somedatasets.",http://arxiv.org/abs/1805.02294v2,,
1859,"A Conjugate Property between Loss Functions and Uncertainty Sets in  Classification Problems","In binary classification problems, mainly two approaches have been proposed;one is loss function approach and the other is uncertainty set approach. Theloss function approach is applied to major learning algorithms such as supportvector machine (SVM) and boosting methods. The loss function represents thepenalty of the decision function on the training samples. In the learningalgorithm, the empirical mean of the loss function is minimized to obtain theclassifier. Against a backdrop of the development of mathematical programming,nowadays learning algorithms based on loss functions are widely applied toreal-world data analysis. In addition, statistical properties of such learningalgorithms are well-understood based on a lots of theoretical works. On theother hand, the learning method using the so-called uncertainty set is used inhard-margin SVM, mini-max probability machine (MPM) and maximum margin MPM. Inthe learning algorithm, firstly, the uncertainty set is defined for each binarylabel based on the training samples. Then, the best separating hyperplanebetween the two uncertainty sets is employed as the decision function. This isregarded as an extension of the maximum-margin approach. The uncertainty setapproach has been studied as an application of robust optimization in the fieldof mathematical programming. The statistical properties of learning algorithmswith uncertainty sets have not been intensively studied. In this paper, weconsider the relation between the above two approaches. We point out that theuncertainty set is described by using the level set of the conjugate of theloss function. Based on such relation, we study statistical properties oflearning algorithms using uncertainty sets.",http://arxiv.org/abs/1204.6583v1,,
1860,Privileged Information for Data Clustering,"Many machine learning algorithms assume that all input samples areindependently and identically distributed from some common distribution oneither the input space X, in the case of unsupervised learning, or the inputand output space X x Y in the case of supervised and semi-supervised learning.In the last number of years the relaxation of this assumption has been exploredand the importance of incorporation of additional information within machinelearning algorithms became more apparent. Traditionally such fusion ofinformation was the domain of semi-supervised learning. More recently theinclusion of knowledge from separate hypothetical spaces has been proposed byVapnik as part of the supervised setting. In this work we are interested inexploring Vapnik's idea of master-class learning and the associated learningusing privileged information, however within the unsupervised setting. Adoptionof the advanced supervised learning paradigm for the unsupervised settinginstigates investigation into the difference between privileged and technicaldata. By means of our proposed aRi-MAX method stability of the KMeans algorithmis improved and identification of the best clustering solution is achieved onan artificial dataset. Subsequently an information theoretic dot product basedalgorithm called P-Dot is proposed. This method has the ability to utilize awide variety of clustering techniques, individually or in combination, whilefusing privileged and technical data for improved clustering. Application ofthe P-Dot method to the task of digit recognition confirms our findings in areal-world scenario.",http://arxiv.org/abs/1305.7454v1,,
1861,Probabilistic supervised learning,"Predictive modelling and supervised learning are central to modern datascience. With predictions from an ever-expanding number of supervised black-boxstrategies - e.g., kernel methods, random forests, deep learning aka neuralnetworks - being employed as a basis for decision making processes, it iscrucial to understand the statistical uncertainty associated with thesepredictions.  As a general means to approach the issue, we present an overarching frameworkfor black-box prediction strategies that not only predict the target but alsotheir own predictions' uncertainty. Moreover, the framework allows for fairassessment and comparison of disparate prediction strategies. For this, weformally consider strategies capable of predicting full distributions fromfeature variables, so-called probabilistic supervised learning strategies.  Our work draws from prior work including Bayesian statistics, informationtheory, and modern supervised machine learning, and in a novel synthesis leadsto (a) new theoretical insights such as a probabilistic bias-variancedecomposition and an entropic formulation of prediction, as well as to (b) newalgorithms and meta-algorithms, such as composite prediction strategies,probabilistic boosting and bagging, and a probabilistic predictive independencetest.  Our black-box formulation also leads (c) to a new modular interface view onprobabilistic supervised learning and a modelling workflow API design, which wehave implemented in the newly released skpro machine learning toolbox,extending the familiar modelling interface and meta-modelling functionality ofsklearn. The skpro package provides interfaces for construction, composition,and tuning of probabilistic supervised learning strategies, together withorchestration features for validation and comparison of any such strategy - beit frequentist, Bayesian, or other.",http://arxiv.org/abs/1801.00753v2,,
1862,Meta-learning with differentiable closed-form solvers,"Adapting deep networks to new concepts from a few examples is challenging,due to the high computational requirements of standard fine-tuning procedures.Most work on few-shot learning has thus focused on simple learning techniquesfor adaptation, such as nearest neighbours or gradient descent. Nonetheless,the machine learning literature contains a wealth of methods that learnnon-deep models very efficiently. In this paper, we propose to use these fastconvergent methods as the main adaptation mechanism for few-shot learning. Themain idea is to teach a deep network to use standard machine learning tools,such as ridge regression, as part of its own internal model, enabling it toquickly adapt to novel data. This requires back-propagating errors through thesolver steps. While normally the cost of the matrix operations involved in sucha process would be significant, by using the Woodbury identity we can make thesmall number of examples work to our advantage. We propose both closed-form anditerative solvers, based on ridge regression and logistic regressioncomponents. Our methods constitute a simple and novel approach to the problemof few-shot learning and achieve performance competitive with or superior tothe state of the art on three benchmarks.",http://arxiv.org/abs/1805.08136v2,,
1863,Privacy-Preserving Deep Learning via Weight Transmission,"This paper considers the scenario that multiple data owners wish to apply amachine learning method over the combined dataset of all owners to obtain thebest possible learning output but do not want to share the local datasets owingto privacy concerns. We design systems for the scenario that the stochasticgradient descent (SGD) algorithm is used as the machine learning method becauseSGD (or its variants) is at the heart of recent deep learning techniques overneural networks. Our systems differ from existing systems in the followingfeatures: {\bf (1)} any activation function can be used, meaning that noprivacy-preserving-friendly approximation is required; {\bf (2)} gradientscomputed by SGD are not shared but the weight parameters are shared instead;and {\bf (3)} robustness against colluding parties even in the extreme casethat only one honest party exists. We prove that our systems, whileprivacy-preserving, achieve the same learning accuracy as SGD and hence retainthe merit of deep learning with respect to accuracy. Finally, we conductseveral experiments using benchmark datasets, and show that our systemsoutperform previous system in terms of learning accuracies.",http://arxiv.org/abs/1809.03272v3,,
1864,"Representation Transfer for Differentially Private Drug Sensitivity  Prediction","Motivation: Human genomic datasets often contain sensitive information thatlimits use and sharing of the data. In particular, simple anonymisationstrategies fail to provide sufficient level of protection for genomic data,because the data are inherently identifiable. Differentially private machinelearning can help by guaranteeing that the published results do not leak toomuch information about any individual data point. Recent research has reachedpromising results on differentially private drug sensitivity prediction usinggene expression data. Differentially private learning with genomic data ischallenging because it is more difficult to guarantee the privacy in highdimensions. Dimensionality reduction can help, but if the dimension reductionmapping is learned from the data, then it needs to be differentially privatetoo, which can carry a significant privacy cost. Furthermore, the selection ofany hyperparameters (such as the target dimensionality) needs to also avoidleaking private information.  Results: We study an approach that uses a large public dataset of similartype to learn a compact representation for differentially private learning. Wecompare three representation learning methods: variational autoencoders, PCAand random projection. We solve two machine learning tasks on gene expressionof cancer cell lines: cancer type classification, and drug sensitivityprediction. The experiments demonstrate significant benefit from allrepresentation learning methods with variational autoencoders providing themost accurate predictions most often. Our results significantly improve overprevious state-of-the-art in accuracy of differentially private drugsensitivity prediction.",http://arxiv.org/abs/1901.10227v1,,
1865,On Lightweight Privacy-Preserving Collaborative Learning for IoT Objects,"The Internet of Things (IoT) will be a main data generation infrastructurefor achieving better system intelligence. This paper considers the design andimplementation of a practical privacy-preserving collaborative learning scheme,in which a curious learning coordinator trains a better machine learning modelbased on the data samples contributed by a number of IoT objects, while theconfidentiality of the raw forms of the training data is protected against thecoordinator. Existing distributed machine learning and data encryptionapproaches incur significant computation and communication overhead, renderingthem ill-suited for resource-constrained IoT objects. We study an approach thatapplies independent Gaussian random projection at each IoT object to obfuscatedata and trains a deep neural network at the coordinator based on the projecteddata from the IoT objects. This approach introduces light computation overheadto the IoT objects and moves most workload to the coordinator that can havesufficient computing resources. Although the independent projections performedby the IoT objects address the potential collusion between the curiouscoordinator and some compromised IoT objects, they significantly increase thecomplexity of the projected data. In this paper, we leverage the superiorlearning capability of deep learning in capturing sophisticated patterns tomaintain good learning performance. Extensive comparative evaluation shows thatthis approach outperforms other lightweight approaches that apply additivenoisification for differential privacy and/or support vector machines forlearning in the applications with light data pattern complexities.",http://arxiv.org/abs/1902.05197v1,,
1866,Surrogate Learning - An Approach for Semi-Supervised Classification,"We consider the task of learning a classifier from the feature space$\mathcal{X}$ to the set of classes $\mathcal{Y} = \{0, 1\}$, when the featurescan be partitioned into class-conditionally independent feature sets$\mathcal{X}_1$ and $\mathcal{X}_2$. We show the surprising fact that theclass-conditional independence can be used to represent the original learningtask in terms of 1) learning a classifier from $\mathcal{X}_2$ to$\mathcal{X}_1$ and 2) learning the class-conditional distribution of thefeature set $\mathcal{X}_1$. This fact can be exploited for semi-supervisedlearning because the former task can be accomplished purely from unlabeledsamples. We present experimental evaluation of the idea in two real worldapplications.",http://arxiv.org/abs/0809.4632v1,,
1867,Pooling-Invariant Image Feature Learning,"Unsupervised dictionary learning has been a key component in state-of-the-artcomputer vision recognition architectures. While highly effective methods existfor patch-based dictionary learning, these methods may learn redundant featuresafter the pooling stage in a given early vision architecture. In this paper, weoffer a novel dictionary learning scheme to efficiently take into account theinvariance of learned features after the spatial pooling stage. The algorithmis built on simple clustering, and thus enjoys efficiency and scalability. Wediscuss the underlying mechanism that justifies the use of clusteringalgorithms, and empirically show that the algorithm finds better dictionariesthan patch-based methods with the same dictionary size.",http://arxiv.org/abs/1302.5056v1,,
1868,A Comparison of learning algorithms on the Arcade Learning Environment,"Reinforcement learning agents have traditionally been evaluated on small toyproblems. With advances in computing power and the advent of the ArcadeLearning Environment, it is now possible to evaluate algorithms on diverse anddifficult problems within a consistent framework. We discuss some challengesposed by the arcade learning environment which do not manifest in simplerenvironments. We then provide a comparison of model-free, linear learningalgorithms on this challenging problem set.",http://arxiv.org/abs/1410.8620v1,,
1869,"Safe Policy Search for Lifelong Reinforcement Learning with Sublinear  Regret","Lifelong reinforcement learning provides a promising framework for developingversatile agents that can accumulate knowledge over a lifetime of experienceand rapidly learn new tasks by building upon prior knowledge. However, currentlifelong learning methods exhibit non-vanishing regret as the amount ofexperience increases and include limitations that can lead to suboptimal orunsafe control policies. To address these issues, we develop a lifelong policygradient learner that operates in an adversarial set- ting to learn multipletasks online while enforcing safety constraints on the learned policies. Wedemonstrate, for the first time, sublinear regret for lifelong policy search,and validate our algorithm on several benchmark dynamical systems and anapplication to quadrotor control.",http://arxiv.org/abs/1505.05798v1,,
1870,Defending Non-Bayesian Learning against Adversarial Attacks,"This paper addresses the problem of non-Bayesian learning over multi-agentnetworks, where agents repeatedly collect partially informative observationsabout an unknown state of the world, and try to collaboratively learn the truestate. We focus on the impact of the adversarial agents on the performance ofconsensus-based non-Bayesian learning, where non-faulty agents combine locallearning updates with consensus primitives. In particular, we consider thescenario where an unknown subset of agents suffer Byzantine faults -- agentssuffering Byzantine faults behave arbitrarily. Two different learning rules areproposed.",http://arxiv.org/abs/1606.08883v1,,
1871,Learning Hidden Markov Models for Regression using Path Aggregation,"We consider the task of learning mappings from sequential data to real-valuedresponses. We present and evaluate an approach to learning a type of hiddenMarkov model (HMM) for regression. The learning process involves inferring thestructure and parameters of a conventional HMM, while simultaneously learning aregression model that maps features that characterize paths through the modelto continuous responses. Our results, in both synthetic and biological domains,demonstrate the value of jointly learning the two components of our approach.",http://arxiv.org/abs/1206.3275v1,,
1872,Abstract Learning via Demodulation in a Deep Neural Network,"Inspired by the brain, deep neural networks (DNN) are thought to learnabstract representations through their hierarchical architecture. However, atpresent, how this happens is not well understood. Here, we demonstrate that DNNlearn abstract representations by a process of demodulation. We introduce abiased sigmoid activation function and use it to show that DNN learn andperform better when optimized for demodulation. Our findings constitute thefirst unambiguous evidence that DNN perform abstract learning in practical use.Our findings may also explain abstract learning in the human brain.",http://arxiv.org/abs/1502.04042v1,,
1873,A Reinforcement Learning Approach to Online Learning of Decision Trees,"Online decision tree learning algorithms typically examine all features of anew data point to update model parameters. We propose a novel alternative,Reinforcement Learning- based Decision Trees (RLDT), that uses ReinforcementLearning (RL) to actively examine a minimal number of features of a data pointto classify it with high accuracy. Furthermore, RLDT optimizes a long termreturn, providing a better alternative to the traditional myopic greedyapproach to growing decision trees. We demonstrate that this approach performsas well as batch learning algorithms and other online decision tree learningalgorithms, while making significantly fewer queries about the features of thedata points. We also show that RLDT can effectively handle concept drift.",http://arxiv.org/abs/1507.06923v1,,
1874,"Conspiracies between Learning Algorithms, Circuit Lower Bounds and  Pseudorandomness","We prove several results giving new and stronger connections betweenlearning, circuit lower bounds and pseudorandomness. Among other results, weshow a generic learning speedup lemma, equivalences between various learningmodels in the exponential time and subexponential time regimes, a dichotomybetween learning and pseudorandomness, consequences of non-trivial learning forcircuit lower bounds, Karp-Lipton theorems for probabilistic exponential time,and NC$^1$-hardness for the Minimum Circuit Size Problem.",http://arxiv.org/abs/1611.01190v1,,
1875,"Meta-Unsupervised-Learning: A supervised approach to unsupervised  learning","We introduce a new paradigm to investigate unsupervised learning, reducingunsupervised learning to supervised learning. Specifically, we mitigate thesubjectivity in unsupervised decision-making by leveraging knowledge acquiredfrom prior, possibly heterogeneous, supervised learning tasks. We demonstratethe versatility of our framework via comprehensive expositions and detailedexperiments on several unsupervised problems such as (a) clustering, (b)outlier detection, and (c) similarity prediction under a common umbrella ofmeta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds toestablish the theoretical foundations of our framework, and show that ourframing of meta-clustering circumvents Kleinberg's impossibility theorem forclustering.",http://arxiv.org/abs/1612.09030v2,,
1876,"Multiple Instance Learning with the Optimal Sub-Pattern Assignment  Metric","Multiple instance data are sets or multi-sets of unordered elements. Usingmetrics or distances for sets, we propose an approach to several multipleinstance learning tasks, such as clustering (unsupervised learning),classification (supervised learning), and novelty detection (semi-supervisedlearning). In particular, we introduce the Optimal Sub-Pattern Assignmentmetric to multiple instance learning so as to provide versatile design choices.Numerical experiments on both simulated and real data are presented toillustrate the versatility of the proposed solution.",http://arxiv.org/abs/1703.08933v1,,
1877,A General Memory-Bounded Learning Algorithm,"In an era of big data there is a growing need for memory-bounded learningalgorithms. In the last few years researchers have investigated what cannot belearned under memory constraints. In this paper we focus on the complementaryquestion of what can be learned under memory constraints. We show that if ahypothesis class fulfills a combinatorial condition defined in this paper,there is a memory-bounded learning algorithm for this class. We prove thatcertain natural classes fulfill this combinatorial property and thus can belearned under memory constraints.",http://arxiv.org/abs/1712.03524v1,,
1878,Deep Reinforcement Learning Boosted by External Knowledge,"Recent improvements in deep reinforcement learning have allowed to solveproblems in many 2D domains such as Atari games. However, in complex 3Denvironments, numerous learning episodes are required which may be too timeconsuming or even impossible especially in real-world scenarios. We present anew architecture to combine external knowledge and deep reinforcement learningusing only visual input. A key concept of our system is augmenting image inputby adding environment feature information and combining two sources ofdecision. We evaluate the performances of our method in a 3Dpartially-observable environment from the Microsoft Malmo platform.Experimental evaluation exhibits higher performance and faster learningcompared to a single reinforcement learning model.",http://arxiv.org/abs/1712.04101v1,,
1879,Lifelong Learning for Sentiment Classification,"This paper proposes a novel lifelong learning (LL) approach to sentimentclassification. LL mimics the human continuous learning process, i.e.,retaining the knowledge learned from past tasks and use it to help futurelearning. In this paper, we first discuss LL in general and then LL forsentiment classification in particular. The proposed LL approach adopts aBayesian optimization framework based on stochastic gradient descent. Ourexperimental results show that the proposed method outperforms baseline methodssignificantly, which demonstrates that lifelong learning is a promisingresearch direction.",http://arxiv.org/abs/1801.02808v1,,
1880,"Analysis of Fast Alternating Minimization for Structured Dictionary  Learning","Methods exploiting sparsity have been popular in imaging and signalprocessing applications including compression, denoising, and imaging inverseproblems. Data-driven approaches such as dictionary learning and transformlearning enable one to discover complex image features from datasets andprovide promising performance over analytical models. Alternating minimizationalgorithms have been particularly popular in dictionary or transform learning.In this work, we study the properties of alternating minimization forstructured (unitary) sparsifying operator learning. While the algorithmconverges to the stationary points of the non-convex problem in general, weprove rapid local linear convergence to the underlying generative model undermild assumptions. Our experiments show that the unitary operator learningalgorithm is robust to initialization.",http://arxiv.org/abs/1802.00518v1,,
1881,Towards continual learning in medical imaging,"This work investigates continual learning of two segmentation tasks in brainMRI with neural networks. To explore in this context the capabilities ofcurrent methods for countering catastrophic forgetting of the first task when anew one is learned, we investigate elastic weight consolidation, a recentlyproposed method based on Fisher information, originally evaluated onreinforcement learning of Atari games. We use it to sequentially learnsegmentation of normal brain structures and then segmentation of white matterlesions. Our findings show this recent method reduces catastrophic forgetting,while large room for improvement exists in these challenging settings forcontinual learning.",http://arxiv.org/abs/1811.02496v1,,
1882,Environments for Lifelong Reinforcement Learning,"To achieve general artificial intelligence, reinforcement learning (RL)agents should learn not only to optimize returns for one specific task but alsoto constantly build more complex skills and scaffold their knowledge about theworld, without forgetting what has already been learned. In this paper, wediscuss the desired characteristics of environments that can support thetraining and evaluation of lifelong reinforcement learning agents, reviewexisting environments from this perspective, and propose recommendations fordevising suitable environments in the future.",http://arxiv.org/abs/1811.10732v2,,
1883,Learning Ontologies with Epistemic Reasoning: The EL Case,"We investigate the problem of learning description logic ontologies fromentailments via queries, using epistemic reasoning. We introduce a new learningmodel consisting of epistemic membership and example queries and show thatpolynomial learnability in this model coincides with polynomial learnability inAngluin's exact learning model with membership and equivalence queries. We theninstantiate our learning framework to EL and show some complexity results foran epistemic extension of EL where epistemic operators can be applied over theaxioms. Finally, we transfer known results for EL ontologies and its fragmentsto our learning model based on epistemic reasoning.",http://arxiv.org/abs/1902.03273v1,,
1884,Differentially Private Empirical Risk Minimization,"Privacy-preserving machine learning algorithms are crucial for theincreasingly common setting in which personal data, such as medical orfinancial records, are analyzed. We provide general techniques to produceprivacy-preserving approximations of classifiers learned via (regularized)empirical risk minimization (ERM). These algorithms are private under the$\epsilon$-differential privacy definition due to Dwork et al. (2006). First weapply the output perturbation ideas of Dwork et al. (2006), to ERMclassification. Then we propose a new method, objective perturbation, forprivacy-preserving machine learning algorithm design. This method entailsperturbing the objective function before optimizing over classifiers. If theloss and regularizer satisfy certain convexity and differentiability criteria,we prove theoretical results showing that our algorithms preserve privacy, andprovide generalization bounds for linear and nonlinear kernels. We furtherpresent a privacy-preserving technique for tuning the parameters in generalmachine learning algorithms, thereby providing end-to-end privacy guaranteesfor the training process. We apply these results to produce privacy-preservinganalogues of regularized logistic regression and support vector machines. Weobtain encouraging results from evaluating their performance on realdemographic and benchmark data sets. Our results show that both theoreticallyand empirically, objective perturbation is superior to the previousstate-of-the-art, output perturbation, in managing the inherent tradeoffbetween privacy and learning performance.",http://arxiv.org/abs/0912.0071v5,,
1885,"Benchmark of structured machine learning methods for microbial  identification from mass-spectrometry data","Microbial identification is a central issue in microbiology, in particular inthe fields of infectious diseases diagnosis and industrial quality control. Theconcept of species is tightly linked to the concept of biological and clinicalclassification where the proximity between species is generally measured interms of evolutionary distances and/or clinical phenotypes. Surprisingly, theinformation provided by this well-known hierarchical structure is rarely usedby machine learning-based automatic microbial identification systems.Structured machine learning methods were recently proposed for taking intoaccount the structure embedded in a hierarchy and using it as additional apriori information, and could therefore allow to improve microbialidentification systems. We test and compare several state-of-the-art machinelearning methods for microbial identification on a new Matrix-Assisted LaserDesorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.We include in the benchmark standard and structured methods, that leverage theknowledge of the underlying hierarchical structure in the learning process. Ourresults show that although some methods perform better than others, structuredmethods do not consistently perform better than their ""flat"" counterparts. Wepostulate that this is partly due to the fact that standard methods alreadyreach a high level of accuracy in this context, and that they mainly confusespecies close to each other in the tree, a case where using the known hierarchyis not helpful.",http://arxiv.org/abs/1506.07251v1,,
1886,"Regularized Dynamic Boltzmann Machine with Delay Pruning for  Unsupervised Learning of Temporal Sequences","We introduce Delay Pruning, a simple yet powerful technique to regularizedynamic Boltzmann machines (DyBM). The recently introduced DyBM provides aparticularly structured Boltzmann machine, as a generative model of amulti-dimensional time-series. This Boltzmann machine can have infinitely manylayers of units but allows exact inference and learning based on itsbiologically motivated structure. DyBM uses the idea of conduction delays inthe form of fixed length first-in first-out (FIFO) queues, with a neuronconnected to another via this FIFO queue, and spikes from a pre-synaptic neurontravel along the queue to the post-synaptic neuron with a constant period ofdelay. Here, we present Delay Pruning as a mechanism to prune the lengths ofthe FIFO queues (making them zero) by setting some delay lengths to one with afixed probability, and finally selecting the best performing model with fixeddelays. The uniqueness of structure and a non-sampling based learning rule inDyBM, make the application of previously proposed regularization techniqueslike Dropout or DropConnect difficult, leading to poor generalization. First,we evaluate the performance of Delay Pruning to let DyBM learn amultidimensional temporal sequence generated by a Markov chain. Finally, weshow the effectiveness of delay pruning in learning high dimensional sequencesusing the moving MNIST dataset, and compare it with Dropout and DropConnectmethods.",http://arxiv.org/abs/1610.01989v1,,
1887,"Privacy Risk in Machine Learning: Analyzing the Connection to  Overfitting","Machine learning algorithms, when applied to sensitive data, pose a distinctthreat to privacy. A growing body of prior work demonstrates that modelsproduced by these algorithms may leak specific private information in thetraining data to an attacker, either through the models' structure or theirobservable behavior. However, the underlying cause of this privacy risk is notwell understood beyond a handful of anecdotal accounts that suggest overfittingand influence might play a role.  This paper examines the effect that overfitting and influence have on theability of an attacker to learn information about the training data frommachine learning models, either through training set membership inference orattribute inference attacks. Using both formal and empirical analyses, weillustrate a clear relationship between these factors and the privacy risk thatarises in several popular machine learning algorithms. We find that overfittingis sufficient to allow an attacker to perform membership inference and, whenthe target attribute meets certain conditions about its influence, attributeinference attacks. Interestingly, our formal analysis also shows thatoverfitting is not necessary for these attacks and begins to shed light on whatother factors may be in play. Finally, we explore the connection betweenmembership inference and attribute inference, showing that there are deepconnections between the two that lead to effective new attacks.",http://arxiv.org/abs/1709.01604v5,,
1888,"Machine Learning by Unitary Tensor Network of Hierarchical Tree  Structure","The resemblance between the methods used in quantum-many body physics and inmachine learning has drawn considerable attention. In particular, tensornetworks (TNs) and deep learning architectures bear striking similarities tothe extent that TNs can be used for machine learning. Previous results usedone-dimensional TNs in image recognition, showing limited scalability andflexibilities. In this work, we train two-dimensional hierarchical TNs to solveimage recognition problems, using a training algorithm derived from themulti-scale entanglement renormalization ansatz. This approach introducesmathematical connections among quantum many-body physics, quantum informationtheory, and machine learning. While keeping the TN unitary in the trainingphase, TN states are defined, which encode classes of images into quantummany-body states. We study the quantum features of the TN states, includingquantum entanglement and fidelity. We find these quantities could be propertiesthat characterize the image classes, as well as the machine learning tasks.",http://arxiv.org/abs/1710.04833v4,,
1889,"Automated Process Incorporating Machine Learning Segmentation and  Correlation of Oral Diseases with Systemic Health","Imaging fluorescent disease biomarkers in tissues and skin is a non-invasivemethod to screen for health conditions. We report an automated process thatcombines intraoral fluorescent porphyrin biomarker imaging, clinicalexaminations and machine learning for correlation of systemic health conditionswith periodontal disease. 1215 intraoral fluorescent images, from 284consenting adults aged 18-90, were analyzed using a machine learning classifierthat can segment periodontal inflammation. The classifier achieved an AUC of0.677 with precision and recall of 0.271 and 0.429, respectively, indicating alearned association between disease signatures in collected images. Periodontaldiseases were more prevalent among males (p=0.0012) and older subjects(p=0.0224) in the screened population. Physicians independently examined thecollected images, assigning localized modified gingival indices (MGIs). MGIsand periodontal disease were then cross-correlated with responses to a medicalhistory questionnaire, blood pressure and body mass index measurements, andoptic nerve, tympanic membrane, neurological, and cardiac rhythm imagingexaminations. Gingivitis and early periodontal disease were associated withsubjects diagnosed with optic nerve abnormalities (p <0.0001) in their retinalscans. We also report significant co-occurrences of periodontal disease insubjects reporting swollen joints (p=0.0422) and a family history of eyedisease (p=0.0337). These results indicate cross-correlation of poorperiodontal health with systemic health outcomes and stress the importance oforal health screenings at the primary care level. Our screening process andanalysis method, using images and machine learning, can be generalized forautomated diagnoses and systemic health screenings for other diseases.",http://arxiv.org/abs/1810.10664v1,,
1890,"Band gap prediction for large organic crystal structures with machine  learning","Large datasets of ab initio calculations have enabled many pioneering studiesof machine learning applied to quantum-chemical systems. For example, machinelearning models already achieve chemical accuracy on the popular QM9 datasetwith small organic molecules. Here, we present a new, more challenging datasetof 12,500 large organic crystal structures and their corresponding DFT bandgap, freely available at https://omdb.diracmaterials.org/dataset. Thecomplexity of the organic crystals in this dataset, which have on average 85atoms per unit cell, makes it a challenging platform for machine learningapplications. We run two recent machine learning models, kernel ridgeregression with the Smooth Overlap of Atomic Positions (SOAP) kernel and thedeep learning model SchNet, on this new dataset and find that an ensemble ofthese two models reaches mean absolute error (MAE) of 0.361 eV, whichcorresponds to a percentage error of 12% on the average band gap of 3.03 eV.The models also provide chemical insights into the data. For example, byvisualizing the SOAP kernel similarity between the crystals, different clustersof materials can be identified, such as organic metals or semiconductors.Finally, the trained models are employed to predict the band gap for 260,092materials contained within the Crystallography Open Database (COD) and madeavailable online so the predictions can be obtained for any arbitrary crystalstructure uploaded by a user.",http://arxiv.org/abs/1810.12814v3,,
1891,"Learning the progression and clinical subtypes of Alzheimer's disease  from longitudinal clinical data","Alzheimer's disease (AD) is a degenerative brain disease impairing a person'sability to perform day to day activities. The clinical manifestations ofAlzheimer's disease are characterized by heterogeneity in age, disease span,progression rate, impairment of memory and cognitive abilities. Due to thesevariabilities, personalized care and treatment planning, as well as patientcounseling about their individual progression is limited. Recent developmentsin machine learning to detect hidden patterns in complex, multi-dimensionaldatasets provides significant opportunities to address this critical need. Inthis work, we use unsupervised and supervised machine learning approaches forsubtype identification and prediction. We apply machine learning methods to theextensive clinical observations available at the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) data set to identify patient subtypes and topredict disease progression. Our analysis depicts the progression space for theAlzheimer's disease into low, moderate and high disease progression zones. Theproposed work will enable early detection and characterization of distinctdisease subtypes based on clinical heterogeneity. We anticipate that our modelswill enable patient counseling, clinical trial design, and ultimatelyindividualized clinical care.",http://arxiv.org/abs/1812.00546v3,,
1892,Reconciling modern machine learning and the bias-variance trade-off,"The question of generalization in machine learning---how algorithms are ableto learn predictors from a training sample to make accurate predictionsout-of-sample---is revisited in light of the recent breakthroughs in modernmachine learning technology.  The classical approach to understanding generalization is based onbias-variance trade-offs, where model complexity is carefully calibrated sothat the fit on the training sample reflects performance out-of-sample.  However, it is now common practice to fit highly complex models like deepneural networks to data with (nearly) zero training error, and yet theseinterpolating predictors are observed to have good out-of-sample accuracy evenfor noisy data.  How can the classical understanding of generalization be reconciled withthese observations from modern machine learning practice?  In this paper, we bridge the two regimes by exhibiting a new ""double descent""risk curve that extends the traditional U-shaped bias-variance curve beyond thepoint of interpolation.  Specifically, the curve shows that as soon as the model complexity is highenough to achieve interpolation on the training sample---a point that we callthe ""interpolation threshold""---the risk of suitably chosen interpolatingpredictors from these models can, in fact, be decreasing as the modelcomplexity increases, often below the risk achieved using non-interpolatingmodels.  The double descent risk curve is demonstrated for a broad range of models,including neural networks and random forests, and a mechanism for producingthis behavior is posited.",http://arxiv.org/abs/1812.11118v1,,
1893,"Generative Adversarial Networks for Black-Box API Attacks with Limited  Training Data","As online systems based on machine learning are offered to public or paidsubscribers via application programming interfaces (APIs), they becomevulnerable to frequent exploits and attacks. This paper studies adversarialmachine learning in the practical case when there are rate limitations on APIcalls. The adversary launches an exploratory (inference) attack by querying theAPI of an online machine learning system (in particular, a classifier) withinput data samples, collecting returned labels to build up the training data,and training an adversarial classifier that is functionally equivalent andstatistically close to the target classifier. The exploratory attack withlimited training data is shown to fail to reliably infer the target classifierof a real text classifier API that is available online to the public. Inreturn, a generative adversarial network (GAN) based on deep learning is builtto generate synthetic training data from a limited number of real training datasamples, thereby extending the training data and improving the performance ofthe inferred classifier. The exploratory attack provides the basis to launchthe causative attack (that aims to poison the training process) and evasionattack (that aims to fool the classifier into making wrong decisions) byselecting training and test data samples, respectively, based on the confidencescores obtained from the inferred classifier. These stealth attacks with smallfootprint (using a small number of API calls) make adversarial machine learningpractical under the realistic case with limited training data available to theadversary.",http://arxiv.org/abs/1901.09113v1,,
1894,Spectrum Data Poisoning with Adversarial Deep Learning,"Machine learning has been widely applied in wireless communications. However,the security aspects of machine learning in wireless applications have not beenwell understood yet. We consider the case that a cognitive transmitter sensesthe spectrum and transmits on idle channels determined by a machine learningalgorithm. We present an adversarial machine learning approach to launch aspectrum data poisoning attack by inferring the transmitter's behavior andattempting to falsify the spectrum sensing data over the air. For that purpose,the adversary transmits for a short period of time when the channel is idle tomanipulate the input for the decision mechanism of the transmitter. Thecognitive engine at the transmitter is a deep neural network model thatpredicts idle channels with minimum sensing error for data transmissions. Thetransmitter collects spectrum sensing data and uses it as the input to itsmachine learning algorithm. In the meantime, the adversary builds a cognitiveengine using another deep neural network model to predict when the transmitterwill have a successful transmission based on its spectrum sensing data. Theadversary then performs the over-the-air spectrum data poisoning attack, whichaims to change the channel occupancy status from idle to busy when thetransmitter is sensing, so that the transmitter is fooled into making incorrecttransmit decisions. This attack is more energy efficient and harder to detectcompared to jamming of data transmissions. We show that this attack is veryeffective and reduces the throughput of the transmitter substantially.",http://arxiv.org/abs/1901.09247v1,,
1895,"When Relaxations Go Bad: ""Differentially-Private"" Machine Learning","Differential privacy is becoming a standard notion for performingprivacy-preserving machine learning over sensitive data. It provides formalguarantees, in terms of the privacy budget, $\epsilon$, on how much informationabout individual training records is leaked by the model. While the privacybudget is directly correlated to the privacy leakage, the calibration of theprivacy budget is not well understood. As a result, many existing works onprivacy-preserving machine learning select large values of $\epsilon$ in orderto get acceptable utility of the model, with little understanding of theconcrete impact of such choices on meaningful privacy. Moreover, in scenarioswhere iterative learning procedures are used which require privacy guaranteesfor each iteration, relaxed definitions of differential privacy are often usedwhich further tradeoff privacy for better utility. In this paper, we evaluatethe impacts of these choices on privacy in experiments with logistic regressionand neural network models. We quantify the privacy leakage in terms ofadvantage of the adversary performing inference attacks and by analyzing thenumber of members at risk for exposure. Our main findings are that currentmechanisms for differential privacy for machine learning rarely offeracceptable utility-privacy tradeoffs: settings that provide limited accuracyloss provide little effective privacy, and settings that provide strong privacyresult in useless models. Open source code is available athttps://github.com/bargavj/EvaluatingDPML.",http://arxiv.org/abs/1902.08874v2,,
1896,"Learning with incomplete information - and the mathematical structure  behind it","Learning and the ability to learn are important factors in development andevolutionary processes [1]. Depending on the level, the complexity of learningcan strongly vary. While associative learning can explain simple learningbehaviour [1,2] much more sophisticated strategies seem to be involved incomplex learning tasks. This is particularly evident in machine learning theory[3] (reinforcement learning [4], statistical learning [5]), but it equallyshows up in trying to model natural learning behaviour [2]. A general settingfor modelling learning processes in which statistical aspects are relevant isprovided by the neural network (NN) paradigm. This is in particular of interestfor natural, learning by experience situations. NN learning models canincorporate elementary learning mechanisms based on neuro-physiologicalanalogies, such as the Hebb rule, and lead to quantitative results concerningthe dynamics of the learning process [6]. The Hebb rule, however, cannot bedirectly applied in all cases, and in particular for realistic problems, suchas ""delayed reinforcement"" [4,6], the sophistication of the algorithms rapidlyincreases. We want to present here a model which can cope with such non trivialtasks, while still being elementary and based only on procedures which one maythink of as natural, without any appeal to higher strategies [7]. We can showthe capability of this model to provide good learning in many, very differentsettings [7,8,9]. It may help therefore understanding some basic features oflearning.",http://arxiv.org/abs/q-bio/0608023v1,,
1897,Learning From An Optimization Viewpoint,"In this dissertation we study statistical and online learning problems froman optimization viewpoint.The dissertation is divided into two parts :  I. We first consider the question of learnability for statistical learningproblems in the general learning setting. The question of learnability is wellstudied and fully characterized for binary classification and for real valuedsupervised learning problems using the theory of uniform convergence. Howeverwe show that for the general learning setting uniform convergence theory failsto characterize learnability. To fill this void we use stability of learningalgorithms to fully characterize statistical learnability in the generalsetting. Next we consider the problem of online learning. Unlike thestatistical learning framework there is a dearth of generic tools that can beused to establish learnability and rates for online learning problems ingeneral. We provide online analogs to classical tools from statistical learningtheory like Rademacher complexity, covering numbers, etc. We further use thesetools to fully characterize learnability for online supervised learningproblems.  II. In the second part, for general classes of convex learning problems, weprovide appropriate mirror descent (MD) updates for online and statisticallearning of these problems. Further, we show that the the MD is near optimalfor online convex learning and for most cases, is also near optimal forstatistical convex learning. We next consider the problem of convexoptimization and show that oracle complexity can be lower bounded by the socalled fat-shattering dimension of the associated linear class. Thus weestablish a strong connection between offline convex optimization problems andstatistical learning problems. We also show that for a large class of highdimensional optimization problems, MD is in fact near optimal even for convexoptimization.",http://arxiv.org/abs/1204.4145v1,,
1898,Cue Phrase Classification Using Machine Learning,"Cue phrases may be used in a discourse sense to explicitly signal discoursestructure, but also in a sentential sense to convey semantic rather thanstructural information. Correctly classifying cue phrases as discourse orsentential is critical in natural language processing systems that exploitdiscourse structure, e.g., for performing tasks such as anaphora resolution andplan recognition. This paper explores the use of machine learning forclassifying cue phrases as discourse or sentential. Two machine learningprograms (Cgrendel and C4.5) are used to induce classification models from setsof pre-classified cue phrases and their features in text and speech. Machinelearning is shown to be an effective technique for not only automating thegeneration of classification models, but also for improving upon previousresults. When compared to manually derived classification models already in theliterature, the learned models often perform with higher accuracy and containnew linguistic insights into the data. In addition, the ability toautomatically construct classification models makes it easier to comparativelyanalyze the utility of alternative feature representations of the data.Finally, the ease of retraining makes the learning approach more scalable andflexible than manual methods.",http://arxiv.org/abs/cmp-lg/9609003v1,,
1899,Cue Phrase Classification Using Machine Learning,"Cue phrases may be used in a discourse sense to explicitly signal discoursestructure, but also in a sentential sense to convey semantic rather thanstructural information. Correctly classifying cue phrases as discourse orsentential is critical in natural language processing systems that exploitdiscourse structure, e.g., for performing tasks such as anaphora resolution andplan recognition. This paper explores the use of machine learning forclassifying cue phrases as discourse or sentential. Two machine learningprograms (Cgrendel and C4.5) are used to induce classification models from setsof pre-classified cue phrases and their features in text and speech. Machinelearning is shown to be an effective technique for not only automating thegeneration of classification models, but also for improving upon previousresults. When compared to manually derived classification models already in theliterature, the learned models often perform with higher accuracy and containnew linguistic insights into the data. In addition, the ability toautomatically construct classification models makes it easier to comparativelyanalyze the utility of alternative feature representations of the data.Finally, the ease of retraining makes the learning approach more scalable andflexible than manual methods.",http://arxiv.org/abs/cs/9609102v1,,
1900,"Concept of E-machine: How does a ""dynamical"" brain learn to process  ""symbolic"" information? Part I","The human brain has many remarkable information processing characteristicsthat deeply puzzle scientists and engineers. Among the most important and themost intriguing of these characteristics are the brain's broad universality asa learning system and its mysterious ability to dynamically change(reconfigure) its behavior depending on a combinatorial number of differentcontexts.  This paper discusses a class of hypothetically brain-like dynamicallyreconfigurable associative learning systems that shed light on the possiblenature of these brain's properties. The systems are arranged on the generalprinciple referred to as the concept of E-machine.  The paper addresses the following questions:  1. How can ""dynamical"" neural networks function as universal programmable""symbolic"" machines?  2. What kind of a universal programmable symbolic machine can formarbitrarily complex software in the process of programming similar to theprocess of biological associative learning?  3. How can a universal learning machine dynamically reconfigure its softwaredepending on a combinatorial number of possible contexts?",http://arxiv.org/abs/cs/0403031v2,,
1901,"Equations of States in Statistical Learning for a Nonparametrizable and  Regular Case","Many learning machines that have hierarchical structure or hidden variablesare now being used in information science, artificial intelligence, andbioinformatics. However, several learning machines used in such fields are notregular but singular statistical models, hence their generalization performanceis still left unknown. To overcome these problems, in the previous papers, weproved new equations in statistical learning, by which we can estimate theBayes generalization loss from the Bayes training loss and the functionalvariance, on the condition that the true distribution is a singularitycontained in a learning machine. In this paper, we prove that the sameequations hold even if a true distribution is not contained in a parametricmodel. Also we prove that, the proposed equations in a regular case areasymptotically equivalent to the Takeuchi information criterion. Therefore, theproposed equations are always applicable without any condition on the unknowntrue distribution.",http://arxiv.org/abs/0906.0211v2,,
1902,"A submodular-supermodular procedure with applications to discriminative  structure learning","In this paper, we present an algorithm for minimizing the difference betweentwo submodular functions using a variational framework which is based on (anextension of) the concave-convex procedure [17]. Because several commonly usedmetrics in machine learning, like mutual information and conditional mutualinformation, are submodular, the problem of minimizing the difference of twosubmodular problems arises naturally in many machine learning applications. Twosuch applications are learning discriminatively structured graphical models andfeature selection under computational complexity constraints. A commonly usedmetric for measuring discriminative capacity is the EAR measure which is thedifference between two conditional mutual information terms. Feature selectiontaking complexity considerations into account also fall into this frameworkbecause both the information that a set of features provide and the cost ofcomputing and using the features can be modeled as submodular functions. Thisproblem is NP-hard, and we give a polynomial time heuristic for it. We alsopresent results on synthetic data to show that classifiers based ondiscriminative graphical models using this algorithm can significantlyoutperform classifiers based on generative graphical models.",http://arxiv.org/abs/1207.1404v1,,
1903,Mixture Representations for Inference and Learning in Boltzmann Machines,"Boltzmann machines are undirected graphical models with two-state stochasticvariables, in which the logarithms of the clique potentials are quadraticfunctions of the node states. They have been widely studied in the neuralcomputing literature, although their practical applicability has been limitedby the difficulty of finding an effective learning algorithm. Onewell-established approach, known as mean field theory, represents thestochastic distribution using a factorized approximation. However, thecorresponding learning algorithm often fails to find a good solution. Weconjecture that this is due to the implicit uni-modality of the mean fieldapproximation which is therefore unable to capture multi-modality in the truedistribution. In this paper we use variational methods to approximate thestochastic distribution using multi-modal mixtures of factorized distributions.We present results for both inference and learning to demonstrate theeffectiveness of this approach.",http://arxiv.org/abs/1301.7393v1,,
1904,Binary Classifier Calibration: Bayesian Non-Parametric Approach,"A set of probabilistic predictions is well calibrated if the events that arepredicted to occur with probability p do in fact occur about p fraction of thetime. Well calibrated predictions are particularly important when machinelearning models are used in decision analysis. This paper presents two newnon-parametric methods for calibrating outputs of binary classification models:a method based on the Bayes optimal selection and a method based on theBayesian model averaging. The advantage of these methods is that they areindependent of the algorithm used to learn a predictive model, and they can beapplied in a post-processing step, after the model is learned. This makes themapplicable to a wide variety of machine learning models and methods. Thesecalibration methods, as well as other methods, are tested on a variety ofdatasets in terms of both discrimination and calibration performance. Theresults show the methods either outperform or are comparable in performance tothe state-of-the-art calibration methods.",http://arxiv.org/abs/1401.2955v1,,
1905,An introduction to quantum machine learning,"Machine learning algorithms learn a desired input-output relation fromexamples in order to interpret new inputs. This is important for tasks such asimage and speech recognition or strategy optimisation, with growingapplications in the IT industry. In the last couple of years, researchersinvestigated if quantum computing can help to improve classical machinelearning algorithms. Ideas range from running computationally costly algorithmsor their subroutines efficiently on a quantum computer to the translation ofstochastic methods into the language of quantum theory. This contribution givesa systematic overview of the emerging field of quantum machine learning. Itpresents the approaches as well as technical details in an accessable way, anddiscusses the potential of a future theory of quantum learning.",http://arxiv.org/abs/1409.3097v1,,
1906,A kernel-based framework for learning graded relations from data,"Driven by a large number of potential applications in areas likebioinformatics, information retrieval and social network analysis, the problemsetting of inferring relations between pairs of data objects has recently beeninvestigated quite intensively in the machine learning community. To this end,current approaches typically consider datasets containing crisp relations, sothat standard classification methods can be adopted. However, relations betweenobjects like similarities and preferences are often expressed in a gradedmanner in real-world applications. A general kernel-based framework forlearning relations from data is introduced here. It extends existing approachesbecause both crisp and graded relations are considered, and it unifies existingapproaches because different types of graded relations can be modeled,including symmetric and reciprocal relations. This framework establishesimportant links between recent developments in fuzzy set theory and machinelearning. Its usefulness is demonstrated through various experiments onsynthetic and real-world data.",http://arxiv.org/abs/1111.6473v1,,
1907,SCOPE: Scalable Composite Optimization for Learning on Spark,"Many machine learning models, such as logistic regression~(LR) and supportvector machine~(SVM), can be formulated as composite optimization problems.Recently, many distributed stochastic optimization~(DSO) methods have beenproposed to solve the large-scale composite optimization problems, which haveshown better performance than traditional batch methods. However, most of theseDSO methods are not scalable enough. In this paper, we propose a novel DSOmethod, called \underline{s}calable \underline{c}omposite\underline{op}timization for l\underline{e}arning~({SCOPE}), and implement iton the fault-tolerant distributed platform \mbox{Spark}. SCOPE is bothcomputation-efficient and communication-efficient. Theoretical analysis showsthat SCOPE is convergent with linear convergence rate when the objectivefunction is convex. Furthermore, empirical results on real datasets show thatSCOPE can outperform other state-of-the-art distributed learning methods onSpark, including both batch learning methods and DSO methods.",http://arxiv.org/abs/1602.00133v5,,
1908,Auxiliary Deep Generative Models,"Deep generative models parameterized by neural networks have recentlyachieved state-of-the-art performance in unsupervised and semi-supervisedlearning. We extend deep generative models with auxiliary variables whichimproves the variational approximation. The auxiliary variables leave thegenerative model unchanged but make the variational distribution moreexpressive. Inspired by the structure of the auxiliary variable we also proposea model with two stochastic layers and skip connections. Our findings suggestthat more expressive and properly specified deep generative models convergefaster with better results. We show state-of-the-art performance withinsemi-supervised learning on MNIST, SVHN and NORB datasets.",http://arxiv.org/abs/1602.05473v4,,
1909,Interpretable Two-level Boolean Rule Learning for Classification,"As a contribution to interpretable machine learning research, we develop anovel optimization framework for learning accurate and sparse two-level Booleanrules. We consider rules in both conjunctive normal form (AND-of-ORs) anddisjunctive normal form (OR-of-ANDs). A principled objective function isproposed to trade classification accuracy and interpretability, where we useHamming loss to characterize accuracy and sparsity to characterizeinterpretability. We propose efficient procedures to optimize these objectivesbased on linear programming (LP) relaxation, block coordinate descent, andalternating minimization. Experiments show that our new algorithms provide verygood tradeoffs between accuracy and interpretability.",http://arxiv.org/abs/1606.05798v1,,
1910,A Generative Process for Sampling Contractive Auto-Encoders,"The contractive auto-encoder learns a representation of the input data thatcaptures the local manifold structure around each data point, through theleading singular vectors of the Jacobian of the transformation from input torepresentation. The corresponding singular values specify how much localvariation is plausible in directions associated with the corresponding singularvectors, while remaining in a high-density region of the input space. Thispaper proposes a procedure for generating samples that are consistent with thelocal structure captured by a contractive auto-encoder. The associatedstochastic process defines a distribution from which one can sample, and whichexperimentally appears to converge quickly and mix well between modes, comparedto Restricted Boltzmann Machines and Deep Belief Networks. The intuitionsbehind this procedure can also be used to train the second layer of contractionthat pools lower-level features and learns to be invariant to the localdirections of variation discovered in the first layer. We show that this canhelp learn and represent invariances present in the data and improveclassification error.",http://arxiv.org/abs/1206.6434v1,,
1911,Bethe Learning of Conditional Random Fields via MAP Decoding,"Many machine learning tasks can be formulated in terms of predictingstructured outputs. In frameworks such as the structured support vector machine(SVM-Struct) and the structured perceptron, discriminative functions arelearned by iteratively applying efficient maximum a posteriori (MAP) decoding.However, maximum likelihood estimation (MLE) of probabilistic models over thesesame structured spaces requires computing partition functions, which isgenerally intractable. This paper presents a method for learning discreteexponential family models using the Bethe approximation to the MLE. Remarkably,this problem also reduces to iterative (MAP) decoding. This connection emergesby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on aconvex dual objective which circumvents the intractable partition function. Theresult is a new single loop algorithm MLE-Struct, which is substantially moreefficient than previous double-loop methods for approximate maximum likelihoodestimation. Our algorithm outperforms existing methods in experiments involvingimage segmentation, matching problems from vision, and a new dataset ofuniversity roommate assignments.",http://arxiv.org/abs/1503.01228v1,,
1912,Dynamic Privacy For Distributed Machine Learning Over Network,"Privacy-preserving distributed machine learning becomes increasinglyimportant due to the recent rapid growth of data. This paper focuses on a classof regularized empirical risk minimization (ERM) machine learning problems, anddevelops two methods to provide differential privacy to distributed learningalgorithms over a network. We first decentralize the learning algorithm usingthe alternating direction method of multipliers (ADMM), and propose the methodsof dual variable perturbation and primal variable perturbation to providedynamic differential privacy. The two mechanisms lead to algorithms that canprovide privacy guarantees under mild conditions of the convexity anddifferentiability of the loss function and the regularizer. We study theperformance of the algorithms, and show that the dual variable perturbationoutperforms its primal counterpart. To design an optimal privacy mechanisms, weanalyze the fundamental tradeoff between privacy and accuracy, and provideguidelines to choose privacy parameters. Numerical experiments using customerinformation database are performed to corroborate the results on privacy andutility tradeoffs and design.",http://arxiv.org/abs/1601.03466v3,,
1913,"Without-Replacement Sampling for Stochastic Gradient Methods:  Convergence Results and Application to Distributed Optimization","Stochastic gradient methods for machine learning and optimization problemsare usually analyzed assuming data points are sampled \emph{with} replacement.In practice, however, sampling \emph{without} replacement is very common,easier to implement in many cases, and often performs better. In this paper, weprovide competitive convergence guarantees for without-replacement sampling,under various scenarios, for three types of algorithms: Any algorithm withonline regret guarantees, stochastic gradient descent, and SVRG. A usefulapplication of our SVRG analysis is a nearly-optimal algorithm for regularizedleast squares in a distributed setting, in terms of both communicationcomplexity and runtime complexity, when the data is randomly partitioned andthe condition number can be as large as the data size per machine (up tologarithmic factors). Our proof techniques combine ideas from stochasticoptimization, adversarial online learning, and transductive learning theory,and can potentially be applied to other stochastic optimization and learningproblems.",http://arxiv.org/abs/1603.00570v3,,
1914,Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,"Performance of machine learning algorithms depends critically on identifyinga good set of hyperparameters. While recent approaches use Bayesianoptimization to adaptively select configurations, we focus on speeding uprandom search through adaptive resource allocation and early-stopping. Weformulate hyperparameter optimization as a pure-exploration non-stochasticinfinite-armed bandit problem where a predefined resource like iterations, datasamples, or features is allocated to randomly sampled configurations. Weintroduce a novel algorithm, Hyperband, for this framework and analyze itstheoretical properties, providing several desirable guarantees. Furthermore, wecompare Hyperband with popular Bayesian optimization methods on a suite ofhyperparameter optimization problems. We observe that Hyperband can provideover an order-of-magnitude speedup over our competitor set on a variety ofdeep-learning and kernel-based learning problems.",http://arxiv.org/abs/1603.06560v4,,
1915,On Generalization and Regularization in Deep Learning,"Why do large neural network generalize so well on complex tasks such as imageclassification or speech recognition? What exactly is the role regularizationfor them? These are arguably among the most important open questions in machinelearning today. In a recent and thought provoking paper [C. Zhang et al.]several authors performed a number of numerical experiments that hint at theneed for novel theoretical concepts to account for this phenomenon. The paperstirred quit a lot of excitement among the machine learning community but atthe same time it created some confusion as discussions on OpenReview.nettestifies. The aim of this pedagogical paper is to make this debate accessibleto a wider audience of data scientists without advanced theoretical knowledgein statistical learning. The focus here is on explicit mathematical definitionsand on a discussion of relevant concepts, not on proofs for which we providereferences.",http://arxiv.org/abs/1704.01312v2,,
1916,Constrained Extreme Learning Machines: A Study on Classification Cases,"Extreme learning machine (ELM) is an extremely fast learning method and has apowerful performance for pattern recognition tasks proven by enormousresearches and engineers. However, its good generalization ability is built onlarge numbers of hidden neurons, which is not beneficial to real time responsein the test process. In this paper, we proposed new ways, named ""constrainedextreme learning machines"" (CELMs), to randomly select hidden neurons based onsample distribution. Compared to completely random selection of hidden nodes inELM, the CELMs randomly select hidden nodes from the constrained vector spacecontaining some basic combinations of original sample vectors. The experimentalresults show that the CELMs have better generalization ability than traditionalELM, SVM and some other related methods. Additionally, the CELMs have a similarfast learning speed as ELM.",http://arxiv.org/abs/1501.06115v2,,
1917,Linear-time Learning on Distributions with Approximate Kernel Embeddings,"Many interesting machine learning problems are best posed by consideringinstances that are distributions, or sample sets drawn from distributions.Previous work devoted to machine learning tasks with distributional inputs hasdone so through pairwise kernel evaluations between pdfs (or sample sets).While such an approach is fine for smaller datasets, the computation of an $N\times N$ Gram matrix is prohibitive in large datasets. Recent scalableestimators that work over pdfs have done so only with kernels that useEuclidean metrics, like the $L_2$ distance. However, there are a myriad ofother useful metrics available, such as total variation, Hellinger distance,and the Jensen-Shannon divergence. This work develops the first random featuresfor pdfs whose dot product approximates kernels using these non-Euclideanmetrics, allowing estimators using such kernels to scale to large datasets byworking in a primal space, without computing large Gram matrices. We provide ananalysis of the approximation error in using our proposed random features andshow empirically the quality of our approximation both in estimating a Grammatrix and in solving learning tasks in real-world and synthetic data.",http://arxiv.org/abs/1509.07553v1,,
1918,Machine Learned Resume-Job Matching Solution,"Job search through online matching engines nowadays are very prominent andbeneficial to both job seekers and employers. But the solutions of traditionalengines without understanding the semantic meanings of different resumes havenot kept pace with the incredible changes in machine learning techniques andcomputing capability. These solutions are usually driven by manual rules andpredefined weights of keywords which lead to an inefficient and frustratingsearch experience. To this end, we present a machine learned solution with richfeatures and deep learning methods. Our solution includes three configurablemodules that can be plugged with little restrictions. Namely, unsupervisedfeature extraction, base classifiers training and ensemble method learning. Inour solution, rather than using manual rules, machine learned methods toautomatically detect the semantic similarity of positions are proposed. Thenfour competitive ""shallow"" estimators and ""deep"" estimators are selected.Finally, ensemble methods to bag these estimators and aggregate theirindividual predictions to form a final prediction are verified. Experimentalresults of over 47 thousand resumes show that our solution can significantlyimprove the predication precision current position, salary, educationalbackground and company scale.",http://arxiv.org/abs/1607.07657v1,,
1919,"A Machine Learning Based Forwarding Algorithm Over Cognitive Radios in  Wireless Mesh Networks","Wireless Mesh Networks improve their capacities by equipping mesh nodes withmulti-radios tuned to non-overlapping channels. Hence the data forwardingbetween two nodes has multiple selections of links and the bandwidth betweenthe pair of nodes varies dynamically. Under this condition, a mesh node adoptsmachine learning mechanisms to choose the possible best next hop which hasmaximum bandwidth when it intends to forward data. In this paper, we present amachine learning based forwarding algorithm to let a forwarding nodedynamically select the next hop with highest potential bandwidth capacity toresume communication based on learning algorithm. Key to this strategy is thata node only maintains three past status, and then it is able to learn andpredict the potential bandwidth capacities of its links. Then, the node selectsthe next hop with potential maximal link bandwidth. Moreover, a geometricalbased algorithm is developed to let the source node figure out the forwardingregion in order to avoid flooding. Simulations demonstrate that our approachsignificantly speeds up the transmission and outperforms other peer algorithms.",http://arxiv.org/abs/1608.03536v1,,
1920,Exact Sampling from Determinantal Point Processes,"Determinantal point processes (DPPs) are an important concept in randommatrix theory and combinatorics. They have also recently attracted interest inthe study of numerical methods for machine learning, as they offer an elegant""missing link"" between independent Monte Carlo sampling and deterministicevaluation on regular grids, applicable to a general set of spaces. This ishelpful whenever an algorithm explores to reduce uncertainty, such as in activelearning, Bayesian optimization, reinforcement learning, and marginalization ingraphical models. To draw samples from a DPP in practice, existing literaturefocuses on approximate schemes of low cost, or comparably inefficient exactalgorithms like rejection sampling. We point out that, for many settings ofrelevance to machine learning, it is also possible to draw exact samples fromDPPs on continuous domains. We start from an intuitive example on the realline, which is then generalized to multivariate real vector spaces. We alsocompare to previously studied approximations, showing that exact sampling,despite higher cost, can be preferable where precision is needed.",http://arxiv.org/abs/1609.06840v2,,
1921,"Learning Deep Architectures for Interaction Prediction in  Structure-based Virtual Screening","We introduce a deep learning architecture for structure-based virtualscreening that generates fixed-sized fingerprints of proteins and smallmolecules by applying learnable atom convolution and softmax operations to eachcompound separately. These fingerprints are further transformed non-linearly,their inner-product is calculated and used to predict the binding potential.Moreover, we show that widely used benchmark datasets may be insufficient fortesting structure-based virtual screening methods that utilize machinelearning. Therefore, we introduce a new benchmark dataset, which we constructedbased on DUD-E and PDBBind databases.",http://arxiv.org/abs/1610.07187v3,,
1922,Towards Wide Learning: Experiments in Healthcare,"In this paper, a Wide Learning architecture is proposed that attempts toautomate the feature engineering portion of the machine learning (ML) pipeline.Feature engineering is widely considered as the most time consuming and expertknowledge demanding portion of any ML task. The proposed feature recommendationapproach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016dataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressureclassification dataset of photoplethysmogram (PPG) signals and c) an emotionclassification dataset of PPG signals. While the proposed method beats thestate of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of theaccuracy level of the winner of PhysioNet Challenge 2016. In all cases, theeffort to reach a satisfactory performance was drastically less (a few days)than manual feature engineering.",http://arxiv.org/abs/1612.05730v2,,
1923,"Compositional Falsification of Cyber-Physical Systems with Machine  Learning Components","Cyber-physical systems (CPS), such as automotive systems, are starting toinclude sophisticated machine learning (ML) components. Their correctness,therefore, depends on properties of the inner ML modules. While learningalgorithms aim to generalize from examples, they are only as good as theexamples provided, and recent efforts have shown that they can produceinconsistent output under small adversarial perturbations. This raises thequestion: can the output from learning components can lead to a failure of theentire CPS? In this work, we address this question by formulating it as aproblem of falsifying signal temporal logic (STL) specifications for CPS withML components. We propose a compositional falsification framework where atemporal logic falsifier and a machine learning analyzer cooperate with the aimof finding falsifying executions of the considered model. The efficacy of theproposed technique is shown on an automatic emergency braking system model witha perception component based on deep neural networks.",http://arxiv.org/abs/1703.00978v3,,
1924,Personalized and Private Peer-to-Peer Machine Learning,"The rise of connected personal devices together with privacy concerns callfor machine learning algorithms capable of leveraging the data of a largenumber of agents to learn personalized models under strong privacyrequirements. In this paper, we introduce an efficient algorithm to address theabove problem in a fully decentralized (peer-to-peer) and asynchronous fashion,with provable convergence rate. We show how to make the algorithmdifferentially private to protect against the disclosure of information aboutthe personal datasets, and formally analyze the trade-off between utility andprivacy. Our experiments show that our approach dramatically outperformsprevious work in the non-private case, and that under privacy constraints, wecan significantly improve over models learned in isolation.",http://arxiv.org/abs/1705.08435v2,,
1925,A Machine Learning Approach for Evaluating Creative Artifacts,"Much work has been done in understanding human creativity and definingmeasures to evaluate creativity. This is necessary mainly for the reason ofhaving an objective and automatic way of quantifying creative artifacts. Inthis work, we propose a regression-based learning framework which takes intoaccount quantitatively the essential criteria for creativity like novelty,influence, value and unexpectedness. As it is often the case with most creativedomains, there is no clear ground truth available for creativity. Our proposedlearning framework is applicable to all creative domains; yet we evaluate it ona dataset of movies created from IMDb and Rotten Tomatoes due to availabilityof audience and critic scores, which can be used as proxy ground truth labelsfor creativity. We report promising results and observations from ourexperiments in the following ways : 1) Correlation of creative criteria withcritic scores, 2) Improvement in movie rating prediction with inclusion ofvarious creative criteria, and 3) Identification of creative movies.",http://arxiv.org/abs/1707.05499v1,,
1926,Spectral Dynamics of Learning Restricted Boltzmann Machines,"The Restricted Boltzmann Machine (RBM), an important tool used in machinelearning in particular for unsupervized learning tasks, is investigated fromthe perspective of its spectral properties. Starting from empiricalobservations, we propose a generic statistical ensemble for the weight matrixof the RBM and characterize its mean evolution. This let us show how in thelinear regime, in which the RBM is found to operate at the beginning of thetraining, the statistical properties of the data drive the selection of theunstable modes of the weight matrix. A set of equations characterizing thenon-linear regime is then derived, unveiling in some way how the selected modesinteract in later stages of the learning procedure and defining a deterministiclearning curve for the RBM.",http://arxiv.org/abs/1708.02917v2,,
1927,"Quantum-assisted Helmholtz machines: A quantum-classical deep learning  framework for industrial datasets in near-term devices","Machine learning has been presented as one of the key applications fornear-term quantum technologies, given its high commercial value and wide rangeof applicability. In this work, we introduce the \textit{quantum-assistedHelmholtz machine:} a hybrid quantum-classical framework with the potential oftackling high-dimensional real-world machine learning datasets on continuousvariables. Instead of using quantum computers only to assist deep learning, asprevious approaches have suggested, we use deep learning to extract alow-dimensional binary representation of data, suitable for processing onrelatively small quantum computers. Then, the quantum hardware and deeplearning architecture work together to train an unsupervised generative model.We demonstrate this concept using 1644 quantum bits of a D-Wave 2000Q quantumdevice to model a sub-sampled version of the MNIST handwritten digit datasetwith 16x16 continuous valued pixels. Although we illustrate this concept on aquantum annealer, adaptations to other quantum platforms, such as ion-traptechnologies or superconducting gate-model architectures, could be exploredwithin this flexible framework.",http://arxiv.org/abs/1708.09784v3,,
1928,Recovery Conditions and Sampling Strategies for Network Lasso,"The network Lasso is a recently proposed convex optimization method formachine learning from massive network structured datasets, i.e., big data overnetworks. It is a variant of the well-known least absolute shrinkage andselection operator (Lasso), which is underlying many methods in learning andsignal processing involving sparse models. Highly scalable implementations ofthe network Lasso can be obtained by state-of-the art proximal methods, e.g.,the alternating direction method of multipliers (ADMM). By generalizing theconcept of the compatibility condition put forward by van de Geer and Buehlmannas a powerful tool for the analysis of plain Lasso, we derive a sufficientcondition, i.e., the network compatibility condition, on the underlying networktopology such that network Lasso accurately learns a clustered underlying graphsignal. This network compatibility condition relates the location of thesampled nodes with the clustering structure of the network. In particular, theNCC informs the choice of which nodes to sample, or in machine learning terms,which data points provide most information if labeled.",http://arxiv.org/abs/1709.01402v1,,
1929,"NSML: A Machine Learning Platform That Enables You to Focus on Your  Models","Machine learning libraries such as TensorFlow and PyTorch simplify modelimplementation. However, researchers are still required to perform anon-trivial amount of manual tasks such as GPU allocation, training statustracking, and comparison of models with different hyperparameter settings. Wepropose a system to handle these tasks and help researchers focus on models. Wepresent the requirements of the system based on a collection of discussionsfrom an online study group comprising 25k members. These include automatic GPUallocation, learning status visualization, handling model parameter snapshotsas well as hyperparameter modification during learning, and comparison ofperformance metrics between models via a leaderboard. We describe the systemarchitecture that fulfills these requirements and present a proof-of-conceptimplementation, NAVER Smart Machine Learning (NSML). We test the system andconfirm substantial efficiency improvements for model development.",http://arxiv.org/abs/1712.05902v1,,
1930,"RadialGAN: Leveraging multiple datasets to improve target-specific  predictive models using Generative Adversarial Networks","Training complex machine learning models for prediction often requires alarge amount of data that is not always readily available. Leveraging theseexternal datasets from related but different sources is therefore an importanttask if good predictive models are to be built for deployment in settings wheredata can be rare. In this paper we propose a novel approach to the problem inwhich we use multiple GAN architectures to learn to translate from one datasetto another, thereby allowing us to effectively enlarge the target dataset, andtherefore learn better predictive models than if we simply used the targetdataset. We show the utility of such an approach, demonstrating that our methodimproves the prediction performance on the target domain over using just thetarget dataset and also show that our framework outperforms several otherbenchmarks on a collection of real-world medical datasets.",http://arxiv.org/abs/1802.06403v2,,
1931,"Learning Dynamic Boltzmann Distributions as Reduced Models of Spatial  Chemical Kinetics","Finding reduced models of spatially-distributed chemical reaction networksrequires an estimation of which effective dynamics are relevant. We propose amachine learning approach to this coarse graining problem, where a maximumentropy approximation is constructed that evolves slowly in time. The dynamicalmodel governing the approximation is expressed as a functional, allowing ageneral treatment of spatial interactions. In contrast to typical machinelearning approaches which estimate the interaction parameters of a graphicalmodel, we derive Boltzmann-machine like learning algorithms to estimatedirectly the functionals dictating the time evolution of these parameters. Byincorporating analytic solutions from simple reaction motifs, an efficientsimulation method is demonstrated for systems ranging from toy problems tobasic biologically relevant networks. The broadly applicable nature of ourapproach to learning spatial dynamics suggests promising applications tomultiscale methods for spatial networks, as well as to further problems inmachine learning.",http://arxiv.org/abs/1803.01063v1,,
1932,DLL: A Blazing Fast Deep Neural Network Library,"Deep Learning Library (DLL) is a new library for machine learning with deepneural networks that focuses on speed. It supports feed-forward neural networkssuch as fully-connected Artificial Neural Networks (ANNs) and ConvolutionalNeural Networks (CNNs). It also has very comprehensive support for RestrictedBoltzmann Machines (RBMs) and Convolutional RBMs. Our main motivation for thiswork was to propose and evaluate novel software engineering strategies withpotential to accelerate runtime for training and inference. Such strategies aremostly independent of the underlying deep learning algorithms. On threedifferent datasets and for four different neural network models, we comparedDLL to five popular deep learning frameworks. Experimentally, it is shown thatthe proposed framework is systematically and significantly faster on CPU andGPU. In terms of classification performance, similar accuracies as the otherframeworks are reported.",http://arxiv.org/abs/1804.04512v1,,
1933,Human Activity Recognition using Recurrent Neural Networks,"Human activity recognition using smart home sensors is one of the bases ofubiquitous computing in smart environments and a topic undergoing intenseresearch in the field of ambient assisted living. The increasingly large amountof data sets calls for machine learning methods. In this paper, we introduce adeep learning model that learns to classify human activities without using anyprior knowledge. For this purpose, a Long Short Term Memory (LSTM) RecurrentNeural Network was applied to three real world smart home datasets. The resultsof these experiments show that the proposed approach outperforms the existingones in terms of accuracy and performance.",http://arxiv.org/abs/1804.07144v1,,
1934,"Automatic classification of trees using a UAV onboard camera and deep  learning","Automatic classification of trees using remotely sensed data has been a dreamof many scientists and land use managers. Recently, Unmanned aerial vehicles(UAV) has been expected to be an easy-to-use, cost-effective tool for remotesensing of forests, and deep learning has attracted attention for its abilityconcerning machine vision. In this study, using a commercially available UAVand a publicly available package for deep learning, we constructed a machinevision system for the automatic classification of trees. In our method, wesegmented a UAV photography image of forest into individual tree crowns andcarried out object-based deep learning. As a result, the system was able toclassify 7 tree types at 89.0% accuracy. This performance is notable because weonly used basic RGB images from a standard UAV. In contrast, most of previousstudies used expensive hardware such as multispectral imagers to improve theperformance. This result means that our method has the potential to classifyindividual trees in a cost-effective manner. This can be a usable tool for manyforest researchers and managements.",http://arxiv.org/abs/1804.10390v1,,
1935,Computing the Shattering Coefficient of Supervised Learning Algorithms,"The Statistical Learning Theory (SLT) provides the theoretical guarantees forsupervised machine learning based on the Empirical Risk Minimization Principle(ERMP). Such principle defines an upper bound to ensure the uniform convergenceof the empirical risk Remp(f), i.e., the error measured on a given data sample,to the expected value of risk R(f) (a.k.a. actual risk), which depends on theJoint Probability Distribution P(X x Y) mapping input examples x in X to classlabels y in Y. The uniform convergence is only ensured when the Shatteringcoefficient N(F,2n) has a polynomial growing behavior. This paper proves theShattering coefficient for any Hilbert space H containing the input space X anddiscusses its effects in terms of learning guarantees for supervised machinealgorithms.",http://arxiv.org/abs/1805.02627v4,,
1936,Deep Learning under Privileged Information Using Heteroscedastic Dropout,"Unlike machines, humans learn through rapid, abstract model-building. Therole of a teacher is not simply to hammer home right or wrong answers, butrather to provide intuitive comments, comparisons, and explanations to a pupil.This is what the Learning Under Privileged Information (LUPI) paradigmendeavors to model by utilizing extra knowledge only available during training.We propose a new LUPI algorithm specifically designed for Convolutional NeuralNetworks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use aheteroscedastic dropout (i.e. dropout with a varying variance) and make thevariance of the dropout a function of privileged information. Intuitively, thiscorresponds to using the privileged information to control the uncertainty ofthe model output. We perform experiments using CNNs and RNNs for the tasks ofimage classification and machine translation. Our method significantlyincreases the sample efficiency during learning, resulting in higher accuracywith a large margin when the number of training examples is limited. We alsotheoretically justify the gains in sample efficiency by providing ageneralization error bound decreasing with $O(\frac{1}{n})$, where $n$ is thenumber of training examples, in an oracle case.",http://arxiv.org/abs/1805.11614v1,,
1937,A Machine Learning Framework for Stock Selection,"This paper demonstrates how to apply machine learning algorithms todistinguish good stocks from the bad stocks. To this end, we construct 244technical and fundamental features to characterize each stock, and label stocksaccording to their ranking with respect to the return-to-volatility ratio.Algorithms ranging from traditional statistical learning methods to recentlypopular deep learning method, e.g. Logistic Regression (LR), Random Forest(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve theclassification task. Genetic Algorithm (GA) is also used to implement featureselection. The effectiveness of the stock selection strategy is validated inChinese stock market in both statistical and practical aspects, showing that:1) Stacking outperforms other models reaching an AUC score of 0.972; 2) GeneticAlgorithm picks a subset of 114 features and the prediction performances of allmodels remain almost unchanged after the selection procedure, which suggestssome features are indeed redundant; 3) LR and DNN are radical models; RF isrisk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfoliosconstructed by our models outperform market average in back tests.",http://arxiv.org/abs/1806.01743v2,,
1938,"Scalable Multi-Class Bayesian Support Vector Machines for Structured and  Unstructured Data","We introduce a new Bayesian multi-class support vector machine by formulatinga pseudo-likelihood for a multi-class hinge loss in the form of alocation-scale mixture of Gaussians. We derive a variational-inference-basedtraining objective for gradient-based learning. Additionally, we employ aninducing point approximation which scales inference to large data sets.Furthermore, we develop hybrid Bayesian neural networks that combine standarddeep learning components with the proposed model to enable learning forunstructured data. We provide empirical evidence that our model outperforms thecompetitor methods with respect to both training time and accuracy inclassification experiments on 68 structured and two unstructured data sets.Finally, we highlight the key capability of our model in yielding predictionuncertainty for classification by demonstrating its effectiveness in the tasksof large-scale active learning and detection of adversarial images.",http://arxiv.org/abs/1806.02659v1,,
1939,Blind Justice: Fairness with Encrypted Sensitive Attributes,"Recent work has explored how to train machine learning models which do notdiscriminate against any subgroup of the population as determined by sensitiveattributes such as gender or race. To avoid disparate treatment, sensitiveattributes should not be considered. On the other hand, in order to avoiddisparate impact, sensitive attributes must be examined, e.g., in order tolearn a fair model, or to check if a given model is fair. We introduce methodsfrom secure multi-party computation which allow us to avoid both. By encryptingsensitive attributes, we show how an outcome-based fair model may be learned,checked, or have its outputs verified and held to account, without usersrevealing their sensitive attributes.",http://arxiv.org/abs/1806.03281v1,,
1940,Piecewise Approximations of Black Box Models for Model Interpretation,"Machine Learning models have proved extremely successful for a wide varietyof supervised learning problems, but the predictions of many of these modelsare difficult to interpret. A recent literature interprets the predictions ofmore general ""black-box"" machine learning models by approximating these modelsin terms of simpler models such as piecewise linear or piecewise constantmodels. Existing literature constructs these approximations in an ad-hocmanner. We provide a tractable dynamic programming algorithm that partitionsthe feature space into clusters in a principled way and then uses thispartition to provide both piecewise constant and piecewise linearinterpretations of an arbitrary ""black-box"" model. When loss is measured interms of mean squared error, our approximation is optimal (under certainconditions); for more general loss functions, our interpretation is probablyapproximately optimal (in the sense of PAC learning). Experiments with real andsynthetic data show that it continues to provide significant improvements (interms of mean squared error) over competing approaches.",http://arxiv.org/abs/1806.10270v2,,
1941,"A Review of Different Word Embeddings for Sentiment Classification using  Deep Learning","The web is loaded with textual content, and Natural Language Processing is astandout amongst the most vital fields in Machine Learning. But when data ishuge simple Machine Learning algorithms are not able to handle it and that iswhen Deep Learning comes into play which based on Neural Networks. Howeversince neural networks cannot process raw text, we have to change over themthrough some diverse strategies of word embedding. This paper demonstratesthose distinctive word embedding strategies implemented on an Amazon ReviewDataset, which has two sentiments to be classified: Happy and Unhappy based onnumerous customer reviews. Moreover we demonstrate the distinction in accuracywith a discourse about which word embedding to apply when.",http://arxiv.org/abs/1807.02471v1,,
1942,"Differentially-Private ""Draw and Discard"" Machine Learning","In this work, we propose a novel framework for privacy-preservingclient-distributed machine learning. It is motivated by the desire to achievedifferential privacy guarantees in the local model of privacy in a way thatsatisfies all systems constraints using asynchronous client-servercommunication and provides attractive model learning properties. We call it""Draw and Discard"" because it relies on random sampling of models for loaddistribution (scalability), which also provides additional server-side privacyprotections and improved model quality through averaging. We present themechanics of client and server components of ""Draw and Discard"" and demonstratehow the framework can be applied to learning Generalized Linear models. We thenanalyze the privacy guarantees provided by our approach against several typesof adversaries and showcase experimental results that provide evidence for theframework's viability in practical deployments.",http://arxiv.org/abs/1807.04369v2,,
1943,"Modular Mechanistic Networks: On Bridging Mechanistic and  Phenomenological Models with Deep Neural Networks in Natural Language  Processing","Natural language processing (NLP) can be done using either top-down (theorydriven) and bottom-up (data driven) approaches, which we call mechanistic andphenomenological respectively. The approaches are frequently considered tostand in opposition to each other. Examining some recent approaches in deeplearning we argue that deep neural networks incorporate both perspectives and,furthermore, that leveraging this aspect of deep learning may help in solvingcomplex problems within language technology, such as modelling language andperception in the domain of spatial cognition.",http://arxiv.org/abs/1807.09844v2,,
1944,"Ensemble Kalman Inversion: A Derivative-Free Technique For Machine  Learning Tasks","The standard probabilistic perspective on machine learning gives rise toempirical risk-minimization tasks that are frequently solved by stochasticgradient descent (SGD) and variants thereof. We present a formulation of thesetasks as classical inverse or filtering problems and, furthermore, we proposean efficient, gradient-free algorithm for finding a solution to these problemsusing ensemble Kalman inversion (EKI). Applications of our approach includeoffline and online supervised learning with deep neural networks, as well asgraph-based semi-supervised learning. The essence of the EKI procedure is anensemble based approximate gradient descent in which derivatives are replacedby differences from within the ensemble. We suggest several modifications tothe basic method, derived from empirically successful heuristics developed inthe context of SGD. Numerical results demonstrate wide applicability androbustness of the proposed algorithm.",http://arxiv.org/abs/1808.03620v1,,
1945,"A novel graph-based model for hybrid recommendations in cold-start  scenarios","Cold-start is a very common and still open problem in the Recommender Systemsliterature. Since cold start items do not have any interaction, collaborativealgorithms are not applicable. One of the main strategies is to use pure orhybrid content-based approaches, which usually yield to lower recommendationquality than collaborative ones. Some techniques to optimize performance ofthis type of approaches have been studied in recent past. One of them is calledfeature weighting, which assigns to every feature a real value, called weight,that estimates its importance. Statistical techniques for feature weightingcommonly used in Information Retrieval, like TF-IDF, have been adapted forRecommender Systems, but they often do not provide sufficient qualityimprovements. More recent approaches, FBSM and LFW, estimate weights byleveraging collaborative information via machine learning, in order to learnthe importance of a feature based on other users opinions. This type of modelshave shown promising results compared to classic statistical analyzes citedpreviously. We propose a novel graph, feature-based machine learning model toface the cold-start item scenario, learning the relevance of features fromprobabilities of item-based collaborative filtering algorithms.",http://arxiv.org/abs/1808.10664v1,,
1946,A Generalized Representer Theorem for Hilbert Space - Valued Functions,"The necessary and sufficient conditions for existence of a generalizedrepresenter theorem are presented for learning Hilbert space-valued functions.Representer theorems involving explicit basis functions and Reproducing Kernelsare a common occurrence in various machine learning algorithms like generalizedleast squares, support vector machines, Gaussian process regression and kernelbased deep neural networks to name a few. Due to the more general structure ofthe underlying variational problems, the theory is also relevant to otherapplication areas like optimal control, signal processing and decision making.We present the generalized representer as a unified view for supervised andsemi-supervised learning methods, using the theory of linear operators andsubspace valued maps. The implications of the theorem are presented withexamples of multi input-multi output regression, kernel based deep neuralnetworks, stochastic regression and sparsity learning problems as being specialcases in this unified view.",http://arxiv.org/abs/1809.07347v1,,
1947,"Analysis of Irregular Spatial Data with Machine Learning: Classification  of Building Patterns with a Graph Convolutional Neural Network","Machine learning methods such as convolutional neural networks (CNNs) arebecoming an integral part of scientific research in many disciplines, spatialvector data often fail to be analyzed using these powerful learning methodsbecause of its irregularities. With the aid of graph Fourier transform andconvolution theorem, it is possible to convert the convolution as a point-wiseproduct in Fourier domain and construct a learning architecture of CNN on graphfor the analysis task of irregular spatial data. In this study, we used theclassification task of building patterns as a case study to test this method,and experiments showed that this method has achieved outstanding results inidentifying regular and irregular patterns, and has significantly improved incomparing with other methods.",http://arxiv.org/abs/1809.08196v1,,
1948,Adversarial Recommendation: Attack of the Learned Fake Users,"Can machine learning models for recommendation be easily fooled? While thequestion has been answered for hand-engineered fake user profiles, it has notbeen explored for machine learned adversarial attacks. This paper attempts toclose this gap.  We propose a framework for generating fake user profiles which, whenincorporated in the training of a recommendation system, can achieve anadversarial intent, while remaining indistinguishable from real user profiles.We formulate this procedure as a repeated general-sum game between two players:an oblivious recommendation system $R$ and an adversarial fake user generator$A$ with two goals: (G1) the rating distribution of the fake users needs to beclose to the real users, and (G2) some objective $f_A$ encoding the attackintent, such as targeting the top-K recommendation quality of $R$ for a subsetof users, needs to be optimized. We propose a learning framework to achieveboth goals, and offer extensive experiments considering multiple types ofattacks highlighting the vulnerability of recommendation systems.",http://arxiv.org/abs/1809.08336v1,,
1949,Optimal Adaptive and Accelerated Stochastic Gradient Descent,"Stochastic gradient descent (\textsc{Sgd}) methods are the most powerfuloptimization tools in training machine learning and deep learning models.Moreover, acceleration (a.k.a. momentum) methods and diagonal scaling (a.k.a.adaptive gradient) methods are the two main techniques to improve the slowconvergence of \textsc{Sgd}. While empirical studies have demonstratedpotential advantages of combining these two techniques, it remains unknownwhether these methods can achieve the optimal rate of convergence forstochastic optimization. In this paper, we present a new class of adaptive andaccelerated stochastic gradient descent methods and show that they exhibit theoptimal sampling and iteration complexity for stochastic optimization. Morespecifically, we show that diagonal scaling, initially designed to improvevanilla stochastic gradient, can be incorporated into accelerated stochasticgradient descent to achieve the optimal rate of convergence for smoothstochastic optimization. We also show that momentum, apart from being known tospeed up the convergence rate of deterministic optimization, also provides usnew ways of designing non-uniform and aggressive moving average schemes instochastic optimization. Finally, we present some heuristics that help toimplement adaptive accelerated stochastic gradient descent methods and tofurther improve their practical performance for machine learning and deeplearning.",http://arxiv.org/abs/1810.00553v1,,
1950,MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales,"We introduce Microsoft Machine Learning for Apache Spark (MMLSpark), anecosystem of enhancements that expand the Apache Spark distributed computinglibrary to tackle problems in Deep Learning, Micro-Service Orchestration,Gradient Boosting, Model Interpretability, and other areas of moderncomputation. Furthermore, we present a novel system called Spark Serving thatallows users to run any Apache Spark program as a distributed, sub-millisecondlatency web service backed by their existing Spark Cluster. All MMLSparkcontributions have the same API to enable simple composition across frameworksand usage across batch, streaming, and RESTful web serving scenarios on static,elastic, or serverless clusters. We showcase MMLSpark by creating a method fordeep object detection capable of learning without human labeled data anddemonstrate its effectiveness for Snow Leopard conservation.",http://arxiv.org/abs/1810.08744v1,,
1951,Learning with Analytical Models,"To understand and predict the performance of scientific applications, severalanalytical and machine learning approaches have been proposed, each having itsadvantages and disadvantages. In this paper, we propose and validate a hybridapproach for performance modeling and prediction, which combines analytical andmachine learning models. The proposed hybrid model aims to minimize predictioncost while providing reasonable prediction accuracy. Our validation resultsshow that the hybrid model is able to learn and correct the analytical modelsto better match the actual performance. Furthermore, the proposed hybrid modelimproves the prediction accuracy in comparison to pure machine learningtechniques while using small training datasets, thus making it suitable forhardware and workload changes.",http://arxiv.org/abs/1810.11772v2,,
1952,"Efficient Online Hyperparameter Optimization for Kernel Ridge Regression  with Applications to Traffic Time Series Prediction","Computational efficiency is an important consideration for deploying machinelearning models for time series prediction in an online setting. Machinelearning algorithms adjust model parameters automatically based on the data,but often require users to set additional parameters, known as hyperparameters.Hyperparameters can significantly impact prediction accuracy. Trafficmeasurements, typically collected online by sensors, are serially correlated.Moreover, the data distribution may change gradually. A typical adaptationstrategy is periodically re-tuning the model hyperparameters, at the cost ofcomputational burden. In this work, we present an efficient and principledonline hyperparameter optimization algorithm for Kernel Ridge regressionapplied to traffic prediction problems. In tests with real traffic measurementdata, our approach requires as little as one-seventh of the computation time ofother tuning methods, while achieving better or similar prediction accuracy.",http://arxiv.org/abs/1811.00620v1,,
1953,"Self Organizing Classifiers: First Steps in Structured Evolutionary  Machine Learning","Learning classifier systems (LCSs) are evolutionary machine learningalgorithms, flexible enough to be applied to reinforcement, supervised andunsupervised learning problems with good performance. Recently, self organizingclassifiers were proposed which are similar to LCSs but have the advantage thatin its structured population no balance between niching and fitness pressure isnecessary. However, more tests and analysis are required to verify itsbenefits. Here, a variation of the first algorithm is proposed which uses aparameterless self organizing map (SOM). This algorithm is applied inchallenging problems such as big, noisy as well as dynamically changingcontinuous input-action mazes (growing and compressing mazes are included) withgood performance. Moreover, a genetic operator is proposed which utilizes thetopological information of the SOM's population structure, improving theresults. Thus, the first steps in structured evolutionary machine learning areshown, nonetheless, the problems faced are more difficult than the state-of-artcontinuous input-action multi-step ones.",http://arxiv.org/abs/1811.08225v1,,
1954,Learning Multiple Defaults for Machine Learning Algorithms,"The performance of modern machine learning methods highly depends on theirhyperparameter configurations. One simple way of selecting a configuration isto use default settings, often proposed along with the publication andimplementation of a new algorithm. Those default values are usually chosen inan ad-hoc manner to work good enough on a wide variety of datasets. To addressthis problem, different automatic hyperparameter configuration algorithms havebeen proposed, which select an optimal configuration per dataset. Thisprincipled approach usually improves performance, but adds additionalalgorithmic complexity and computational costs to the training procedure. As analternative to this, we propose learning a set of complementary default valuesfrom a large database of prior empirical results. Selecting an appropriateconfiguration on a new dataset then requires only a simple, efficient andembarrassingly parallel search over this set. We demonstrate the effectivenessand efficiency of the approach we propose in comparison to random search andBayesian Optimization.",http://arxiv.org/abs/1811.09409v1,,
1955,How to improve the interpretability of kernel learning,"In recent years, machine learning researchers have focused on methods toconstruct flexible and interpretable prediction models. However, theinterpretability evaluation, the relationship between the generalizationperformance and the interpretability of the model and the method for improvingthe interpretability are very important factors to consider. In this paper, thequantitative index of the interpretability is proposed and its rationality isgiven, and the relationship between the interpretability and the generalizationperformance is analyzed. For traditional supervised kernel machine learningproblem, a universal learning framework is put forward to solve the equilibriumproblem between the two performances. The uniqueness of solution of the problemis proved and condition of unique solution is obtained. Probability upper boundof the sum of the two performances is analyzed.",http://arxiv.org/abs/1811.10469v1,,
1956,Recurrent machines for likelihood-free inference,"Likelihood-free inference is concerned with the estimation of the parametersof a non-differentiable stochastic simulator that best reproduce realobservations. In the absence of a likelihood function, most of the existinginference methods optimize the simulator parameters through a handcraftediterative procedure that tries to make the simulated data more similar to theobservations. In this work, we explore whether meta-learning can be used in thelikelihood-free context, for learning automatically from data an iterativeoptimization procedure that would solve likelihood-free inference problems. Wedesign a recurrent inference machine that learns a sequence of parameterupdates leading to good parameter estimates, without ever specifying someexplicit notion of divergence between the simulated data and the real datadistributions. We demonstrate our approach on toy simulators, showing promisingresults both in terms of performance and robustness.",http://arxiv.org/abs/1811.12932v2,,
1957,Modeling Irregularly Sampled Clinical Time Series,"While the volume of electronic health records (EHR) data continues to grow,it remains rare for hospital systems to capture dense physiological datastreams, even in the data-rich intensive care unit setting. Instead, typicalEHR records consist of sparse and irregularly observed multivariate timeseries, which are well understood to present particularly challenging problemsfor machine learning methods. In this paper, we present a new deep learningarchitecture for addressing this problem based on the use of a semi-parametricinterpolation network followed by the application of a prediction network. Theinterpolation network allows for information to be shared across multipledimensions during the interpolation stage, while any standard deep learningmodel can be used for the prediction network. We investigate the performance ofthis architecture on the problems of mortality and length of stay prediction.",http://arxiv.org/abs/1812.00531v1,,
1958,"Machine Learning for Yield Curve Feature Extraction: Application to  Illiquid Corporate Bonds","This paper studies an application of machine learning in extracting featuresfrom the historical market implied corporate bond yields. We consider anexample of a hypothetical illiquid fixed income market. After choosing asurrogate liquid market, we apply the Denoising Autoencoder (DAE) algorithm tolearn the features of the missing yield parameters from the historical data ofthe instruments traded in the chosen liquid market. The DAE algorithm is thenchallenged by two ""point-in-time"" inpainting algorithms taken from the imageprocessing and computer vision domain. It is observed that, when tested onunobserved rate surfaces, the DAE algorithm exhibits superior performancethanks to the features it has learned from the historical shapes of yieldcurves.",http://arxiv.org/abs/1812.01102v1,,
1959,Learning Cheap and Novel Flight Itineraries,"We consider the problem of efficiently constructing cheap and novel roundtrip flight itineraries by combining legs from different airlines. We analysethe factors that contribute towards the price of such itineraries and find thatmany result from the combination of just 30% of airlines and that the closerthe departure of such itineraries is to the user's search date the more likelythey are to be cheaper than the tickets from one airline. We use these insightsto formulate the problem as a trade-off between the recall of cheap itineraryconstructions and the costs associated with building them.  We propose a supervised learning solution with location embeddings whichachieves an AUC=80.48, a substantial improvement over simpler baselines. Wediscuss various practical considerations for dealing with the staleness and thestability of the model and present the design of the machine learning pipeline.Finally, we present an analysis of the model's performance in production andits impact on Skyscanner's users.",http://arxiv.org/abs/1812.01735v1,,
1960,Secure Federated Transfer Learning,"Machine learning relies on the availability of a vast amount of data fortraining. However, in reality, most data are scattered across differentorganizations and cannot be easily integrated under many legal and practicalconstraints. In this paper, we introduce a new technique and framework, knownas federated transfer learning (FTL), to improve statistical models under adata federation. The federation allows knowledge to be shared withoutcompromising user privacy, and enables complimentary knowledge to betransferred in the network. As a result, a target-domain party can build moreflexible and powerful models by leveraging rich labels from a source-domainparty. A secure transfer cross validation approach is also proposed to guardthe FTL performance under the federation. The framework requires minimalmodifications to the existing model structure and provides the same level ofaccuracy as the non-privacy-preserving approach. This framework is veryflexible and can be effectively adapted to various secure multi-party machinelearning tasks.",http://arxiv.org/abs/1812.03337v1,,
1961,Training Set Camouflage,"We introduce a form of steganography in the domain of machine learning whichwe call training set camouflage. Imagine Alice has a training set on an illicitmachine learning classification task. Alice wants Bob (a machine learningsystem) to learn the task. However, sending either the training set or thetrained model to Bob can raise suspicion if the communication is monitored.Training set camouflage allows Alice to compute a second training set on acompletely different -- and seemingly benign -- classification task. Byconstruction, sending the second training set will not raise suspicion. WhenBob applies his standard (public) learning algorithm to the second trainingset, he approximately recovers the classifier on the original task. Trainingset camouflage is a novel form of steganography in machine learning. Weformulate training set camouflage as a combinatorial bilevel optimizationproblem and propose solvers based on nonlinear programming and local search.Experiments on real classification tasks demonstrate the feasibility of suchcamouflage.",http://arxiv.org/abs/1812.05725v1,,
1962,"Generalization Studies of Neural Network Models for Cardiac Disease  Detection Using Limited Channel ECG","Acceleration of machine learning research in healthcare is challenged by lackof large annotated and balanced datasets. Furthermore, dealing with measurementinaccuracies and exploiting unsupervised data are considered to be central toimproving existing solutions. In particular, a primary objective in predictivemodeling is to generalize well to both unseen variations within the observedclasses, and unseen classes. In this work, we consider such a challengingproblem in machine learning driven diagnosis: detecting a gamut ofcardiovascular conditions (e.g. infarction, dysrhythmia etc.) from limitedchannel ECG measurements. Though deep neural networks have achievedunprecedented success in predictive modeling, they rely solely ondiscriminative models that can generalize poorly to unseen classes. We arguethat unsupervised learning can be utilized to construct effective latent spacesthat facilitate better generalization. This work extensively compares thegeneralization of our proposed approach against a state-of-the-art deeplearning solution. Our results show significant improvements in F1-scores.",http://arxiv.org/abs/1901.03295v1,,
1963,Impact of Data Pruning on Machine Learning Algorithm Performance,"Dataset pruning is the process of removing sub-optimal tuples from a datasetto improve the learning of a machine learning model. In this paper, we comparedthe performance of different algorithms, first on an unpruned dataset and thenon an iteratively pruned dataset. The goal was to understand whether analgorithm (say A) on an unpruned dataset performs better than another algorithm(say B), will algorithm B perform better on the pruned data or vice-versa. Thedataset chosen for our analysis is a subset of the largest movie ratingsdatabase publicly available on the internet, IMDb [1]. The learning objectiveof the model was to predict the categorical rating of a movie among 5 bins:poor, average, good, very good, excellent. The results indicated that analgorithm that performed better on an unpruned dataset also performed better ona pruned dataset.",http://arxiv.org/abs/1901.10539v1,,
1964,AutoML @ NeurIPS 2018 challenge: Design and Results,"We organized a competition on Autonomous Lifelong Machine Learning with Driftthat was part of the competition program of NeurIPS 2018. This data drivencompetition asked participants to develop computer programs capable of solvingsupervised learning problems where the i.i.d. assumption did not hold. Largedata sets were arranged in a lifelong learning and evaluation scenario andCodaLab was used as the challenge platform. The challenge attracted more than300 participants in its two month duration. This chapter describes the designof the challenge and summarizes its main results.",http://arxiv.org/abs/1903.05263v2,,
1965,Task-oriented Design through Deep Reinforcement Learning,"We propose a new low-cost machine-learning-based methodology which assistsdesigners in reducing the gap between the problem and the solution in thedesign process. Our work applies reinforcement learning (RL) to find theoptimal task-oriented design solution through the construction of the designaction for each task. For this task-oriented design, the 3D design process inproduct design is assigned to an action space in Deep RL, and the desired 3Dmodel is obtained by training each design action according to the task. Byshowing that this method achieves satisfactory design even when applied to atask pursuing multiple goals, we suggest the direction of how machine learningcan contribute to the design process. Also, we have validated with productdesigners that this methodology can assist the creative part in the process ofdesign.",http://arxiv.org/abs/1903.05271v1,,
1966,Machine Learning Methods Economists Should Know About,"We discuss the relevance of the recent Machine Learning (ML) literature foreconomics and econometrics. First we discuss the differences in goals, methodsand settings between the ML literature and the traditional econometrics andstatistics literatures. Then we discuss some specific methods from the machinelearning literature that we view as important for empirical researchers ineconomics. These include supervised learning methods for regression andclassification, unsupervised learning methods, as well as matrix completionmethods. Finally, we highlight newly developed methods at the intersection ofML and econometrics, methods that typically perform better than eitheroff-the-shelf ML or more traditional econometric methods when applied toparticular classes of problems, problems that include causal inference foraverage treatment effects, optimal policy estimation, and estimation of thecounterfactual effect of price changes in consumer choice models.",http://arxiv.org/abs/1903.10075v1,,
1967,"Unsupervised Learning in a Framework of Information Compression by  Multiple Alignment, Unification and Search","This paper describes a novel approach to unsupervised learning that has beendeveloped within a framework of ""information compression by multiple alignment,unification and search"" (ICMAUS), designed to integrate learning with other AIfunctions such as parsing and production of language, fuzzy patternrecognition, probabilistic and exact forms of reasoning, and others.",http://arxiv.org/abs/cs/0302015v1,,
1968,Robustness and Generalization,"We derive generalization bounds for learning algorithms based on theirrobustness: the property that if a testing sample is ""similar"" to a trainingsample, then the testing error is close to the training error. This provides anovel approach, different from the complexity or stability arguments, to studygeneralization of learning algorithms. We further show that a weak notion ofrobustness is both sufficient and necessary for generalizability, which impliesthat robustness is a fundamental property for learning algorithms to work.",http://arxiv.org/abs/1005.2243v1,,
1969,Visual Transfer Learning: Informal Introduction and Literature Overview,"Transfer learning techniques are important to handle small training sets andto allow for quick generalization even from only a few examples. The followingpaper is the introduction as well as the literature overview part of my thesisrelated to the topic of transfer learning for visual recognition problems.",http://arxiv.org/abs/1211.1127v1,,
1970,A C++ library for Multimodal Deep Learning,"MDL, Multimodal Deep Learning Library, is a deep learning framework thatsupports multiple models, and this document explains its philosophy andfunctionality. MDL runs on Linux, Mac, and Unix platforms. It depends onOpenCV.",http://arxiv.org/abs/1512.06927v4,,
1971,Node-By-Node Greedy Deep Learning for Interpretable Features,"Multilayer networks have seen a resurgence under the umbrella of deeplearning. Current deep learning algorithms train the layers of the networksequentially, improving algorithmic performance as well as providing someregularization. We present a new training algorithm for deep networks whichtrains \emph{each node in the network} sequentially. Our algorithm is orders ofmagnitude faster, creates more interpretable internal representations at thenode level, while not sacrificing on the ultimate out-of-sample performance.",http://arxiv.org/abs/1602.06183v1,,
1972,Learning AMP Chain Graphs under Faithfulness,"This paper deals with chain graphs under the alternativeAndersson-Madigan-Perlman (AMP) interpretation. In particular, we present aconstraint based algorithm for learning an AMP chain graph a given probabilitydistribution is faithful to. We also show that the extension of Meek'sconjecture to AMP chain graphs does not hold, which compromises the developmentof efficient and correct score+search learning algorithms under assumptionsweaker than faithfulness.",http://arxiv.org/abs/1204.5357v1,,
1973,Learning States Representations in POMDP,"We propose to deal with sequential processes where only partial observationsare available by learning a latent representation space on which policies maybe accurately learned.",http://arxiv.org/abs/1312.6042v4,,
1974,"Unsupervised Representation Learning of Structured Radio Communication  Signals","We explore unsupervised representation learning of radio communicationsignals in raw sampled time series representation. We demonstrate that we canlearn modulation basis functions using convolutional autoencoders and visuallyrecognize their relationship to the analytic bases used in digitalcommunications. We also propose and evaluate quantitative met- rics for qualityof encoding using domain relevant performance metrics.",http://arxiv.org/abs/1604.07078v1,,
1975,Quantile Reinforcement Learning,"In reinforcement learning, the standard criterion to evaluate policies in astate is the expectation of (discounted) sum of rewards. However, thiscriterion may not always be suitable, we consider an alternative criterionbased on the notion of quantiles. In the case of episodic reinforcementlearning problems, we propose an algorithm based on stochastic approximationwith two timescales. We evaluate our proposition on a simple model of the TVshow, Who wants to be a millionaire.",http://arxiv.org/abs/1611.00862v1,,
1976,Learning from partial correction,"We introduce a new model of interactive learning in which an expert examinesthe predictions of a learner and partially fixes them if they are wrong.Although this kind of feedback is not i.i.d., we show statisticalgeneralization bounds on the quality of the learned model.",http://arxiv.org/abs/1705.08076v4,,
1977,"Proceedings of the First International Workshop on Deep Learning and  Music","Proceedings of the First International Workshop on Deep Learning and Music,joint with IJCNN, Anchorage, US, May 17-18, 2017",http://arxiv.org/abs/1706.08675v1,,
1978,Mathematics of Deep Learning,"Recently there has been a dramatic increase in the performance of recognitionsystems due to the introduction of deep architectures for representationlearning and classification. However, the mathematical reasons for this successremain elusive. This tutorial will review recent work that aims to provide amathematical justification for several properties of deep networks, such asglobal optimality, geometric stability, and invariance of the learnedrepresentations.",http://arxiv.org/abs/1712.04741v1,,
1979,A Model for Learned Bloom Filters and Related Structures,"Recent work has suggested enhancing Bloom filters by using a pre-filter,based on applying machine learning to model the data set the Bloom filter ismeant to represent. Here we model such learned Bloom filters, clarifying whatguarantees can and cannot be associated with such a structure.",http://arxiv.org/abs/1802.00884v1,,
1980,Towards Learning Fine-Grained Disentangled Representations from Speech,"Learning disentangled representations of high-dimensional data is currentlyan active research area. However, compared to the field of computer vision,less work has been done for speech processing. In this paper, we provide areview of two representative efforts on this topic and propose the novelconcept of fine-grained disentangled speech representation learning.",http://arxiv.org/abs/1808.02939v1,,
1981,Virtual Vector Machine for Bayesian Online Classification,"In a typical online learning scenario, a learner is required to process alarge data stream using a small memory buffer. Such a requirement is usually inconflict with a learner's primary pursuit of prediction accuracy. To addressthis dilemma, we introduce a novel Bayesian online classi cation algorithm,called the Virtual Vector Machine. The virtual vector machine allows you tosmoothly trade-off prediction accuracy with memory size. The virtual vectormachine summarizes the information contained in the preceding data stream by aGaussian distribution over the classi cation weights plus a constant number ofvirtual data points. The virtual data points are designed to add extranon-Gaussian information about the classi cation weights. To maintain theconstant number of virtual points, the virtual vector machine adds the currentreal data point into the virtual point set, merges two most similar virtualpoints into a new virtual point or deletes a virtual point that is far from thedecision boundary. The information lost in this process is absorbed into theGaussian distribution. The extra information provided by the virtual pointsleads to improved predictive accuracy over previous online classificationalgorithms.",http://arxiv.org/abs/1205.2623v1,,
1982,A Support Tensor Train Machine,"There has been growing interest in extending traditional vector-based machinelearning techniques to their tensor forms. An example is the support tensormachine (STM) that utilizes a rank-one tensor to capture the data structure,thereby alleviating the overfitting and curse of dimensionality problems in theconventional support vector machine (SVM). However, the expressive power of arank-one tensor is restrictive for many real-world data. To overcome thislimitation, we introduce a support tensor train machine (STTM) by replacing therank-one tensor in an STM with a tensor train. Experiments validate and confirmthe superiority of an STTM over the SVM and STM.",http://arxiv.org/abs/1804.06114v1,,
1983,Meta-learning within Projective Simulation,"Learning models of artificial intelligence can nowadays perform very well ona large variety of tasks. However, in practice different task environments arebest handled by different learning models, rather than a single, universal,approach. Most non-trivial models thus require the adjustment of several tomany learning parameters, which is often done on a case-by-case basis by anexternal party. Meta-learning refers to the ability of an agent to autonomouslyand dynamically adjust its own learning parameters, or meta-parameters. In thiswork we show how projective simulation, a recently developed model ofartificial intelligence, can naturally be extended to account for meta-learningin reinforcement learning settings. The projective simulation approach is basedon a random walk process over a network of clips. The suggested meta-learningscheme builds upon the same design and employs clip networks to monitor theagent's performance and to adjust its meta-parameters ""on the fly"". Wedistinguish between ""reflexive adaptation"" and ""adaptation through learning"",and show the utility of both approaches. In addition, a trade-off betweenflexibility and learning-time is addressed. The extended model is examined onthree different kinds of reinforcement learning tasks, in which the agent hasdifferent optimal values of the meta-parameters, and is shown to perform well,reaching near-optimal to optimal success rates in all of them, without everneeding to manually adjust any meta-parameter.",http://arxiv.org/abs/1602.08017v1,,
1984,"An Extensive Evaluation of Filtering Misclassified Instances in  Supervised Classification Tasks","Removing or filtering outliers and mislabeled instances prior to training alearning algorithm has been shown to increase classification accuracy. Apopular approach for handling outliers and mislabeled instances is to removeany instance that is misclassified by a learning algorithm. However, anexamination of which learning algorithms to use for filtering as well as theireffects on multiple learning algorithms over a large set of data sets has notbeen done. Previous work has generally been limited due to the largecomputational requirements to run such an experiment, and, thus, theexamination has generally been limited to learning algorithms that arecomputationally inexpensive and using a small number of data sets. In thispaper, we examine 9 learning algorithms as filtering algorithms as well asexamining the effects of filtering in the 9 chosen learning algorithms on a setof 54 data sets. In addition to using each learning algorithm individually as afilter, we also use the set of learning algorithms as an ensemble filter anduse an adaptive algorithm that selects a subset of the learning algorithms forfiltering for a specific task and learning algorithm. We find that for mostcases, using an ensemble of learning algorithms for filtering produces thegreatest increase in classification accuracy. We also compare filtering with amajority voting ensemble. The voting ensemble significantly outperformsfiltering unless there are high amounts of noise present in the data set.Additionally, we find that a majority voting ensemble is robust to noise asfiltering with a voting ensemble does not increase the classification accuracyof the voting ensemble.",http://arxiv.org/abs/1312.3970v1,,
1985,"Intrinsically Motivated Goal Exploration Processes with Automatic  Curriculum Learning","Intrinsically motivated spontaneous exploration is a key enabler ofautonomous lifelong learning in human children. It allows them to discover andacquire large repertoires of skills through self-generation, self-selection,self-ordering and self-experimentation of learning goals. We present theunsupervised multi-goal reinforcement learning formal framework as well as analgorithmic approach called intrinsically motivated goal exploration processes(IMGEP) to enable similar properties of autonomous learning in machines. TheIMGEP algorithmic architecture relies on several principles: 1) self-generationof goals as parameterized reinforcement learning problems; 2) selection ofgoals based on intrinsic rewards; 3) exploration with parameterizedtime-bounded policies and fast incremental goal-parameterized policy search; 4)systematic reuse of information acquired when targeting a goal for improvingother goals. We present a particularly efficient form of IMGEP that uses amodular representation of goal spaces as well as intrinsic rewards based onlearning progress. We show how IMGEPs automatically generate a learningcurriculum within an experimental setup where a real humanoid robot can exploremultiple spaces of goals with several hundred continuous dimensions. While noparticular target goal is provided to the system beforehand, this curriculumallows the discovery of skills of increasing complexity, that act as steppingstone for learning more complex skills (like nested tool use). We show thatlearning several spaces of diverse problems can be more efficient for learningcomplex skills than only trying to directly learn these complex skills. Weillustrate the computational efficiency of IMGEPs as these robotic experimentsuse a simple memory-based low-level policy representations and searchalgorithm, enabling the whole system to learn online and incrementally on aRaspberry Pi 3.",http://arxiv.org/abs/1708.02190v1,,
1986,Learning from Informants: Relations between Learning Success Criteria,"Learning from positive and negative information, so-called \emph{informants},being one of the models for human and machine learning introduced by Gold, isinvestigated. Particularly, naturally arising questions about this learningsetting, originating in results on learning from solely positive information,are answered. By a carefully arranged argument learners can be assumed to onlychange their hypothesis in case it is inconsistent with the data (such alearning behavior is called \emph{conservative}). The deduced main theoremstates the relations between the most important delayable learning successcriteria, being the ones not ruined by a delayed in time hypothesis output.Additionally, our investigations concerning the non-delayable requirement ofconsistent learning underpin the claim for \emph{delayability} being the rightstructural property to gain a deeper understanding concerning the nature oflearning success criteria. Moreover, we obtain an anomalous \emph{hierarchy}when allowing for an increasing finite number of \emph{anomalies} of thehypothesized language by the learner compared with the language to be learned.In contrast to the vacillatory hierarchy for learning from solely positiveinformation, we observe a \emph{duality} depending on whether infinitely many\emph{vacillations} between different (almost) correct hypotheses are stillconsidered a successful learning behavior.",http://arxiv.org/abs/1801.10502v3,,
1987,"Tree Edit Distance Learning via Adaptive Symbol Embeddings:  Supplementary Materials and Results","Metric learning has the aim to improve classification accuracy by learning adistance measure which brings data points from the same class closer togetherand pushes data points from different classes further apart. Recent researchhas demonstrated that metric learning approaches can also be applied to trees,such as molecular structures, abstract syntax trees of computer programs, orsyntax trees of natural language, by learning the cost function of an editdistance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.However, learning such costs directly may yield an edit distance which violatesmetric axioms, is challenging to interpret, and may not generalize well. Inthis contribution, we propose a novel metric learning approach for trees whichlearns an edit distance indirectly by embedding the tree nodes as vectors, suchthat the Euclidean distance between those vectors supports classdiscrimination. We learn such embeddings by reducing the distance toprototypical trees from the same class and increasing the distance toprototypical trees from different classes. In our experiments, we show that ourproposed metric learning approach improves upon the state-of-the-art in metriclearning for trees on six benchmark data sets, ranging from computer scienceover biomedical data to a natural-language processing data set containing over300,000 nodes.",http://arxiv.org/abs/1805.07123v1,,
1988,"Learning to Propagate Labels: Transductive Propagation Network for  Few-shot Learning","The goal of few-shot learning is to learn a classifier that generalizes welleven when trained with a limited number of training instances per class. Therecently introduced meta-learning approaches tackle this problem by learning ageneric classifier across a large number of multiclass classification tasks andgeneralizing the model to a new task. Yet, even with such meta-learning, thelow-data problem in the novel classification task still remains. In this paper,we propose Transductive Propagation Network (TPN), a novel meta-learningframework for transductive inference that classifies the entire test set atonce to alleviate the low-data problem. Specifically, we propose to learn topropagate labels from labeled instances to unlabeled test instances, bylearning a graph construction module that exploits the manifold structure inthe data. TPN jointly learns both the parameters of feature embedding and thegraph construction in an end-to-end manner. We validate TPN on multiplebenchmark datasets, on which it largely outperforms existing few-shot learningapproaches and achieves the state-of-the-art results.",http://arxiv.org/abs/1805.10002v5,,
1989,"Effective Feature Learning with Unsupervised Learning for Improving the  Predictive Models in Massive Open Online Courses","The effectiveness of learning in massive open online courses (MOOCs) can besignificantly enhanced by introducing personalized intervention schemes whichrely on building predictive models of student learning behaviors such as someengagement or performance indicators. A major challenge that has to beaddressed when building such models is to design handcrafted features that areeffective for the prediction task at hand. In this paper, we make the firstattempt to solve the feature learning problem by taking the unsupervisedlearning approach to learn a compact representation of the raw features with alarge degree of redundancy. Specifically, in order to capture the underlyinglearning patterns in the content domain and the temporal nature of theclickstream data, we train a modified auto-encoder (AE) combined with the longshort-term memory (LSTM) network to obtain a fixed-length embedding for eachinput sequence. When compared with the original features, the new features thatcorrespond to the embedding obtained by the modified LSTM-AE are not only moreparsimonious but also more discriminative for our prediction task. Using simplesupervised learning models, the learned features can improve the predictionaccuracy by up to 17% compared with the supervised neural networks and reduceoverfitting to the dominant low-performing group of students, specifically inthe task of predicting students' performance. Our approach is generic in thesense that it is not restricted to a specific supervised learning model nor aspecific prediction task for MOOC learning analytics.",http://arxiv.org/abs/1812.05044v2,,
1990,"Evaluation of Transfer Learning for Classification of: (1) Diabetic  Retinopathy by Digital Fundus Photography and (2) Diabetic Macular Edema,  Choroidal Neovascularization and Drusen by Optical Coherence Tomography","Deep learning has been successfully applied to a variety of imageclassification tasks. There has been keen interest to apply deep learning inthe medical domain, particularly specialties that heavily utilize imaging, suchas ophthalmology. One issue that may hinder application of deep learning to themedical domain is the vast amount of data necessary to train deep neuralnetworks (DNNs). Because of regulatory and privacy issues associated withmedicine, and the generally proprietary nature of data in medical domains,obtaining large datasets to train DNNs is a challenge, particularly in theophthalmology domain.  Transfer learning is a technique developed to address the issue of applyingDNNs for domains with limited data. Prior reports on transfer learning haveexamined custom networks to fully train or used a particular DNN for transferlearning. However, to the best of my knowledge, no work has systematicallyexamined a suite of DNNs for transfer learning for classification of diabeticretinopathy, diabetic macular edema, and two key features of age-relatedmacular degeneration. This work attempts to investigate transfer learning forclassification of these ophthalmic conditions. Part I gives a condensedoverview of neural networks and the DNNs under evaluation. Part II gives thereader the necessary background concerning diabetic retinopathy and prior workon classification using retinal fundus photographs. The methodology and resultsof transfer learning for diabetic retinopathy classification are presented,showing that transfer learning towards this domain is feasible, with promisingaccuracy. Part III gives an overview of diabetic macular edema, choroidalneovascularization and drusen (features associated with age-related maculardegeneration), and presents results for transfer learning evaluation usingoptical coherence tomography to classify these entities.",http://arxiv.org/abs/1902.04151v1,,
1991,"A Machine Learning Based Analytical Framework for Semantic Annotation  Requirements","The Semantic Web is an extension of the current web in which information isgiven well-defined meaning. The perspective of Semantic Web is to promote thequality and intelligence of the current web by changing its contents intomachine understandable form. Therefore, semantic level information is one ofthe cornerstones of the Semantic Web. The process of adding semantic metadatato web resources is called Semantic Annotation. There are many obstaclesagainst the Semantic Annotation, such as multilinguality, scalability, andissues which are related to diversity and inconsistency in content of differentweb pages. Due to the wide range of domains and the dynamic environments thatthe Semantic Annotation systems must be performed on, the problem of automatingannotation process is one of the significant challenges in this domain. Toovercome this problem, different machine learning approaches such as supervisedlearning, unsupervised learning and more recent ones like, semi-supervisedlearning and active learning have been utilized. In this paper we present aninclusive layered classification of Semantic Annotation challenges and discussthe most important issues in this field. Also, we review and analyze machinelearning applications for solving semantic annotation problems. For this goal,the article tries to closely study and categorize related researches for betterunderstanding and to reach a framework that can map machine learning techniquesinto the Semantic Annotation challenges and requirements.",http://arxiv.org/abs/1104.4950v1,,
1992,Practical Bayesian Optimization of Machine Learning Algorithms,"Machine learning algorithms frequently require careful tuning of modelhyperparameters, regularization terms, and optimization parameters.Unfortunately, this tuning is often a ""black art"" that requires expertexperience, unwritten rules of thumb, or sometimes brute-force search. Muchmore appealing is the idea of developing automatic approaches which canoptimize the performance of a given learning algorithm to the task at hand. Inthis work, we consider the automatic tuning problem within the framework ofBayesian optimization, in which a learning algorithm's generalizationperformance is modeled as a sample from a Gaussian process (GP). The tractableposterior distribution induced by the GP leads to efficient use of theinformation gathered by previous experiments, enabling optimal choices aboutwhat parameters to try next. Here we show how the effects of the Gaussianprocess prior and the associated inference procedure can have a large impact onthe success or failure of Bayesian optimization. We show that thoughtfulchoices can lead to results that exceed expert-level performance in tuningmachine learning algorithms. We also describe new algorithms that take intoaccount the variable cost (duration) of learning experiments and that canleverage the presence of multiple cores for parallel experimentation. We showthat these proposed algorithms improve on previous automatic procedures and canreach or surpass human expert-level optimization on a diverse set ofcontemporary algorithms including latent Dirichlet allocation, structured SVMsand convolutional neural networks.",http://arxiv.org/abs/1206.2944v2,,
1993,An Approach with Toric Varieties for Singular Learning Machines,"The Computational Algebraic Geometry applied in Algebraic Statistics; arebeginning to exploring new branches and applications; in artificialintelligence and others areas. Currently, the development of the mathematics isvery extensive and it is difficult to see the immediate application of fewtheorems in different areas, such as is the case of the Theorem 3.9 given in[10] and proved in part of here. Also this work has the intention to show theHilbert basis as a powerful tool in data science; and for that reason wecompile important results proved in works by, S. Watanabe [27], D. Cox, J.Little and H. Schenck [8], B. Sturmfels [16] and G. Ewald [10]. In this work westudy, first, the fundamental concepts in Toric Algebraic Geometry. Theprincipal contribution of this work is the application of Hilbert basis (as onerealization of Theorem 3.9) for the resolution of singularities with toricvarieties, and a background in Lattice Polytope. In the second part we applythis theorem to problems in statistical learning, principally in a recent areaas is the Singular Learning Theory. We define the singular machines and theproblem of Singular Learning through the computing of learning curves on thesestatistical machines. We review and compile results on the work of S. Watanabein Singular Learning Theory, ref.; [17], [20], [21], also revising theimportant result in [26], about almost the machines are singular, we formalizethis theory withtoric resolution morphism in a theorem proved here (Theorem5.4), characterizing these Learning Machines as toric varieties, and wereproduce results previously published in Singular Statistical Learning seen in[19], [20], [23].",http://arxiv.org/abs/1708.02273v1,,
1994,On better training the infinite restricted Boltzmann machines,"The infinite restricted Boltzmann machine (iRBM) is an extension of theclassic RBM. It enjoys a good property of automatically deciding the size ofthe hidden layer according to specific training data. With sufficient training,the iRBM can achieve a competitive performance with that of the classic RBM.However, the convergence of learning the iRBM is slow, due to the fact that theiRBM is sensitive to the ordering of its hidden units, the learned filterschange slowly from the left-most hidden unit to right. To break this dependencybetween neighboring hidden units and speed up the convergence of training, anovel training strategy is proposed. The key idea of the proposed trainingstrategy is randomly regrouping the hidden units before each gradient descentstep. Potentially, a mixing of infinite many iRBMs with different permutationsof the hidden units can be achieved by this learning method, which has asimilar effect of preventing the model from over-fitting as the dropout. Theoriginal iRBM is also modified to be capable of carrying out discriminativetraining. To evaluate the impact of our method on convergence speed of learningand the model's generalization ability, several experiments have been performedon the binarized MNIST and CalTech101 Silhouettes datasets. Experimentalresults indicate that the proposed training strategy can greatly acceleratelearning and enhance generalization ability of iRBMs.",http://arxiv.org/abs/1709.03239v2,,
1995,"Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning  Architectures","Objective: A clinical decision support tool that automatically interpretsEEGs can reduce time to diagnosis and enhance real-time applications such asICU monitoring. Clinicians have indicated that a sensitivity of 95% with aspecificity below 5% was the minimum requirement for clinical acceptance. Wepropose a highperformance classification system based on principles of big dataand machine learning. Methods: A hybrid machine learning system that useshidden Markov models (HMM) for sequential decoding and deep learning networksfor postprocessing is proposed. These algorithms were trained and evaluatedusing the TUH EEG Corpus, which is the world's largest publicly availabledatabase of clinical EEG data. Results: Our approach delivers a sensitivityabove 90% while maintaining a specificity below 5%. This system detects threeevents of clinical interest: (1) spike and/or sharp waves, (2) periodiclateralized epileptiform discharges, (3) generalized periodic epileptiformdischarges. It also detects three events used to model background noise: (1)artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deeplearning system can deliver a low false alarm rate on EEG event detection,making automated analysis a viable option for clinicians. Significance: The TUHEEG Corpus enables application of highly data consumptive machine learningalgorithms to EEG analysis. Performance is approaching clinical acceptance forreal-time applications.",http://arxiv.org/abs/1712.09771v1,,
1996,Faster Learning by Reduction of Data Access Time,"Nowadays, the major challenge in machine learning is the Big Data challenge.The big data problems due to large number of data points or large number offeatures in each data point, or both, the training of models have become veryslow. The training time has two major components: Time to access the data andtime to process (learn from) the data. So far, the research has focused only onthe second part, i.e., learning from the data. In this paper, we have proposedone possible solution to handle the big data problems in machine learning. Theidea is to reduce the training time through reducing data access time byproposing systematic sampling and cyclic/sequential sampling to selectmini-batches from the dataset. To prove the effectiveness of proposed samplingtechniques, we have used Empirical Risk Minimization, which is commonly usedmachine learning problem, for strongly convex and smooth case. The problem hasbeen solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), eachusing two step determination techniques, namely, constant step size andbacktracking line search method. Theoretical results prove the same convergencefor systematic sampling, cyclic sampling and the widely used random samplingtechnique, in expectation. Experimental results with bench marked datasetsprove the efficacy of the proposed sampling techniques and show up to six timesfaster training.",http://arxiv.org/abs/1801.05931v4,,
1997,"DeepDRR -- A Catalyst for Machine Learning in Fluoroscopy-guided  Procedures","Machine learning-based approaches outperform competing methods in mostdisciplines relevant to diagnostic radiology. Interventional radiology,however, has not yet benefited substantially from the advent of deep learning,in particular because of two reasons: 1) Most images acquired during theprocedure are never archived and are thus not available for learning, and 2)even if they were available, annotations would be a severe challenge due to thevast amounts of data. When considering fluoroscopy-guided procedures, aninteresting alternative to true interventional fluoroscopy is in silicosimulation of the procedure from 3D diagnostic CT. In this case, labeling iscomparably easy and potentially readily available, yet, the appropriateness ofresulting synthetic data is dependent on the forward model. In this work, wepropose DeepDRR, a framework for fast and realistic simulation of fluoroscopyand digital radiography from CT scans, tightly integrated with the softwareplatforms native to deep learning. We use machine learning for materialdecomposition and scatter estimation in 3D and 2D, respectively, combined withanalytic forward projection and noise injection to achieve the requiredperformance. On the example of anatomical landmark detection in X-ray images ofthe pelvis, we demonstrate that machine learning models trained on DeepDRRsgeneralize to unseen clinically acquired data without the need for re-trainingor domain adaptation. Our results are promising and promote the establishmentof machine learning in fluoroscopy-guided procedures.",http://arxiv.org/abs/1803.08606v1,,
1998,Supervised learning with generalized tensor networks,"Tensor networks have found a wide use in a variety of applications in physicsand computer science, recently leading to both theoretical insights as well aspractical algorithms in machine learning. In this work we explore theconnection between tensor networks and probabilistic graphical models, and showthat it motivates the definition of generalized tensor networks whereinformation from a tensor can be copied and reused in other parts of thenetwork. We discuss the relationship between generalized tensor networkarchitectures used in quantum physics, such as String-Bond States and EntangledPlaquette States, and architectures commonly used in machine learning. Weprovide an algorithm to train these networks in a supervised learning contextand show that they overcome the limitations of regular tensor networks inhigher dimensions, while keeping the computation efficient. A method to combineneural networks and tensor networks as part of a common deep learningarchitecture is also introduced. We benchmark our algorithm for severalgeneralized tensor network architectures on the task of classifying images andsounds, and show that they outperform previously introduced tensor networkalgorithms. Some of the models we consider can be realized on a quantumcomputer and may guide the development of near-term quantum machine learningarchitectures.",http://arxiv.org/abs/1806.05964v1,,
1999,A feature agnostic approach for glaucoma detection in OCT volumes,"Optical coherence tomography (OCT) based measurements of retinal layerthickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cellwith inner plexiform layer (GCIPL) are commonly used for the diagnosis andmonitoring of glaucoma. Previously, machine learning techniques have utilizedsegmentation-based imaging features such as the peripapillary RNFL thicknessand the cup-to-disc ratio. Here, we propose a deep learning technique thatclassifies eyes as healthy or glaucomatous directly from raw, unsegmented OCTvolumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network(CNN). We compared the accuracy of this technique with various feature-basedmachine learning algorithms and demonstrated the superiority of the proposeddeep learning based method.  Logistic regression was found to be the best performing classical machinelearning technique with an AUC of 0.89. In direct comparison, the deep learningapproach achieved a substantially higher AUC of 0.94 with the additionaladvantage of providing insight into which regions of an OCT volume areimportant for glaucoma detection.  Computing Class Activation Maps (CAM), we found that the CNN identifiedneuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) andits surrounding areas as the regions significantly associated with the glaucomaclassification. These regions anatomically correspond to the well establishedand commonly used clinical markers for glaucoma diagnosis such as increased cupvolume, cup diameter, and neuroretinal rim thinning at the superior andinferior segments.",http://arxiv.org/abs/1807.04855v2,,