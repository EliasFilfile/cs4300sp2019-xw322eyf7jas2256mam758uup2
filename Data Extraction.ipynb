{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import feedparser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"machine+learning\"\n",
    "max_results = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://export.arxiv.org/api/query?search_query=all:\"+search_term+\"&start=0&max_results=\"+str(max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://export.arxiv.org/api/query?search_query=all:machine+learning&start=0&max_results=1000'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Amachine%20learning%26id_list%3D%26start%3D0%26max_results%3D1000\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=all:machine learning&amp;id_list=&amp;start=0&amp;max_results=1000</title>\\n  <id>http://arxiv.org/api/cJ94CoIB99FKowsEQVatCUfLlTs</id>\\n  <updated>2019-04-12T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">77475</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1000</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.04422v1</id>\\n    <updated>2018-11-11T14:28:34Z</updated>\\n    <published>2018-11-11T14:28:34Z</published>\\n    <title>An Optimal Control View of Adversarial Machine Learning</title>\\n    <summary>  I describe an optimal control view of adversarial machine learning, where the\\ndynamical system is the machine learner, the input are adversarial actions, and\\nthe control costs are defined by the adversary\\'s goals to do harm and be hard\\nto detect. This view encompasses many types of adversarial machine learning,\\nincluding test-item attacks, training-data poisoning, and adversarial reward\\nshaping. The view encourages adversarial machine learning researcher to utilize\\nadvances in control theory and reinforcement learning.\\n</summary>\\n    <author>\\n      <name>Xiaojin Zhu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.04422v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.04422v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.04849v1</id>\\n    <updated>2017-07-16T09:15:08Z</updated>\\n    <published>2017-07-16T09:15:08Z</published>\\n    <title>Minimax deviation strategies for machine learning and recognition with\\n  short learning samples</title>\\n    <summary>  The article is devoted to the problem of small learning samples in machine\\nlearning. The flaws of maximum likelihood learning and minimax learning are\\nlooked into and the concept of minimax deviation learning is introduced that is\\nfree of those flaws.\\n</summary>\\n    <author>\\n      <name>Michail Schlesinger</name>\\n    </author>\\n    <author>\\n      <name>Evgeniy Vodolazskiy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.04849v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.04849v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0904.3664v1</id>\\n    <updated>2009-04-23T11:40:57Z</updated>\\n    <published>2009-04-23T11:40:57Z</published>\\n    <title>Introduction to Machine Learning: Class Notes 67577</title>\\n    <summary>  Introduction to Machine learning covering Statistical Inference (Bayes, EM,\\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).\\n</summary>\\n    <author>\\n      <name>Amnon Shashua</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">109 pages, class notes of Machine Learning course given at the Hebrew\\n  University of Jerusalem</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0904.3664v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.3664v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.08801v1</id>\\n    <updated>2019-03-21T02:17:08Z</updated>\\n    <published>2019-03-21T02:17:08Z</published>\\n    <title>A Unified Analytical Framework for Trustable Machine Learning and\\n  Automation Running with Blockchain</title>\\n    <summary>  Traditional machine learning algorithms use data from databases that are\\nmutable, and therefore the data cannot be fully trusted. Also, the machine\\nlearning process is difficult to automate. This paper proposes building a\\ntrustable machine learning system by using blockchain technology, which can\\nstore data in a permanent and immutable way. In addition, smart contracts are\\nused to automate the machine learning process. This paper makes three\\ncontributions. First, it establishes a link between machine learning technology\\nand blockchain technology. Previously, machine learning and blockchain have\\nbeen considered two independent technologies without an obvious link. Second,\\nit proposes a unified analytical framework for trustable machine learning by\\nusing blockchain technology. This unified framework solves both the\\ntrustability and automation issues in machine learning. Third, it enables a\\ncomputer to translate core machine learning implementation from a single thread\\non a single machine to multiple threads on multiple machines running with\\nblockchain by using a unified approach. The paper uses association rule mining\\nas an example to demonstrate how trustable machine learning can be implemented\\nwith blockchain, and it shows how this approach can be used to analyze opioid\\nprescriptions to help combat the opioid crisis.\\n</summary>\\n    <author>\\n      <name>Tao Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, IEEE Big Data Workshops, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.08801v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.08801v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.09562v3</id>\\n    <updated>2017-10-16T11:13:32Z</updated>\\n    <published>2017-07-29T21:59:18Z</published>\\n    <title>MLBench: How Good Are Machine Learning Clouds for Binary Classification\\n  Tasks on Structured Data?</title>\\n    <summary>  We conduct an empirical study of machine learning functionalities provided by\\nmajor cloud service providers, which we call machine learning clouds. Machine\\nlearning clouds hold the promise of hiding all the sophistication of running\\nlarge-scale machine learning: Instead of specifying how to run a machine\\nlearning task, users only specify what machine learning task to run and the\\ncloud figures out the rest. Raising the level of abstraction, however, rarely\\ncomes free - a performance penalty is possible. How good, then, are current\\nmachine learning clouds on real-world machine learning workloads?\\n  We study this question with a focus on binary classication problems. We\\npresent mlbench, a novel benchmark constructed by harvesting datasets from\\nKaggle competitions. We then compare the performance of the top winning code\\navailable from Kaggle with that of running machine learning clouds from both\\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\\nweakness of existing machine learning clouds and points out potential future\\ndirections for improvement.\\n</summary>\\n    <author>\\n      <name>Yu Liu</name>\\n    </author>\\n    <author>\\n      <name>Hantian Zhang</name>\\n    </author>\\n    <author>\\n      <name>Luyuan Zeng</name>\\n    </author>\\n    <author>\\n      <name>Wentao Wu</name>\\n    </author>\\n    <author>\\n      <name>Ce Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.09562v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.09562v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.02188v1</id>\\n    <updated>2015-07-08T15:07:39Z</updated>\\n    <published>2015-07-08T15:07:39Z</published>\\n    <title>AutoCompete: A Framework for Machine Learning Competition</title>\\n    <summary>  In this paper, we propose AutoCompete, a highly automated machine learning\\nframework for tackling machine learning competitions. This framework has been\\nlearned by us, validated and improved over a period of more than two years by\\nparticipating in online machine learning competitions. It aims at minimizing\\nhuman interference required to build a first useful predictive model and to\\nassess the practical difficulty of a given machine learning challenge. The\\nproposed system helps in identifying data types, choosing a machine learn- ing\\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\\nprovided evaluation metric. We also observe that the proposed system produces\\nbetter (or comparable) results with less runtime as compared to other\\napproaches.\\n</summary>\\n    <author>\\n      <name>Abhishek Thakur</name>\\n    </author>\\n    <author>\\n      <name>Artus Krohn-Grimberghe</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Paper at AutoML workshop in ICML, 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.02188v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.02188v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.02450v2</id>\\n    <updated>2016-08-28T15:23:47Z</updated>\\n    <published>2016-07-08T16:55:31Z</published>\\n    <title>Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in\\n  Social Good Applications</title>\\n    <summary>  This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\\nin Social Good Applications, which was held on June 24, 2016 in New York.\\n</summary>\\n    <author>\\n      <name>Kush R. Varshney</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1607.02450v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.02450v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.2686v1</id>\\n    <updated>2012-12-12T01:59:27Z</updated>\\n    <published>2012-12-12T01:59:27Z</published>\\n    <title>Joint Training of Deep Boltzmann Machines</title>\\n    <summary>  We introduce a new method for training deep Boltzmann machines jointly. Prior\\nmethods require an initial learning pass that trains the deep Boltzmann machine\\ngreedily, one layer at a time, or do not perform well on classifi- cation\\ntasks.\\n</summary>\\n    <author>\\n      <name>Ian Goodfellow</name>\\n    </author>\\n    <author>\\n      <name>Aaron Courville</name>\\n    </author>\\n    <author>\\n      <name>Yoshua Bengio</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1212.2686v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.2686v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.05353v1</id>\\n    <updated>2019-01-16T15:47:51Z</updated>\\n    <published>2019-01-16T15:47:51Z</published>\\n    <title>A Primer on PAC-Bayesian Learning</title>\\n    <summary>  Generalized Bayesian learning algorithms are increasingly popular in machine\\nlearning, due to their PAC generalization properties and flexibility. The\\npresent paper aims at providing a self-contained survey on the resulting\\nPAC-Bayes framework and some of its main theoretical and algorithmic\\ndevelopments.\\n</summary>\\n    <author>\\n      <name>Benjamin Guedj</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.05353v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.05353v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.08001v1</id>\\n    <updated>2017-06-24T20:56:27Z</updated>\\n    <published>2017-06-24T20:56:27Z</published>\\n    <title>Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of\\n  learning relational order via reinforcement learning procedure?</title>\\n    <summary>  In this article, we extend the conventional framework of\\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\\namong abitrary number of time related input maps by constructing a layer of\\nmultiplicative units, which capture the relations among inputs. In many cases,\\nmore than two maps are strongly related, so it is wise to make multiplicative\\nunit learn relations among more input maps, in other words, to find the optimal\\nrelational-order of each unit. In order to enable our machine to learn\\nrelational order, we developed a reinforcement-learning method whose optimality\\nis proven to train the network.\\n</summary>\\n    <author>\\n      <name>Zizhuang Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Keywords: Convolutional-Restricted-Boltzmann-Machine, Reinforcement\\n  learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.08001v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.08001v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.4676v2</id>\\n    <updated>2012-09-16T11:24:54Z</updated>\\n    <published>2012-07-19T14:08:22Z</published>\\n    <title>Proceedings of the 29th International Conference on Machine Learning\\n  (ICML-12)</title>\\n    <summary>  This is an index to the papers that appear in the Proceedings of the 29th\\nInternational Conference on Machine Learning (ICML-12). The conference was held\\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.\\n</summary>\\n    <author>\\n      <name>John Langford</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Editors</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Joelle Pineau</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Editors</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 29th International Conference on Machine Learning\\n  (ICML-12). Editors: John Langford and Joelle Pineau. Publisher: Omnipress,\\n  2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1207.4676v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.4676v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.02185v1</id>\\n    <updated>2016-03-07T18:11:54Z</updated>\\n    <published>2016-03-07T18:11:54Z</published>\\n    <title>Distributed Multi-Task Learning with Shared Representation</title>\\n    <summary>  We study the problem of distributed multi-task learning with shared\\nrepresentation, where each machine aims to learn a separate, but related, task\\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\\nhas low rank. We consider a setting where each task is handled by a different\\nmachine, with samples for the task available locally on the machine, and study\\ncommunication-efficient methods for exploiting the shared structure.\\n</summary>\\n    <author>\\n      <name>Jialei Wang</name>\\n    </author>\\n    <author>\\n      <name>Mladen Kolar</name>\\n    </author>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1603.02185v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.02185v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1509.00913v3</id>\\n    <updated>2015-09-29T16:57:42Z</updated>\\n    <published>2015-09-03T01:30:29Z</published>\\n    <title>On-the-Fly Learning in a Perpetual Learning Machine</title>\\n    <summary>  Despite the promise of brain-inspired machine learning, deep neural networks\\n(DNN) have frustratingly failed to bridge the deceptively large gap between\\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\\ntype of DNN that is capable of brain-like dynamic \\'on the fly\\' learning because\\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\\nThus, we provide the means to unify learning and memory within a machine\\nlearning framework. We also explore the elegant duality of abstraction and\\nsynthesis: the Yin and Yang of deep learning.\\n</summary>\\n    <author>\\n      <name>Andrew J. R. Simpson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1509.00913v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1509.00913v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Txx\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1510.00633v1</id>\\n    <updated>2015-10-02T16:15:30Z</updated>\\n    <published>2015-10-02T16:15:30Z</published>\\n    <title>Distributed Multitask Learning</title>\\n    <summary>  We consider the problem of distributed multi-task learning, where each\\nmachine learns a separate, but related, task. Specifically, each machine learns\\na linear predictor in high-dimensional space,where all tasks share the same\\nsmall support. We present a communication-efficient estimator based on the\\ndebiased lasso and show that it is comparable with the optimal centralized\\nmethod.\\n</summary>\\n    <author>\\n      <name>Jialei Wang</name>\\n    </author>\\n    <author>\\n      <name>Mladen Kolar</name>\\n    </author>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1510.00633v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1510.00633v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.03830v1</id>\\n    <updated>2018-02-11T22:23:34Z</updated>\\n    <published>2018-02-11T22:23:34Z</published>\\n    <title>Distributed Stochastic Multi-Task Learning with Graph Regularization</title>\\n    <summary>  We propose methods for distributed graph-based multi-task learning that are\\nbased on weighted averaging of messages from other machines. Uniform averaging\\nor diminishing stepsize in these methods would yield consensus (single task)\\nlearning. We show how simply skewing the averaging weights or controlling the\\nstepsize allows learning different, but related, tasks on the different\\nmachines.\\n</summary>\\n    <author>\\n      <name>Weiran Wang</name>\\n    </author>\\n    <author>\\n      <name>Jialei Wang</name>\\n    </author>\\n    <author>\\n      <name>Mladen Kolar</name>\\n    </author>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.03830v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.03830v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.01400v1</id>\\n    <updated>2016-07-05T20:04:57Z</updated>\\n    <published>2016-07-05T20:04:57Z</published>\\n    <title>An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\\n  in Machine Learning</title>\\n    <summary>  We propose a clustering-based iterative algorithm to solve certain\\noptimization problems in machine learning, where we start the algorithm by\\naggregating the original data, solving the problem on aggregated data, and then\\nin subsequent steps gradually disaggregate the aggregated data. We apply the\\nalgorithm to common machine learning problems such as the least absolute\\ndeviation regression problem, support vector machines, and semi-supervised\\nsupport vector machines. We derive model-specific data aggregation and\\ndisaggregation procedures. We also show optimality, convergence, and the\\noptimality gap of the approximated solution in each iteration. A computational\\nstudy is provided.\\n</summary>\\n    <author>\\n      <name>Young Woong Park</name>\\n    </author>\\n    <author>\\n      <name>Diego Klabjan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s10994-016-5562-z</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s10994-016-5562-z\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning 105 (2016) 199 - 232</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1607.01400v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.01400v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.04858v1</id>\\n    <updated>2016-12-14T22:04:33Z</updated>\\n    <published>2016-12-14T22:04:33Z</published>\\n    <title>Bayesian Optimization for Machine Learning : A Practical Guidebook</title>\\n    <summary>  The engineering of machine learning systems is still a nascent field; relying\\non a seemingly daunting collection of quickly evolving tools and best\\npractices. It is our hope that this guidebook will serve as a useful resource\\nfor machine learning practitioners looking to take advantage of Bayesian\\noptimization techniques. We outline four example machine learning problems that\\ncan be solved using open source machine learning libraries, and highlight the\\nbenefits of using Bayesian optimization in the context of these common machine\\nlearning applications.\\n</summary>\\n    <author>\\n      <name>Ian Dewancker</name>\\n    </author>\\n    <author>\\n      <name>Michael McCourt</name>\\n    </author>\\n    <author>\\n      <name>Scott Clark</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.04858v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.04858v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.08608v2</id>\\n    <updated>2017-03-02T19:32:10Z</updated>\\n    <published>2017-02-28T02:19:20Z</published>\\n    <title>Towards A Rigorous Science of Interpretable Machine Learning</title>\\n    <summary>  As machine learning systems become ubiquitous, there has been a surge of\\ninterest in interpretable machine learning: systems that provide explanation\\nfor their outputs. These explanations are often used to qualitatively assess\\nother criteria such as safety or non-discrimination. However, despite the\\ninterest in interpretability, there is very little consensus on what\\ninterpretable machine learning is and how it should be measured. In this\\nposition paper, we first define interpretability and describe when\\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\\nfor rigorous evaluation and expose open questions towards a more rigorous\\nscience of interpretable machine learning.\\n</summary>\\n    <author>\\n      <name>Finale Doshi-Velez</name>\\n    </author>\\n    <author>\\n      <name>Been Kim</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1702.08608v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.08608v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.07538v2</id>\\n    <updated>2017-06-09T02:13:09Z</updated>\\n    <published>2017-05-22T02:28:19Z</published>\\n    <title>Infrastructure for Usable Machine Learning: The Stanford DAWN Project</title>\\n    <summary>  Despite incredible recent advances in machine learning, building machine\\nlearning applications remains prohibitively time-consuming and expensive for\\nall but the best-trained, best-funded engineering organizations. This expense\\ncomes not from a need for new and improved statistical models but instead from\\na lack of systems and tools for supporting end-to-end machine learning\\napplication development, from data preparation and labeling to\\nproductionization and monitoring. In this document, we outline opportunities\\nfor infrastructure supporting usable, end-to-end machine learning applications\\nin the context of the nascent DAWN (Data Analytics for What\\'s Next) project at\\nStanford.\\n</summary>\\n    <author>\\n      <name>Peter Bailis</name>\\n    </author>\\n    <author>\\n      <name>Kunle Olukotun</name>\\n    </author>\\n    <author>\\n      <name>Christopher Re</name>\\n    </author>\\n    <author>\\n      <name>Matei Zaharia</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.07538v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.07538v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.02969v3</id>\\n    <updated>2018-06-27T06:43:29Z</updated>\\n    <published>2018-04-09T13:28:56Z</published>\\n    <title>A review of possible effects of cognitive biases on interpretation of\\n  rule-based machine learning models</title>\\n    <summary>  This paper investigates to what extent cognitive biases may affect human\\nunderstanding of interpretable machine learning models, in particular of rules\\ndiscovered from data. Twenty cognitive biases are covered, as are possible\\ndebiasing techniques that can be adopted by designers of machine learning\\nalgorithms and software. Our review transfers results obtained in cognitive\\npsychology to the domain of machine learning, aiming to bridge the current gap\\nbetween these two areas. It needs to be followed by empirical studies\\nspecifically aimed at the machine learning domain.\\n</summary>\\n    <author>\\n      <name>Tom\\xc3\\xa1\\xc5\\xa1 Kliegr</name>\\n    </author>\\n    <author>\\n      <name>\\xc5\\xa0t\\xc4\\x9bp\\xc3\\xa1n Bahn\\xc3\\xadk</name>\\n    </author>\\n    <author>\\n      <name>Johannes F\\xc3\\xbcrnkranz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.02969v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.02969v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00033v2</id>\\n    <updated>2018-12-10T17:24:35Z</updated>\\n    <published>2018-07-31T19:14:39Z</published>\\n    <title>Techniques for Interpretable Machine Learning</title>\\n    <summary>  Interpretable machine learning tackles the important problem that humans\\ncannot understand the behaviors of complex machine learning models and how\\nthese models arrive at a particular decision. Although many approaches have\\nbeen proposed, a comprehensive understanding of the achievements and challenges\\nis still lacking. We provide a survey covering existing techniques to increase\\nthe interpretability of machine learning models. We also discuss crucial issues\\nthat the community should consider in future work such as designing\\nuser-friendly explanations and developing comprehensive evaluation metrics to\\nfurther push forward the area of interpretable machine learning.\\n</summary>\\n    <author>\\n      <name>Mengnan Du</name>\\n    </author>\\n    <author>\\n      <name>Ninghao Liu</name>\\n    </author>\\n    <author>\\n      <name>Xia Hu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.00033v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00033v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.05331v2</id>\\n    <updated>2019-01-21T16:34:59Z</updated>\\n    <published>2019-01-16T15:04:16Z</published>\\n    <title>Optimization Models for Machine Learning: A Survey</title>\\n    <summary>  This paper surveys the machine learning literature and presents machine\\nlearning as optimization models. Such models can benefit from the advancement\\nof numerical optimization techniques which have already played a distinctive\\nrole in several machine learning settings. Particularly, mathematical\\noptimization models are presented for commonly used machine learning approaches\\nfor regression, classification, clustering, and deep neural networks as well\\nnew emerging applications in machine teaching and empirical model learning. The\\nstrengths and the shortcomings of these models are discussed and potential\\nresearch directions are highlighted.\\n</summary>\\n    <author>\\n      <name>Claudio Gambella</name>\\n    </author>\\n    <author>\\n      <name>Bissan Ghaddar</name>\\n    </author>\\n    <author>\\n      <name>Joe Naoum-Sawaya</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.05331v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.05331v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.03548v1</id>\\n    <updated>2018-10-08T16:07:11Z</updated>\\n    <published>2018-10-08T16:07:11Z</published>\\n    <title>Meta-Learning: A Survey</title>\\n    <summary>  Meta-learning, or learning to learn, is the science of systematically\\nobserving how different machine learning approaches perform on a wide range of\\nlearning tasks, and then learning from this experience, or meta-data, to learn\\nnew tasks much faster than otherwise possible. Not only does this dramatically\\nspeed up and improve the design of machine learning pipelines or neural\\narchitectures, it also allows us to replace hand-engineered algorithms with\\nnovel approaches learned in a data-driven way. In this chapter, we provide an\\noverview of the state of the art in this fascinating and continuously evolving\\nfield.\\n</summary>\\n    <author>\\n      <name>Joaquin Vanschoren</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.03548v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.03548v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.06552v1</id>\\n    <updated>2017-11-15T16:52:48Z</updated>\\n    <published>2017-11-15T16:52:48Z</published>\\n    <title>Introduction to intelligent computing unit 1</title>\\n    <summary>  This brief note highlights some basic concepts required toward understanding\\nthe evolution of machine learning and deep learning models. The note starts\\nwith an overview of artificial intelligence and its relationship to biological\\nneuron that ultimately led to the evolution of todays intelligent models.\\n</summary>\\n    <author>\\n      <name>Isa Inuwa-Dutse</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">23 Pages and 10 figures document</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.06552v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.06552v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.01410v1</id>\\n    <updated>2018-12-04T13:50:11Z</updated>\\n    <published>2018-12-04T13:50:11Z</published>\\n    <title>Compressive Classification (Machine Learning without learning)</title>\\n    <summary>  Compressive learning is a framework where (so far unsupervised) learning\\ntasks use not the entire dataset but a compressed summary (sketch) of it. We\\npropose a compressive learning classification method, and a novel sketch\\nfunction for images.\\n</summary>\\n    <author>\\n      <name>Vincent Schellekens</name>\\n    </author>\\n    <author>\\n      <name>Laurent Jacques</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">in Proceedings of iTWIST\\'18, Paper-ID: 8, Marseille, France,\\n  November, 21-23, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.01410v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.01410v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06722v2</id>\\n    <updated>2018-07-25T07:23:45Z</updated>\\n    <published>2018-07-18T00:50:18Z</published>\\n    <title>Machine Learning Interpretability: A Science rather than a tool</title>\\n    <summary>  The term \"interpretability\" is oftenly used by machine learning researchers\\neach with their own intuitive understanding of it. There is no universal well\\nagreed upon definition of interpretability in machine learning. As any type of\\nscience discipline is mainly driven by the set of formulated questions rather\\nthan by different tools in that discipline, e.g. astrophysics is the discipline\\nthat learns the composition of stars, not as the discipline that use the\\nspectroscopes. Similarly, we propose that machine learning interpretability\\nshould be a discipline that answers specific questions related to\\ninterpretability. These questions can be of statistical, causal and\\ncounterfactual nature. Therefore, there is a need to look into the\\ninterpretability problem of machine learning in the context of questions that\\nneed to be addressed rather than different tools. We discuss about a\\nhypothetical interpretability framework driven by a question based scientific\\napproach rather than some specific machine learning model. Using a question\\nbased notion of interpretability, we can step towards understanding the science\\nof machine learning rather than its engineering. This notion will also help us\\nunderstanding any specific problem more in depth rather than relying solely on\\nmachine learning methods.\\n</summary>\\n    <author>\\n      <name>Abdul Karim</name>\\n    </author>\\n    <author>\\n      <name>Avinash Mishra</name>\\n    </author>\\n    <author>\\n      <name>MA Hakim Newton</name>\\n    </author>\\n    <author>\\n      <name>Abdul Sattar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.06722v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06722v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.03184v1</id>\\n    <updated>2017-07-11T09:15:46Z</updated>\\n    <published>2017-07-11T09:15:46Z</published>\\n    <title>A Survey on Resilient Machine Learning</title>\\n    <summary>  Machine learning based system are increasingly being used for sensitive tasks\\nsuch as security surveillance, guiding autonomous vehicle, taking investment\\ndecisions, detecting and blocking network intrusion and malware etc. However,\\nrecent research has shown that machine learning models are venerable to attacks\\nby adversaries at all phases of machine learning (eg, training data collection,\\ntraining, operation). All model classes of machine learning systems can be\\nmisled by providing carefully crafted inputs making them wrongly classify\\ninputs. Maliciously created input samples can affect the learning process of a\\nML system by either slowing down the learning process, or affecting the\\nperformance of the learned mode, or causing the system make error(s) only in\\nattacker\\'s planned scenario. Because of these developments, understanding\\nsecurity of machine learning algorithms and systems is emerging as an important\\nresearch area among computer security and machine learning researchers and\\npractitioners. We present a survey of this emerging area in machine learning.\\n</summary>\\n    <author>\\n      <name>Atul Kumar</name>\\n    </author>\\n    <author>\\n      <name>Sameep Mehta</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.03184v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.03184v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1602.00198v1</id>\\n    <updated>2016-01-31T04:05:50Z</updated>\\n    <published>2016-01-31T04:05:50Z</published>\\n    <title>Discussion on Mechanical Learning and Learning Machine</title>\\n    <summary>  Mechanical learning is a computing system that is based on a set of simple\\nand fixed rules, and can learn from incoming data. A learning machine is a\\nsystem that realizes mechanical learning. Importantly, we emphasis that it is\\nbased on a set of simple and fixed rules, contrasting to often called machine\\nlearning that is sophisticated software based on very complicated mathematical\\ntheory, and often needs human intervene for software fine tune and manual\\nadjustments. Here, we discuss some basic facts and principles of such system,\\nand try to lay down a framework for further study. We propose 2 directions to\\napproach mechanical learning, just like Church-Turing pair: one is trying to\\nrealize a learning machine, another is trying to well describe the mechanical\\nlearning.\\n</summary>\\n    <author>\\n      <name>Chuyu Xiong</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1602.00198v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.00198v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.04309v1</id>\\n    <updated>2015-01-18T14:57:02Z</updated>\\n    <published>2015-01-18T14:57:02Z</published>\\n    <title>Information Theory and its Relation to Machine Learning</title>\\n    <summary>  In this position paper, I first describe a new perspective on machine\\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\\nTowards this primary problem within the four levels, I briefly review the\\nexisting studies about the connection between information theoretical learning\\n(ITL [1]) and machine learning. A theorem is given on the relation between the\\nempirically-defined similarity measure and information measures. Finally, a\\nconjecture is proposed for pursuing a unified mathematical interpretation to\\nlearning target selection.\\n</summary>\\n    <author>\\n      <name>Bao-Gang Hu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 6 figures, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1501.04309v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.04309v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.03969v1</id>\\n    <updated>2016-11-12T08:18:38Z</updated>\\n    <published>2016-11-12T08:18:38Z</published>\\n    <title>An Introduction to MM Algorithms for Machine Learning and Statistical</title>\\n    <summary>  MM (majorization--minimization) algorithms are an increasingly popular tool\\nfor solving optimization problems in machine learning and statistical\\nestimation. This article introduces the MM algorithm framework in general and\\nvia three popular example applications: Gaussian mixture regressions,\\nmultinomial logistic regressions, and support vector machines. Specific\\nalgorithms for the three examples are derived and numerical demonstrations are\\npresented. Theoretical and practical aspects of MM algorithm design are\\ndiscussed.\\n</summary>\\n    <author>\\n      <name>Hien D. Nguyen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1611.03969v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.03969v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.11383v2</id>\\n    <updated>2018-11-08T01:03:50Z</updated>\\n    <published>2018-10-25T02:53:14Z</published>\\n    <title>Some Requests for Machine Learning Research from the East African Tech\\n  Scene</title>\\n    <summary>  Based on 46 in-depth interviews with scientists, engineers, and CEOs, this\\ndocument presents a list of concrete machine research problems, progress on\\nwhich would directly benefit tech ventures in East Africa.\\n</summary>\\n    <author>\\n      <name>Milan Cvitkovic</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2018 Workshop on Machine Learning for the\\n  Developing World</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.11383v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.11383v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.1304v1</id>\\n    <updated>2014-05-03T14:26:42Z</updated>\\n    <published>2014-05-03T14:26:42Z</published>\\n    <title>Application of Machine Learning Techniques in Aquaculture</title>\\n    <summary>  In this paper we present applications of different machine learning\\nalgorithms in aquaculture. Machine learning algorithms learn models from\\nhistorical data. In aquaculture historical data are obtained from farm\\npractices, yields, and environmental data sources. Associations between these\\ndifferent variables can be obtained by applying machine learning algorithms to\\nhistorical data. In this paper we present applications of different machine\\nlearning algorithms in aquaculture applications.\\n</summary>\\n    <author>\\n      <name>Akhlaqur Rahman</name>\\n    </author>\\n    <author>\\n      <name>Sumaira Tasnim</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.14445/22312803/IJCTT-V10P137</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.14445/22312803/IJCTT-V10P137\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Computer Trends and Technology (IJCTT)\\n  V10(3):214-215 Apr 2014. ISSN:2231-2803</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1405.1304v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.1304v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.04251v1</id>\\n    <updated>2016-12-13T16:00:51Z</updated>\\n    <published>2016-12-13T16:00:51Z</published>\\n    <title>TF.Learn: TensorFlow\\'s High-level Module for Distributed Machine\\n  Learning</title>\\n    <summary>  TF.Learn is a high-level Python module for distributed machine learning\\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\\nsimplify the process of creating, configuring, training, evaluating, and\\nexperimenting a machine learning model. TF.Learn integrates a wide range of\\nstate-of-art machine learning algorithms built on top of TensorFlow\\'s low level\\nAPIs for small to large-scale supervised and unsupervised problems. This module\\nfocuses on bringing machine learning to non-specialists using a general-purpose\\nhigh-level language as well as researchers who want to implement, benchmark,\\nand compare their new methods in a structured environment. Emphasis is put on\\nease of use, performance, documentation, and API consistency.\\n</summary>\\n    <author>\\n      <name>Yuan Tang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.04251v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.04251v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.07826v1</id>\\n    <updated>2017-08-24T20:19:20Z</updated>\\n    <published>2017-08-24T20:19:20Z</published>\\n    <title>Logistic Regression as Soft Perceptron Learning</title>\\n    <summary>  We comment on the fact that gradient ascent for logistic regression has a\\nconnection with the perceptron learning algorithm. Logistic learning is the\\n\"soft\" variant of perceptron learning.\\n</summary>\\n    <author>\\n      <name>Raul Rojas</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1708.07826v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.07826v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62M45, 68Q32\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"K.3.2; I.5.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.05639v2</id>\\n    <updated>2019-02-01T14:01:29Z</updated>\\n    <published>2019-01-17T06:26:32Z</published>\\n    <title>Artificial Neural Networks</title>\\n    <summary>  These are lecture notes for my course on Artificial Neural Networks that I\\nhave given at Chalmers and Gothenburg University. This course describes the use\\nof neural networks in machine learning: deep learning, recurrent networks, and\\nother supervised and unsupervised machine-learning algorithms.\\n</summary>\\n    <author>\\n      <name>B. Mehlig</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Lecture notes, revised version, 206 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.05639v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.05639v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.04622v1</id>\\n    <updated>2019-02-12T20:28:09Z</updated>\\n    <published>2019-02-12T20:28:09Z</published>\\n    <title>Learning Theory and Support Vector Machines - a primer</title>\\n    <summary>  The main goal of statistical learning theory is to provide a fundamental\\nframework for the problem of decision making and model construction based on\\nsets of data. Here, we present a brief introduction to the fundamentals of\\nstatistical learning theory, in particular the difference between empirical and\\nstructural risk minimization, including one of its most prominent\\nimplementations, i.e. the Support Vector Machine.\\n</summary>\\n    <author>\\n      <name>Michael Banf</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.04622v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.04622v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.08251v1</id>\\n    <updated>2016-10-26T09:35:11Z</updated>\\n    <published>2016-10-26T09:35:11Z</published>\\n    <title>Quantum-enhanced machine learning</title>\\n    <summary>  The emerging field of quantum machine learning has the potential to\\nsubstantially aid in the problems and scope of artificial intelligence. This is\\nonly enhanced by recent successes in the field of classical machine learning.\\nIn this work we propose an approach for the systematic treatment of machine\\nlearning, from the perspective of quantum information. Our approach is general\\nand covers all three main branches of machine learning: supervised,\\nunsupervised and reinforcement learning. While quantum improvements in\\nsupervised and unsupervised learning have been reported, reinforcement learning\\nhas received much less attention. Within our approach, we tackle the problem of\\nquantum enhancements in reinforcement learning as well, and propose a\\nsystematic scheme for providing improvements. As an example, we show that\\nquadratic improvements in learning efficiency, and exponential improvements in\\nperformance over limited time periods, can be obtained for a broad class of\\nlearning problems.\\n</summary>\\n    <author>\\n      <name>Vedran Dunjko</name>\\n    </author>\\n    <author>\\n      <name>Jacob M. Taylor</name>\\n    </author>\\n    <author>\\n      <name>Hans J. Briegel</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevLett.117.130501</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevLett.117.130501\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5+15 pages. This paper builds upon and mostly supersedes\\n  arXiv:1507.08482. In addition to results provided in this previous work, here\\n  we achieve learning improvements in more general environments, and provide\\n  connections to other work in quantum machine learning. Explicit constructions\\n  of oracularized environments given in arXiv:1507.08482 are omitted in this\\n  version</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Phys. Rev. Lett. 117, 130501 (2016)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1610.08251v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.08251v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.07805v2</id>\\n    <updated>2016-09-02T09:27:40Z</updated>\\n    <published>2016-05-25T10:11:03Z</published>\\n    <title>Learning Moore Machines from Input-Output Traces</title>\\n    <summary>  The problem of learning automata from example traces (but no equivalence or\\nmembership queries) is fundamental in automata learning theory and practice. In\\nthis paper we study this problem for finite state machines with inputs and\\noutputs, and in particular for Moore machines. We develop three algorithms for\\nsolving this problem: (1) the PTAP algorithm, which transforms a set of\\ninput-output traces into an incomplete Moore machine and then completes the\\nmachine with self-loops; (2) the PRPNI algorithm, which uses the well-known\\nRPNI algorithm for automata learning to learn a product of automata encoding a\\nMoore machine; and (3) the MooreMI algorithm, which directly learns a Moore\\nmachine using PTAP extended with state merging. We prove that MooreMI has the\\nfundamental identification in the limit property. We also compare the\\nalgorithms experimentally in terms of the size of the learned machine and\\nseveral notions of accuracy, introduced in this paper. Finally, we compare with\\nOSTIA, an algorithm that learns a more general class of transducers, and find\\nthat OSTIA generally does not learn a Moore machine, even when fed with a\\ncharacteristic sample.\\n</summary>\\n    <author>\\n      <name>Georgios Giantamidis</name>\\n    </author>\\n    <author>\\n      <name>Stavros Tripakis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1605.07805v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.07805v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.FL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.FL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.10311v2</id>\\n    <updated>2018-05-17T22:16:31Z</updated>\\n    <published>2018-03-27T20:38:05Z</published>\\n    <title>How Developers Iterate on Machine Learning Workflows -- A Survey of the\\n  Applied Machine Learning Literature</title>\\n    <summary>  Machine learning workflow development is anecdotally regarded to be an\\niterative process of trial-and-error with humans-in-the-loop. However, we are\\nnot aware of quantitative evidence corroborating this popular belief. A\\nquantitative characterization of iteration can serve as a benchmark for machine\\nlearning workflow development in practice, and can aid the development of\\nhuman-in-the-loop machine learning systems. To this end, we conduct a\\nsmall-scale survey of the applied machine learning literature from five\\ndistinct application domains. We collect and distill statistics on the role of\\niteration within machine learning workflow development, and report preliminary\\ntrends and insights from our investigation, as a starting point towards this\\nbenchmark. Based on our findings, we finally describe desiderata for effective\\nand versatile human-in-the-loop machine learning systems that can cater to\\nusers in diverse domains.\\n</summary>\\n    <author>\\n      <name>Doris Xin</name>\\n    </author>\\n    <author>\\n      <name>Litian Ma</name>\\n    </author>\\n    <author>\\n      <name>Shuchen Song</name>\\n    </author>\\n    <author>\\n      <name>Aditya Parameswaran</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.10311v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.10311v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.0406v1</id>\\n    <updated>2013-02-02T17:20:47Z</updated>\\n    <published>2013-02-02T17:20:47Z</published>\\n    <title>Generalization Guarantees for a Binary Classification Framework for\\n  Two-Stage Multiple Kernel Learning</title>\\n    <summary>  We present generalization bounds for the TS-MKL framework for two stage\\nmultiple kernel learning. We also present bounds for sparse kernel learning\\nformulations within the TS-MKL framework.\\n</summary>\\n    <author>\\n      <name>Purushottam Kar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1302.0406v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.0406v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.01477v1</id>\\n    <updated>2018-07-04T08:25:17Z</updated>\\n    <published>2018-07-04T08:25:17Z</published>\\n    <title>Diversity in Machine Learning</title>\\n    <summary>  Machine learning methods have achieved good performance and been widely\\napplied in various real-world applications. It can learn the model adaptively\\nand be better fit for special requirements of different tasks. Many factors can\\naffect the performance of the machine learning process, among which diversity\\nof the machine learning is an important one. Generally, a good machine learning\\nsystem is composed of plentiful training data, a good model training process,\\nand an accurate inference. The diversity could help each procedure to guarantee\\na total good machine learning: diversity of the training data ensures the data\\ncontain enough discriminative information, diversity of the learned model\\n(diversity in parameters of each model or diversity in models) makes each\\nparameter/model capture unique or complement information and the diversity in\\ninference can provide multiple choices each of which corresponds to a plausible\\nresult. However, there is no systematical analysis of the diversification in\\nmachine learning system. In this paper, we systematically summarize the methods\\nto make data diversification, model diversification, and inference\\ndiversification in machine learning process, respectively. In addition, the\\ntypical applications where the diversity technology improved the machine\\nlearning performances have been surveyed, including the remote sensing imaging\\ntasks, machine translation, camera relocalization, image segmentation, object\\ndetection, topic modeling, and others. Finally, we discuss some challenges of\\ndiversity technology in machine learning and point out some directions in\\nfuture work. Our analysis provides a deeper understanding of the diversity\\ntechnology in machine learning tasks, and hence can help design and learn more\\neffective models for specific tasks.\\n</summary>\\n    <author>\\n      <name>Zhiqiang Gong</name>\\n    </author>\\n    <author>\\n      <name>Ping Zhong</name>\\n    </author>\\n    <author>\\n      <name>Weidong Hu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.01477v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.01477v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.03259v1</id>\\n    <updated>2019-04-05T20:05:46Z</updated>\\n    <published>2019-04-05T20:05:46Z</published>\\n    <title>Is \\'Unsupervised Learning\\' a Misconceived Term?</title>\\n    <summary>  Is all of machine learning supervised to some degree? The field of machine\\nlearning has traditionally been categorized pedagogically into\\n$supervised~vs~unsupervised~learning$; where supervised learning has typically\\nreferred to learning from labeled data, while unsupervised learning has\\ntypically referred to learning from unlabeled data. In this paper, we assert\\nthat all machine learning is in fact supervised to some degree, and that the\\nscope of supervision is necessarily commensurate to the scope of learning\\npotential. In particular, we argue that clustering algorithms such as k-means,\\nand dimensionality reduction algorithms such as principal component analysis,\\nvariational autoencoders, and deep belief networks are each internally\\nsupervised by the data itself to learn their respective representations of its\\nfeatures. Furthermore, these algorithms are not capable of external inference\\nuntil their respective outputs (clusters, principal components, or\\nrepresentation codes) have been identified and externally labeled in effect. As\\nsuch, they do not suffice as examples of unsupervised learning. We propose that\\nthe categorization `supervised vs unsupervised learning\\' be dispensed with, and\\ninstead, learning algorithms be categorized as either\\n$internally~or~externally~supervised$ (or both). We believe this change in\\nperspective will yield new fundamental insights into the structure and\\ncharacter of data and of learning algorithms.\\n</summary>\\n    <author>\\n      <name>Stephen G. Odaibo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.03259v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.03259v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.01431v1</id>\\n    <updated>2017-11-04T12:54:35Z</updated>\\n    <published>2017-11-04T12:54:35Z</published>\\n    <title>The Case for Meta-Cognitive Machine Learning: On Model Entropy and\\n  Concept Formation in Deep Learning</title>\\n    <summary>  Machine learning is usually defined in behaviourist terms, where external\\nvalidation is the primary mechanism of learning. In this paper, I argue for a\\nmore holistic interpretation in which finding more probable, efficient and\\nabstract representations is as central to learning as performance. In other\\nwords, machine learning should be extended with strategies to reason over its\\nown learning process, leading to so-called meta-cognitive machine learning. As\\nsuch, the de facto definition of machine learning should be reformulated in\\nthese intrinsically multi-objective terms, taking into account not only the\\ntask performance but also internal learning objectives. To this end, we suggest\\na \"model entropy function\" to be defined that quantifies the efficiency of the\\ninternal learning processes. It is conjured that the minimization of this model\\nentropy leads to concept formation. Besides philosophical aspects, some initial\\nillustrations are included to support the claims.\\n</summary>\\n    <author>\\n      <name>Johan Loeckx</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.01431v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.01431v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.2104v1</id>\\n    <updated>2013-03-08T20:46:27Z</updated>\\n    <published>2013-03-08T20:46:27Z</published>\\n    <title>Transfer Learning for Voice Activity Detection: A Denoising Deep Neural\\n  Network Perspective</title>\\n    <summary>  Mismatching problem between the source and target noisy corpora severely\\nhinder the practical use of the machine-learning-based voice activity detection\\n(VAD). In this paper, we try to address this problem in the transfer learning\\nprospective. Transfer learning tries to find a common learning machine or a\\ncommon feature subspace that is shared by both the source corpus and the target\\ncorpus. The denoising deep neural network is used as the learning machine.\\nThree transfer techniques, which aim to learn common feature representations,\\nare used for analysis. Experimental results demonstrate the effectiveness of\\nthe transfer learning schemes on the mismatch problem.\\n</summary>\\n    <author>\\n      <name>Xiao-Lei Zhang</name>\\n    </author>\\n    <author>\\n      <name>Ji Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been submitted to the conference \"INTERSPEECH2013\" in\\n  March 4, 2013 for review</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1303.2104v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.2104v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.10681v1</id>\\n    <updated>2018-07-27T15:21:43Z</updated>\\n    <published>2018-07-27T15:21:43Z</published>\\n    <title>Learnable: Theory vs Applications</title>\\n    <summary>  Two different views on machine learning problem: Applied learning (machine\\nlearning with business applications) and Agnostic PAC learning are formalized\\nand compared here. I show that, under some conditions, the theory of PAC\\nLearnable provides a way to solve the Applied learning problem. However, the\\ntheory requires to have the training sets so large, that it would make the\\nlearning practically useless. I suggest shedding some theoretical\\nmisconceptions about learning to make the theory more aligned with the needs\\nand experience of practitioners.\\n</summary>\\n    <author>\\n      <name>Marina Sapir</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.10681v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.10681v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.07640v1</id>\\n    <updated>2016-12-16T04:56:30Z</updated>\\n    <published>2016-12-16T04:56:30Z</published>\\n    <title>Deep Learning and Its Applications to Machine Health Monitoring: A\\n  Survey</title>\\n    <summary>  Since 2006, deep learning (DL) has become a rapidly growing research\\ndirection, redefining state-of-the-art performances in a wide range of areas\\nsuch as object recognition, image segmentation, speech recognition and machine\\ntranslation. In modern manufacturing systems, data-driven machine health\\nmonitoring is gaining in popularity due to the widespread deployment of\\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\\nprovides useful tools for processing and analyzing these big machinery data.\\nThe main purpose of this paper is to review and summarize the emerging research\\nwork of deep learning on machine health monitoring. After the brief\\nintroduction of deep learning techniques, the applications of deep learning in\\nmachine health monitoring systems are reviewed mainly from the following\\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\\nFinally, some new trends of DL-based machine health monitoring methods are\\ndiscussed.\\n</summary>\\n    <author>\\n      <name>Rui Zhao</name>\\n    </author>\\n    <author>\\n      <name>Ruqiang Yan</name>\\n    </author>\\n    <author>\\n      <name>Zhenghua Chen</name>\\n    </author>\\n    <author>\\n      <name>Kezhi Mao</name>\\n    </author>\\n    <author>\\n      <name>Peng Wang</name>\\n    </author>\\n    <author>\\n      <name>Robert X. Gao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 9 figures, submitted to IEEE Transactions on Neural\\n  Networks and Learning Systems</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1612.07640v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.07640v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.05052v10</id>\\n    <updated>2019-03-17T17:52:27Z</updated>\\n    <published>2018-05-14T08:08:33Z</published>\\n    <title>Machine Learning: Basic Principles</title>\\n    <summary>  This tutorial is based on the lecture notes for, and the plentiful student\\nfeedback received from, the courses \"Machine Learning: Basic Principles\" and\\n\"Artificial Intelligence\", which I have co-taught since 2015 at Aalto\\nUniversity. The aim is to provide an accessible introduction to some of the\\nmain concepts and methods within machine learning. Many of the current systems\\nwhich are considered as (artificially) intelligent are based on combinations of\\nfew basic machine learning methods. After formalizing the main building blocks\\nof a machine learning problem, some popular algorithmic design patterns for\\nmachine learning methods are discussed in some detail.\\n</summary>\\n    <author>\\n      <name>Alexander Jung</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning, Artificial Intelligence, Deep Learning, Data\\n  Science</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.05052v10\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.05052v10\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.05749v1</id>\\n    <updated>2017-06-19T00:16:24Z</updated>\\n    <published>2017-06-19T00:16:24Z</published>\\n    <title>Dex: Incremental Learning for Complex Environments in Deep Reinforcement\\n  Learning</title>\\n    <summary>  This paper introduces Dex, a reinforcement learning environment toolkit\\nspecialized for training and evaluation of continual learning methods as well\\nas general reinforcement learning problems. We also present the novel continual\\nlearning method of incremental learning, where a challenging environment is\\nsolved using optimal weight initialization learned from first solving a similar\\neasier environment. We show that incremental learning can produce vastly\\nsuperior results than standard methods by providing a strong baseline method\\nacross ten Dex environments. We finally develop a saliency method for\\nqualitative analysis of reinforcement learning, which shows the impact\\nincremental learning has on network attention.\\n</summary>\\n    <author>\\n      <name>Nick Erickson</name>\\n    </author>\\n    <author>\\n      <name>Qi Zhao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2017 submission, 10 pages, 26 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.05749v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.05749v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.6674v1</id>\\n    <updated>2014-04-26T19:24:24Z</updated>\\n    <published>2014-04-26T19:24:24Z</published>\\n    <title>A Comparison of First-order Algorithms for Machine Learning</title>\\n    <summary>  Using an optimization algorithm to solve a machine learning problem is one of\\nmainstreams in the field of science. In this work, we demonstrate a\\ncomprehensive comparison of some state-of-the-art first-order optimization\\nalgorithms for convex optimization problems in machine learning. We concentrate\\non several smooth and non-smooth machine learning problems with a loss function\\nplus a regularizer. The overall experimental results show the superiority of\\nprimal-dual algorithms in solving a machine learning problem from the\\nperspectives of the ease to construct, running time and accuracy.\\n</summary>\\n    <author>\\n      <name>Yu Wei</name>\\n    </author>\\n    <author>\\n      <name>Pock Thomas</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Part of the OAGM 2014 proceedings (arXiv:1404.3538)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.6674v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.6674v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.00279v1</id>\\n    <updated>2016-07-01T15:07:52Z</updated>\\n    <published>2016-07-01T15:07:52Z</published>\\n    <title>Meaningful Models: Utilizing Conceptual Structure to Improve Machine\\n  Learning Interpretability</title>\\n    <summary>  The last decade has seen huge progress in the development of advanced machine\\nlearning models; however, those models are powerless unless human users can\\ninterpret them. Here we show how the mind\\'s construction of concepts and\\nmeaning can be used to create more interpretable machine learning models. By\\nproposing a novel method of classifying concepts, in terms of \\'form\\' and\\n\\'function\\', we elucidate the nature of meaning and offer proposals to improve\\nmodel understandability. As machine learning begins to permeate daily life,\\ninterpretable models may serve as a bridge between domain-expert authors and\\nnon-expert users.\\n</summary>\\n    <author>\\n      <name>Nick Condry</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 3 figures, presented at 2016 ICML Workshop on Human\\n  Interpretability in Machine Learning (WHI 2016), New York, NY</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1607.00279v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.00279v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.04016v1</id>\\n    <updated>2018-01-11T23:37:48Z</updated>\\n    <published>2018-01-11T23:37:48Z</published>\\n    <title>Theoretical Impediments to Machine Learning With Seven Sparks from the\\n  Causal Revolution</title>\\n    <summary>  Current machine learning systems operate, almost exclusively, in a\\nstatistical, or model-free mode, which entails severe theoretical limits on\\ntheir power and performance. Such systems cannot reason about interventions and\\nretrospection and, therefore, cannot serve as the basis for strong AI. To\\nachieve human level intelligence, learning machines need the guidance of a\\nmodel of reality, similar to the ones used in causal inference tasks. To\\ndemonstrate the essential role of such models, I will present a summary of\\nseven tasks which are beyond reach of current machine learning systems and\\nwhich have been accomplished using the tools of causal modeling.\\n</summary>\\n    <author>\\n      <name>Judea Pearl</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.04016v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.04016v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07072v1</id>\\n    <updated>2018-05-18T07:04:37Z</updated>\\n    <published>2018-05-18T07:04:37Z</published>\\n    <title>Optimizing for Generalization in Machine Learning with Cross-Validation\\n  Gradients</title>\\n    <summary>  Cross-validation is the workhorse of modern applied statistics and machine\\nlearning, as it provides a principled framework for selecting the model that\\nmaximizes generalization performance. In this paper, we show that the\\ncross-validation risk is differentiable with respect to the hyperparameters and\\ntraining data for many common machine learning algorithms, including logistic\\nregression, elastic-net regression, and support vector machines. Leveraging\\nthis property of differentiability, we propose a cross-validation gradient\\nmethod (CVGM) for hyperparameter optimization. Our method enables efficient\\noptimization in high-dimensional hyperparameter spaces of the cross-validation\\nrisk, the best surrogate of the true generalization ability of our learning\\nalgorithm.\\n</summary>\\n    <author>\\n      <name>Shane Barratt</name>\\n    </author>\\n    <author>\\n      <name>Rishi Sharma</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.07072v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07072v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.11959v2</id>\\n    <updated>2018-05-31T03:03:40Z</updated>\\n    <published>2018-05-26T20:54:13Z</published>\\n    <title>Algebraic Expression of Subjective Spatial and Temporal Patterns</title>\\n    <summary>  Universal learning machine is a theory trying to study machine learning from\\nmathematical point of view. The outside world is reflected inside an universal\\nlearning machine according to pattern of incoming data. This is subjective\\npattern of learning machine. In [2,4], we discussed subjective spatial pattern,\\nand established a powerful tool -- X-form, which is an algebraic expression for\\nsubjective spatial pattern. However, as the initial stage of study, there we\\nonly discussed spatial pattern. Here, we will discuss spatial and temporal\\npatterns, and algebraic expression for them.\\n</summary>\\n    <author>\\n      <name>Chuyu Xiong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.11959v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.11959v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.10422v1</id>\\n    <updated>2018-12-13T11:02:01Z</updated>\\n    <published>2018-12-13T11:02:01Z</published>\\n    <title>Machine Learning in Official Statistics</title>\\n    <summary>  In the first half of 2018, the Federal Statistical Office of Germany\\n(Destatis) carried out a \"Proof of Concept Machine Learning\" as part of its\\nDigital Agenda. A major component of this was surveys on the use of machine\\nlearning methods in official statistics, which were conducted at selected\\nnational and international statistical institutions and among the divisions of\\nDestatis. It was of particular interest to find out in which statistical areas\\nand for which tasks machine learning is used and which methods are applied.\\nThis paper is intended to make the results of the surveys publicly accessible.\\n</summary>\\n    <author>\\n      <name>Martin Beck</name>\\n    </author>\\n    <author>\\n      <name>Florian Dumpert</name>\\n    </author>\\n    <author>\\n      <name>Joerg Feuerhake</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.10422v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.10422v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.06789v2</id>\\n    <updated>2019-02-22T07:33:33Z</updated>\\n    <published>2019-02-18T20:38:14Z</published>\\n    <title>Seven Myths in Machine Learning Research</title>\\n    <summary>  We present seven myths commonly believed to be true in machine learning\\nresearch, circa Feb 2019. This is an archival copy of the blog post at\\nhttps://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/\\n  Myth 1: TensorFlow is a Tensor manipulation library\\n  Myth 2: Image datasets are representative of real images found in the wild\\n  Myth 3: Machine Learning researchers do not use the test set for validation\\n  Myth 4: Every datapoint is used in training a neural network\\n  Myth 5: We need (batch) normalization to train very deep residual networks\\n  Myth 6: Attention $&gt;$ Convolution\\n  Myth 7: Saliency maps are robust ways to interpret neural networks\\n</summary>\\n    <author>\\n      <name>Oscar Chang</name>\\n    </author>\\n    <author>\\n      <name>Hod Lipson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.06789v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.06789v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.00092v2</id>\\n    <updated>2019-03-12T20:29:57Z</updated>\\n    <published>2019-02-28T22:34:46Z</published>\\n    <title>Optimal Algorithms for Ski Rental with Soft Machine-Learned Predictions</title>\\n    <summary>  We consider a variant of the classic Ski Rental online algorithm with\\napplications to machine learning. In our variant, we allow the skier access to\\na black-box machine-learning algorithm that provides an estimate of the\\nprobability that there will be at most a threshold number of ski-days. We\\nderive a class of optimal randomized algorithms to determine the strategy that\\nminimizes the worst-case expected competitive ratio for the skier given a\\nprediction from the machine learning algorithm,and analyze the performance and\\nrobustness of these algorithms.\\n</summary>\\n    <author>\\n      <name>Rohan Kodialam</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.00092v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.00092v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06689v1</id>\\n    <updated>2018-07-17T22:18:19Z</updated>\\n    <published>2018-07-17T22:18:19Z</published>\\n    <title>Efficient Deep Learning on Multi-Source Private Data</title>\\n    <summary>  Machine learning models benefit from large and diverse datasets. Using such\\ndatasets, however, often requires trusting a centralized data aggregator. For\\nsensitive applications like healthcare and finance this is undesirable as it\\ncould compromise patient privacy or divulge trade secrets. Recent advances in\\nsecure and privacy-preserving computation, including trusted hardware enclaves\\nand differential privacy, offer a way for mutually distrusting parties to\\nefficiently train a machine learning model without revealing the training data.\\nIn this work, we introduce Myelin, a deep learning framework which combines\\nthese privacy-preservation primitives, and use it to establish a baseline level\\nof performance for fully private machine learning.\\n</summary>\\n    <author>\\n      <name>Nick Hynes</name>\\n    </author>\\n    <author>\\n      <name>Raymond Cheng</name>\\n    </author>\\n    <author>\\n      <name>Dawn Song</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.06689v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06689v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.00542v1</id>\\n    <updated>2018-10-31T22:54:12Z</updated>\\n    <published>2018-10-31T22:54:12Z</published>\\n    <title>Pymc-learn: Practical Probabilistic Machine Learning in Python</title>\\n    <summary>  $\\\\textit{Pymc-learn}$ is a Python package providing a variety of\\nstate-of-the-art probabilistic models for supervised and unsupervised machine\\nlearning. It is inspired by $\\\\textit{scikit-learn}$ and focuses on bringing\\nprobabilistic machine learning to non-specialists. It uses a general-purpose\\nhigh-level language that mimics $\\\\textit{scikit-learn}$. Emphasis is put on\\nease of use, productivity, flexibility, performance, documentation, and an API\\nconsistent with $\\\\textit{scikit-learn}$. It depends on $\\\\textit{scikit-learn}$\\nand $\\\\textit{pymc3}$ and is distributed under the new BSD-3 license,\\nencouraging its use in both academia and industry. Source code, binaries, and\\ndocumentation are available on http://github.com/pymc-learn/pymc-learn.\\n</summary>\\n    <author>\\n      <name>Daniel Emaasit</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.00542v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.00542v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.05381v1</id>\\n    <updated>2019-04-10T18:26:16Z</updated>\\n    <published>2019-04-10T18:26:16Z</published>\\n    <title>ReinBo: Machine Learning pipeline search and configuration with Bayesian\\n  Optimization embedded Reinforcement Learning</title>\\n    <summary>  Machine learning pipeline potentially consists of several stages of\\noperations like data preprocessing, feature engineering and machine learning\\nmodel training. Each operation has a set of hyper-parameters, which can become\\nirrelevant for the pipeline when the operation is not selected. This gives rise\\nto a hierarchical conditional hyper-parameter space. To optimize this mixed\\ncontinuous and discrete conditional hierarchical hyper-parameter space, we\\npropose an efficient pipeline search and configuration algorithm which combines\\nthe power of Reinforcement Learning and Bayesian Optimization. Empirical\\nresults show that our method performs favorably compared to state of the art\\nmethods like Auto-sklearn , TPOT, Tree Parzen Window, and Random Search.\\n</summary>\\n    <author>\\n      <name>Xudong Sun</name>\\n    </author>\\n    <author>\\n      <name>Jiali Lin</name>\\n    </author>\\n    <author>\\n      <name>Bernd Bischl</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.05381v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.05381v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.08531v1</id>\\n    <updated>2016-06-28T01:43:38Z</updated>\\n    <published>2016-06-28T01:43:38Z</published>\\n    <title>A Learning Algorithm for Relational Logistic Regression: Preliminary\\n  Results</title>\\n    <summary>  Relational logistic regression (RLR) is a representation of conditional\\nprobability in terms of weighted formulae for modelling multi-relational data.\\nIn this paper, we develop a learning algorithm for RLR models. Learning an RLR\\nmodel from data consists of two steps: 1- learning the set of formulae to be\\nused in the model (a.k.a. structure learning) and learning the weight of each\\nformula (a.k.a. parameter learning). For structure learning, we deploy Schmidt\\nand Murphy\\'s hierarchical assumption: first we learn a model with simple\\nformulae, then more complex formulae are added iteratively only if all their\\nsub-formulae have proven effective in previous learned models. For parameter\\nlearning, we convert the problem into a non-relational learning problem and use\\nan off-the-shelf logistic regression learning algorithm from Weka, an\\nopen-source machine learning tool, to learn the weights. We also indicate how\\nhidden features about the individuals can be incorporated into RLR to boost the\\nlearning performance. We compare our learning algorithm to other structure and\\nparameter learning algorithms in the literature, and compare the performance of\\nRLR models to standard logistic regression and RDN-Boost on a modified version\\nof the MovieLens data-set.\\n</summary>\\n    <author>\\n      <name>Bahare Fatemi</name>\\n    </author>\\n    <author>\\n      <name>Seyed Mehran Kazemi</name>\\n    </author>\\n    <author>\\n      <name>David Poole</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In IJCAI-16 Statistical Relational AI Workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.08531v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.08531v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.09139v1</id>\\n    <updated>2016-11-28T14:34:15Z</updated>\\n    <published>2016-11-28T14:34:15Z</published>\\n    <title>Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for\\n  Complex Systems</title>\\n    <summary>  This is the Proceedings of NIPS 2016 Workshop on Interpretable Machine\\nLearning for Complex Systems, held in Barcelona, Spain on December 9, 2016\\n</summary>\\n    <author>\\n      <name>Andrew Gordon Wilson</name>\\n    </author>\\n    <author>\\n      <name>Been Kim</name>\\n    </author>\\n    <author>\\n      <name>William Herlands</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">31 papers</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.09139v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.09139v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.01977v1</id>\\n    <updated>2017-02-26T10:41:26Z</updated>\\n    <published>2017-02-26T10:41:26Z</published>\\n    <title>Linear, Machine Learning and Probabilistic Approaches for Time Series\\n  Analysis</title>\\n    <summary>  In this paper we study different approaches for time series modeling. The\\nforecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine\\nlearning algorithm are described. Results of different model combinations are\\nshown. For probabilistic modeling the approaches using copulas and Bayesian\\ninference are considered.\\n</summary>\\n    <author>\\n      <name>B. M. Pavlyshenko</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.01977v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.01977v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.09522v2</id>\\n    <updated>2017-12-12T14:09:38Z</updated>\\n    <published>2017-11-27T03:27:17Z</published>\\n    <title>Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing\\n  World</title>\\n    <summary>  This is the Proceedings of NIPS 2017 Workshop on Machine Learning for the\\nDeveloping World, held in Long Beach, California, USA on December 8, 2017\\n</summary>\\n    <author>\\n      <name>Maria De-Arteaga</name>\\n    </author>\\n    <author>\\n      <name>William Herlands</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 papers</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.09522v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.09522v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.00382v1</id>\\n    <updated>2018-02-01T16:46:00Z</updated>\\n    <published>2018-02-01T16:46:00Z</published>\\n    <title>Classifying medical notes into standard disease codes using Machine\\n  Learning</title>\\n    <summary>  We investigate the automatic classification of patient discharge notes into\\nstandard disease labels. We find that Convolutional Neural Networks with\\nAttention outperform previous algorithms used in this task, and suggest further\\nareas for improvement.\\n</summary>\\n    <author>\\n      <name>Amitabha Karmakar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.00382v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.00382v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.03857v1</id>\\n    <updated>2018-06-11T08:33:04Z</updated>\\n    <published>2018-06-11T08:33:04Z</published>\\n    <title>Deep Learning for Classification Tasks on Geospatial Vector Polygons</title>\\n    <summary>  In this paper, we evaluate the accuracy of deep learning approaches on\\ngeospatial vector geometry classification tasks. The purpose of this evaluation\\nis to investigate the ability of deep learning models to learn from geometry\\ncoordinates directly. Previous machine learning research applied to geospatial\\npolygon data did not use geometries directly, but derived properties thereof.\\nThese are produced by way of extracting geometry properties such as Fourier\\ndescriptors. Instead, our introduced deep neural net architectures are able to\\nlearn on sequences of coordinates mapped directly from polygons. In three\\nclassification tasks we show that the deep learning architectures are\\ncompetitive with common learning algorithms that require extracted features.\\n</summary>\\n    <author>\\n      <name>Rein van \\'t Veer</name>\\n    </author>\\n    <author>\\n      <name>Peter Bloem</name>\\n    </author>\\n    <author>\\n      <name>Erwin Folmer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.03857v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.03857v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1504.03874v1</id>\\n    <updated>2015-04-15T12:04:58Z</updated>\\n    <published>2015-04-15T12:04:58Z</published>\\n    <title>Bridging belief function theory to modern machine learning</title>\\n    <summary>  Machine learning is a quickly evolving field which now looks really different\\nfrom what it was 15 years ago, when classification and clustering were major\\nissues. This document proposes several trends to explore the new questions of\\nmodern machine learning, with the strong afterthought that the belief function\\nframework has a major role to play.\\n</summary>\\n    <author>\\n      <name>Thomas Burger</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1504.03874v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1504.03874v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06614v1</id>\\n    <updated>2015-05-25T13:02:32Z</updated>\\n    <published>2015-05-25T13:02:32Z</published>\\n    <title>Electre Tri-Machine Learning Approach to the Record Linkage Problem</title>\\n    <summary>  In this short paper, the Electre Tri-Machine Learning Method, generally used\\nto solve ordinal classification problems, is proposed for solving the Record\\nLinkage problem. Preliminary experimental results show that, using the Electre\\nTri method, high accuracy can be achieved and more than 99% of the matches and\\nnonmatches were correctly identified by the procedure.\\n</summary>\\n    <author>\\n      <name>Renato De Leone</name>\\n    </author>\\n    <author>\\n      <name>Valentina Minnetti</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.06614v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06614v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.02767v2</id>\\n    <updated>2016-06-23T13:27:01Z</updated>\\n    <published>2016-06-08T21:46:20Z</published>\\n    <title>Theoretical Robopsychology: Samu Has Learned Turing Machines</title>\\n    <summary>  From the point of view of a programmer, the robopsychology is a synonym for\\nthe activity is done by developers to implement their machine learning\\napplications. This robopsychological approach raises some fundamental\\ntheoretical questions of machine learning. Our discussion of these questions is\\nconstrained to Turing machines. Alan Turing had given an algorithm (aka the\\nTuring Machine) to describe algorithms. If it has been applied to describe\\nitself then this brings us to Turing\\'s notion of the universal machine. In the\\npresent paper, we investigate algorithms to write algorithms. From a pedagogy\\npoint of view, this way of writing programs can be considered as a combination\\nof learning by listening and learning by doing due to it is based on applying\\nagent technology and machine learning. As the main result we introduce the\\nproblem of learning and then we show that it cannot easily be handled in\\nreality therefore it is reasonable to use machine learning algorithm for\\nlearning Turing machines.\\n</summary>\\n    <author>\\n      <name>Norbert B\\xc3\\xa1tfai</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, added a missing cc* value and the appearance of Table 1 is\\n  improved</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.02767v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.02767v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.05386v1</id>\\n    <updated>2016-06-16T23:39:41Z</updated>\\n    <published>2016-06-16T23:39:41Z</published>\\n    <title>Model-Agnostic Interpretability of Machine Learning</title>\\n    <summary>  Understanding why machine learning models behave the way they do empowers\\nboth system designers and end-users in many ways: in model selection, feature\\nengineering, in order to trust and act upon the predictions, and in more\\nintuitive user interfaces. Thus, interpretability has become a vital concern in\\nmachine learning, and work in the area of interpretable models has found\\nrenewed interest. In some applications, such models are as accurate as\\nnon-interpretable ones, and thus are preferred for their transparency. Even\\nwhen they are not accurate, they may still be preferred when interpretability\\nis of paramount importance. However, restricting machine learning to\\ninterpretable models is often a severe limitation. In this paper we argue for\\nexplaining machine learning predictions using model-agnostic approaches. By\\ntreating the machine learning models as black-box functions, these approaches\\nprovide crucial flexibility in the choice of models, explanations, and\\nrepresentations, improving debugging, comparison, and interfaces for a variety\\nof users and models. We also outline the main challenges for such methods, and\\nreview a recently-introduced model-agnostic explanation approach (LIME) that\\naddresses these challenges.\\n</summary>\\n    <author>\\n      <name>Marco Tulio Ribeiro</name>\\n    </author>\\n    <author>\\n      <name>Sameer Singh</name>\\n    </author>\\n    <author>\\n      <name>Carlos Guestrin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">presented at 2016 ICML Workshop on Human Interpretability in Machine\\n  Learning (WHI 2016), New York, NY</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.05386v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.05386v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.02531v2</id>\\n    <updated>2016-07-27T19:00:49Z</updated>\\n    <published>2016-07-08T21:07:54Z</published>\\n    <title>Proceedings of the 2016 ICML Workshop on Human Interpretability in\\n  Machine Learning (WHI 2016)</title>\\n    <summary>  This is the Proceedings of the 2016 ICML Workshop on Human Interpretability\\nin Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.\\n  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,\\nand Hanna Wallach.\\n</summary>\\n    <author>\\n      <name>Been Kim</name>\\n    </author>\\n    <author>\\n      <name>Dmitry M. Malioutov</name>\\n    </author>\\n    <author>\\n      <name>Kush R. Varshney</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1607.02531v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.02531v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.10121v1</id>\\n    <updated>2017-03-29T16:29:04Z</updated>\\n    <published>2017-03-29T16:29:04Z</published>\\n    <title>The Top 10 Topics in Machine Learning Revisited: A Quantitative\\n  Meta-Study</title>\\n    <summary>  Which topics of machine learning are most commonly addressed in research?\\nThis question was initially answered in 2007 by doing a qualitative survey\\namong distinguished researchers. In our study, we revisit this question from a\\nquantitative perspective. Concretely, we collect 54K abstracts of papers\\npublished between 2007 and 2016 in leading machine learning journals and\\nconferences. We then use machine learning in order to determine the top 10\\ntopics in machine learning. We not only include models, but provide a holistic\\nview across optimization, data, features, etc. This quantitative approach\\nallows reducing the bias of surveys. It reveals new and up-to-date insights\\ninto what the 10 most prolific topics in machine learning research are. This\\nallows researchers to identify popular topics as well as new and rising topics\\nfor their research.\\n</summary>\\n    <author>\\n      <name>Patrick Glauner</name>\\n    </author>\\n    <author>\\n      <name>Manxing Du</name>\\n    </author>\\n    <author>\\n      <name>Victor Paraschiv</name>\\n    </author>\\n    <author>\\n      <name>Andrey Boytsov</name>\\n    </author>\\n    <author>\\n      <name>Isabel Lopez Andrade</name>\\n    </author>\\n    <author>\\n      <name>Jorge Meira</name>\\n    </author>\\n    <author>\\n      <name>Petko Valtchev</name>\\n    </author>\\n    <author>\\n      <name>Radu State</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 25th European Symposium on Artificial Neural\\n  Networks, Computational Intelligence and Machine Learning (ESANN 2017)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1703.10121v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.10121v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.08519v1</id>\\n    <updated>2017-06-26T17:41:20Z</updated>\\n    <published>2017-06-26T17:41:20Z</published>\\n    <title>On conditional parity as a notion of non-discrimination in machine\\n  learning</title>\\n    <summary>  We identify conditional parity as a general notion of non-discrimination in\\nmachine learning. In fact, several recently proposed notions of\\nnon-discrimination, including a few counterfactual notions, are instances of\\nconditional parity. We show that conditional parity is amenable to statistical\\nanalysis by studying randomization as a general mechanism for achieving\\nconditional parity and a kernel-based test of conditional parity.\\n</summary>\\n    <author>\\n      <name>Ya\\'acov Ritov</name>\\n    </author>\\n    <author>\\n      <name>Yuekai Sun</name>\\n    </author>\\n    <author>\\n      <name>Ruofei Zhao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.08519v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.08519v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.02666v1</id>\\n    <updated>2017-08-08T22:21:11Z</updated>\\n    <published>2017-08-08T22:21:11Z</published>\\n    <title>Proceedings of the 2017 ICML Workshop on Human Interpretability in\\n  Machine Learning (WHI 2017)</title>\\n    <summary>  This is the Proceedings of the 2017 ICML Workshop on Human Interpretability\\nin Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,\\n2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.\\n</summary>\\n    <author>\\n      <name>Been Kim</name>\\n    </author>\\n    <author>\\n      <name>Dmitry M. Malioutov</name>\\n    </author>\\n    <author>\\n      <name>Kush R. Varshney</name>\\n    </author>\\n    <author>\\n      <name>Adrian Weller</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.02666v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.02666v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.11238v1</id>\\n    <updated>2018-03-27T15:10:31Z</updated>\\n    <published>2018-03-27T15:10:31Z</published>\\n    <title>Privacy Preserving Machine Learning: Threats and Solutions</title>\\n    <summary>  For privacy concerns to be addressed adequately in current machine learning\\nsystems, the knowledge gap between the machine learning and privacy communities\\nmust be bridged. This article aims to provide an introduction to the\\nintersection of both fields with special emphasis on the techniques used to\\nprotect the data.\\n</summary>\\n    <author>\\n      <name>Mohammad Al-Rubaie</name>\\n    </author>\\n    <author>\\n      <name>J. Morris Chang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PPML Overview, 18 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1804.11238v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.11238v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.04272v2</id>\\n    <updated>2018-08-15T16:24:39Z</updated>\\n    <published>2018-05-11T08:28:55Z</published>\\n    <title>An $O(N)$ Sorting Algorithm: Machine Learning Sort</title>\\n    <summary>  We propose an $O(N\\\\cdot M)$ sorting algorithm by Machine Learning method,\\nwhich shows a huge potential sorting big data. This sorting algorithm can be\\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\\nFurthermore, we discuss the application of this algorithm to sparse hash table.\\n</summary>\\n    <author>\\n      <name>Hanqing Zhao</name>\\n    </author>\\n    <author>\\n      <name>Yuehan Luo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.04272v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.04272v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.01308v1</id>\\n    <updated>2018-07-03T17:49:14Z</updated>\\n    <published>2018-07-03T17:49:14Z</published>\\n    <title>Proceedings of the 2018 ICML Workshop on Human Interpretability in\\n  Machine Learning (WHI 2018)</title>\\n    <summary>  This is the Proceedings of the 2018 ICML Workshop on Human Interpretability\\nin Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14,\\n2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda\\nVi\\\\\\'egas, and Martin Wattenberg.\\n</summary>\\n    <author>\\n      <name>Been Kim</name>\\n    </author>\\n    <author>\\n      <name>Kush R. Varshney</name>\\n    </author>\\n    <author>\\n      <name>Adrian Weller</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.01308v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.01308v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.04162v3</id>\\n    <updated>2018-10-04T20:26:11Z</updated>\\n    <published>2018-07-11T14:39:17Z</published>\\n    <title>TherML: Thermodynamics of Machine Learning</title>\\n    <summary>  In this work we offer a framework for reasoning about a wide class of\\nexisting objectives in machine learning. We develop a formal correspondence\\nbetween this work and thermodynamics and discuss its implications.\\n</summary>\\n    <author>\\n      <name>Alexander A. Alemi</name>\\n    </author>\\n    <author>\\n      <name>Ian Fischer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the ICML 2018 workshop on Theoretical Foundations and\\n  Applications of Deep Generative Models</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.04162v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.04162v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.05351v1</id>\\n    <updated>2018-07-14T08:07:31Z</updated>\\n    <published>2018-07-14T08:07:31Z</published>\\n    <title>ML-Schema: Exposing the Semantics of Machine Learning with Schemas and\\n  Ontologies</title>\\n    <summary>  The ML-Schema, proposed by the W3C Machine Learning Schema Community Group,\\nis a top-level ontology that provides a set of classes, properties, and\\nrestrictions for representing and interchanging information on machine learning\\nalgorithms, datasets, and experiments. It can be easily extended and\\nspecialized and it is also mapped to other more domain-specific ontologies\\ndeveloped in the area of machine learning and data mining. In this paper we\\noverview existing state-of-the-art machine learning interchange formats and\\npresent the first release of ML-Schema, a canonical format resulted of more\\nthan seven years of experience among different research institutions. We argue\\nthat exposing semantics of machine learning algorithms, models, and experiments\\nthrough a canonical format may pave the way to better interpretability and to\\nrealistically achieve the full interoperability of experiments regardless of\\nplatform or adopted workflow solution.\\n</summary>\\n    <author>\\n      <name>Gustavo Correa Publio</name>\\n    </author>\\n    <author>\\n      <name>Diego Esteves</name>\\n    </author>\\n    <author>\\n      <name>Agnieszka \\xc5\\x81awrynowicz</name>\\n    </author>\\n    <author>\\n      <name>Pan\\xc4\\x8de Panov</name>\\n    </author>\\n    <author>\\n      <name>Larisa Soldatova</name>\\n    </author>\\n    <author>\\n      <name>Tommaso Soru</name>\\n    </author>\\n    <author>\\n      <name>Joaquin Vanschoren</name>\\n    </author>\\n    <author>\\n      <name>Hamid Zafar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Poster, selected for the 2nd Reproducibility in Machine Learning\\n  Workshop at ICML 2018, Stockholm, Sweden</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.05351v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.05351v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.04871v1</id>\\n    <updated>2018-11-12T17:32:24Z</updated>\\n    <published>2018-11-12T17:32:24Z</published>\\n    <title>Characterizing machine learning process: A maturity framework</title>\\n    <summary>  Academic literature on machine learning modeling fails to address how to make\\nmachine learning models work for enterprises. For example, existing machine\\nlearning processes cannot address how to define business use cases for an AI\\napplication, how to convert business requirements from offering managers into\\ndata requirements for data scientists, and how to continuously improve AI\\napplications in term of accuracy and fairness, and how to customize general\\npurpose machine learning models with industry, domain, and use case specific\\ndata to make them more accurate for specific situations etc. Making AI work for\\nenterprises requires special considerations, tools, methods and processes. In\\nthis paper we present a maturity framework for machine learning model lifecycle\\nmanagement for enterprises. Our framework is a re-interpretation of the\\nsoftware Capability Maturity Model (CMM) for machine learning model development\\nprocess. We present a set of best practices from our personal experience of\\nbuilding large scale real-world machine learning models to help organizations\\nachieve higher levels of maturity independent of their starting point.\\n</summary>\\n    <author>\\n      <name>Rama Akkiraju</name>\\n    </author>\\n    <author>\\n      <name>Vibha Sinha</name>\\n    </author>\\n    <author>\\n      <name>Anbang Xu</name>\\n    </author>\\n    <author>\\n      <name>Jalal Mahmud</name>\\n    </author>\\n    <author>\\n      <name>Pritam Gundecha</name>\\n    </author>\\n    <author>\\n      <name>Zhe Liu</name>\\n    </author>\\n    <author>\\n      <name>Xiaotong Liu</name>\\n    </author>\\n    <author>\\n      <name>John Schumacher</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 1 figure, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.04871v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.04871v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.11669v1</id>\\n    <updated>2018-11-28T16:49:37Z</updated>\\n    <published>2018-11-28T16:49:37Z</published>\\n    <title>Towards Identifying and Managing Sources of Uncertainty in AI and\\n  Machine Learning Models - An Overview</title>\\n    <summary>  Quantifying and managing uncertainties that occur when data-driven models\\nsuch as those provided by AI and machine learning methods are applied is\\ncrucial. This whitepaper provides a brief motivation and first overview of the\\nstate of the art in identifying and quantifying sources of uncertainty for\\ndata-driven components as well as means for analyzing their impact.\\n</summary>\\n    <author>\\n      <name>Michael Kl\\xc3\\xa4s</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.11669v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.11669v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T01\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.10398v2</id>\\n    <updated>2019-02-18T22:25:48Z</updated>\\n    <published>2018-12-21T03:29:09Z</published>\\n    <title>Proceedings of NeurIPS 2018 Workshop on Machine Learning for the\\n  Developing World: Achieving Sustainable Impact</title>\\n    <summary>  This is the Proceedings of NeurIPS 2018 Workshop on Machine Learning for the\\nDeveloping World: Achieving Sustainable Impact, held in Montreal, Canada on\\nDecember 8, 2018\\n</summary>\\n    <author>\\n      <name>Maria De-Arteaga</name>\\n    </author>\\n    <author>\\n      <name>Amanda Coston</name>\\n    </author>\\n    <author>\\n      <name>William Herlands</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 papers in the proceedings. 10 additional papers were presented at\\n  the workshop but not included in the proceedings</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.10398v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.10398v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.11726v1</id>\\n    <updated>2019-03-27T23:11:15Z</updated>\\n    <published>2019-03-27T23:11:15Z</published>\\n    <title>Radiological images and machine learning: trends, perspectives, and\\n  prospects</title>\\n    <summary>  The application of machine learning to radiological images is an increasingly\\nactive research area that is expected to grow in the next five to ten years.\\nRecent advances in machine learning have the potential to recognize and\\nclassify complex patterns from different radiological imaging modalities such\\nas x-rays, computed tomography, magnetic resonance imaging and positron\\nemission tomography imaging. In many applications, machine learning based\\nsystems have shown comparable performance to human decision-making. The\\napplications of machine learning are the key ingredients of future clinical\\ndecision making and monitoring systems. This review covers the fundamental\\nconcepts behind various machine learning techniques and their applications in\\nseveral radiological imaging areas, such as medical image segmentation, brain\\nfunction studies and neurological disease diagnosis, as well as computer-aided\\nsystems, image registration, and content-based image retrieval systems.\\nSynchronistically, we will briefly discuss current challenges and future\\ndirections regarding the application of machine learning in radiological\\nimaging. By giving insight on how take advantage of machine learning powered\\napplications, we expect that clinicians can prevent and diagnose diseases more\\naccurately and efficiently.\\n</summary>\\n    <author>\\n      <name>Zhenwei Zhang</name>\\n    </author>\\n    <author>\\n      <name>Ervin Sejdic</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.compbiomed.2019.02.017</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.compbiomed.2019.02.017\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Computers in Biology and Medicine (2019)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1903.11726v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.11726v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.01957v1</id>\\n    <updated>2019-04-02T09:37:44Z</updated>\\n    <published>2019-04-02T09:37:44Z</published>\\n    <title>A Game of Dice: Machine Learning and the Question Concerning Art</title>\\n    <summary>  We review some practical and philosophical questions raised by the use of\\nmachine learning in creative practice. Beyond the obvious problems regarding\\nplagiarism and authorship, we argue that the novelty in AI Art relies mostly on\\na narrow machine learning contribution : manifold approximation. Nevertheless,\\nthis contribution creates a radical shift in the way we have to consider this\\nmovement. Is this omnipotent tool a blessing or a curse for the artists?\\n</summary>\\n    <author>\\n      <name>Paul Todorov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.01957v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.01957v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.01487v1</id>\\n    <updated>2016-06-05T09:59:32Z</updated>\\n    <published>2016-06-05T09:59:32Z</published>\\n    <title>Bounds for Vector-Valued Function Estimation</title>\\n    <summary>  We present a framework to derive risk bounds for vector-valued learning with\\na broad class of feature maps and loss functions. Multi-task learning and\\none-vs-all multi-category learning are treated as examples. We discuss in\\ndetail vector-valued functions with one hidden layer, and demonstrate that the\\nconditions under which shared representations are beneficial for multi- task\\nlearning are equally applicable to multi-category learning.\\n</summary>\\n    <author>\\n      <name>Andreas Maurer</name>\\n    </author>\\n    <author>\\n      <name>Massimiliano Pontil</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1606.01487v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.01487v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.08618v1</id>\\n    <updated>2018-08-26T20:26:11Z</updated>\\n    <published>2018-08-26T20:26:11Z</published>\\n    <title>Deep Learning: Computational Aspects</title>\\n    <summary>  In this article we review computational aspects of Deep Learning (DL). Deep\\nlearning uses network architectures consisting of hierarchical layers of latent\\nvariables to construct predictors for high-dimensional input-output models.\\nTraining a deep learning architecture is computationally intensive, and\\nefficient linear algebra libraries is the key for training and inference.\\nStochastic gradient descent (SGD) optimization and batch sampling are used to\\nlearn from massive data sets.\\n</summary>\\n    <author>\\n      <name>Nicholas Polson</name>\\n    </author>\\n    <author>\\n      <name>Vadim Sokolov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.08618v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.08618v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.06622v1</id>\\n    <updated>2018-11-15T23:13:26Z</updated>\\n    <published>2018-11-15T23:13:26Z</published>\\n    <title>Concept-Oriented Deep Learning: Generative Concept Representations</title>\\n    <summary>  Generative concept representations have three major advantages over\\ndiscriminative ones: they can represent uncertainty, they support integration\\nof learning and reasoning, and they are good for unsupervised and\\nsemi-supervised learning. We discuss probabilistic and generative deep\\nlearning, which generative concept representations are based on, and the use of\\nvariational autoencoders and generative adversarial networks for learning\\ngenerative concept representations, particularly for concepts whose data are\\nsequences, structured data or graphs.\\n</summary>\\n    <author>\\n      <name>Daniel T. Chang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.06622v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.06622v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.01709v1</id>\\n    <updated>2015-06-04T19:58:56Z</updated>\\n    <published>2015-06-04T19:58:56Z</published>\\n    <title>The Preference Learning Toolbox</title>\\n    <summary>  Preference learning (PL) is a core area of machine learning that handles\\ndatasets with ordinal relations. As the number of generated data of ordinal\\nnature is increasing, the importance and role of the PL field becomes central\\nwithin machine learning research and practice. This paper introduces an open\\nsource, scalable, efficient and accessible preference learning toolbox that\\nsupports the key phases of the data training process incorporating various\\npopular data preprocessing, feature selection and preference learning methods.\\n</summary>\\n    <author>\\n      <name>Vincent E. Farrugia</name>\\n    </author>\\n    <author>\\n      <name>H\\xc3\\xa9ctor P. Mart\\xc3\\xadnez</name>\\n    </author>\\n    <author>\\n      <name>Georgios N. Yannakakis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1506.01709v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.01709v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.10444v1</id>\\n    <updated>2017-03-30T12:46:46Z</updated>\\n    <published>2017-03-30T12:46:46Z</published>\\n    <title>On Fundamental Limits of Robust Learning</title>\\n    <summary>  We consider the problems of robust PAC learning from distributed and\\nstreaming data, which may contain malicious errors and outliers, and analyze\\ntheir fundamental complexity questions. In particular, we establish lower\\nbounds on the communication complexity for distributed robust learning\\nperformed on multiple machines, and on the space complexity for robust learning\\nfrom streaming data on a single machine. These results demonstrate that gaining\\nrobustness of learning algorithms is usually at the expense of increased\\ncomplexities. As far as we know, this work gives the first complexity results\\nfor distributed and online robust PAC learning.\\n</summary>\\n    <author>\\n      <name>Jiashi Feng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.10444v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.10444v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.08118v3</id>\\n    <updated>2018-10-18T17:11:55Z</updated>\\n    <published>2018-03-21T20:30:34Z</published>\\n    <title>Seglearn: A Python Package for Learning Sequences and Time Series</title>\\n    <summary>  Seglearn is an open-source python package for machine learning time series or\\nsequences using a sliding window segmentation approach. The implementation\\nprovides a flexible pipeline for tackling classification, regression, and\\nforecasting problems with multivariate sequence and contextual data. This\\npackage is compatible with scikit-learn and is listed under scikit-learn\\nRelated Projects. The package depends on numpy, scipy, and scikit-learn.\\nSeglearn is distributed under the BSD 3-Clause License. Documentation includes\\na detailed API description, user guide, and examples. Unit tests provide a high\\ndegree of code coverage.\\n</summary>\\n    <author>\\n      <name>David M. Burns</name>\\n    </author>\\n    <author>\\n      <name>Cari M. Whyne</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research 19 (2018) 1-7</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1803.08118v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.08118v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.10201v2</id>\\n    <updated>2017-08-31T15:53:28Z</updated>\\n    <published>2017-05-29T14:07:33Z</published>\\n    <title>Machine Learned Learning Machines</title>\\n    <summary>  There are two common approaches for optimizing the performance of a machine:\\ngenetic algorithms and machine learning. A genetic algorithm is applied over\\nmany generations whereas machine learning works by applying feedback until the\\nsystem meets a performance threshold. Though these are methods that typically\\noperate separately, we combine evolutionary adaptation and machine learning\\ninto one approach. Our focus is on machines that can learn during their\\nlifetime, but instead of equipping them with a machine learning algorithm we\\naim to let them evolve their ability to learn by themselves. We use evolvable\\nnetworks of probabilistic and deterministic logic gates, known as Markov\\nBrains, as our computational model organism. The ability of Markov Brains to\\nlearn is augmented by a novel adaptive component that can change its\\ncomputational behavior based on feedback. We show that Markov Brains can indeed\\nevolve to incorporate these feedback gates to improve their adaptability to\\nvariable environments. By combining these two methods, we now also implemented\\na computational model that can be used to study the evolution of learning.\\n</summary>\\n    <author>\\n      <name>Leigh Sheneman</name>\\n    </author>\\n    <author>\\n      <name>Arend Hintze</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.10201v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.10201v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.06072v1</id>\\n    <updated>2016-10-19T15:46:30Z</updated>\\n    <published>2016-10-19T15:46:30Z</published>\\n    <title>Learning to Learn Neural Networks</title>\\n    <summary>  Meta-learning consists in learning learning algorithms. We use a Long Short\\nTerm Memory (LSTM) based network to learn to compute on-line updates of the\\nparameters of another neural network. These parameters are stored in the cell\\nstate of the LSTM. Our framework allows to compare learned algorithms to\\nhand-made algorithms within the traditional train and test methodology. In an\\nexperiment, we learn a learning algorithm for a one-hidden layer Multi-Layer\\nPerceptron (MLP) on non-linearly separable datasets. The learned algorithm is\\nable to update parameters of both layers and generalise well on similar\\ndatasets.\\n</summary>\\n    <author>\\n      <name>Tom Bosc</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">presented at \"Reasoning, Attention, Memory\" workshop, NIPS 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.06072v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.06072v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.00379v1</id>\\n    <updated>2016-11-01T20:35:46Z</updated>\\n    <published>2016-11-01T20:35:46Z</published>\\n    <title>The Machine Learning Algorithm as Creative Musical Tool</title>\\n    <summary>  Machine learning is the capacity of a computational system to learn\\nstructures from datasets in order to make predictions on newly seen data. Such\\nan approach offers a significant advantage in music scenarios in which\\nmusicians can teach the system to learn an idiosyncratic style, or can break\\nthe rules to explore the system\\'s capacity in unexpected ways. In this chapter\\nwe draw on music, machine learning, and human-computer interaction to elucidate\\nan understanding of machine learning algorithms as creative tools for music and\\nthe sonic arts. We motivate a new understanding of learning algorithms as\\nhuman-computer interfaces. We show that, like other interfaces, learning\\nalgorithms can be characterised by the ways their affordances intersect with\\ngoals of human users. We also argue that the nature of interaction between\\nusers and algorithms impacts the usability and usefulness of those algorithms\\nin profound ways. This human-centred view of machine learning motivates our\\nconcluding discussion of what it means to employ machine learning as a creative\\ntool.\\n</summary>\\n    <author>\\n      <name>Rebecca Fiebrink</name>\\n    </author>\\n    <author>\\n      <name>Baptiste Caramiaux</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pre-print to appear in the Oxford Handbook on Algorithmic Music.\\n  Oxford University Press</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.00379v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.00379v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.03678v1</id>\\n    <updated>2019-01-11T18:06:05Z</updated>\\n    <published>2019-01-11T18:06:05Z</published>\\n    <title>Machine Learning Automation Toolbox (MLaut)</title>\\n    <summary>  In this paper we present MLaut (Machine Learning AUtomation Toolbox) for the\\npython data science ecosystem. MLaut automates large-scale evaluation and\\nbenchmarking of machine learning algorithms on a large number of datasets.\\nMLaut provides a high-level workflow interface to machine algorithm algorithms,\\nimplements a local back-end to a database of dataset collections, trained\\nalgorithms, and experimental results, and provides easy-to-use interfaces to\\nthe scikit-learn and keras modelling libraries. Experiments are easy to set up\\nwith default settings in a few lines of code, while remaining fully\\ncustomizable to the level of hyper-parameter tuning, pipeline composition, or\\ndeep learning architecture.\\n  As a principal test case for MLaut, we conducted a large-scale supervised\\nclassification study in order to benchmark the performance of a number of\\nmachine learning algorithms - to our knowledge also the first larger-scale\\nstudy on standard supervised learning data sets to include deep learning\\nalgorithms. While corroborating a number of previous findings in literature, we\\nfound (within the limitations of our study) that deep neural networks do not\\nperform well on basic supervised learning, i.e., outside the more specialized,\\nimage-, audio-, or text-based tasks.\\n</summary>\\n    <author>\\n      <name>Viktor Kazakov</name>\\n    </author>\\n    <author>\\n      <name>Franz J. Kir\\xc3\\xa1ly</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.03678v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.03678v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.00001v1</id>\\n    <updated>2019-04-01T13:57:27Z</updated>\\n    <published>2019-04-01T13:57:27Z</published>\\n    <title>Open Problems in Engineering Machine Learning Systems and the Quality\\n  Model</title>\\n    <summary>  Fatal accidents are a major issue hindering the wide acceptance of\\nsafety-critical systems that use machine learning and deep learning models,\\nsuch as automated driving vehicles. To use machine learning in a\\nsafety-critical system, it is necessary to demonstrate the safety and security\\nof the system to society through the engineering process. However, there have\\nbeen no such total concepts or frameworks established for these systems that\\nhave been widely accepted, and needs or open problems are not organized in a\\nway researchers can select a theme and work on. The key to using a machine\\nlearning model in a deductively engineered system, developed in a rigorous\\ndevelopment lifecycle consisting of requirement, design, and verification, cf.\\nV-Model, is decomposing the data-driven training of machine-learning models\\ninto requirement, design, and verification, especially for machine learning\\nmodels used in safety-critical systems. In this study, we identify, classify,\\nand explore the open problems in engineering (safety-critical) machine learning\\nsystems, i.e., requirement, design, and verification of machine learning models\\nand systems, as well as related works and research directions, using automated\\ndriving vehicles as an example. We also discuss the introduction of\\nmachine-learning models into a conventional system quality model such as SQuARE\\nto study the quality model for machine learning systems.\\n</summary>\\n    <author>\\n      <name>Hiroshi Kuwajima</name>\\n    </author>\\n    <author>\\n      <name>Hirotoshi Yasuoka</name>\\n    </author>\\n    <author>\\n      <name>Toshihiro Nakae</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 3 figures, 5 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.00001v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.00001v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1103.3095v1</id>\\n    <updated>2011-03-16T04:54:58Z</updated>\\n    <published>2011-03-16T04:54:58Z</published>\\n    <title>A note on active learning for smooth problems</title>\\n    <summary>  We show that the disagreement coefficient of certain smooth hypothesis\\nclasses is $O(m)$, where $m$ is the dimension of the hypothesis space, thereby\\nanswering a question posed in \\\\cite{friedman09}.\\n</summary>\\n    <author>\\n      <name>Satyaki Mahalanabis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1103.3095v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1103.3095v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Q32: Computational learning theory\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.3900v2</id>\\n    <updated>2012-12-21T19:55:53Z</updated>\\n    <published>2012-12-17T06:49:14Z</published>\\n    <title>A Tutorial on Probabilistic Latent Semantic Analysis</title>\\n    <summary>  In this tutorial, I will discuss the details about how Probabilistic Latent\\nSemantic Analysis (PLSA) is formalized and how different learning algorithms\\nare proposed to learn the model.\\n</summary>\\n    <author>\\n      <name>Liangjie Hong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1212.3900v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.3900v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.6618v1</id>\\n    <updated>2014-08-28T03:29:06Z</updated>\\n    <published>2014-08-28T03:29:06Z</published>\\n    <title>Falsifiable implies Learnable</title>\\n    <summary>  The paper demonstrates that falsifiability is fundamental to learning. We\\nprove the following theorem for statistical learning and sequential prediction:\\nIf a theory is falsifiable then it is learnable -- i.e. admits a strategy that\\npredicts optimally. An analogous result is shown for universal induction.\\n</summary>\\n    <author>\\n      <name>David Balduzzi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1408.6618v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.6618v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.2477v1</id>\\n    <updated>2012-04-11T15:35:43Z</updated>\\n    <published>2012-04-11T15:35:43Z</published>\\n    <title>A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov\\n  Models</title>\\n    <summary>  A simple linear algebraic explanation of the algorithm in \"A Spectral\\nAlgorithm for Learning Hidden Markov Models\" (COLT 2009). Most of the content\\nis in Figure 2; the text just makes everything precise in four nearly-trivial\\nclaims.\\n</summary>\\n    <author>\\n      <name>Matthew James Johnson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1204.2477v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.2477v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.02704v1</id>\\n    <updated>2015-02-09T22:05:25Z</updated>\\n    <published>2015-02-09T22:05:25Z</published>\\n    <title>Learning Reductions that Really Work</title>\\n    <summary>  We provide a summary of the mathematical and computational techniques that\\nhave enabled learning reductions to effectively address a wide class of\\nproblems, and show that this approach to solving machine learning problems can\\nbe broadly useful.\\n</summary>\\n    <author>\\n      <name>Alina Beygelzimer</name>\\n    </author>\\n    <author>\\n      <name>Hal Daum\\xc3\\xa9 III</name>\\n    </author>\\n    <author>\\n      <name>John Langford</name>\\n    </author>\\n    <author>\\n      <name>Paul Mineiro</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.02704v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.02704v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.06586v1</id>\\n    <updated>2018-03-17T23:39:57Z</updated>\\n    <published>2018-03-17T23:39:57Z</published>\\n    <title>Structural query-by-committee</title>\\n    <summary>  In this work, we describe a framework that unifies many different interactive\\nlearning tasks. We present a generalization of the {\\\\it query-by-committee}\\nactive learning algorithm for this setting, and we study its consistency and\\nrate of convergence, both theoretically and empirically, with and without\\nnoise.\\n</summary>\\n    <author>\\n      <name>Christopher Tosh</name>\\n    </author>\\n    <author>\\n      <name>Sanjoy Dasgupta</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.06586v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.06586v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.07904v2</id>\\n    <updated>2018-09-24T20:08:42Z</updated>\\n    <published>2018-09-21T01:02:48Z</published>\\n    <title>Automatic Rule Learning for Autonomous Driving Using Semantic Memory</title>\\n    <summary>  This paper presents a novel approach for automatic rule learning applicable\\nto an autonomous driving system using real driving data.\\n</summary>\\n    <author>\\n      <name>Dmitriy Korchev</name>\\n    </author>\\n    <author>\\n      <name>Aruna Jammalamadaka</name>\\n    </author>\\n    <author>\\n      <name>Rajan Bhattacharyya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 23 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.07904v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.07904v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.02388v1</id>\\n    <updated>2018-03-06T19:16:08Z</updated>\\n    <published>2018-03-06T19:16:08Z</published>\\n    <title>Learning SMaLL Predictors</title>\\n    <summary>  We present a new machine learning technique for training small\\nresource-constrained predictors. Our algorithm, the Sparse Multiprototype\\nLinear Learner (SMaLL), is inspired by the classic machine learning problem of\\nlearning $k$-DNF Boolean formulae. We present a formal derivation of our\\nalgorithm and demonstrate the benefits of our approach with a detailed\\nempirical study.\\n</summary>\\n    <author>\\n      <name>Vikas K. Garg</name>\\n    </author>\\n    <author>\\n      <name>Ofer Dekel</name>\\n    </author>\\n    <author>\\n      <name>Lin Xiao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.02388v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.02388v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.00066v1</id>\\n    <updated>2017-05-31T19:42:41Z</updated>\\n    <published>2017-05-31T19:42:41Z</published>\\n    <title>Descriptions of Objectives and Processes of Mechanical Learning</title>\\n    <summary>  In [1], we introduced mechanical learning and proposed 2 approaches to\\nmechanical learning. Here, we follow one such approach to well describe the\\nobjects and the processes of learning. We discuss 2 kinds of patterns:\\nobjective and subjective pattern. Subjective pattern is crucial for learning\\nmachine. We prove that for any objective pattern we can find a proper\\nsubjective pattern based upon least base patterns to express the objective\\npattern well. X-form is algebraic expression for subjective pattern. Collection\\nof X-forms form internal representation space, which is center of learning\\nmachine. We discuss learning by teaching and without teaching. We define data\\nsufficiency by X-form. We then discussed some learning strategies. We show, in\\neach strategy, with sufficient data, and with certain capabilities, learning\\nmachine indeed can learn any pattern (universal learning machine). In appendix,\\nwith knowledge of learning machine, we try to view deep learning from a\\ndifferent angle, i.e. its internal representation space and its learning\\ndynamics.\\n</summary>\\n    <author>\\n      <name>Chuyu Xiong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.00066v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.00066v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07938v1</id>\\n    <updated>2018-05-21T08:15:09Z</updated>\\n    <published>2018-05-21T08:15:09Z</published>\\n    <title>Transductive Boltzmann Machines</title>\\n    <summary>  We present transductive Boltzmann machines (TBMs), which firstly achieve\\ntransductive learning of the Gibbs distribution. While exact learning of the\\nGibbs distribution is impossible by the family of existing Boltzmann machines\\ndue to combinatorial explosion of the sample space, TBMs overcome the problem\\nby adaptively constructing the minimum required sample space from data to avoid\\nunnecessary generalization. We theoretically provide bias-variance\\ndecomposition of the KL divergence in TBMs to analyze its learnability, and\\nempirically demonstrate that TBMs are superior to the fully visible Boltzmann\\nmachines and popularly used restricted Boltzmann machines in terms of\\nefficiency and effectiveness.\\n</summary>\\n    <author>\\n      <name>Mahito Sugiyama</name>\\n    </author>\\n    <author>\\n      <name>Koji Tsuda</name>\\n    </author>\\n    <author>\\n      <name>Hiroyuki Nakahara</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 1 figure, 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.07938v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07938v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.12587v1</id>\\n    <updated>2018-11-30T02:40:40Z</updated>\\n    <published>2018-11-30T02:40:40Z</published>\\n    <title>Restricted Boltzmann Machine with Multivalued Hidden Variables: a model\\n  suppressing over-fitting</title>\\n    <summary>  Generalization is one of the most important issues in machine learning\\nproblems. In this paper, we consider the generalization in restricted Boltzmann\\nmachines. We propose a restricted Boltzmann machine with multivalued hidden\\nvariables, which is a simple extension of conventional restricted Boltzmann\\nmachines. We demonstrate that our model is better than the conventional one via\\nnumerical experiments: experiments for a contrastive divergence learning with\\nartificial data and for a classification problem with MNIST.\\n</summary>\\n    <author>\\n      <name>Yuuki Yokoyama</name>\\n    </author>\\n    <author>\\n      <name>Tomu Katsumata</name>\\n    </author>\\n    <author>\\n      <name>Muneki Yasuda</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.12587v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.12587v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.3783v1</id>\\n    <updated>2012-03-16T19:01:10Z</updated>\\n    <published>2012-03-16T19:01:10Z</published>\\n    <title>Learning Feature Hierarchies with Centered Deep Boltzmann Machines</title>\\n    <summary>  Deep Boltzmann machines are in principle powerful models for extracting the\\nhierarchical structure of data. Unfortunately, attempts to train layers jointly\\n(without greedy layer-wise pretraining) have been largely unsuccessful. We\\npropose a modification of the learning algorithm that initially recenters the\\noutput of the activation functions to zero. This modification leads to a better\\nconditioned Hessian and thus makes learning easier. We test the algorithm on\\nreal data and demonstrate that our suggestion, the centered deep Boltzmann\\nmachine, learns a hierarchy of increasingly abstract representations and a\\nbetter generative model of data.\\n</summary>\\n    <author>\\n      <name>Gr\\xc3\\xa9goire Montavon</name>\\n    </author>\\n    <author>\\n      <name>Klaus-Robert M\\xc3\\xbcller</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-642-35289-8_33</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-642-35289-8_33\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1203.3783v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.3783v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.03198v1</id>\\n    <updated>2015-11-10T17:41:48Z</updated>\\n    <published>2015-11-10T17:41:48Z</published>\\n    <title>Sliced Wasserstein Kernels for Probability Distributions</title>\\n    <summary>  Optimal transport distances, otherwise known as Wasserstein distances, have\\nrecently drawn ample attention in computer vision and machine learning as a\\npowerful discrepancy measure for probability distributions. The recent\\ndevelopments on alternative formulations of the optimal transport have allowed\\nfor faster solutions to the problem and has revamped its practical applications\\nin machine learning. In this paper, we exploit the widely used kernel methods\\nand provide a family of provably positive definite kernels based on the Sliced\\nWasserstein distance and demonstrate the benefits of these kernels in a variety\\nof learning tasks. Our work provides a new perspective on the application of\\noptimal transport flavored distances through kernel methods in machine learning\\ntasks.\\n</summary>\\n    <author>\\n      <name>Soheil Kolouri</name>\\n    </author>\\n    <author>\\n      <name>Yang Zou</name>\\n    </author>\\n    <author>\\n      <name>Gustavo K. Rohde</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1511.03198v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.03198v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.03643v3</id>\\n    <updated>2016-02-26T02:21:52Z</updated>\\n    <published>2015-11-11T20:27:54Z</published>\\n    <title>Unifying distillation and privileged information</title>\\n    <summary>  Distillation (Hinton et al., 2015) and privileged information (Vapnik &amp;\\nIzmailov, 2015) are two techniques that enable machines to learn from other\\nmachines. This paper unifies these two techniques into generalized\\ndistillation, a framework to learn from multiple machines and data\\nrepresentations. We provide theoretical and causal insight about the inner\\nworkings of generalized distillation, extend it to unsupervised, semisupervised\\nand multitask learning scenarios, and illustrate its efficacy on a variety of\\nnumerical simulations on both synthetic and real-world data.\\n</summary>\\n    <author>\\n      <name>David Lopez-Paz</name>\\n    </author>\\n    <author>\\n      <name>L\\xc3\\xa9on Bottou</name>\\n    </author>\\n    <author>\\n      <name>Bernhard Sch\\xc3\\xb6lkopf</name>\\n    </author>\\n    <author>\\n      <name>Vladimir Vapnik</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the International Conference on Learning\\n  Representations (2016) 1-10</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1511.03643v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.03643v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.02127v2</id>\\n    <updated>2015-04-06T15:44:52Z</updated>\\n    <published>2015-02-07T11:46:22Z</published>\\n    <title>Hyperparameter Search in Machine Learning</title>\\n    <summary>  We introduce the hyperparameter search problem in the field of machine\\nlearning and discuss its main challenges from an optimization perspective.\\nMachine learning methods attempt to build models that capture some element of\\ninterest based on given data. Most common learning algorithms feature a set of\\nhyperparameters that must be determined before training commences. The choice\\nof hyperparameters can significantly affect the resulting model\\'s performance,\\nbut determining good values can be complex; hence a disciplined, theoretically\\nsound search strategy is essential.\\n</summary>\\n    <author>\\n      <name>Marc Claesen</name>\\n    </author>\\n    <author>\\n      <name>Bart De Moor</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, accepted for MIC 2015: The XI Metaheuristics International\\n  Conference in Agadir, Morocco</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1502.02127v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.02127v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1.6; I.2.6; I.2.8; I.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1510.00772v1</id>\\n    <updated>2015-10-03T02:57:47Z</updated>\\n    <published>2015-10-03T02:57:47Z</published>\\n    <title>Machine Learning for Machine Data from a CATI Network</title>\\n    <summary>  This is a machine learning application paper involving big data. We present\\nhigh-accuracy prediction methods of rare events in semi-structured machine log\\nfiles, which are produced at high velocity and high volume by NORC\\'s\\ncomputer-assisted telephone interviewing (CATI) network for conducting surveys.\\nWe judiciously apply natural language processing (NLP) techniques and\\ndata-mining strategies to train effective learning and prediction models for\\nclassifying uncommon error messages in the log---without access to source code,\\nupdated documentation or dictionaries. In particular, our simple but effective\\napproach of features preallocation for learning from imbalanced data coupled\\nwith naive Bayes classifiers can be conceivably generalized to supervised or\\nsemi-supervised learning and prediction methods for other critical events such\\nas cyberattack detection.\\n</summary>\\n    <author>\\n      <name>Sou-Cheng T. Choi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1510.00772v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1510.00772v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.10775v1</id>\\n    <updated>2018-11-27T02:17:19Z</updated>\\n    <published>2018-11-27T02:17:19Z</published>\\n    <title>Machine learning in protein engineering</title>\\n    <summary>  Machine learning-guided protein engineering is a new paradigm that enables\\nthe optimization of complex protein functions. Machine-learning methods use\\ndata to predict protein function without requiring a detailed model of the\\nunderlying physics or biological pathways. They accelerate protein engineering\\nby learning from information contained in all measured variants and using it to\\nselect variants that are likely to be improved. In this review, we introduce\\nthe steps required to collect protein data, train machine-learning models, and\\nuse trained models to guide engineering. We make recommendations at each stage\\nand look to future opportunities for machine learning to enable the discovery\\nof new protein functions and uncover the relationship between protein sequence\\nand function.\\n</summary>\\n    <author>\\n      <name>Kevin K. Yang</name>\\n    </author>\\n    <author>\\n      <name>Zachary Wu</name>\\n    </author>\\n    <author>\\n      <name>Frances H. Arnold</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.10775v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.10775v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.01354v1</id>\\n    <updated>2016-03-22T18:46:13Z</updated>\\n    <published>2016-03-22T18:46:13Z</published>\\n    <title>Learning Discriminative Features using Encoder-Decoder type Deep Neural\\n  Nets</title>\\n    <summary>  As machine learning is applied to an increasing variety of complex problems,\\nwhich are defined by high dimensional and complex data sets, the necessity for\\ntask oriented feature learning grows in importance. With the advancement of\\nDeep Learning algorithms, various successful feature learning techniques have\\nevolved. In this paper, we present a novel way of learning discriminative\\nfeatures by training Deep Neural Nets which have Encoder or Decoder type\\narchitecture similar to an Autoencoder. We demonstrate that our approach can\\nlearn discriminative features which can perform better at pattern\\nclassification tasks when the number of training samples is relatively small in\\nsize.\\n</summary>\\n    <author>\\n      <name>Vishwajeet Singh</name>\\n    </author>\\n    <author>\\n      <name>Killamsetti Ravi Kumar</name>\\n    </author>\\n    <author>\\n      <name>K Eswaran</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 8 figures and 8 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1607.01354v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.01354v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5; I.5.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.07883v2</id>\\n    <updated>2018-01-30T07:20:41Z</updated>\\n    <published>2018-01-24T07:32:29Z</published>\\n    <title>Deep Learning for Sentiment Analysis : A Survey</title>\\n    <summary>  Deep learning has emerged as a powerful machine learning technique that\\nlearns multiple layers of representations or features of the data and produces\\nstate-of-the-art prediction results. Along with the success of deep learning in\\nmany other application domains, deep learning is also popularly used in\\nsentiment analysis in recent years. This paper first gives an overview of deep\\nlearning and then provides a comprehensive survey of its current applications\\nin sentiment analysis.\\n</summary>\\n    <author>\\n      <name>Lei Zhang</name>\\n    </author>\\n    <author>\\n      <name>Shuai Wang</name>\\n    </author>\\n    <author>\\n      <name>Bing Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">34 pages, 9 figures, 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.07883v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.07883v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.08250v2</id>\\n    <updated>2018-05-09T08:59:04Z</updated>\\n    <published>2018-02-22T10:23:36Z</published>\\n    <title>SeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural\\n  Networks by Selective Network Augmentation</title>\\n    <summary>  Lifelong learning aims to develop machine learning systems that can learn new\\ntasks while preserving the performance on previous learned tasks. In this paper\\nwe present a method to overcome catastrophic forgetting on convolutional neural\\nnetworks, that learns new tasks and preserves the performance on old tasks\\nwithout accessing the data of the original model, by selective network\\naugmentation. The experiment results showed that SeNA-CNN, in some scenarios,\\noutperforms the state-of-art Learning without Forgetting algorithm. Results\\nalso showed that in some situations it is better to use SeNA-CNN instead of\\ntraining a neural network using isolated learning.\\n</summary>\\n    <author>\\n      <name>Abel S. Zacarias</name>\\n    </author>\\n    <author>\\n      <name>Lu\\xc3\\xads A. Alexandre</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.08250v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.08250v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.02038v1</id>\\n    <updated>2017-11-06T17:44:03Z</updated>\\n    <published>2017-11-06T17:44:03Z</published>\\n    <title>An efficient quantum algorithm for generative machine learning</title>\\n    <summary>  A central task in the field of quantum computing is to find applications\\nwhere quantum computer could provide exponential speedup over any classical\\ncomputer. Machine learning represents an important field with broad\\napplications where quantum computer may offer significant speedup. Several\\nquantum algorithms for discriminative machine learning have been found based on\\nefficient solving of linear algebraic problems, with potential exponential\\nspeedup in runtime under the assumption of effective input from a quantum\\nrandom access memory. In machine learning, generative models represent another\\nlarge class which is widely used for both supervised and unsupervised learning.\\nHere, we propose an efficient quantum algorithm for machine learning based on a\\nquantum generative model. We prove that our proposed model is exponentially\\nmore powerful to represent probability distributions compared with classical\\ngenerative models and has exponential speedup in training and inference at\\nleast for some instances under a reasonable assumption in computational\\ncomplexity theory. Our result opens a new direction for quantum machine\\nlearning and offers a remarkable example in which a quantum algorithm shows\\nexponential improvement over any classical algorithm in an important\\napplication field.\\n</summary>\\n    <author>\\n      <name>Xun Gao</name>\\n    </author>\\n    <author>\\n      <name>Zhengyu Zhang</name>\\n    </author>\\n    <author>\\n      <name>Luming Duan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7+15 pages, 3+6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.02038v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.02038v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.05351v2</id>\\n    <updated>2018-02-20T17:21:27Z</updated>\\n    <published>2018-02-14T22:58:31Z</published>\\n    <title>Stealing Hyperparameters in Machine Learning</title>\\n    <summary>  Hyperparameters are critical in machine learning, as different\\nhyperparameters often result in models with significantly different\\nperformance. Hyperparameters may be deemed confidential because of their\\ncommercial value and the confidentiality of the proprietary algorithms that the\\nlearner uses to learn them. In this work, we propose attacks on stealing the\\nhyperparameters that are learned by a learner. We call our attacks\\nhyperparameter stealing attacks. Our attacks are applicable to a variety of\\npopular machine learning algorithms such as ridge regression, logistic\\nregression, support vector machine, and neural network. We evaluate the\\neffectiveness of our attacks both theoretically and empirically. For instance,\\nwe evaluate our attacks on Amazon Machine Learning. Our results demonstrate\\nthat our attacks can accurately steal hyperparameters. We also study\\ncountermeasures. Our results highlight the need for new defenses against our\\nhyperparameter stealing attacks for certain machine learning algorithms.\\n</summary>\\n    <author>\\n      <name>Binghui Wang</name>\\n    </author>\\n    <author>\\n      <name>Neil Zhenqiang Gong</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in the 39th IEEE Symposium on Security and Privacy, May\\n  2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.05351v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.05351v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.05409v2</id>\\n    <updated>2018-09-11T15:32:10Z</updated>\\n    <published>2018-05-11T14:30:30Z</published>\\n    <title>Machine Learning for Public Administration Research, with Application to\\n  Organizational Reputation</title>\\n    <summary>  Machine learning methods have gained a great deal of popularity in recent\\nyears among public administration scholars and practitioners. These techniques\\nopen the door to the analysis of text, image and other types of data that allow\\nus to test foundational theories of public administration and to develop new\\ntheories. Despite the excitement surrounding machine learning methods, clarity\\nregarding their proper use and potential pitfalls is lacking. This paper\\nattempts to fill this gap in the literature through providing a machine\\nlearning \"guide to practice\" for public administration scholars and\\npractitioners. Here, we take a foundational view of machine learning and\\ndescribe how these methods can enrich public administration research and\\npractice through their ability develop new measures, tap into new sources of\\ndata and conduct statistical inference and causal inference in a principled\\nmanner. We then turn our attention to the pitfalls of using these methods such\\nas unvalidated measures and lack of interpretability. Finally, we demonstrate\\nhow machine learning techniques can help us learn about organizational\\nreputation in federal agencies through an illustrated example using tweets from\\n13 executive federal agencies.\\n</summary>\\n    <author>\\n      <name>L. Jason Anastasopoulos</name>\\n    </author>\\n    <author>\\n      <name>Andrew B. Whitford</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.05409v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.05409v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.03402v1</id>\\n    <updated>2018-11-08T13:37:46Z</updated>\\n    <published>2018-11-08T13:37:46Z</published>\\n    <title>A Survey on Data Collection for Machine Learning: a Big Data - AI\\n  Integration Perspective</title>\\n    <summary>  Data collection is a major bottleneck in machine learning and an active\\nresearch topic in multiple communities. There are largely two reasons data\\ncollection has recently become a critical issue. First, as machine learning is\\nbecoming more widely-used, we are seeing new applications that do not\\nnecessarily have enough labeled data. Second, unlike traditional machine\\nlearning where feature engineering is the bottleneck, deep learning techniques\\nautomatically generate features, but instead require large amounts of labeled\\ndata. Interestingly, recent research in data collection comes not only from the\\nmachine learning, natural language, and computer vision communities, but also\\nfrom the data management community due to the importance of handling large\\namounts of data. In this survey, we perform a comprehensive study of data\\ncollection from a data management point of view. Data collection largely\\nconsists of data acquisition, data labeling, and improvement of existing data\\nor models. We provide a research landscape of these operations, provide\\nguidelines on which technique to use when, and identify interesting research\\nchallenges. The integration of machine learning and data management for data\\ncollection is part of a larger trend of Big data and Artificial Intelligence\\n(AI) integration and opens many opportunities for new research.\\n</summary>\\n    <author>\\n      <name>Yuji Roh</name>\\n    </author>\\n    <author>\\n      <name>Geon Heo</name>\\n    </author>\\n    <author>\\n      <name>Steven Euijong Whang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.03402v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.03402v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.03057v1</id>\\n    <updated>2018-12-07T15:02:40Z</updated>\\n    <published>2018-12-07T15:02:40Z</published>\\n    <title>Open Problems in Engineering and Quality Assurance of Safety Critical\\n  Machine Learning Systems</title>\\n    <summary>  Fatal accidents are a major issue hindering the wide acceptance of\\nsafety-critical systems using machine-learning and deep-learning models, such\\nas automated-driving vehicles. Quality assurance frameworks are required for\\nsuch machine learning systems, but there are no widely accepted and established\\nquality-assurance concepts and techniques. At the same time, open problems and\\nthe relevant technical fields are not organized. To establish standard quality\\nassurance frameworks, it is necessary to visualize and organize these open\\nproblems in an interdisciplinary way, so that the experts from many different\\ntechnical fields may discuss these problems in depth and develop solutions. In\\nthe present study, we identify, classify, and explore the open problems in\\nquality assurance of safety-critical machine-learning systems, and their\\nrelevant corresponding industry and technological trends, using\\nautomated-driving vehicles as an example. Our results show that addressing\\nthese open problems requires incorporating knowledge from several different\\ntechnological and industrial fields, including the automobile industry,\\nstatistics, software engineering, and machine learning.\\n</summary>\\n    <author>\\n      <name>Hiroshi Kuwajima</name>\\n    </author>\\n    <author>\\n      <name>Hirotoshi Yasuoka</name>\\n    </author>\\n    <author>\\n      <name>Toshihiro Nakae</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">DISE1: Joint Workshop on Deep (or Machine) Learning for\\n  Safety-Critical Applications in Engineering</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.03057v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.03057v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.04708v1</id>\\n    <updated>2017-11-13T17:16:38Z</updated>\\n    <published>2017-11-13T17:16:38Z</published>\\n    <title>Machine Learning for the Geosciences: Challenges and Opportunities</title>\\n    <summary>  Geosciences is a field of great societal relevance that requires solutions to\\nseveral urgent problems facing our humanity and the planet. As geosciences\\nenters the era of big data, machine learning (ML) -- that has been widely\\nsuccessful in commercial domains -- offers immense potential to contribute to\\nproblems in geosciences. However, problems in geosciences have several unique\\nchallenges that are seldom found in traditional applications, requiring novel\\nproblem formulations and methodologies in machine learning. This article\\nintroduces researchers in the machine learning (ML) community to these\\nchallenges offered by geoscience problems and the opportunities that exist for\\nadvancing both machine learning and geosciences. We first highlight typical\\nsources of geoscience data and describe their properties that make it\\nchallenging to use traditional machine learning techniques. We then describe\\nsome of the common categories of geoscience problems where machine learning can\\nplay a role, and discuss some of the existing efforts and promising directions\\nfor methodological development in machine learning. We conclude by discussing\\nsome of the emerging research themes in machine learning that are applicable\\nacross all problems in the geosciences, and the importance of a deep\\ncollaboration between machine learning and geosciences for synergistic\\nadvancements in both disciplines.\\n</summary>\\n    <author>\\n      <name>Anuj Karpatne</name>\\n    </author>\\n    <author>\\n      <name>Imme Ebert-Uphoff</name>\\n    </author>\\n    <author>\\n      <name>Sai Ravela</name>\\n    </author>\\n    <author>\\n      <name>Hassan Ali Babaie</name>\\n    </author>\\n    <author>\\n      <name>Vipin Kumar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Under review at IEEE Transactions on Knowledge and Data Engineering</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.04708v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.04708v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.geo-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.01315v2</id>\\n    <updated>2019-04-01T19:40:52Z</updated>\\n    <published>2018-11-04T02:55:49Z</published>\\n    <title>Modeling Stated Preference for Mobility-on-Demand Transit: A Comparison\\n  of Machine Learning and Logit Models</title>\\n    <summary>  Logit models are usually applied when studying individual travel behavior,\\ni.e., to predict travel mode choice and to gain behavioral insights on traveler\\npreferences. Recently, some studies have applied machine learning to model\\ntravel mode choice and reported higher out-of-sample predictive accuracy than\\ntraditional logit models (e.g., multinomial logit). However, little research\\nfocuses on comparing the interpretability of machine learning with logit\\nmodels. In other words, how to draw behavioral insights from the\\nhigh-performance \"black-box\" machine-learning models remains largely unsolved\\nin the field of travel behavior modeling.\\n  This paper aims at providing a comprehensive comparison between the two\\napproaches by examining the key similarities and differences in model\\ndevelopment, evaluation, and behavioral interpretation between logit and\\nmachine-learning models for travel mode choice modeling. To complement the\\ntheoretical discussions, the paper also empirically evaluates the two\\napproaches on the stated-preference survey data for a new type of transit\\nsystem integrating high-frequency fixed-route services and ridesourcing. The\\nresults show that machine learning can produce significantly higher predictive\\naccuracy than logit models. Moreover, machine learning and logit models largely\\nagree on many aspects of behavioral interpretations. In addition, machine\\nlearning can automatically capture the nonlinear relationship between the input\\nfeatures and choice outcomes. The paper concludes that there is great potential\\nin merging ideas from machine learning and conventional statistical methods to\\ndevelop refined models for travel behavior research and suggests some new\\nresearch directions.\\n</summary>\\n    <author>\\n      <name>Xilei Zhao</name>\\n    </author>\\n    <author>\\n      <name>Xiang Yan</name>\\n    </author>\\n    <author>\\n      <name>Alan Yu</name>\\n    </author>\\n    <author>\\n      <name>Pascal Van Hentenryck</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">30 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.01315v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.01315v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.01034v1</id>\\n    <updated>2018-02-03T21:20:03Z</updated>\\n    <published>2018-02-03T21:20:03Z</published>\\n    <title>Multi-task Learning for Continuous Control</title>\\n    <summary>  Reliable and effective multi-task learning is a prerequisite for the\\ndevelopment of robotic agents that can quickly learn to accomplish related,\\neveryday tasks. However, in the reinforcement learning domain, multi-task\\nlearning has not exhibited the same level of success as in other domains, such\\nas computer vision. In addition, most reinforcement learning research on\\nmulti-task learning has been focused on discrete action spaces, which are not\\nused for robotic control in the real-world. In this work, we apply multi-task\\nlearning methods to continuous action spaces and benchmark their performance on\\na series of simulated continuous control tasks. Most notably, we show that\\nmulti-task learning outperforms our baselines and alternative knowledge sharing\\nmethods.\\n</summary>\\n    <author>\\n      <name>Himani Arora</name>\\n    </author>\\n    <author>\\n      <name>Rajath Kumar</name>\\n    </author>\\n    <author>\\n      <name>Jason Krone</name>\\n    </author>\\n    <author>\\n      <name>Chong Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.01034v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.01034v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.02908v1</id>\\n    <updated>2017-05-08T15:06:32Z</updated>\\n    <published>2017-05-08T15:06:32Z</published>\\n    <title>Machine Learning with World Knowledge: The Position and Survey</title>\\n    <summary>  Machine learning has become pervasive in multiple domains, impacting a wide\\nvariety of applications, such as knowledge discovery and data mining, natural\\nlanguage processing, information retrieval, computer vision, social and health\\ninformatics, ubiquitous computing, etc. Two essential problems of machine\\nlearning are how to generate features and how to acquire labels for machines to\\nlearn. Particularly, labeling large amount of data for each domain-specific\\nproblem can be very time consuming and costly. It has become a key obstacle in\\nmaking learning protocols realistic in applications. In this paper, we will\\ndiscuss how to use the existing general-purpose world knowledge to enhance\\nmachine learning processes, by enriching the features or reducing the labeling\\nwork. We start from the comparison of world knowledge with domain-specific\\nknowledge, and then introduce three key problems in using world knowledge in\\nlearning processes, i.e., explicit and implicit feature representation,\\ninference for knowledge linking and disambiguation, and learning with direct or\\nindirect supervision. Finally we discuss the future directions of this research\\ntopic.\\n</summary>\\n    <author>\\n      <name>Yangqiu Song</name>\\n    </author>\\n    <author>\\n      <name>Dan Roth</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.02908v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.02908v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.00709v1</id>\\n    <updated>2018-04-02T19:23:06Z</updated>\\n    <published>2018-04-02T19:23:06Z</published>\\n    <title>Generative Adversarial Learning for Spectrum Sensing</title>\\n    <summary>  A novel approach of training data augmentation and domain adaptation is\\npresented to support machine learning applications for cognitive radio. Machine\\nlearning provides effective tools to automate cognitive radio functionalities\\nby reliably extracting and learning intrinsic spectrum dynamics. However, there\\nare two important challenges to overcome, in order to fully utilize the machine\\nlearning benefits with cognitive radios. First, machine learning requires\\nsignificant amount of truthed data to capture complex channel and emitter\\ncharacteristics, and train the underlying algorithm (e.g., a classifier).\\nSecond, the training data that has been identified for one spectrum environment\\ncannot be used for another one (e.g., after channel and emitter conditions\\nchange). To address these challenges, a generative adversarial network (GAN)\\nwith deep learning structures is used to 1)~generate additional synthetic\\ntraining data to improve classifier accuracy, and 2) adapt training data to\\nspectrum dynamics. This approach is applied to spectrum sensing by assuming\\nonly limited training data without knowledge of spectrum statistics. Machine\\nlearning classifiers are trained with limited, augmented and adapted training\\ndata to detect signals. Results show that training data augmentation increases\\nthe classifier accuracy significantly and this increase is sustained with\\ndomain adaptation as spectrum conditions change.\\n</summary>\\n    <author>\\n      <name>Kemal Davaslioglu</name>\\n    </author>\\n    <author>\\n      <name>Yalin E. Sagduyu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE ICC 2018 COGNITIVE RADIO AND NETWORKING (ICC\\'18 CRN)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1804.00709v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.00709v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.05061v1</id>\\n    <updated>2019-04-10T08:36:46Z</updated>\\n    <published>2019-04-10T08:36:46Z</published>\\n    <title>A review on Neural Turing Machine</title>\\n    <summary>  One of the major objectives of Artificial Intelligence is to design learning\\nalgorithms that are executed on a general purposes computational machines such\\nas human brain. Neural Turing Machine (NTM) is a step towards realizing such a\\ncomputational machine. The attempt is made here to run a systematic review on\\nNeural Turing Machine. First, the mind-map and taxonomy of machine learning,\\nneural networks, and Turing machine are introduced. Next, NTM is inspected in\\nterms of concepts, structure, variety of versions, implemented tasks,\\ncomparisons, etc. Finally, the paper discusses on issues and ends up with\\nseveral future works.\\n</summary>\\n    <author>\\n      <name>Soroor Malekmohammadi Faradonbeh</name>\\n    </author>\\n    <author>\\n      <name>Faramarz Safi-Esfahani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.05061v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.05061v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T01\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.8353v1</id>\\n    <updated>2012-10-31T14:55:50Z</updated>\\n    <published>2012-10-31T14:55:50Z</published>\\n    <title>Temporal Autoencoding Restricted Boltzmann Machine</title>\\n    <summary>  Much work has been done refining and characterizing the receptive fields\\nlearned by deep learning algorithms. A lot of this work has focused on the\\ndevelopment of Gabor-like filters learned when enforcing sparsity constraints\\non a natural image dataset. Little work however has investigated how these\\nfilters might expand to the temporal domain, namely through training on natural\\nmovies. Here we investigate exactly this problem in established temporal deep\\nlearning algorithms as well as a new learning paradigm suggested here, the\\nTemporal Autoencoding Restricted Boltzmann Machine (TARBM).\\n</summary>\\n    <author>\\n      <name>Chris H\\xc3\\xa4usler</name>\\n    </author>\\n    <author>\\n      <name>Alex Susemihl</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1210.8353v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.8353v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.4044v1</id>\\n    <updated>2014-09-14T10:25:23Z</updated>\\n    <published>2014-09-14T10:25:23Z</published>\\n    <title>A new approach in machine learning</title>\\n    <summary>  In this technical report we presented a novel approach to machine learning.\\nOnce the new framework is presented, we will provide a simple and yet very\\npowerful learning algorithm which will be benchmark on various dataset.\\n  The framework we proposed is based on booleen circuits; more specifically the\\nclassifier produced by our algorithm have that form. Using bits and boolean\\ngates instead of real numbers and multiplication enable the the learning\\nalgorithm and classifier to use very efficient boolean vector operations. This\\nenable both the learning algorithm and classifier to be extremely efficient.\\nThe accuracy of the classifier we obtain with our framework compares very\\nfavorably those produced by conventional techniques, both in terms of\\nefficiency and accuracy.\\n</summary>\\n    <author>\\n      <name>Alain Tapp</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Preliminary report</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1409.4044v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.4044v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.06275v1</id>\\n    <updated>2018-01-19T02:42:57Z</updated>\\n    <published>2018-01-19T02:42:57Z</published>\\n    <title>IoT Security Techniques Based on Machine Learning</title>\\n    <summary>  Internet of things (IoT) that integrate a variety of devices into networks to\\nprovide advanced and intelligent services have to protect user privacy and\\naddress attacks such as spoofing attacks, denial of service attacks, jamming\\nand eavesdropping. In this article, we investigate the attack model for IoT\\nsystems, and review the IoT security solutions based on machine learning\\ntechniques including supervised learning, unsupervised learning and\\nreinforcement learning. We focus on the machine learning based IoT\\nauthentication, access control, secure offloading and malware detection schemes\\nto protect data privacy. In this article, we discuss the challenges that need\\nto be addressed to implement these machine learning based security schemes in\\npractical IoT systems.\\n</summary>\\n    <author>\\n      <name>Liang Xiao</name>\\n    </author>\\n    <author>\\n      <name>Xiaoyue Wan</name>\\n    </author>\\n    <author>\\n      <name>Xiaozhen Lu</name>\\n    </author>\\n    <author>\\n      <name>Yanyong Zhang</name>\\n    </author>\\n    <author>\\n      <name>Di Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1801.06275v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.06275v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.05202v2</id>\\n    <updated>2019-03-18T14:44:53Z</updated>\\n    <published>2019-03-12T20:41:36Z</published>\\n    <title>Continual Learning in Practice</title>\\n    <summary>  This paper describes a reference architecture for self-maintaining systems\\nthat can learn continually, as data arrives. In environments where data\\nevolves, we need architectures that manage Machine Learning (ML) models in\\nproduction, adapt to shifting data distributions, cope with outliers, retrain\\nwhen necessary, and adapt to new tasks. This represents continual AutoML or\\nAutomatically Adaptive Machine Learning. We describe the challenges and\\nproposes a reference architecture.\\n</summary>\\n    <author>\\n      <name>Tom Diethe</name>\\n    </author>\\n    <author>\\n      <name>Tom Borchert</name>\\n    </author>\\n    <author>\\n      <name>Eno Thereska</name>\\n    </author>\\n    <author>\\n      <name>Borja Balle</name>\\n    </author>\\n    <author>\\n      <name>Neil Lawrence</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the NeurIPS 2018 workshop on Continual Learning\\n  https://sites.google.com/view/continual2018/home</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.05202v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.05202v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.03392v1</id>\\n    <updated>2018-11-08T13:09:05Z</updated>\\n    <published>2018-11-08T13:09:05Z</published>\\n    <title>Transformative Machine Learning</title>\\n    <summary>  The key to success in machine learning (ML) is the use of effective data\\nrepresentations. Traditionally, data representations were hand-crafted.\\nRecently it has been demonstrated that, given sufficient data, deep neural\\nnetworks can learn effective implicit representations from simple input\\nrepresentations. However, for most scientific problems, the use of deep\\nlearning is not appropriate as the amount of available data is limited, and/or\\nthe output models must be explainable. Nevertheless, many scientific problems\\ndo have significant amounts of data available on related tasks, which makes\\nthem amenable to multi-task learning, i.e. learning many related problems\\nsimultaneously. Here we propose a novel and general representation learning\\napproach for multi-task learning that works successfully with small amounts of\\ndata. The fundamental new idea is to transform an input intrinsic data\\nrepresentation (i.e., handcrafted features), to an extrinsic representation\\nbased on what a pre-trained set of models predict about the examples. This\\ntransformation has the dual advantages of producing significantly more accurate\\npredictions, and providing explainable models. To demonstrate the utility of\\nthis transformative learning approach, we have applied it to three real-world\\nscientific problems: drug-design (quantitative structure activity relationship\\nlearning), predicting human gene expression (across different tissue types and\\ndrug treatments), and meta-learning for machine learning (predicting which\\nmachine learning methods work best for a given problem). In all three problems,\\ntransformative machine learning significantly outperforms the best intrinsic\\nrepresentation.\\n</summary>\\n    <author>\\n      <name>Ivan Olier</name>\\n    </author>\\n    <author>\\n      <name>Oghenejokpeme I. Orhobor</name>\\n    </author>\\n    <author>\\n      <name>Joaquin Vanschoren</name>\\n    </author>\\n    <author>\\n      <name>Ross D. King</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.03392v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.03392v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.03854v1</id>\\n    <updated>2017-09-12T14:07:13Z</updated>\\n    <published>2017-09-12T14:07:13Z</published>\\n    <title>Meta-QSAR: a large-scale application of meta-learning to drug design and\\n  discovery</title>\\n    <summary>  We investigate the learning of quantitative structure activity relationships\\n(QSARs) as a case-study of meta-learning. This application area is of the\\nhighest societal importance, as it is a key step in the development of new\\nmedicines. The standard QSAR learning problem is: given a target (usually a\\nprotein) and a set of chemical compounds (small molecules) with associated\\nbioactivities (e.g. inhibition of the target), learn a predictive mapping from\\nmolecular representation to activity. Although almost every type of machine\\nlearning method has been applied to QSAR learning there is no agreed single\\nbest way of learning QSARs, and therefore the problem area is well-suited to\\nmeta-learning. We first carried out the most comprehensive ever comparison of\\nmachine learning methods for QSAR learning: 18 regression methods, 6 molecular\\nrepresentations, applied to more than 2,700 QSAR problems. (These results have\\nbeen made publicly available on OpenML and represent a valuable resource for\\ntesting novel meta-learning methods.) We then investigated the utility of\\nalgorithm selection for QSAR problems. We found that this meta-learning\\napproach outperformed the best individual QSAR learning method (random forests\\nusing a molecular fingerprint representation) by up to 13%, on average. We\\nconclude that meta-learning outperforms base-learning methods for QSAR\\nlearning, and as this investigation is one of the most extensive ever\\ncomparisons of base and meta-learning methods ever made, it provides evidence\\nfor the general effectiveness of meta-learning over base-learning.\\n</summary>\\n    <author>\\n      <name>Ivan Olier</name>\\n    </author>\\n    <author>\\n      <name>Noureddin Sadawi</name>\\n    </author>\\n    <author>\\n      <name>G. Richard Bickerton</name>\\n    </author>\\n    <author>\\n      <name>Joaquin Vanschoren</name>\\n    </author>\\n    <author>\\n      <name>Crina Grosan</name>\\n    </author>\\n    <author>\\n      <name>Larisa Soldatova</name>\\n    </author>\\n    <author>\\n      <name>Ross D. King</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">33 pages and 15 figures. Manuscript accepted for publication in\\n  Machine Learning Journal. This is the author\\'s pre-print version</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1709.03854v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.03854v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.02910v1</id>\\n    <updated>2017-03-08T16:53:57Z</updated>\\n    <published>2017-03-08T16:53:57Z</published>\\n    <title>Deep Bayesian Active Learning with Image Data</title>\\n    <summary>  Even though active learning forms an important pillar of machine learning,\\ndeep learning tools are not prevalent within it. Deep learning poses several\\ndifficulties when used in an active learning setting. First, active learning\\n(AL) methods generally rely on being able to learn and update models from small\\namounts of data. Recent advances in deep learning, on the other hand, are\\nnotorious for their dependence on large amounts of data. Second, many AL\\nacquisition functions rely on model uncertainty, yet deep learning methods\\nrarely represent such model uncertainty. In this paper we combine recent\\nadvances in Bayesian deep learning into the active learning framework in a\\npractical way. We develop an active learning framework for high dimensional\\ndata, a task which has been extremely challenging so far, with very sparse\\nexisting literature. Taking advantage of specialised models such as Bayesian\\nconvolutional neural networks, we demonstrate our active learning techniques\\nwith image data, obtaining a significant improvement on existing active\\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\\nfor skin cancer diagnosis from lesion images (ISIC2016 task).\\n</summary>\\n    <author>\\n      <name>Yarin Gal</name>\\n    </author>\\n    <author>\\n      <name>Riashat Islam</name>\\n    </author>\\n    <author>\\n      <name>Zoubin Ghahramani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.02910v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.02910v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.3382v1</id>\\n    <updated>2014-02-14T06:46:44Z</updated>\\n    <published>2014-02-14T06:46:44Z</published>\\n    <title>Machine Learning of Phonologically Conditioned Noun Declensions For\\n  Tamil Morphological Generators</title>\\n    <summary>  This paper presents machine learning solutions to a practical problem of\\nNatural Language Generation (NLG), particularly the word formation in\\nagglutinative languages like Tamil, in a supervised manner. The morphological\\ngenerator is an important component of Natural Language Processing in\\nArtificial Intelligence. It generates word forms given a root and affixes. The\\nmorphophonemic changes like addition, deletion, alternation etc., occur when\\ntwo or more morphemes or words joined together. The Sandhi rules should be\\nexplicitly specified in the rule based morphological analyzers and generators.\\nIn machine learning framework, these rules can be learned automatically by the\\nsystem from the training samples and subsequently be applied for new inputs. In\\nthis paper we proposed the machine learning models which learn the\\nmorphophonemic rules for noun declensions from the given training data. These\\nmodels are trained to learn sandhi rules using various learning algorithms and\\nthe performance of those algorithms are presented. From this we conclude that\\nmachine learning of morphological processing such as word form generation can\\nbe successfully learned in a supervised manner, without explicit description of\\nrules. The performance of Decision trees and Bayesian machine learning\\nalgorithms on noun declensions are discussed.\\n</summary>\\n    <author>\\n      <name>K. Rajan</name>\\n    </author>\\n    <author>\\n      <name>Dr. V. Ramalingam</name>\\n    </author>\\n    <author>\\n      <name>Dr. M. Ganesan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages. International Journal of Computer Engineering and\\n  Applications, 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1402.3382v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.3382v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0508073v1</id>\\n    <updated>2005-08-16T16:27:25Z</updated>\\n    <published>2005-08-16T16:27:25Z</published>\\n    <title>Universal Learning of Repeated Matrix Games</title>\\n    <summary>  We study and compare the learning dynamics of two universal learning\\nalgorithms, one based on Bayesian learning and the other on prediction with\\nexpert advice. Both approaches have strong asymptotic performance guarantees.\\nWhen confronted with the task of finding good long-term strategies in repeated\\n2x2 matrix games, they behave quite differently.\\n</summary>\\n    <author>\\n      <name>Jan Poland</name>\\n    </author>\\n    <author>\\n      <name>Marcus Hutter</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 LaTeX pages, 8 eps figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. 15th Annual Machine Learning Conf. of Belgium and The\\n  Netherlands (Benelearn 2006) pages 7-14</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0508073v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0508073v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.4294v1</id>\\n    <updated>2012-04-19T09:29:10Z</updated>\\n    <published>2012-04-19T09:29:10Z</published>\\n    <title>Learning in Riemannian Orbifolds</title>\\n    <summary>  Learning in Riemannian orbifolds is motivated by existing machine learning\\nalgorithms that directly operate on finite combinatorial structures such as\\npoint patterns, trees, and graphs. These methods, however, lack statistical\\njustification. This contribution derives consistency results for learning\\nproblems in structured domains and thereby generalizes learning in vector\\nspaces and manifolds.\\n</summary>\\n    <author>\\n      <name>Brijnesh J. Jain</name>\\n    </author>\\n    <author>\\n      <name>Klaus Obermayer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1001.0921</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.4294v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.4294v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.00797v1</id>\\n    <updated>2017-07-04T02:05:52Z</updated>\\n    <published>2017-07-04T02:05:52Z</published>\\n    <title>Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE</title>\\n    <summary>  We propose a number of new algorithms for learning deep energy models and\\ndemonstrate their properties. We show that our SteinCD performs well in term of\\ntest likelihood, while SteinGAN performs well in terms of generating realistic\\nlooking images. Our results suggest promising directions for learning better\\nmodels by combining GAN-style methods with traditional energy-based learning.\\n</summary>\\n    <author>\\n      <name>Qiang Liu</name>\\n    </author>\\n    <author>\\n      <name>Dilin Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.00797v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.00797v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.06742v3</id>\\n    <updated>2017-08-11T00:16:49Z</updated>\\n    <published>2017-07-21T02:37:04Z</published>\\n    <title>Machine Teaching: A New Paradigm for Building Machine Learning Systems</title>\\n    <summary>  The current processes for building machine learning systems require\\npractitioners with deep knowledge of machine learning. This significantly\\nlimits the number of machine learning systems that can be created and has led\\nto a mismatch between the demand for machine learning systems and the ability\\nfor organizations to build them. We believe that in order to meet this growing\\ndemand for machine learning systems we must significantly increase the number\\nof individuals that can teach machines. We postulate that we can achieve this\\ngoal by making the process of teaching machines easy, fast and above all,\\nuniversally accessible.\\n  While machine learning focuses on creating new algorithms and improving the\\naccuracy of \"learners\", the machine teaching discipline focuses on the efficacy\\nof the \"teachers\". Machine teaching as a discipline is a paradigm shift that\\nfollows and extends principles of software engineering and programming\\nlanguages. We put a strong emphasis on the teacher and the teacher\\'s\\ninteraction with data, as well as crucial components such as techniques and\\ndesign principles of interaction and visualization.\\n  In this paper, we present our position regarding the discipline of machine\\nteaching and articulate fundamental machine teaching principles. We also\\ndescribe how, by decoupling knowledge about machine learning algorithms from\\nthe process of teaching, we can accelerate innovation and empower millions of\\nnew uses for machine learning models.\\n</summary>\\n    <author>\\n      <name>Patrice Y. Simard</name>\\n    </author>\\n    <author>\\n      <name>Saleema Amershi</name>\\n    </author>\\n    <author>\\n      <name>David M. Chickering</name>\\n    </author>\\n    <author>\\n      <name>Alicia Edelman Pelton</name>\\n    </author>\\n    <author>\\n      <name>Soroush Ghorashi</name>\\n    </author>\\n    <author>\\n      <name>Christopher Meek</name>\\n    </author>\\n    <author>\\n      <name>Gonzalo Ramos</name>\\n    </author>\\n    <author>\\n      <name>Jina Suh</name>\\n    </author>\\n    <author>\\n      <name>Johan Verwey</name>\\n    </author>\\n    <author>\\n      <name>Mo Wang</name>\\n    </author>\\n    <author>\\n      <name>John Wernsing</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Also available at: http://aka.ms/machineteachingpaper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.06742v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.06742v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.06401v1</id>\\n    <updated>2018-03-16T21:14:02Z</updated>\\n    <published>2018-03-16T21:14:02Z</published>\\n    <title>Evaluating Conditional Cash Transfer Policies with Machine Learning\\n  Methods</title>\\n    <summary>  This paper presents an out-of-sample prediction comparison between major\\nmachine learning models and the structural econometric model. Over the past\\ndecade, machine learning has established itself as a powerful tool in many\\nprediction applications, but this approach is still not widely adopted in\\nempirical economic studies. To evaluate the benefits of this approach, I use\\nthe most common machine learning algorithms, CART, C4.5, LASSO, random forest,\\nand adaboost, to construct prediction models for a cash transfer experiment\\nconducted by the Progresa program in Mexico, and I compare the prediction\\nresults with those of a previous structural econometric study. Two prediction\\ntasks are performed in this paper: the out-of-sample forecast and the long-term\\nwithin-sample simulation. For the out-of-sample forecast, both the mean\\nabsolute error and the root mean square error of the school attendance rates\\nfound by all machine learning models are smaller than those found by the\\nstructural model. Random forest and adaboost have the highest accuracy for the\\nindividual outcomes of all subgroups. For the long-term within-sample\\nsimulation, the structural model has better performance than do all of the\\nmachine learning models. The poor within-sample fitness of the machine learning\\nmodel results from the inaccuracy of the income and pregnancy prediction\\nmodels. The result shows that the machine learning model performs better than\\ndoes the structural model when there are many data to learn; however, when the\\ndata are limited, the structural model offers a more sensible prediction. The\\nfindings of this paper show promise for adopting machine learning in economic\\npolicy analyses in the era of big data.\\n</summary>\\n    <author>\\n      <name>Tzai-Shuen Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.06401v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.06401v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.5738v1</id>\\n    <updated>2013-10-21T22:02:17Z</updated>\\n    <published>2013-10-21T22:02:17Z</published>\\n    <title>A Kernel for Hierarchical Parameter Spaces</title>\\n    <summary>  We define a family of kernels for mixed continuous/discrete hierarchical\\nparameter spaces and show that they are positive definite.\\n</summary>\\n    <author>\\n      <name>Frank Hutter</name>\\n    </author>\\n    <author>\\n      <name>Michael A. Osborne</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1310.5738v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.5738v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.4072v1</id>\\n    <updated>2014-08-15T07:21:48Z</updated>\\n    <published>2014-08-15T07:21:48Z</published>\\n    <title>Indexing Cost Sensitive Prediction</title>\\n    <summary>  Predictive models are often used for real-time decision making. However,\\ntypical machine learning techniques ignore feature evaluation cost, and focus\\nsolely on the accuracy of the machine learning models obtained utilizing all\\nthe features available. We develop algorithms and indexes to support\\ncost-sensitive prediction, i.e., making decisions using machine learning models\\ntaking feature evaluation cost into account. Given an item and a online\\ncomputation cost (i.e., time) budget, we present two approaches to return an\\nappropriately chosen machine learning model that will run within the specified\\ntime on the given item. The first approach returns the optimal machine learning\\nmodel, i.e., one with the highest accuracy, that runs within the specified\\ntime, but requires significant up-front precomputation time. The second\\napproach returns a possibly sub- optimal machine learning model, but requires\\nlittle up-front precomputation time. We study these two algorithms in detail\\nand characterize the scenarios (using real and synthetic data) in which each\\nperforms well. Unlike prior work that focuses on a narrow domain or a specific\\nalgorithm, our techniques are very general: they apply to any cost-sensitive\\nprediction scenario on any machine learning algorithm.\\n</summary>\\n    <author>\\n      <name>Leilani Battle</name>\\n    </author>\\n    <author>\\n      <name>Edward Benson</name>\\n    </author>\\n    <author>\\n      <name>Aditya Parameswaran</name>\\n    </author>\\n    <author>\\n      <name>Eugene Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1408.4072v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.4072v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.1575v1</id>\\n    <updated>2013-01-06T04:03:29Z</updated>\\n    <published>2013-01-06T04:03:29Z</published>\\n    <title>BigDB: Automatic Machine Learning Optimizer</title>\\n    <summary>  In this short vision paper, we introduce a machine learning optimizer for\\ndata management and describe its architecture and main functionality.\\n</summary>\\n    <author>\\n      <name>Anna Pyayt</name>\\n    </author>\\n    <author>\\n      <name>Michael Gubanov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1301.1575v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.1575v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.5088v1</id>\\n    <updated>2013-01-22T07:10:34Z</updated>\\n    <published>2013-01-22T07:10:34Z</published>\\n    <title>Piecewise Linear Multilayer Perceptrons and Dropout</title>\\n    <summary>  We propose a new type of hidden layer for a multilayer perceptron, and\\ndemonstrate that it obtains the best reported performance for an MLP on the\\nMNIST dataset.\\n</summary>\\n    <author>\\n      <name>Ian J. Goodfellow</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1301.5088v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.5088v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.08878v1</id>\\n    <updated>2016-07-29T18:06:39Z</updated>\\n    <published>2016-07-29T18:06:39Z</published>\\n    <title>Identifying and Harnessing the Building Blocks of Machine Learning\\n  Pipelines for Sensible Initialization of a Data Science Automation Tool</title>\\n    <summary>  As data science continues to grow in popularity, there will be an increasing\\nneed to make data science tools more scalable, flexible, and accessible. In\\nparticular, automated machine learning (AutoML) systems seek to automate the\\nprocess of designing and optimizing machine learning pipelines. In this\\nchapter, we present a genetic programming-based AutoML system called TPOT that\\noptimizes a series of feature preprocessors and machine learning models with\\nthe goal of maximizing classification accuracy on a supervised classification\\nproblem. Further, we analyze a large database of pipelines that were previously\\nused to solve various supervised classification problems and identify 100 short\\nseries of machine learning operations that appear the most frequently, which we\\ncall the building blocks of machine learning pipelines. We harness these\\nbuilding blocks to initialize TPOT with promising solutions, and find that this\\nsensible initialization method significantly improves TPOT\\'s performance on one\\nbenchmark at no cost of significantly degrading performance on the others.\\nThus, sensible initialization with machine learning pipeline building blocks\\nshows promise for GP-based AutoML systems, and should be further refined in\\nfuture work.\\n</summary>\\n    <author>\\n      <name>Randal S. Olson</name>\\n    </author>\\n    <author>\\n      <name>Jason H. Moore</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 5 figures, preprint of chapter to appear in GPTP 2016 book</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1607.08878v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.08878v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.00403v1</id>\\n    <updated>2018-04-02T05:44:59Z</updated>\\n    <published>2018-04-02T05:44:59Z</published>\\n    <title>A Note on Kaldi\\'s PLDA Implementation</title>\\n    <summary>  Some explanations to Kaldi\\'s PLDA implementation to make formula derivation\\neasier to catch.\\n</summary>\\n    <author>\\n      <name>Ke Ding</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.00403v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.00403v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.00297v1</id>\\n    <updated>2018-07-01T09:01:52Z</updated>\\n    <published>2018-07-01T09:01:52Z</published>\\n    <title>Exponential Convergence of the Deep Neural Network Approximation for\\n  Analytic Functions</title>\\n    <summary>  We prove that for analytic functions in low dimension, the convergence rate\\nof the deep neural network approximation is exponential.\\n</summary>\\n    <author>\\n      <name>Weinan E</name>\\n    </author>\\n    <author>\\n      <name>Qingcan Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.00297v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.00297v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06228v1</id>\\n    <updated>2018-07-17T05:29:10Z</updated>\\n    <published>2018-07-17T05:29:10Z</published>\\n    <title>RuleMatrix: Visualizing and Understanding Classifiers with Rules</title>\\n    <summary>  With the growing adoption of machine learning techniques, there is a surge of\\nresearch interest towards making machine learning systems more transparent and\\ninterpretable. Various visualizations have been developed to help model\\ndevelopers understand, diagnose, and refine machine learning models. However, a\\nlarge number of potential but neglected users are the domain experts with\\nlittle knowledge of machine learning but are expected to work with machine\\nlearning systems. In this paper, we present an interactive visualization\\ntechnique to help users with little expertise in machine learning to\\nunderstand, explore and validate predictive models. By viewing the model as a\\nblack box, we extract a standardized rule-based knowledge representation from\\nits input-output behavior. We design RuleMatrix, a matrix-based visualization\\nof rules to help users navigate and verify the rules and the black-box model.\\nWe evaluate the effectiveness of RuleMatrix via two use cases and a usability\\nstudy.\\n</summary>\\n    <author>\\n      <name>Yao Ming</name>\\n    </author>\\n    <author>\\n      <name>Huamin Qu</name>\\n    </author>\\n    <author>\\n      <name>Enrico Bertini</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by IEEE Conference of Visual Analytics Science and\\n  Technology 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.06228v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06228v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.09856v1</id>\\n    <updated>2018-08-29T14:43:12Z</updated>\\n    <published>2018-08-29T14:43:12Z</published>\\n    <title>Application of Machine Learning in Rock Facies Classification with\\n  Physics-Motivated Feature Augmentation</title>\\n    <summary>  With recent progress in algorithms and the availability of massive amounts of\\ncomputation power, application of machine learning techniques is becoming a hot\\ntopic in the oil and gas industry. One of the most promising aspects to apply\\nmachine learning to the upstream field is the rock facies classification in\\nreservoir characterization, which is crucial in determining the net pay\\nthickness of reservoirs, thus a definitive factor in drilling decision making\\nprocess. For complex machine learning tasks like facies classification, feature\\nengineering is often critical. This paper shows the inclusion of\\nphysics-motivated feature interaction in feature augmentation can further\\nimprove the capability of machine learning in rock facies classification. We\\ndemonstrate this approach with the SEG 2016 machine learning contest dataset\\nand the top winning algorithms. The improvement is roboust and can be $\\\\sim5\\\\%$\\nbetter than current existing best F-1 score, where F-1 is an evaluation metric\\nused to quantify average prediction accuracy.\\n</summary>\\n    <author>\\n      <name>Jie Chen</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Corresponding author</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Yu Zeng</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Corresponding author</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.09856v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.09856v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.geo-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.05266v1</id>\\n    <updated>2018-11-13T13:02:55Z</updated>\\n    <published>2018-11-13T13:02:55Z</published>\\n    <title>A conjugate prior for the Dirichlet distribution</title>\\n    <summary>  This note investigates a conjugate class for the Dirichlet distribution class\\nin the exponential family.\\n</summary>\\n    <author>\\n      <name>Jean-Marc Andreoli</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.05266v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.05266v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"60E99\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.10455v1</id>\\n    <updated>2018-11-26T15:35:57Z</updated>\\n    <published>2018-11-26T15:35:57Z</published>\\n    <title>A Framework for Implementing Machine Learning on Omics Data</title>\\n    <summary>  The potential benefits of applying machine learning methods to -omics data\\nare becoming increasingly apparent, especially in clinical settings. However,\\nthe unique characteristics of these data are not always well suited to machine\\nlearning techniques. These data are often generated across different\\ntechnologies in different labs, and frequently with high dimensionality. In\\nthis paper we present a framework for combining -omics data sets, and for\\nhandling high dimensional data, making -omics research more accessible to\\nmachine learning applications. We demonstrate the success of this framework\\nthrough integration and analysis of multi-analyte data for a set of 3,533\\nbreast cancers. We then use this data-set to predict breast cancer patient\\nsurvival for individuals at risk of an impending event, with higher accuracy\\nand lower variance than methods trained on individual data-sets. We hope that\\nour pipelines for data-set generation and transformation will open up -omics\\ndata to machine learning researchers. We have made these freely available for\\nnoncommercial use at www.ccg.ai.\\n</summary>\\n    <author>\\n      <name>Geoffroy Dubourg-Felonneau</name>\\n    </author>\\n    <author>\\n      <name>Timothy Cannings</name>\\n    </author>\\n    <author>\\n      <name>Fergal Cotter</name>\\n    </author>\\n    <author>\\n      <name>Hannah Thompson</name>\\n    </author>\\n    <author>\\n      <name>Nirmesh Patel</name>\\n    </author>\\n    <author>\\n      <name>John W Cassidy</name>\\n    </author>\\n    <author>\\n      <name>Harry W Clifford</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  arXiv:1811.07216</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.10455v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.10455v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.02322v1</id>\\n    <updated>2019-02-06T18:38:02Z</updated>\\n    <published>2019-02-06T18:38:02Z</published>\\n    <title>Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?</title>\\n    <summary>  No.\\n</summary>\\n    <author>\\n      <name>Nicholas Carlini</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.02322v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.02322v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.07167v1</id>\\n    <updated>2019-03-17T20:44:25Z</updated>\\n    <published>2019-03-17T20:44:25Z</published>\\n    <title>Machine Learning: A Dark Side of Cancer Computing</title>\\n    <summary>  Cancer analysis and prediction is the utmost important research field for\\nwell-being of humankind. The Cancer data are analyzed and predicted using\\nmachine learning algorithms. Most of the researcher claims the accuracy of the\\npredicted results within 99%. However, we show that machine learning algorithms\\ncan easily predict with an accuracy of 100% on Wisconsin Diagnostic Breast\\nCancer dataset. We show that the method of gaining accuracy is an unethical\\napproach that we can easily mislead the algorithms. In this paper, we exploit\\nthe weakness of Machine Learning algorithms. We perform extensive experiments\\nfor the correctness of our results to exploit the weakness of machine learning\\nalgorithms. The methods are rigorously evaluated to validate our claim. In\\naddition, this paper focuses on correctness of accuracy. This paper report\\nthree key outcomes of the experiments, namely, correctness of accuracies,\\nsignificance of minimum accuracy, and correctness of machine learning\\nalgorithms.\\n</summary>\\n    <author>\\n      <name>Ripon Patgiri</name>\\n    </author>\\n    <author>\\n      <name>Sabuzima Nayak</name>\\n    </author>\\n    <author>\\n      <name>Tanya Akutota</name>\\n    </author>\\n    <author>\\n      <name>Bishal Paul</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 Pages, 21 Figures, 2 Tables, Proceedings of the 2018 International\\n  Conference on Bioinformatics and Computational Biology, pp. 92-98, 2018</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 2018 International Conference on Bioinformatics\\n  and Computational Biology, pp. 92-98, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1903.07167v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.07167v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.09731v2</id>\\n    <updated>2019-04-01T21:22:36Z</updated>\\n    <published>2019-03-22T23:32:22Z</published>\\n    <title>Expert-Augmented Machine Learning</title>\\n    <summary>  Machine Learning is proving invaluable across disciplines. However, its\\nsuccess is often limited by the quality and quantity of available data, while\\nits adoption by the level of trust that models afford users. Human vs. machine\\nperformance is commonly compared empirically to decide whether a certain task\\nshould be performed by a computer or an expert. In reality, the optimal\\nlearning strategy may involve combining the complementary strengths of man and\\nmachine. Here we present Expert-Augmented Machine Learning (EAML), an automated\\nmethod that guides the extraction of expert knowledge and its integration into\\nmachine-learned models. We use a large dataset of intensive care patient data\\nto predict mortality and show that we can extract expert knowledge using an\\nonline platform, help reveal hidden confounders, improve generalizability on a\\ndifferent population and learn using less data. EAML presents a novel framework\\nfor high performance and dependable machine learning in critical applications.\\n</summary>\\n    <author>\\n      <name>E. D. Gennatas</name>\\n    </author>\\n    <author>\\n      <name>J. H. Friedman</name>\\n    </author>\\n    <author>\\n      <name>L. H. Ungar</name>\\n    </author>\\n    <author>\\n      <name>R. Pirracchio</name>\\n    </author>\\n    <author>\\n      <name>E. Eaton</name>\\n    </author>\\n    <author>\\n      <name>L. Reichman</name>\\n    </author>\\n    <author>\\n      <name>Y. Interian</name>\\n    </author>\\n    <author>\\n      <name>C. B. Simone</name>\\n    </author>\\n    <author>\\n      <name>A. Auerbach</name>\\n    </author>\\n    <author>\\n      <name>E. Delgado</name>\\n    </author>\\n    <author>\\n      <name>M. J. Van der Laan</name>\\n    </author>\\n    <author>\\n      <name>T. D. Solberg</name>\\n    </author>\\n    <author>\\n      <name>G. Valdes</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.09731v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.09731v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.07426v3</id>\\n    <updated>2019-03-06T22:23:14Z</updated>\\n    <published>2018-02-21T05:03:52Z</published>\\n    <title>Generalization in Machine Learning via Analytical Learning Theory</title>\\n    <summary>  This paper introduces a novel measure-theoretic theory for machine learning\\nthat does not require statistical assumptions. Based on this theory, a new\\nregularization method in deep learning is derived and shown to outperform\\nprevious methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed\\ntheory provides a theoretical basis for a family of practically successful\\nregularization methods in deep learning. We discuss several consequences of our\\nresults on one-shot learning, representation learning, deep learning, and\\ncurriculum learning. Unlike statistical learning theory, the proposed learning\\ntheory analyzes each problem instance individually via measure theory, rather\\nthan a set of problem instances via statistics. As a result, it provides\\ndifferent types of results and insights when compared to statistical learning\\ntheory.\\n</summary>\\n    <author>\\n      <name>Kenji Kawaguchi</name>\\n    </author>\\n    <author>\\n      <name>Yoshua Bengio</name>\\n    </author>\\n    <author>\\n      <name>Vikas Verma</name>\\n    </author>\\n    <author>\\n      <name>Leslie Pack Kaelbling</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.07426v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.07426v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.08169v1</id>\\n    <updated>2018-07-21T15:40:10Z</updated>\\n    <published>2018-07-21T15:40:10Z</published>\\n    <title>Recent Advances in Deep Learning: An Overview</title>\\n    <summary>  Deep Learning is one of the newest trends in Machine Learning and Artificial\\nIntelligence research. It is also one of the most popular scientific research\\ntrends now-a-days. Deep learning methods have brought revolutionary advances in\\ncomputer vision and machine learning. Every now and then, new and new deep\\nlearning techniques are being born, outperforming state-of-the-art machine\\nlearning and even existing deep learning techniques. In recent years, the world\\nhas seen many major breakthroughs in this field. Since deep learning is\\nevolving at a huge speed, its kind of hard to keep track of the regular\\nadvances especially for new researchers. In this paper, we are going to briefly\\ndiscuss about recent advances in Deep Learning for past few years.\\n</summary>\\n    <author>\\n      <name>Matiur Rahman Minar</name>\\n    </author>\\n    <author>\\n      <name>Jibon Naher</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.13140/RG.2.2.24831.10403</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.13140/RG.2.2.24831.10403\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">31 pages including bibliography</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.08169v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.08169v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1008.1643v2</id>\\n    <updated>2010-12-12T06:13:31Z</updated>\\n    <published>2010-08-10T07:44:08Z</published>\\n    <title>A Learning Algorithm based on High School Teaching Wisdom</title>\\n    <summary>  A learning algorithm based on primary school teaching and learning is\\npresented. The methodology is to continuously evaluate a student and to give\\nthem training on the examples for which they repeatedly fail, until, they can\\ncorrectly answer all types of questions. This incremental learning procedure\\nproduces better learning curves by demanding the student to optimally dedicate\\ntheir learning time on the failed examples. When used in machine learning, the\\nalgorithm is found to train a machine on a data with maximum variance in the\\nfeature space so that the generalization ability of the network improves. The\\nalgorithm has interesting applications in data mining, model evaluations and\\nrare objects discovery.\\n</summary>\\n    <author>\\n      <name>Ninan Sajeeth Philip</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1008.1643v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1008.1643v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.06988v1</id>\\n    <updated>2016-07-24T01:14:19Z</updated>\\n    <published>2016-07-24T01:14:19Z</published>\\n    <title>Interactive Learning from Multiple Noisy Labels</title>\\n    <summary>  Interactive learning is a process in which a machine learning algorithm is\\nprovided with meaningful, well-chosen examples as opposed to randomly chosen\\nexamples typical in standard supervised learning. In this paper, we propose a\\nnew method for interactive learning from multiple noisy labels where we exploit\\nthe disagreement among annotators to quantify the easiness (or meaningfulness)\\nof an example. We demonstrate the usefulness of this method in estimating the\\nparameters of a latent variable classification model, and conduct experimental\\nanalyses on a range of synthetic and benchmark datasets. Furthermore, we\\ntheoretically analyze the performance of perceptron in this interactive\\nlearning framework.\\n</summary>\\n    <author>\\n      <name>Shankar Vembu</name>\\n    </author>\\n    <author>\\n      <name>Sandra Zilles</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1607.06988v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.06988v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.05748v2</id>\\n    <updated>2018-07-31T09:25:36Z</updated>\\n    <published>2018-07-16T09:21:38Z</published>\\n    <title>Learning Stochastic Differential Equations With Gaussian Processes\\n  Without Gradient Matching</title>\\n    <summary>  We introduce a novel paradigm for learning non-parametric drift and diffusion\\nfunctions for stochastic differential equation (SDE). The proposed model learns\\nto simulate path distributions that match observations with non-uniform time\\nincrements and arbitrary sparseness, which is in contrast with gradient\\nmatching that does not optimize simulated responses. We formulate sensitivity\\nequations for learning and demonstrate that our general stochastic distribution\\noptimisation leads to robust and efficient learning of SDE systems.\\n</summary>\\n    <author>\\n      <name>Cagatay Yildiz</name>\\n    </author>\\n    <author>\\n      <name>Markus Heinonen</name>\\n    </author>\\n    <author>\\n      <name>Jukka Intosalmi</name>\\n    </author>\\n    <author>\\n      <name>Henrik Mannerstr\\xc3\\xb6m</name>\\n    </author>\\n    <author>\\n      <name>Harri L\\xc3\\xa4hdesm\\xc3\\xa4ki</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The accepted version of the paper to be presented in 2018 IEEE\\n  International Workshop on Machine Learning for Signal Processing</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.05748v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.05748v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.06995v1</id>\\n    <updated>2018-09-19T03:23:35Z</updated>\\n    <published>2018-09-19T03:23:35Z</published>\\n    <title>Interpretable Reinforcement Learning with Ensemble Methods</title>\\n    <summary>  We propose to use boosted regression trees as a way to compute\\nhuman-interpretable solutions to reinforcement learning problems. Boosting\\ncombines several regression trees to improve their accuracy without\\nsignificantly reducing their inherent interpretability. Prior work has focused\\nindependently on reinforcement learning and on interpretable machine learning,\\nbut there has been little progress in interpretable reinforcement learning. Our\\nexperimental results show that boosted regression trees compute solutions that\\nare both interpretable and match the quality of leading reinforcement learning\\nmethods.\\n</summary>\\n    <author>\\n      <name>Alexander Brown</name>\\n    </author>\\n    <author>\\n      <name>Marek Petrik</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1809.06995v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.06995v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.02664v1</id>\\n    <updated>2016-09-09T06:04:17Z</updated>\\n    <published>2016-09-09T06:04:17Z</published>\\n    <title>Machine Learning with Guarantees using Descriptive Complexity and SMT\\n  Solvers</title>\\n    <summary>  Machine learning is a thriving part of computer science. There are many\\nefficient approaches to machine learning that do not provide strong theoretical\\nguarantees, and a beautiful general learning theory. Unfortunately, machine\\nlearning approaches that give strong theoretical guarantees have not been\\nefficient enough to be applicable. In this paper we introduce a logical\\napproach to machine learning. Models are represented by tuples of logical\\nformulas and inputs and outputs are logical structures. We present our\\nframework together with several applications where we evaluate it using SAT and\\nSMT solvers. We argue that this approach to machine learning is particularly\\nsuited to bridge the gap between efficiency and theoretical soundness. We\\nexploit results from descriptive complexity theory to prove strong theoretical\\nguarantees for our approach. To show its applicability, we present experimental\\nresults including learning complexity-theoretic reductions rules for board\\ngames. We also explain how neural networks fit into our framework, although the\\ncurrent implementation does not scale to provide guarantees for real-world\\nneural networks.\\n</summary>\\n    <author>\\n      <name>Charles Jordan</name>\\n    </author>\\n    <author>\\n      <name>\\xc5\\x81ukasz Kaiser</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1609.02664v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.02664v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.07567v1</id>\\n    <updated>2016-11-22T22:36:31Z</updated>\\n    <published>2016-11-22T22:36:31Z</published>\\n    <title>Feature Importance Measure for Non-linear Learning Algorithms</title>\\n    <summary>  Complex problems may require sophisticated, non-linear learning methods such\\nas kernel machines or deep neural networks to achieve state of the art\\nprediction accuracies. However, high prediction accuracies are not the only\\nobjective to consider when solving problems using machine learning. Instead,\\nparticular scientific applications require some explanation of the learned\\nprediction function. Unfortunately, most methods do not come with out of the\\nbox straight forward interpretation. Even linear prediction functions are not\\nstraight forward to explain if features exhibit complex correlation structure.\\n  In this paper, we propose the Measure of Feature Importance (MFI). MFI is\\ngeneral and can be applied to any arbitrary learning machine (including kernel\\nmachines and deep learning). MFI is intrinsically non-linear and can detect\\nfeatures that by itself are inconspicuous and only impact the prediction\\nfunction through their interaction with other features. Lastly, MFI can be used\\nfor both --- model-based feature importance and instance-based feature\\nimportance (i.e, measuring the importance of a feature for a particular data\\npoint).\\n</summary>\\n    <author>\\n      <name>Marina M. -C. Vidovic</name>\\n    </author>\\n    <author>\\n      <name>Nico G\\xc3\\xb6rnitz</name>\\n    </author>\\n    <author>\\n      <name>Klaus-Robert M\\xc3\\xbcller</name>\\n    </author>\\n    <author>\\n      <name>Marius Kloft</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\\n  Complex Systems</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.07567v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.07567v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.12629v2</id>\\n    <updated>2018-12-19T16:39:50Z</updated>\\n    <published>2018-11-30T06:05:21Z</published>\\n    <title>LoAdaBoost:Loss-Based AdaBoost Federated Machine Learning on medical\\n  Data</title>\\n    <summary>  Medical data are valuable for improvement of health care, policy making and\\nmany other purposes. Vast amount of medical data are stored in different\\nlocations, on many different devices and in different data silos. Sharing\\nmedical data among different sources is a big challenge due to regulatory,\\noperational and security reasons. One potential solution is federated machine\\nlearning ,which is a method that sends machine learning algorithms\\nsimultaneously to all data sources, train models in each source and aggregates\\nthe learned models. This strategy allows utilization of valuable data without\\nmoving them.One challenge in applying federated machine learning is the\\nheterogeneity of data from different sources. To tackle this problem, we\\nproposed an adaptive boosting method that increases the efficiency of federated\\nmachine learning. Using intensive care unit data from hospital, we showed that\\nLoAdaBoost federated learning outperformed baseline method and increased\\ncommunication efficiency at negligible additional cost.\\n</summary>\\n    <author>\\n      <name>Li Huang</name>\\n    </author>\\n    <author>\\n      <name>Yifeng Yin</name>\\n    </author>\\n    <author>\\n      <name>Zeng Fu</name>\\n    </author>\\n    <author>\\n      <name>Shifa Zhang</name>\\n    </author>\\n    <author>\\n      <name>Hao Deng</name>\\n    </author>\\n    <author>\\n      <name>Dianbo Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.12629v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.12629v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.8320v1</id>\\n    <updated>2013-10-30T20:56:50Z</updated>\\n    <published>2013-10-30T20:56:50Z</published>\\n    <title>Safe and Efficient Screening For Sparse Support Vector Machine</title>\\n    <summary>  Screening is an effective technique for speeding up the training process of a\\nsparse learning model by removing the features that are guaranteed to be\\ninactive the process. In this paper, we present a efficient screening technique\\nfor sparse support vector machine based on variational inequality. The\\ntechnique is both efficient and safe.\\n</summary>\\n    <author>\\n      <name>Zheng Zhao</name>\\n    </author>\\n    <author>\\n      <name>Jun Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1310.8320v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.8320v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.3726v1</id>\\n    <updated>2014-06-14T13:08:30Z</updated>\\n    <published>2014-06-14T13:08:30Z</published>\\n    <title>Evaluation of Machine Learning Techniques for Green Energy Prediction</title>\\n    <summary>  We evaluate the following Machine Learning techniques for Green Energy (Wind,\\nSolar) Prediction: Bayesian Inference, Neural Networks, Support Vector\\nMachines, Clustering techniques (PCA). Our objective is to predict green energy\\nusing weather forecasts, predict deviations from forecast green energy, find\\ncorrelation amongst different weather parameters and green energy availability,\\nrecover lost or missing energy (/ weather) data. We use historical weather data\\nand weather forecasts for the same.\\n</summary>\\n    <author>\\n      <name>Ankur Sahai</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.3726v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.3726v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.00231v3</id>\\n    <updated>2019-01-09T15:45:11Z</updated>\\n    <published>2018-11-01T05:07:49Z</published>\\n    <title>Towards learning-to-learn</title>\\n    <summary>  In good old-fashioned artificial intelligence (GOFAI), humans specified\\nsystems that solved problems. Much of the recent progress in AI has come from\\nreplacing human insights by learning. However, learning itself is still usually\\nbuilt by humans -- specifically the choice that parameter updates should follow\\nthe gradient of a cost function. Yet, in analogy with GOFAI, there is no reason\\nto believe that humans are particularly good at defining such learning systems:\\nwe may expect learning itself to be better if we learn it. Recent research in\\nmachine learning has started to realize the benefits of that strategy. We\\nshould thus expect this to be relevant for neuroscience: how could the correct\\nlearning rules be acquired? Indeed, cognitive science has long shown that\\nhumans learn-to-learn, which is potentially responsible for their impressive\\nlearning abilities. Here we discuss ideas across machine learning,\\nneuroscience, and cognitive science that matter for the principle of\\nlearning-to-learn.\\n</summary>\\n    <author>\\n      <name>Benjamin James Lansdell</name>\\n    </author>\\n    <author>\\n      <name>Konrad Paul Kording</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.00231v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.00231v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.02865v1</id>\\n    <updated>2018-06-07T19:02:18Z</updated>\\n    <published>2018-06-07T19:02:18Z</published>\\n    <title>Kernel Machines With Missing Responses</title>\\n    <summary>  Missing responses is a missing data format in which outcomes are not always\\nobserved. In this work we develop kernel machines that can handle missing\\nresponses. First, we propose a kernel machine family that uses mainly the\\ncomplete cases. For the quadratic loss, we then propose a family of\\ndoubly-robust kernel machines. The proposed kernel-machine estimators can be\\napplied to both regression and classification problems. We prove oracle\\ninequalities for the finite-sample differences between the kernel machine risk\\nand Bayes risk. We use these oracle inequalities to prove consistency and to\\ncalculate convergence rates. We demonstrate the performance of the two proposed\\nkernel machine families using both a simulation study and a real-world data\\nanalysis.\\n</summary>\\n    <author>\\n      <name>Tiantian Liu</name>\\n    </author>\\n    <author>\\n      <name>Yair Goldberg</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.02865v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.02865v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.1766v1</id>\\n    <updated>2011-08-08T18:04:02Z</updated>\\n    <published>2011-08-08T18:04:02Z</published>\\n    <title>Activized Learning: Transforming Passive to Active with Improved Label\\n  Complexity</title>\\n    <summary>  We study the theoretical advantages of active learning over passive learning.\\nSpecifically, we prove that, in noise-free classifier learning for VC classes,\\nany passive learning algorithm can be transformed into an active learning\\nalgorithm with asymptotically strictly superior label complexity for all\\nnontrivial target functions and distributions. We further provide a general\\ncharacterization of the magnitudes of these improvements in terms of a novel\\ngeneralization of the disagreement coefficient. We also extend these results to\\nactive learning in the presence of label noise, and find that even under broad\\nclasses of noise distributions, we can typically guarantee strict improvements\\nover the known results for passive learning.\\n</summary>\\n    <author>\\n      <name>Steve Hanneke</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1108.1766v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.1766v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.10612v1</id>\\n    <updated>2018-10-24T20:41:13Z</updated>\\n    <published>2018-10-24T20:41:13Z</published>\\n    <title>Continual Classification Learning Using Generative Models</title>\\n    <summary>  Continual learning is the ability to sequentially learn over time by\\naccommodating knowledge while retaining previously learned experiences. Neural\\nnetworks can learn multiple tasks when trained on them jointly, but cannot\\nmaintain performance on previously learned tasks when tasks are presented one\\nat a time. This problem is called catastrophic forgetting. In this work, we\\npropose a classification model that learns continuously from sequentially\\nobserved tasks, while preventing catastrophic forgetting. We build on the\\nlifelong generative capabilities of [10] and extend it to the classification\\nsetting by deriving a new variational bound on the joint log likelihood, $\\\\log\\np(x; y)$.\\n</summary>\\n    <author>\\n      <name>Frantzeska Lavda</name>\\n    </author>\\n    <author>\\n      <name>Jason Ramapuram</name>\\n    </author>\\n    <author>\\n      <name>Magda Gregorova</name>\\n    </author>\\n    <author>\\n      <name>Alexandros Kalousis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 4 figures, under review in Continual learning Workshop NIPS\\n  2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.10612v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.10612v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0904.3667v1</id>\\n    <updated>2009-04-23T11:48:38Z</updated>\\n    <published>2009-04-23T11:48:38Z</published>\\n    <title>Considerations upon the Machine Learning Technologies</title>\\n    <summary>  Artificial intelligence offers superior techniques and methods by which\\nproblems from diverse domains may find an optimal solution. The Machine\\nLearning technologies refer to the domain of artificial intelligence aiming to\\ndevelop the techniques allowing the computers to \"learn\". Some systems based on\\nMachine Learning technologies tend to eliminate the necessity of the human\\nintelligence while the others adopt a man-machine collaborative approach.\\n</summary>\\n    <author>\\n      <name>Alin Munteanu</name>\\n    </author>\\n    <author>\\n      <name>Cristina Ofelia Sofran</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages,exposed on 1st \"European Conference on Computer Sciences &amp;\\n  Applications\" - XA2006, Timisoara, Romania</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 133-138</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0904.3667v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.3667v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0911.1386v1</id>\\n    <updated>2009-11-07T02:52:53Z</updated>\\n    <published>2009-11-07T02:52:53Z</published>\\n    <title>Machine Learning: When and Where the Horses Went Astray?</title>\\n    <summary>  Machine Learning is usually defined as a subfield of AI, which is busy with\\ninformation extraction from raw data sets. Despite of its common acceptance and\\nwidespread recognition, this definition is wrong and groundless. Meaningful\\ninformation does not belong to the data that bear it. It belongs to the\\nobservers of the data and it is a shared agreement and a convention among them.\\nTherefore, this private information cannot be extracted from the data by any\\nmeans. Therefore, all further attempts of Machine Learning apologists to\\njustify their funny business are inappropriate.\\n</summary>\\n    <author>\\n      <name>Emanuel Diamant</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The paper is accepted to be published in the Machine Learning serie\\n  of the InTech</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0911.1386v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0911.1386v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.6548v2</id>\\n    <updated>2012-03-01T13:31:54Z</updated>\\n    <published>2012-02-29T13:49:10Z</published>\\n    <title>mlpy: Machine Learning Python</title>\\n    <summary>  mlpy is a Python Open Source Machine Learning library built on top of\\nNumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of\\nstate-of-the-art machine learning methods for supervised and unsupervised\\nproblems and it is aimed at finding a reasonable compromise among modularity,\\nmaintainability, reproducibility, usability and efficiency. mlpy is\\nmultiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at\\nthe website http://mlpy.fbk.eu.\\n</summary>\\n    <author>\\n      <name>Davide Albanese</name>\\n    </author>\\n    <author>\\n      <name>Roberto Visintainer</name>\\n    </author>\\n    <author>\\n      <name>Stefano Merler</name>\\n    </author>\\n    <author>\\n      <name>Samantha Riccadonna</name>\\n    </author>\\n    <author>\\n      <name>Giuseppe Jurman</name>\\n    </author>\\n    <author>\\n      <name>Cesare Furlanello</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Corrected a few typos; rephrased two sentences in the Overview\\n  section</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1202.6548v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.6548v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1308.4214v1</id>\\n    <updated>2013-08-20T02:50:43Z</updated>\\n    <published>2013-08-20T02:50:43Z</published>\\n    <title>Pylearn2: a machine learning research library</title>\\n    <summary>  Pylearn2 is a machine learning research library. This does not just mean that\\nit is a collection of machine learning algorithms that share a common API; it\\nmeans that it has been designed for flexibility and extensibility in order to\\nfacilitate research projects that involve new or unusual use cases. In this\\npaper we give a brief history of the library, an overview of its basic\\nphilosophy, a summary of the library\\'s architecture, and a description of how\\nthe Pylearn2 community functions socially.\\n</summary>\\n    <author>\\n      <name>Ian J. Goodfellow</name>\\n    </author>\\n    <author>\\n      <name>David Warde-Farley</name>\\n    </author>\\n    <author>\\n      <name>Pascal Lamblin</name>\\n    </author>\\n    <author>\\n      <name>Vincent Dumoulin</name>\\n    </author>\\n    <author>\\n      <name>Mehdi Mirza</name>\\n    </author>\\n    <author>\\n      <name>Razvan Pascanu</name>\\n    </author>\\n    <author>\\n      <name>James Bergstra</name>\\n    </author>\\n    <author>\\n      <name>Fr\\xc3\\xa9d\\xc3\\xa9ric Bastien</name>\\n    </author>\\n    <author>\\n      <name>Yoshua Bengio</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1308.4214v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1308.4214v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.01042v1</id>\\n    <updated>2016-06-03T10:58:37Z</updated>\\n    <published>2016-06-03T10:58:37Z</published>\\n    <title>Machine Learning for E-mail Spam Filtering: Review,Techniques and Trends</title>\\n    <summary>  We present a comprehensive review of the most effective content-based e-mail\\nspam filtering techniques. We focus primarily on Machine Learning-based spam\\nfilters and their variants, and report on a broad review ranging from surveying\\nthe relevant ideas, efforts, effectiveness, and the current progress. The\\ninitial exposition of the background examines the basics of e-mail spam\\nfiltering, the evolving nature of spam, spammers playing cat-and-mouse with\\ne-mail service providers (ESPs), and the Machine Learning front in fighting\\nspam. We conclude by measuring the impact of Machine Learning-based filters and\\nexplore the promising offshoots of latest developments.\\n</summary>\\n    <author>\\n      <name>Alexy Bhowmick</name>\\n    </author>\\n    <author>\\n      <name>Shyamanta M. Hazarika</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal. 27 Pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.01042v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.01042v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.05685v2</id>\\n    <updated>2016-06-21T18:06:13Z</updated>\\n    <published>2016-06-17T21:56:43Z</published>\\n    <title>Using Visual Analytics to Interpret Predictive Machine Learning Models</title>\\n    <summary>  It is commonly believed that increasing the interpretability of a machine\\nlearning model may decrease its predictive power. However, inspecting\\ninput-output relationships of those models using visual analytics, while\\ntreating them as black-box, can help to understand the reasoning behind\\noutcomes without sacrificing predictive quality. We identify a space of\\npossible solutions and provide two examples of where such techniques have been\\nsuccessfully used in practice.\\n</summary>\\n    <author>\\n      <name>Josua Krause</name>\\n    </author>\\n    <author>\\n      <name>Adam Perer</name>\\n    </author>\\n    <author>\\n      <name>Enrico Bertini</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">presented at 2016 ICML Workshop on Human Interpretability in Machine\\n  Learning (WHI 2016), New York, NY</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.05685v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.05685v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.4656v1</id>\\n    <updated>2012-06-18T15:26:13Z</updated>\\n    <published>2012-06-18T15:26:13Z</published>\\n    <title>Machine Learning that Matters</title>\\n    <summary>  Much of current machine learning (ML) research has lost its connection to\\nproblems of import to the larger world of science and society. From this\\nperspective, there exist glaring limitations in the data sets we investigate,\\nthe metrics we employ for evaluation, and the degree to which results are\\ncommunicated back to their originating domains. What changes are needed to how\\nwe conduct research to increase the impact that ML has? We present six Impact\\nChallenges to explicitly focus the field?s energy and attention, and we discuss\\nexisting obstacles that must be addressed. We aim to inspire ongoing discussion\\nand focus on ML that matters.\\n</summary>\\n    <author>\\n      <name>Kiri Wagstaff</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Jet Propulsion Laboratory</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML2012</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the Twenty-Ninth International Conference on\\n  Machine Learning (ICML), p. 529-536</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1206.4656v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.4656v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.0745v1</id>\\n    <updated>2014-03-04T11:28:59Z</updated>\\n    <published>2014-03-04T11:28:59Z</published>\\n    <title>EnsembleSVM: A Library for Ensemble Learning Using Support Vector\\n  Machines</title>\\n    <summary>  EnsembleSVM is a free software package containing efficient routines to\\nperform ensemble learning with support vector machine (SVM) base models. It\\ncurrently offers ensemble methods based on binary SVM models. Our\\nimplementation avoids duplicate storage and evaluation of support vectors which\\nare shared between constituent models. Experimental results show that using\\nensemble approaches can drastically reduce training complexity while\\nmaintaining high predictive accuracy. The EnsembleSVM software package is\\nfreely available online at http://esat.kuleuven.be/stadius/ensemblesvm.\\n</summary>\\n    <author>\\n      <name>Marc Claesen</name>\\n    </author>\\n    <author>\\n      <name>Frank De Smet</name>\\n    </author>\\n    <author>\\n      <name>Johan Suykens</name>\\n    </author>\\n    <author>\\n      <name>Bart De Moor</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 1 table</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research. 15 (2014) 141-145</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1403.0745v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.0745v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.3; I.2.6; I.5.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1601.03642v1</id>\\n    <updated>2016-01-12T23:28:07Z</updated>\\n    <published>2016-01-12T23:28:07Z</published>\\n    <title>Creativity in Machine Learning</title>\\n    <summary>  Recent machine learning techniques can be modified to produce creative\\nresults. Those results did not exist before; it is not a trivial combination of\\nthe data which was fed into the machine learning system. The obtained results\\ncome in multiple forms: As images, as text and as audio.\\n  This paper gives a high level overview of how they are created and gives some\\nexamples. It is meant to be a summary of the current work and give people who\\nare new to machine learning some starting points.\\n</summary>\\n    <author>\\n      <name>Martin Thoma</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1601.03642v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1601.03642v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.04776v1</id>\\n    <updated>2015-06-15T21:20:06Z</updated>\\n    <published>2015-06-15T21:20:06Z</published>\\n    <title>Encog: Library of Interchangeable Machine Learning Models for Java and\\n  C#</title>\\n    <summary>  This paper introduces the Encog library for Java and C#, a scalable,\\nadaptable, multiplatform machine learning framework that was 1st released in\\n2008. Encog allows a variety of machine learning models to be applied to\\ndatasets using regression, classification, and clustering. Various supported\\nmachine learning models can be used interchangeably with minimal recoding.\\nEncog uses efficient multithreaded code to reduce training time by exploiting\\nmodern multicore processors. The current version of Encog can be downloaded\\nfrom http://www.encog.org.\\n</summary>\\n    <author>\\n      <name>Jeff Heaton</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1506.04776v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.04776v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T01\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.05740v1</id>\\n    <updated>2016-12-17T11:57:45Z</updated>\\n    <published>2016-12-17T11:57:45Z</published>\\n    <title>Machine Learning, Linear and Bayesian Models for Logistic Regression in\\n  Failure Detection Problems</title>\\n    <summary>  In this work, we study the use of logistic regression in manufacturing\\nfailures detection. As a data set for the analysis, we used the data from\\nKaggle competition Bosch Production Line Performance. We considered the use of\\nmachine learning, linear and Bayesian models. For machine learning approach, we\\nanalyzed XGBoost tree based classifier to obtain high scored classification.\\nUsing the generalized linear model for logistic regression makes it possible to\\nanalyze the influence of the factors under study. The Bayesian approach for\\nlogistic regression gives the statistical distribution for the parameters of\\nthe model. It can be useful in the probabilistic analysis, e.g. risk\\nassessment.\\n</summary>\\n    <author>\\n      <name>B. Pavlyshenko</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.05740v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.05740v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.04680v1</id>\\n    <updated>2017-08-11T11:19:44Z</updated>\\n    <published>2017-08-11T11:19:44Z</published>\\n    <title>Augmentor: An Image Augmentation Library for Machine Learning</title>\\n    <summary>  The generation of artificial data based on existing observations, known as\\ndata augmentation, is a technique used in machine learning to improve model\\naccuracy, generalisation, and to control overfitting. Augmentor is a software\\npackage, available in both Python and Julia versions, that provides a high\\nlevel API for the expansion of image data using a stochastic, pipeline-based\\napproach which effectively allows for images to be sampled from a distribution\\nof augmented images at runtime. Augmentor provides methods for most standard\\naugmentation practices as well as several advanced features such as\\nlabel-preserving, randomised elastic distortions, and provides many helper\\nfunctions for typical augmentation tasks used in machine learning.\\n</summary>\\n    <author>\\n      <name>Marcus D. Bloice</name>\\n    </author>\\n    <author>\\n      <name>Christof Stocker</name>\\n    </author>\\n    <author>\\n      <name>Andreas Holzinger</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.04680v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.04680v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.08022v1</id>\\n    <updated>2017-08-26T23:23:39Z</updated>\\n    <published>2017-08-26T23:23:39Z</published>\\n    <title>On the Protection of Private Information in Machine Learning Systems:\\n  Two Recent Approaches</title>\\n    <summary>  The recent, remarkable growth of machine learning has led to intense interest\\nin the privacy of the data on which machine learning relies, and to new\\ntechniques for preserving privacy. However, older ideas about privacy may well\\nremain valid and useful. This note reviews two recent works on privacy in the\\nlight of the wisdom of some of the early literature, in particular the\\nprinciples distilled by Saltzer and Schroeder in the 1970s.\\n</summary>\\n    <author>\\n      <name>Mart\\xc3\\xadn Abadi</name>\\n    </author>\\n    <author>\\n      <name>\\xc3\\x9alfar Erlingsson</name>\\n    </author>\\n    <author>\\n      <name>Ian Goodfellow</name>\\n    </author>\\n    <author>\\n      <name>H. Brendan McMahan</name>\\n    </author>\\n    <author>\\n      <name>Ilya Mironov</name>\\n    </author>\\n    <author>\\n      <name>Nicolas Papernot</name>\\n    </author>\\n    <author>\\n      <name>Kunal Talwar</name>\\n    </author>\\n    <author>\\n      <name>Li Zhang</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CSF.2017.10</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CSF.2017.10\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE 30th Computer Security Foundations Symposium (CSF), pages\\n  1--6, 2017</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1708.08022v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.08022v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.00001v1</id>\\n    <updated>2017-10-30T19:02:13Z</updated>\\n    <published>2017-10-30T19:02:13Z</published>\\n    <title>Gene Ontology (GO) Prediction using Machine Learning Methods</title>\\n    <summary>  We applied machine learning to predict whether a gene is involved in axon\\nregeneration. We extracted 31 features from different databases and trained\\nfive machine learning models. Our optimal model, a Random Forest Classifier\\nwith 50 submodels, yielded a test score of 85.71%, which is 4.1% higher than\\nthe baseline score. We concluded that our models have some predictive\\ncapability. Similar methodology and features could be applied to predict other\\nGene Ontology (GO) terms.\\n</summary>\\n    <author>\\n      <name>Haoze Wu</name>\\n    </author>\\n    <author>\\n      <name>Yangyu Zhou</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 8 figures, 3 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.00001v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.00001v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.03532v2</id>\\n    <updated>2018-02-16T22:38:34Z</updated>\\n    <published>2018-02-10T07:14:53Z</published>\\n    <title>Bayesian Optimization Using Monotonicity Information and Its Application\\n  in Machine Learning Hyperparameter</title>\\n    <summary>  We propose an algorithm for a family of optimization problems where the\\nobjective can be decomposed as a sum of functions with monotonicity properties.\\nThe motivating problem is optimization of hyperparameters of machine learning\\nalgorithms, where we argue that the objective, validation error, can be\\ndecomposed as monotonic functions of the hyperparameters. Our proposed\\nalgorithm adapts Bayesian optimization methods to incorporate the monotonicity\\nconstraints. We illustrate the advantages of exploiting monotonicity using\\nillustrative examples and demonstrate the improvements in optimization\\nefficiency for some machine learning hyperparameter tuning applications.\\n</summary>\\n    <author>\\n      <name>Wenyi Wang</name>\\n    </author>\\n    <author>\\n      <name>William J. Welch</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Citation style errors fixed</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.03532v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.03532v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.04193v1</id>\\n    <updated>2018-03-12T11:12:48Z</updated>\\n    <published>2018-03-12T11:12:48Z</published>\\n    <title>Extreme Learning Machine for Graph Signal Processing</title>\\n    <summary>  In this article, we improve extreme learning machines for regression tasks\\nusing a graph signal processing based regularization. We assume that the target\\nsignal for prediction or regression is a graph signal. With this assumption, we\\nuse the regularization to enforce that the output of an extreme learning\\nmachine is smooth over a given graph. Simulation results with real data confirm\\nthat such regularization helps significantly when the available training data\\nis limited in size and corrupted by noise.\\n</summary>\\n    <author>\\n      <name>Arun Venkitaraman</name>\\n    </author>\\n    <author>\\n      <name>Saikat Chatterjee</name>\\n    </author>\\n    <author>\\n      <name>Peter H\\xc3\\xa4ndel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.04193v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.04193v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.01382v1</id>\\n    <updated>2018-04-03T01:17:16Z</updated>\\n    <published>2018-04-03T01:17:16Z</published>\\n    <title>Vanlearning: A Machine Learning SaaS Application for People Without\\n  Programming Backgrounds</title>\\n    <summary>  Although we have tons of machine learning tools to analyze data, most of them\\nrequire users have some programming backgrounds. Here we introduce a SaaS\\napplication which allows users analyze their data without any coding and even\\nwithout any knowledge of machine learning. Users can upload, train, predict and\\ndownload their data by simply clicks their mouses. Our system uses data\\npre-processor and validator to relieve the computational cost of our server.\\nThe simple architecture of Vanlearning helps developers can easily maintain and\\nextend it.\\n</summary>\\n    <author>\\n      <name>Chaochen Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.01382v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.01382v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.03441v1</id>\\n    <updated>2018-05-09T10:04:28Z</updated>\\n    <published>2018-05-09T10:04:28Z</published>\\n    <title>Machine Learning in Compiler Optimisation</title>\\n    <summary>  In the last decade, machine learning based compilation has moved from an an\\nobscure research niche to a mainstream activity. In this article, we describe\\nthe relationship between machine learning and compiler optimisation and\\nintroduce the main concepts of features, models, training and deployment. We\\nthen provide a comprehensive survey and provide a road map for the wide variety\\nof different research areas. We conclude with a discussion on open issues in\\nthe area and potential research directions. This paper provides both an\\naccessible introduction to the fast moving area of machine learning based\\ncompilation and a detailed bibliography of its main achievements.\\n</summary>\\n    <author>\\n      <name>Zheng Wang</name>\\n    </author>\\n    <author>\\n      <name>Michael O\\'Boyle</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to be published at Proceedings of the IEEE</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.03441v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.03441v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08239v2</id>\\n    <updated>2018-11-26T15:36:21Z</updated>\\n    <published>2018-05-21T18:11:26Z</published>\\n    <title>The Roles of Supervised Machine Learning in Systems Neuroscience</title>\\n    <summary>  Over the last several years, the use of machine learning (ML) in neuroscience\\nhas been rapidly increasing. Here, we review ML\\'s contributions, both realized\\nand potential, across several areas of systems neuroscience. We describe four\\nprimary roles of ML within neuroscience: 1) creating solutions to engineering\\nproblems, 2) identifying predictive variables, 3) setting benchmarks for simple\\nmodels of the brain, and 4) serving itself as a model for the brain. The\\nbreadth and ease of its applicability suggests that machine learning should be\\nin the toolbox of most systems neuroscientists.\\n</summary>\\n    <author>\\n      <name>Joshua I. Glaser</name>\\n    </author>\\n    <author>\\n      <name>Ari S. Benjamin</name>\\n    </author>\\n    <author>\\n      <name>Roozbeh Farhoodi</name>\\n    </author>\\n    <author>\\n      <name>Konrad P. Kording</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.08239v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08239v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.00388v2</id>\\n    <updated>2018-06-05T17:11:13Z</updated>\\n    <published>2018-06-01T15:12:20Z</published>\\n    <title>Opportunities in Machine Learning for Healthcare</title>\\n    <summary>  Healthcare is a natural arena for the application of machine learning,\\nespecially as modern electronic health records (EHRs) provide increasingly\\nlarge amounts of data to answer clinically meaningful questions. However,\\nclinical data and practice present unique challenges that complicate the use of\\ncommon methodologies. This article serves as a primer on addressing these\\nchallenges and highlights opportunities for members of the machine learning and\\ndata science communities to contribute to this growing domain.\\n</summary>\\n    <author>\\n      <name>Marzyeh Ghassemi</name>\\n    </author>\\n    <author>\\n      <name>Tristan Naumann</name>\\n    </author>\\n    <author>\\n      <name>Peter Schulam</name>\\n    </author>\\n    <author>\\n      <name>Andrew L. Beam</name>\\n    </author>\\n    <author>\\n      <name>Rajesh Ranganath</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Corrected preprint footer</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.00388v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.00388v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.03200v2</id>\\n    <updated>2018-11-14T10:46:48Z</updated>\\n    <published>2018-07-06T13:40:58Z</published>\\n    <title>The CodRep Machine Learning on Source Code Competition</title>\\n    <summary>  CodRep is a machine learning competition on source code data. It is carefully\\ndesigned so that anybody can enter the competition, whether professional\\nresearchers, students or independent scholars, without specific knowledge in\\nmachine learning or program analysis. In particular, it aims at being a common\\nplayground on which the machine learning and the software engineering research\\ncommunities can interact. The competition has started on April 14th 2018 and\\nhas ended on October 14th 2018. The CodRep data is hosted at\\nhttps://github.com/KTH/CodRep-competition/.\\n</summary>\\n    <author>\\n      <name>Zimin Chen</name>\\n    </author>\\n    <author>\\n      <name>Martin Monperrus</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.03200v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.03200v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06574v1</id>\\n    <updated>2018-07-17T17:31:59Z</updated>\\n    <published>2018-07-17T17:31:59Z</published>\\n    <title>Jensen: An Easily-Extensible C++ Toolkit for Production-Level Machine\\n  Learning and Convex Optimization</title>\\n    <summary>  This paper introduces Jensen, an easily extensible and scalable toolkit for\\nproduction-level machine learning and convex optimization. Jensen implements a\\nframework of convex (or loss) functions, convex optimization algorithms\\n(including Gradient Descent, L-BFGS, Stochastic Gradient Descent, Conjugate\\nGradient, etc.), and a family of machine learning classifiers and regressors\\n(Logistic Regression, SVMs, Least Square Regression, etc.). This framework\\nmakes it possible to deploy and train models with a few lines of code, and also\\nextend and build upon this by integrating new loss functions and optimization\\nalgorithms.\\n</summary>\\n    <author>\\n      <name>Rishabh Iyer</name>\\n    </author>\\n    <author>\\n      <name>John T. Halloran</name>\\n    </author>\\n    <author>\\n      <name>Kai Wei</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.06574v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06574v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.08655v1</id>\\n    <updated>2018-06-29T21:06:06Z</updated>\\n    <published>2018-06-29T21:06:06Z</published>\\n    <title>Training Humans and Machines</title>\\n    <summary>  For many years, researchers in psychology, education, statistics, and machine\\nlearning have been developing practical methods to improve learning speed,\\nretention, and generalizability, and this work has been successful. Many of\\nthese methods are rooted in common underlying principles that seem to drive\\nlearning and overlearning in both humans and machines. I present a review of a\\nsmall part of this work to point to potentially novel applications in both\\nmachine and human learning that may be worth exploring.\\n</summary>\\n    <author>\\n      <name>Aki Nikolaidis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, Computational Cognitive Neuroscience</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.08655v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.08655v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.04491v1</id>\\n    <updated>2018-10-10T12:56:06Z</updated>\\n    <published>2018-10-10T12:56:06Z</published>\\n    <title>Multi-class Classification Model Inspired by Quantum Detection Theory</title>\\n    <summary>  Machine Learning has become very famous currently which assist in identifying\\nthe patterns from the raw data. Technological advancement has led to\\nsubstantial improvement in Machine Learning which, thus helping to improve\\nprediction. Current Machine Learning models are based on Classical Theory,\\nwhich can be replaced by Quantum Theory to improve the effectiveness of the\\nmodel. In the previous work, we developed binary classifier inspired by Quantum\\nDetection Theory. In this extended abstract, our main goal is to develop\\nmulti-class classifier. We generally use the terminology multinomial\\nclassification or multi-class classification when we have a classification\\nproblem for classifying observations or instances into one of three or more\\nclasses.\\n</summary>\\n    <author>\\n      <name>Prayag Tiwari</name>\\n    </author>\\n    <author>\\n      <name>Massimo Melucci</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Future Directions in Information Access (FDIA) 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.04491v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.04491v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.07924v1</id>\\n    <updated>2018-10-18T07:04:39Z</updated>\\n    <published>2018-10-18T07:04:39Z</published>\\n    <title>Entropic Variable Boosting for Explainability and Interpretability in\\n  Machine Learning</title>\\n    <summary>  In this paper, we present a new explainability formalism to make clear the\\nimpact of each variable on the predictions given by black-box decision rules.\\nOur method consists in evaluating the decision rules on test samples generated\\nin such a way that each variable is stressed incrementally while preserving the\\noriginal distribution of the machine learning problem. We then propose a new\\ncomputation-ally efficient algorithm to stress the variables, which only\\nreweights the reference observations and predictions. This makes our\\nmethodology scalable to large datasets. Results obtained on standard machine\\nlearning datasets are presented and discussed.\\n</summary>\\n    <author>\\n      <name>Francois Bachoc</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IMT</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fabrice Gamboa</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IMT</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Jean-Michel Loubes</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IMT</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Laurent Risser</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IMT</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.07924v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.07924v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.04548v1</id>\\n    <updated>2018-11-12T04:07:46Z</updated>\\n    <published>2018-11-12T04:07:46Z</published>\\n    <title>Recent Research Advances on Interactive Machine Learning</title>\\n    <summary>  Interactive Machine Learning (IML) is an iterative learning process that\\ntightly couples a human with a machine learner, which is widely used by\\nresearchers and practitioners to effectively solve a wide variety of real-world\\napplication problems. Although recent years have witnessed the proliferation of\\nIML in the field of visual analytics, most recent surveys either focus on a\\nspecific area of IML or aim to summarize a visualization field that is too\\ngeneric for IML. In this paper, we systematically review the recent literature\\non IML and classify them into a task-oriented taxonomy built by us. We conclude\\nthe survey with a discussion of open challenges and research opportunities that\\nwe believe are inspiring for future work in IML.\\n</summary>\\n    <author>\\n      <name>Liu Jiang</name>\\n    </author>\\n    <author>\\n      <name>Shixia Liu</name>\\n    </author>\\n    <author>\\n      <name>Changjian Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.04548v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.04548v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.09323v1</id>\\n    <updated>2019-01-27T05:37:49Z</updated>\\n    <published>2019-01-27T05:37:49Z</published>\\n    <title>Prediction of Silicate Glasses\\' Stiffness by High-Throughput Molecular\\n  Dynamics Simulations and Machine Learning</title>\\n    <summary>  The development by machine learning of models predicting materials\\'\\nproperties usually requires the use of a large number of consistent data for\\ntraining. However, quality experimental datasets are not always available or\\nself-consistent. Here, as an alternative route, we combine machine learning\\nwith high-throughput molecular dynamics simulations to predict the Young\\'s\\nmodulus of silicate glasses. We demonstrate that this combined approach offers\\nexcellent predictions over the entire compositional domain. By comparing the\\nperformance of select machine learning algorithms, we discuss the nature of the\\nbalance between accuracy, simplicity, and interpretability in machine learning.\\n</summary>\\n    <author>\\n      <name>Kai Yang</name>\\n    </author>\\n    <author>\\n      <name>Xinyi Xu</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Yang</name>\\n    </author>\\n    <author>\\n      <name>Brian Cook</name>\\n    </author>\\n    <author>\\n      <name>Herbert Ramos</name>\\n    </author>\\n    <author>\\n      <name>Mathieu Bauchy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.09323v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.09323v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.08356v1</id>\\n    <updated>2019-03-20T06:32:10Z</updated>\\n    <published>2019-03-20T06:32:10Z</published>\\n    <title>Machine Learning for Data-Driven Movement Generation: a Review of the\\n  State of the Art</title>\\n    <summary>  The rise of non-linear and interactive media such as video games has\\nincreased the need for automatic movement animation generation. In this survey,\\nwe review and analyze different aspects of building automatic movement\\ngeneration systems using machine learning techniques and motion capture data.\\nWe cover topics such as high-level movement characterization, training data,\\nfeatures representation, machine learning models, and evaluation methods. We\\nconclude by presenting a discussion of the reviewed literature and outlining\\nthe research gaps and remaining challenges for future work.\\n</summary>\\n    <author>\\n      <name>Omid Alemi</name>\\n    </author>\\n    <author>\\n      <name>Philippe Pasquier</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.08356v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.08356v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1308.3750v1</id>\\n    <updated>2013-08-17T03:56:03Z</updated>\\n    <published>2013-08-17T03:56:03Z</published>\\n    <title>Comment on \"robustness and regularization of support vector machines\" by\\n  H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510,\\n  2009, arXiv:0803.3490)</title>\\n    <summary>  This paper comments on the published work dealing with robustness and\\nregularization of support vector machines (Journal of Machine Learning\\nResearch, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. They\\nproposed a theorem to show that it is possible to relate robustness in the\\nfeature space and robustness in the sample space directly. In this paper, we\\npropose a counter example that rejects their theorem.\\n</summary>\\n    <author>\\n      <name>Yahya Forghani</name>\\n    </author>\\n    <author>\\n      <name>Hadi Sadoghi Yazdi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages. This paper has been accepted with minor revision in journal\\n  of machine learning research (JMLR)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1308.3750v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1308.3750v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.3100v1</id>\\n    <updated>2014-06-12T02:08:31Z</updated>\\n    <published>2014-06-12T02:08:31Z</published>\\n    <title>Learning ELM network weights using linear discriminant analysis</title>\\n    <summary>  We present an alternative to the pseudo-inverse method for determining the\\nhidden to output weight values for Extreme Learning Machines performing\\nclassification tasks. The method is based on linear discriminant analysis and\\nprovides Bayes optimal single point estimates for the weight values.\\n</summary>\\n    <author>\\n      <name>Philip de Chazal</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Tapson</name>\\n    </author>\\n    <author>\\n      <name>Andr\\xc3\\xa9 van Schaik</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In submission to the ELM 2014 conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1406.3100v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.3100v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.09557v1</id>\\n    <updated>2017-06-29T02:52:25Z</updated>\\n    <published>2017-06-29T02:52:25Z</published>\\n    <title>Machine listening intelligence</title>\\n    <summary>  This manifesto paper will introduce machine listening intelligence, an\\nintegrated research framework for acoustic and musical signals modelling, based\\non signal processing, deep learning and computational musicology.\\n</summary>\\n    <author>\\n      <name>C. E. Cella</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the First International Conference on Deep Learning\\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the First International Workshop on Deep Learning\\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 50-55 (2017)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1706.09557v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.09557v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Txx\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"C.1.3; H.5.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.03074v2</id>\\n    <updated>2017-11-11T09:10:04Z</updated>\\n    <published>2017-08-10T04:33:09Z</published>\\n    <title>A Machine Learning Approach to Routing</title>\\n    <summary>  Can ideas and techniques from machine learning be leveraged to automatically\\ngenerate \"good\" routing configurations? We investigate the power of data-driven\\nrouting protocols. Our results suggest that applying ideas and techniques from\\ndeep reinforcement learning to this context yields high performance, motivating\\nfurther research along these lines.\\n</summary>\\n    <author>\\n      <name>Asaf Valadarsky</name>\\n    </author>\\n    <author>\\n      <name>Michael Schapira</name>\\n    </author>\\n    <author>\\n      <name>Dafna Shahaf</name>\\n    </author>\\n    <author>\\n      <name>Aviv Tamar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.03074v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.03074v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"C.2.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.00156v1</id>\\n    <updated>2018-03-01T01:35:27Z</updated>\\n    <published>2018-03-01T01:35:27Z</published>\\n    <title>Autoencoding topology</title>\\n    <summary>  The problem of learning a manifold structure on a dataset is framed in terms\\nof a generative model, to which we use ideas behind autoencoders (namely\\nadversarial/Wasserstein autoencoders) to fit deep neural networks. From a\\nmachine learning perspective, the resulting structure, an atlas of a manifold,\\nmay be viewed as a combination of dimensionality reduction and \"fuzzy\"\\nclustering.\\n</summary>\\n    <author>\\n      <name>Eric O. Korman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.00156v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.00156v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.09103v1</id>\\n    <updated>2018-03-24T13:08:56Z</updated>\\n    <published>2018-03-24T13:08:56Z</published>\\n    <title>Machine Learning and Applied Linguistics</title>\\n    <summary>  This entry introduces the topic of machine learning and provides an overview\\nof its relevance for applied linguistics and language learning. The discussion\\nwill focus on giving an introduction to the methods and applications of machine\\nlearning in applied linguistics, and will provide references for further study.\\n</summary>\\n    <author>\\n      <name>Sowmya Vajjala</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1002/9781405198431.wbeal1486</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1002/9781405198431.wbeal1486\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Pre-print version of the article that is accepted for publication in\\n  \"Encyclopedia of Applied Linguistics\"</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.09103v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.09103v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.02543v1</id>\\n    <updated>2018-04-07T10:24:04Z</updated>\\n    <published>2018-04-07T10:24:04Z</published>\\n    <title>Not quite unreasonable effectiveness of machine learning algorithms</title>\\n    <summary>  State-of-the-art machine learning algorithms demonstrate close to absolute\\nperformance in selected challenges. We provide arguments that the reason can be\\nin low variability of the samples and high effectiveness in learning typical\\npatterns. Due to this fact, standard performance metrics do not reveal model\\ncapacity and new metrics are required for the better understanding of\\nstate-of-the-art.\\n</summary>\\n    <author>\\n      <name>Egor Illarionov</name>\\n    </author>\\n    <author>\\n      <name>Roman Khudorozhkov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.02543v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.02543v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1001.2709v1</id>\\n    <updated>2010-01-15T15:10:39Z</updated>\\n    <published>2010-01-15T15:10:39Z</published>\\n    <title>Kernel machines with two layers and multiple kernel learning</title>\\n    <summary>  In this paper, the framework of kernel machines with two layers is\\nintroduced, generalizing classical kernel methods. The new learning methodology\\nprovide a formal connection between computational architectures with multiple\\nlayers and the theme of kernel learning in standard regularization methods.\\nFirst, a representer theorem for two-layer networks is presented, showing that\\nfinite linear combinations of kernels on each layer are optimal architectures\\nwhenever the corresponding functions solve suitable variational problems in\\nreproducing kernel Hilbert spaces (RKHS). The input-output map expressed by\\nthese architectures turns out to be equivalent to a suitable single-layer\\nkernel machines in which the kernel function is also learned from the data.\\nRecently, the so-called multiple kernel learning methods have attracted\\nconsiderable attention in the machine learning literature. In this paper,\\nmultiple kernel learning methods are shown to be specific cases of kernel\\nmachines with two layers in which the second layer is linear. Finally, a simple\\nand effective multiple kernel learning method called RLS2 (regularized least\\nsquares with two layers) is introduced, and his performances on several\\nlearning problems are extensively analyzed. An open source MATLAB toolbox to\\ntrain and validate RLS2 models with a Graphic User Interface is available.\\n</summary>\\n    <author>\\n      <name>Francesco Dinuzzo</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1001.2709v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1001.2709v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.08523v1</id>\\n    <updated>2017-12-20T23:28:03Z</updated>\\n    <published>2017-12-20T23:28:03Z</published>\\n    <title>Contemporary machine learning: a guide for practitioners in the physical\\n  sciences</title>\\n    <summary>  Machine learning is finding increasingly broad application in the physical\\nsciences. This most often involves building a model relationship between a\\ndependent, measurable output and an associated set of controllable, but\\ncomplicated, independent inputs. We present a tutorial on current techniques in\\nmachine learning -- a jumping-off point for interested researchers to advance\\ntheir work. We focus on deep neural networks with an emphasis on demystifying\\ndeep learning. We begin with background ideas in machine learning and some\\nexample applications from current research in plasma physics. We discuss\\nsupervised learning techniques for modeling complicated functions, beginning\\nwith familiar regression schemes, then advancing to more sophisticated deep\\nlearning methods. We also address unsupervised learning and techniques for\\nreducing the dimensionality of input spaces. Along the way, we describe methods\\nfor practitioners to help ensure that their models generalize from their\\ntraining data to as-yet-unseen test data. We describe classes of tasks --\\npredicting scalars, handling images, fitting time-series -- and prepare the\\nreader to choose an appropriate technique. We finally point out some\\nlimitations to modern machine learning and speculate on some ways that\\npractitioners from the physical sciences may be particularly suited to help.\\n</summary>\\n    <author>\\n      <name>Brian K. Spears</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.5020791</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.5020791\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">29 pages, 16 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.08523v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.08523v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.MP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.02046v3</id>\\n    <updated>2019-03-19T16:40:02Z</updated>\\n    <published>2019-01-07T20:10:55Z</published>\\n    <title>A New Perspective on Machine Learning: How to do Perfect Supervised\\n  Learning</title>\\n    <summary>  In this work, we introduce the concept of bandlimiting into the theory of\\nmachine learning because all physical processes are bandlimited by nature,\\nincluding real-world machine learning tasks. After the bandlimiting constraint\\nis taken into account, our theoretical analysis has shown that all practical\\nmachine learning tasks are asymptotically solvable in a perfect sense.\\nFurthermore, the key towards this solvability almost solely relies on two\\nfactors: i) a sufficiently large amount of training samples beyond a threshold\\ndetermined by a difficulty measurement of the underlying task; ii) a\\nsufficiently complex and bandlimited model. Moreover, for some special cases,\\nwe have derived new error bounds for perfect learning, which can quantify the\\ndifficulty of learning. These generalization bounds are not only asymptotically\\nconvergent but also irrelevant to model complexity. Our new results on\\ngeneralization have provided a new perspective to explain the recent successes\\nof large-scale supervised learning using complex models like neural networks.\\n</summary>\\n    <author>\\n      <name>Hui Jiang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.02046v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.02046v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06054v1</id>\\n    <updated>2018-07-16T18:41:16Z</updated>\\n    <published>2018-07-16T18:41:16Z</published>\\n    <title>On the Information Theoretic Distance Measures and Bidirectional\\n  Helmholtz Machines</title>\\n    <summary>  By establishing a connection between bi-directional Helmholtz machines and\\ninformation theory, we propose a generalized Helmholtz machine. Theoretical and\\nexperimental results show that given \\\\textit{shallow} architectures, the\\ngeneralized model outperforms the previous ones substantially.\\n</summary>\\n    <author>\\n      <name>Mahdi Azarafrooz</name>\\n    </author>\\n    <author>\\n      <name>Xuan Zhao</name>\\n    </author>\\n    <author>\\n      <name>Sepehr Akhavan-Masouleh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.06054v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06054v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.05767v4</id>\\n    <updated>2018-02-05T15:57:57Z</updated>\\n    <published>2015-02-20T04:20:47Z</published>\\n    <title>Automatic differentiation in machine learning: a survey</title>\\n    <summary>  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\\nmachine learning. Automatic differentiation (AD), also called algorithmic\\ndifferentiation or simply \"autodiff\", is a family of techniques similar to but\\nmore general than backpropagation for efficiently and accurately evaluating\\nderivatives of numeric functions expressed as computer programs. AD is a small\\nbut established field with applications in areas including computational fluid\\ndynamics, atmospheric sciences, and engineering design optimization. Until very\\nrecently, the fields of machine learning and AD have largely been unaware of\\neach other and, in some cases, have independently discovered each other\\'s\\nresults. Despite its relevance, general-purpose AD has been missing from the\\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\\nunder the names \"dynamic computational graphs\" and \"differentiable\\nprogramming\". We survey the intersection of AD and machine learning, cover\\napplications where AD has direct relevance, and address the main implementation\\ntechniques. By precisely defining the main differentiation techniques and their\\ninterrelationships, we aim to bring clarity to the usage of the terms\\n\"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as\\nthese are encountered more and more in machine learning settings.\\n</summary>\\n    <author>\\n      <name>Atilim Gunes Baydin</name>\\n    </author>\\n    <author>\\n      <name>Barak A. Pearlmutter</name>\\n    </author>\\n    <author>\\n      <name>Alexey Andreyevich Radul</name>\\n    </author>\\n    <author>\\n      <name>Jeffrey Mark Siskind</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">43 pages, 5 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich\\n  Radul, Jeffrey Mark Siskind. Automatic differentiation in machine learning: a\\n  survey. The Journal of Machine Learning Research, 18(153):1--43, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1502.05767v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.05767v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68W30, 65D25, 68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1.4; I.2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.02855v1</id>\\n    <updated>2018-12-06T23:46:15Z</updated>\\n    <published>2018-12-06T23:46:15Z</published>\\n    <title>Progressive Sampling-Based Bayesian Optimization for Efficient and\\n  Automatic Machine Learning Model Selection</title>\\n    <summary>  Purpose: Machine learning is broadly used for clinical data analysis. Before\\ntraining a model, a machine learning algorithm must be selected. Also, the\\nvalues of one or more model parameters termed hyper-parameters must be set.\\nSelecting algorithms and hyper-parameter values requires advanced machine\\nlearning knowledge and many labor-intensive manual iterations. To lower the bar\\nto machine learning, miscellaneous automatic selection methods for algorithms\\nand/or hyper-parameter values have been proposed. Existing automatic selection\\nmethods are inefficient on large data sets. This poses a challenge for using\\nmachine learning in the clinical big data era. Methods: To address the\\nchallenge, this paper presents progressive sampling-based Bayesian\\noptimization, an efficient and automatic selection method for both algorithms\\nand hyper-parameter values. Results: We report an implementation of the method.\\nWe show that compared to a state of the art automatic selection method, our\\nmethod can significantly reduce search time, classification error rate, and\\nstandard deviation of error rate due to randomization. Conclusions: This is\\nmajor progress towards enabling fast turnaround in identifying high-quality\\nsolutions required by many machine learning-based clinical data analysis tasks.\\n</summary>\\n    <author>\\n      <name>Xueqiang Zeng</name>\\n    </author>\\n    <author>\\n      <name>Gang Luo</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s13755-017-0023-z</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s13755-017-0023-z\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Xueqiang Zeng, Gang Luo. Progressive Sampling-Based Bayesian\\n  Optimization for Efficient and Automatic Machine Learning Model Selection.\\n  Health Information Science and Systems, Vol. 5, No. 1, Article 2, Sep. 2017</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1812.02855v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.02855v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0212037v1</id>\\n    <updated>2002-12-12T18:14:38Z</updated>\\n    <published>2002-12-12T18:14:38Z</published>\\n    <title>The Management of Context-Sensitive Features: A Review of Strategies</title>\\n    <summary>  In this paper, we review five heuristic strategies for handling\\ncontext-sensitive features in supervised machine learning from examples. We\\ndiscuss two methods for recovering lost (implicit) contextual information. We\\nmention some evidence that hybrid strategies can have a synergetic effect. We\\nthen show how the work of several machine learning researchers fits into this\\nframework. While we do not claim that these strategies exhaust the\\npossibilities, it appears that the framework includes all of the techniques\\nthat can be found in the published literature on contextsensitive learning.\\n</summary>\\n    <author>\\n      <name>Peter D. Turney</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">National Research Council of Canada</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13th International Conference on Machine Learning, Workshop on\\n  Learning in Context-Sensitive Domains, Bari, Italy, (1996), 60-66</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0212037v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0212037v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6; I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0803.2976v2</id>\\n    <updated>2008-03-31T05:41:41Z</updated>\\n    <published>2008-03-20T16:11:41Z</published>\\n    <title>Quantum Learning Machine</title>\\n    <summary>  We propose a novel notion of a quantum learning machine for automatically\\ncontrolling quantum coherence and for developing quantum algorithms. A quantum\\nlearning machine can be trained to learn a certain task with no a priori\\nknowledge on its algorithm. As an example, it is demonstrated that the quantum\\nlearning machine learns Deutsch\\'s task and finds itself a quantum algorithm,\\nthat is different from but equivalent to the original one.\\n</summary>\\n    <author>\\n      <name>Jeongho Bang</name>\\n    </author>\\n    <author>\\n      <name>James Lim</name>\\n    </author>\\n    <author>\\n      <name>M. S. Kim</name>\\n    </author>\\n    <author>\\n      <name>Jinhyoung Lee</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 8 figures; Revisions made to improve the introduction and\\n  motivation</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0803.2976v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0803.2976v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.3101v1</id>\\n    <updated>2013-10-11T12:14:00Z</updated>\\n    <published>2013-10-11T12:14:00Z</published>\\n    <title>Deep Multiple Kernel Learning</title>\\n    <summary>  Deep learning methods have predominantly been applied to large artificial\\nneural networks. Despite their state-of-the-art performance, these large\\nnetworks typically do not generalize well to datasets with limited sample\\nsizes. In this paper, we take a different approach by learning multiple layers\\nof kernels. We combine kernels at each layer and then optimize over an estimate\\nof the support vector machine leave-one-out error rather than the dual\\nobjective function. Our experiments on a variety of datasets show that each\\nlayer successively increases performance with only a few base kernels.\\n</summary>\\n    <author>\\n      <name>Eric Strobl</name>\\n    </author>\\n    <author>\\n      <name>Shyam Visweswaran</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ICMLA.2013.84</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ICMLA.2013.84\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 1 figure, 1 table, conference paper</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE 12th International Conference on Machine Learning and\\n  Applications (ICMLA 2013)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1310.3101v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.3101v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.2622v1</id>\\n    <updated>2014-06-10T16:40:56Z</updated>\\n    <published>2014-06-10T16:40:56Z</published>\\n    <title>Equivalence of Learning Algorithms</title>\\n    <summary>  The purpose of this paper is to introduce a concept of equivalence between\\nmachine learning algorithms. We define two notions of algorithmic equivalence,\\nnamely, weak and strong equivalence. These notions are of paramount importance\\nfor identifying when learning prop erties from one learning algorithm can be\\ntransferred to another. Using regularized kernel machines as a case study, we\\nillustrate the importance of the introduced equivalence concept by analyzing\\nthe relation between kernel ridge regression (KRR) and m-power regularized\\nleast squares regression (M-RLSR) algorithms.\\n</summary>\\n    <author>\\n      <name>Julien Audiffren</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CMLA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Hachem Kadri</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LIF</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1310.2451</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1406.2622v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.2622v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.07647v2</id>\\n    <updated>2016-11-10T19:35:11Z</updated>\\n    <published>2016-10-24T20:48:04Z</published>\\n    <title>Learning to Reason With Adaptive Computation</title>\\n    <summary>  Multi-hop inference is necessary for machine learning systems to successfully\\nsolve tasks such as Recognising Textual Entailment and Machine Reading. In this\\nwork, we demonstrate the effectiveness of adaptive computation for learning the\\nnumber of inference steps required for examples of different complexity and\\nthat learning the correct number of inference steps is difficult. We introduce\\nthe first model involving Adaptive Computation Time which provides a small\\nperformance benefit on top of a similar model without an adaptive component as\\nwell as enabling considerable insight into the reasoning process of the model.\\n</summary>\\n    <author>\\n      <name>Mark Neumann</name>\\n    </author>\\n    <author>\\n      <name>Pontus Stenetorp</name>\\n    </author>\\n    <author>\\n      <name>Sebastian Riedel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\\n  Complex Systems</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.07647v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.07647v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.07644v1</id>\\n    <updated>2017-08-25T08:14:22Z</updated>\\n    <published>2017-08-25T08:14:22Z</published>\\n    <title>Joint Structured Learning and Predictions under Logical Constraints in\\n  Conditional Random Fields</title>\\n    <summary>  This paper is concerned with structured machine learning, in a supervised\\nmachine learning context. It discusses how to make joint structured learning on\\ninterdependent objects of different nature, as well as how to enforce logical\\ncon-straints when predicting labels. We explain how this need arose in a\\nDocument Understanding task. We then discuss a general extension to Conditional\\nRandom Field (CRF) for this purpose and present the contributed open source\\nimplementation on top of the open source PyStruct library. We evaluate its\\nperformance on a publicly available dataset.\\n</summary>\\n    <author>\\n      <name>Jean-Luc Meunier</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CAp 2017 (Conf\\\\\\'erence sur l\\'Apprentissage automatique)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1708.07644v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.07644v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.08621v3</id>\\n    <updated>2017-12-14T13:47:21Z</updated>\\n    <published>2017-11-23T08:54:05Z</published>\\n    <title>Counterfactual Learning for Machine Translation: Degeneracies and\\n  Solutions</title>\\n    <summary>  Counterfactual learning is a natural scenario to improve web-based machine\\ntranslation services by offline learning from feedback logged during user\\ninteractions. In order to avoid the risk of showing inferior translations to\\nusers, in such scenarios mostly exploration-free deterministic logging policies\\nare in place. We analyze possible degeneracies of inverse and reweighted\\npropensity scoring estimators, in stochastic and deterministic settings, and\\nrelate them to recently proposed techniques for counterfactual learning under\\ndeterministic logging.\\n</summary>\\n    <author>\\n      <name>Carolin Lawrence</name>\\n    </author>\\n    <author>\\n      <name>Pratik Gajane</name>\\n    </author>\\n    <author>\\n      <name>Stefan Riezler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Workshop \"From \\'What If?\\' To \\'What Next?\\'\" at the 31st Conference on\\n  Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.08621v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.08621v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.02361v1</id>\\n    <updated>2018-11-06T14:16:29Z</updated>\\n    <published>2018-11-06T14:16:29Z</published>\\n    <title>Kalman Filter Modifier for Neural Networks in Non-stationary\\n  Environments</title>\\n    <summary>  Learning in a non-stationary environment is an inevitable problem when\\napplying machine learning algorithm to real world environment. Learning new\\ntasks without forgetting the previous knowledge is a challenge issue in machine\\nlearning. We propose a Kalman Filter based modifier to maintain the performance\\nof Neural Network models under non-stationary environments. The result shows\\nthat our proposed model can preserve the key information and adapts better to\\nthe changes. The accuracy of proposed model decreases by 0.4% in our\\nexperiments, while the accuracy of conventional model decreases by 90% in the\\ndrifts environment.\\n</summary>\\n    <author>\\n      <name>Honglin Li</name>\\n    </author>\\n    <author>\\n      <name>Frieder Ganz</name>\\n    </author>\\n    <author>\\n      <name>Shirin Enshaeifar</name>\\n    </author>\\n    <author>\\n      <name>Payam Barnaghi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.02361v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.02361v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.3919v1</id>\\n    <updated>2014-12-12T08:38:35Z</updated>\\n    <published>2014-12-12T08:38:35Z</published>\\n    <title>Machine Learning for Neuroimaging with Scikit-Learn</title>\\n    <summary>  Statistical machine learning methods are increasingly used for neuroimaging\\ndata analysis. Their main virtue is their ability to model high-dimensional\\ndatasets, e.g. multivariate analysis of activation images or resting-state time\\nseries. Supervised learning is typically used in decoding or encoding settings\\nto relate brain images to behavioral or clinical observations, while\\nunsupervised learning can uncover hidden structures in sets of images (e.g.\\nresting state functional MRI) or find sub-populations in large cohorts. By\\nconsidering different functional neuroimaging applications, we illustrate how\\nscikit-learn, a Python machine learning library, can be used to perform some\\nkey analysis steps. Scikit-learn contains a very large set of statistical\\nlearning algorithms, both supervised and unsupervised, and its application to\\nneuroimaging data provides a versatile tool to study the brain.\\n</summary>\\n    <author>\\n      <name>Alexandre Abraham</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NEUROSPIN, INRIA Saclay - Ile de France</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fabian Pedregosa</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Saclay - Ile de France</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Michael Eickenberg</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LNAO, INRIA Saclay - Ile de France</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Philippe Gervais</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NEUROSPIN, INRIA Saclay - Ile de France, LNAO</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Andreas Muller</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NEUROSPIN, LTCI</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Jean Kossaifi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NEUROSPIN, LTCI</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Alexandre Gramfort</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NEUROSPIN, LTCI</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Bertrand Thirion</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NEUROSPIN, INRIA Saclay - Ile de France</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>G\\xc3\\xa4el Varoquaux</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NEUROSPIN, INRIA Saclay - Ile de France, LNAO</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Frontiers in neuroscience, Frontiers Research Foundation, 2013, pp.15</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.3919v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.3919v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06279v2</id>\\n    <updated>2016-03-25T11:03:42Z</updated>\\n    <published>2015-05-23T06:37:17Z</published>\\n    <title>The Benefit of Multitask Representation Learning</title>\\n    <summary>  We discuss a general method to learn data representations from multiple\\ntasks. We provide a justification for this method in both settings of multitask\\nlearning and learning-to-learn. The method is illustrated in detail in the\\nspecial case of linear feature learning. Conditions on the theoretical\\nadvantage offered by multitask representation learning over independent task\\nlearning are established. In particular, focusing on the important example of\\nhalf-space learning, we derive the regime in which multitask representation\\nlearning is beneficial over independent task learning, as a function of the\\nsample size, the number of tasks and the intrinsic data dimensionality. Other\\npotential applications of our results include multitask feature learning in\\nreproducing kernel Hilbert spaces and multilayer, deep networks.\\n</summary>\\n    <author>\\n      <name>Andreas Maurer</name>\\n    </author>\\n    <author>\\n      <name>Massimiliano Pontil</name>\\n    </author>\\n    <author>\\n      <name>Bernardino Romera-Paredes</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in Journal of Machine Learning Research (JMLR). 31 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.06279v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06279v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.04601v1</id>\\n    <updated>2017-06-14T17:35:21Z</updated>\\n    <published>2017-06-14T17:35:21Z</published>\\n    <title>Provable benefits of representation learning</title>\\n    <summary>  There is general consensus that learning representations is useful for a\\nvariety of reasons, e.g. efficient use of labeled data (semi-supervised\\nlearning), transfer learning and understanding hidden structure of data.\\nPopular techniques for representation learning include clustering, manifold\\nlearning, kernel-learning, autoencoders, Boltzmann machines, etc.\\n  To study the relative merits of these techniques, it\\'s essential to formalize\\nthe definition and goals of representation learning, so that they are all\\nbecome instances of the same definition. This paper introduces such a formal\\nframework that also formalizes the utility of learning the representation. It\\nis related to previous Bayesian notions, but with some new twists. We show the\\nusefulness of our framework by exhibiting simple and natural settings -- linear\\nmixture models and loglinear models, where the power of representation learning\\ncan be formally shown. In these examples, representation learning can be\\nperformed provably and efficiently under plausible assumptions (despite being\\nNP-hard), and furthermore: (i) it greatly reduces the need for labeled data\\n(semi-supervised learning) and (ii) it allows solving classification tasks when\\nsimpler approaches like nearest neighbors require too much data (iii) it is\\nmore powerful than manifold learning methods.\\n</summary>\\n    <author>\\n      <name>Sanjeev Arora</name>\\n    </author>\\n    <author>\\n      <name>Andrej Risteski</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.04601v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.04601v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08355v1</id>\\n    <updated>2018-05-22T02:12:33Z</updated>\\n    <published>2018-05-22T02:12:33Z</published>\\n    <title>Opening the black box of deep learning</title>\\n    <summary>  The great success of deep learning shows that its technology contains\\nprofound truth, and understanding its internal mechanism not only has important\\nimplications for the development of its technology and effective application in\\nvarious fields, but also provides meaningful insights into the understanding of\\nhuman brain mechanism. At present, most of the theoretical research on deep\\nlearning is based on mathematics. This dissertation proposes that the neural\\nnetwork of deep learning is a physical system, examines deep learning from\\nthree different perspectives: microscopic, macroscopic, and physical world\\nviews, answers multiple theoretical puzzles in deep learning by using physics\\nprinciples. For example, from the perspective of quantum mechanics and\\nstatistical physics, this dissertation presents the calculation methods for\\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\\nMachine, as well as the selection of cost functions, explains why deep learning\\nmust be deep, what characteristics are learned in deep learning, why\\nConvolutional Neural Networks do not have to be trained layer by layer, and the\\nlimitations of deep learning, etc., and proposes the theoretical direction and\\nbasis for the further development of deep learning now and in the future. The\\nbrilliance of physics flashes in deep learning, we try to establish the deep\\nlearning technology based on the scientific theory of physics.\\n</summary>\\n    <author>\\n      <name>Dian Lei</name>\\n    </author>\\n    <author>\\n      <name>Xiaoxiao Chen</name>\\n    </author>\\n    <author>\\n      <name>Jianfei Zhao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.08355v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08355v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.04572v3</id>\\n    <updated>2018-08-22T14:48:43Z</updated>\\n    <published>2018-08-14T08:01:07Z</published>\\n    <title>Small Sample Learning in Big Data Era</title>\\n    <summary>  As a promising area in artificial intelligence, a new learning paradigm,\\ncalled Small Sample Learning (SSL), has been attracting prominent research\\nattention in the recent years. In this paper, we aim to present a survey to\\ncomprehensively introduce the current techniques proposed on this topic.\\nSpecifically, current SSL techniques can be mainly divided into two categories.\\nThe first category of SSL approaches can be called \"concept learning\", which\\nemphasizes learning new concepts from only few related observations. The\\npurpose is mainly to simulate human learning behaviors like recognition,\\ngeneration, imagination, synthesis and analysis. The second category is called\\n\"experience learning\", which usually co-exists with the large sample learning\\nmanner of conventional machine learning. This category mainly focuses on\\nlearning with insufficient samples, and can also be called small data learning\\nin some literatures. More extensive surveys on both categories of SSL\\ntechniques are introduced and some neuroscience evidences are provided to\\nclarify the rationality of the entire SSL regime, and the relationship with\\nhuman learning process. Some discussions on the main challenges and possible\\nfuture research directions along this line are also presented.\\n</summary>\\n    <author>\\n      <name>Jun Shu</name>\\n    </author>\\n    <author>\\n      <name>Zongben Xu</name>\\n    </author>\\n    <author>\\n      <name>Deyu Meng</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">76 pages, 15 figures, survey of small sample learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.04572v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.04572v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.01110v2</id>\\n    <updated>2018-03-23T21:18:28Z</updated>\\n    <published>2015-06-03T03:06:42Z</published>\\n    <title>Multi-View Factorization Machines</title>\\n    <summary>  For a learning task, data can usually be collected from different sources or\\nbe represented from multiple views. For example, laboratory results from\\ndifferent medical examinations are available for disease diagnosis, and each of\\nthem can only reflect the health state of a person from a particular\\naspect/view. Therefore, different views provide complementary information for\\nlearning tasks. An effective integration of the multi-view information is\\nexpected to facilitate the learning performance. In this paper, we propose a\\ngeneral predictor, named multi-view machines (MVMs), that can effectively\\ninclude all the possible interactions between features from multiple views. A\\njoint factorization is embedded for the full-order interaction parameters which\\nallows parameter estimation under sparsity. Moreover, MVMs can work in\\nconjunction with different loss functions for a variety of machine learning\\ntasks. A stochastic gradient descent method is presented to learn the MVM\\nmodel. We further illustrate the advantages of MVMs through comparison with\\nother methods for multi-view classification, including support vector machines\\n(SVMs), support tensor machines (STMs) and factorization machines (FMs).\\n</summary>\\n    <author>\\n      <name>Bokai Cao</name>\\n    </author>\\n    <author>\\n      <name>Hucheng Zhou</name>\\n    </author>\\n    <author>\\n      <name>Guoqiang Li</name>\\n    </author>\\n    <author>\\n      <name>Philip S. Yu</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/2835776.2835777</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/2835776.2835777\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">WSDM 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1506.01110v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.01110v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"H.2.8\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.09050v1</id>\\n    <updated>2017-07-27T21:16:41Z</updated>\\n    <published>2017-07-27T21:16:41Z</published>\\n    <title>A Shared Task on Bandit Learning for Machine Translation</title>\\n    <summary>  We introduce and describe the results of a novel shared task on bandit\\nlearning for machine translation. The task was organized jointly by Amazon and\\nHeidelberg University for the first time at the Second Conference on Machine\\nTranslation (WMT 2017). The goal of the task is to encourage research on\\nlearning machine translation from weak user feedback instead of human\\nreferences or post-edits. On each of a sequence of rounds, a machine\\ntranslation system is required to propose a translation for an input, and\\nreceives a real-valued estimate of the quality of the proposed translation for\\nlearning. This paper describes the shared task\\'s learning and evaluation setup,\\nusing services hosted on Amazon Web Services (AWS), the data and evaluation\\nmetrics, and the results of various machine translation architectures and\\nlearning protocols.\\n</summary>\\n    <author>\\n      <name>Artem Sokolov</name>\\n    </author>\\n    <author>\\n      <name>Julia Kreutzer</name>\\n    </author>\\n    <author>\\n      <name>Kellen Sunderland</name>\\n    </author>\\n    <author>\\n      <name>Pavel Danchenko</name>\\n    </author>\\n    <author>\\n      <name>Witold Szymaniak</name>\\n    </author>\\n    <author>\\n      <name>Hagen F\\xc3\\xbcrstenau</name>\\n    </author>\\n    <author>\\n      <name>Stefan Riezler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Conference on Machine Translation (WMT) 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.09050v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.09050v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.10028v1</id>\\n    <updated>2019-01-10T14:53:27Z</updated>\\n    <published>2019-01-10T14:53:27Z</published>\\n    <title>Performance Analysis of Machine Learning Techniques to Predict Diabetes\\n  Mellitus</title>\\n    <summary>  Diabetes mellitus is a common disease of human body caused by a group of\\nmetabolic disorders where the sugar levels over a prolonged period is very\\nhigh. It affects different organs of the human body which thus harm a large\\nnumber of the body\\'s system, in particular the blood veins and nerves. Early\\nprediction in such disease can be controlled and save human life. To achieve\\nthe goal, this research work mainly explores various risk factors related to\\nthis disease using machine learning techniques. Machine learning techniques\\nprovide efficient result to extract knowledge by constructing predicting models\\nfrom diagnostic medical datasets collected from the diabetic patients.\\nExtracting knowledge from such data can be useful to predict diabetic patients.\\nIn this work, we employ four popular machine learning algorithms, namely\\nSupport Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbor (KNN) and\\nC4.5 Decision Tree, on adult population data to predict diabetic mellitus. Our\\nexperimental results show that C4.5 decision tree achieved higher accuracy\\ncompared to other machine learning techniques.\\n</summary>\\n    <author>\\n      <name>Md. Faisal Faruque</name>\\n    </author>\\n    <author>\\n      <name> Asaduzzaman</name>\\n    </author>\\n    <author>\\n      <name>Iqbal H. Sarker</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE International Conference on Electrical, Computer and\\n  Communication Engineering (ECCE 2019), Cox\\'s Bazar, Bangladesh</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.10028v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.10028v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.09471v1</id>\\n    <updated>2018-06-25T14:04:44Z</updated>\\n    <published>2018-06-25T14:04:44Z</published>\\n    <title>Does data interpolation contradict statistical optimality?</title>\\n    <summary>  We show that learning methods interpolating the training data can achieve\\noptimal rates for the problems of nonparametric regression and prediction with\\nsquare loss.\\n</summary>\\n    <author>\\n      <name>Mikhail Belkin</name>\\n    </author>\\n    <author>\\n      <name>Alexander Rakhlin</name>\\n    </author>\\n    <author>\\n      <name>Alexandre B. Tsybakov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.09471v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.09471v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00418v1</id>\\n    <updated>2018-08-01T17:00:43Z</updated>\\n    <published>2018-08-01T17:00:43Z</published>\\n    <title>Stock Chart Pattern recognition with Deep Learning</title>\\n    <summary>  This study evaluates the performances of CNN and LSTM for recognizing common\\ncharts patterns in a stock historical data. It presents two common patterns,\\nthe method used to build the training set, the neural networks architectures\\nand the accuracies obtained.\\n</summary>\\n    <author>\\n      <name>Marc Velay</name>\\n    </author>\\n    <author>\\n      <name>Fabrice Daniel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.00418v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00418v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.07049v1</id>\\n    <updated>2018-08-20T21:49:13Z</updated>\\n    <published>2018-08-20T21:49:13Z</published>\\n    <title>Catastrophic Importance of Catastrophic Forgetting</title>\\n    <summary>  This paper describes some of the possibilities of artificial neural networks\\nthat open up after solving the problem of catastrophic forgetting. A simple\\nmodel and reinforcement learning applications of existing methods are also\\nproposed.\\n</summary>\\n    <author>\\n      <name>Albert Ierusalem</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.07049v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.07049v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.06186v3</id>\\n    <updated>2018-11-29T16:57:47Z</updated>\\n    <published>2018-09-17T13:27:43Z</published>\\n    <title>Study and Observation of the Variation of Accuracies of KNN, SVM, LMNN,\\n  ENN Algorithms on Eleven Different Datasets from UCI Machine Learning\\n  Repository</title>\\n    <summary>  Machine learning qualifies computers to assimilate with data, without being\\nsolely programmed [1, 2]. Machine learning can be classified as supervised and\\nunsupervised learning. In supervised learning, computers learn an objective\\nthat portrays an input to an output hinged on training input-output pairs [3].\\nMost efficient and widely used supervised learning algorithms are K-Nearest\\nNeighbors (KNN), Support Vector Machine (SVM), Large Margin Nearest Neighbor\\n(LMNN), and Extended Nearest Neighbor (ENN). The main contribution of this\\npaper is to implement these elegant learning algorithms on eleven different\\ndatasets from the UCI machine learning repository to observe the variation of\\naccuracies for each of the algorithms on all datasets. Analyzing the accuracy\\nof the algorithms will give us a brief idea about the relationship of the\\nmachine learning algorithms and the data dimensionality. All the algorithms are\\ndeveloped in Matlab. Upon such accuracy observation, the comparison can be\\nbuilt among KNN, SVM, LMNN, and ENN regarding their performances on each\\ndataset.\\n</summary>\\n    <author>\\n      <name>Mohammad Mahmudur Rahman Khan</name>\\n    </author>\\n    <author>\\n      <name>Rezoana Bente Arif</name>\\n    </author>\\n    <author>\\n      <name>Md. Abu Bakr Siddique</name>\\n    </author>\\n    <author>\\n      <name>Mahjabin Rahman Oishe</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CEEICT.2018.8628041</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CEEICT.2018.8628041\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To be published in the 4th IEEE International Conference on\\n  Electrical Engineering and Information &amp; Communication Technology (iCEEiCT\\n  2018)</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2018 4th International Conference on Electrical Engineering and\\n  Information &amp; Communication Technology (iCEEiCT)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1809.06186v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.06186v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cond-mat/0209234v1</id>\\n    <updated>2002-09-10T09:19:56Z</updated>\\n    <published>2002-09-10T09:19:56Z</published>\\n    <title>Mutual learning in a tree parity machine and its application to\\n  cryptography</title>\\n    <summary>  Mutual learning of a pair of tree parity machines with continuous and\\ndiscrete weight vectors is studied analytically. The analysis is based on a\\nmapping procedure that maps the mutual learning in tree parity machines onto\\nmutual learning in noisy perceptrons. The stationary solution of the mutual\\nlearning in the case of continuous tree parity machines depends on the learning\\nrate where a phase transition from partial to full synchronization is observed.\\nIn the discrete case the learning process is based on a finite increment and a\\nfull synchronized state is achieved in a finite number of steps. The\\nsynchronization of discrete parity machines is introduced in order to construct\\nan ephemeral key-exchange protocol. The dynamic learning of a third tree parity\\nmachine (an attacker) that tries to imitate one of the two machines while the\\ntwo still update their weight vectors is also analyzed. In particular, the\\nsynchronization times of the naive attacker and the flipping attacker recently\\nintroduced in [1] are analyzed. All analytical results are found to be in good\\nagreement with simulation results.\\n</summary>\\n    <author>\\n      <name>Michal Rosen-Zvi</name>\\n    </author>\\n    <author>\\n      <name>Einat Klein</name>\\n    </author>\\n    <author>\\n      <name>Ido Kanter</name>\\n    </author>\\n    <author>\\n      <name>Wolfgang Kinzel</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevE.66.066135</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevE.66.066135\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/cond-mat/0209234v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cond-mat/0209234v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.07563v1</id>\\n    <updated>2015-06-24T21:07:47Z</updated>\\n    <published>2015-06-24T21:07:47Z</published>\\n    <title>Benchmarking Machine Learning Technologies for Software Defect Detection</title>\\n    <summary>  Machine Learning approaches are good in solving problems that have less\\ninformation. In most cases, the software domain problems characterize as a\\nprocess of learning that depend on the various circumstances and changes\\naccordingly. A predictive model is constructed by using machine learning\\napproaches and classified them into defective and non-defective modules.\\nMachine learning techniques help developers to retrieve useful information\\nafter the classification and enable them to analyse data from different\\nperspectives. Machine learning techniques are proven to be useful in terms of\\nsoftware bug prediction. This study used public available data sets of software\\nmodules and provides comparative performance analysis of different machine\\nlearning techniques for software bug prediction. Results showed most of the\\nmachine learning methods performed well on software bug datasets.\\n</summary>\\n    <author>\\n      <name>Saiqa Aleem</name>\\n    </author>\\n    <author>\\n      <name>Luiz Fernando Capretz</name>\\n    </author>\\n    <author>\\n      <name>Faheem Ahmed</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Software Engineering &amp; Applications\\n  (IJSEA), Volume 6, No.3, pp. 11-23, May 2015</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1506.07563v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.07563v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.00084v1</id>\\n    <updated>2017-02-28T23:16:19Z</updated>\\n    <published>2017-02-28T23:16:19Z</published>\\n    <title>Multi-Sensor Data Pattern Recognition for Multi-Target Localization: A\\n  Machine Learning Approach</title>\\n    <summary>  Data-target pairing is an important step towards multi-target localization\\nfor the intelligent operation of unmanned systems. Target localization plays a\\ncrucial role in numerous applications, such as search, and rescue missions,\\ntraffic management and surveillance. The objective of this paper is to present\\nan innovative target location learning approach, where numerous machine\\nlearning approaches, including K-means clustering and supported vector machines\\n(SVM), are used to learn the data pattern across a list of spatially\\ndistributed sensors. To enable the accurate data association from different\\nsensors for accurate target localization, appropriate data pre-processing is\\nessential, which is then followed by the application of different machine\\nlearning algorithms to appropriately group data from different sensors for the\\naccurate localization of multiple targets. Through simulation examples, the\\nperformance of these machine learning algorithms is quantified and compared.\\n</summary>\\n    <author>\\n      <name>Kasthurirengan Suresh</name>\\n    </author>\\n    <author>\\n      <name>Samuel Silva</name>\\n    </author>\\n    <author>\\n      <name>Johnathan Votion</name>\\n    </author>\\n    <author>\\n      <name>Yongcan Cao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">submitted for conference publication</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.00084v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.00084v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.05355v2</id>\\n    <updated>2018-05-01T19:52:31Z</updated>\\n    <published>2017-05-15T17:47:26Z</published>\\n    <title>Probabilistic Matrix Factorization for Automated Machine Learning</title>\\n    <summary>  In order to achieve state-of-the-art performance, modern machine learning\\ntechniques require careful data pre-processing and hyperparameter tuning.\\nMoreover, given the ever increasing number of machine learning models being\\ndeveloped, model selection is becoming increasingly important. Automating the\\nselection and tuning of machine learning pipelines consisting of data\\npre-processing methods and machine learning models, has long been one of the\\ngoals of the machine learning community. In this paper, we tackle this\\nmeta-learning task by combining ideas from collaborative filtering and Bayesian\\noptimization. Using probabilistic matrix factorization techniques and\\nacquisition functions from Bayesian optimization, we exploit experiments\\nperformed in hundreds of different datasets to guide the exploration of the\\nspace of possible pipelines. In our experiments, we show that our approach\\nquickly identifies high-performing pipelines across a wide range of datasets,\\nsignificantly outperforming the current state-of-the-art.\\n</summary>\\n    <author>\\n      <name>Nicolo Fusi</name>\\n    </author>\\n    <author>\\n      <name>Rishit Sheth</name>\\n    </author>\\n    <author>\\n      <name>Huseyn Melih Elibol</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.05355v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.05355v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.00754v1</id>\\n    <updated>2017-08-02T14:14:49Z</updated>\\n    <published>2017-08-02T14:14:49Z</published>\\n    <title>Fairness-aware machine learning: a perspective</title>\\n    <summary>  Algorithms learned from data are increasingly used for deciding many aspects\\nin our life: from movies we see, to prices we pay, or medicine we get. Yet\\nthere is growing evidence that decision making by inappropriately trained\\nalgorithms may unintentionally discriminate people. For example, in automated\\nmatching of candidate CVs with job descriptions, algorithms may capture and\\npropagate ethnicity related biases. Several repairs for selected algorithms\\nhave already been proposed, but the underlying mechanisms how such\\ndiscrimination happens from the computational perspective are not yet\\nscientifically understood. We need to develop theoretical understanding how\\nalgorithms may become discriminatory, and establish fundamental machine\\nlearning principles for prevention. We need to analyze machine learning process\\nas a whole to systematically explain the roots of discrimination occurrence,\\nwhich will allow to devise global machine learning optimization criteria for\\nguaranteed prevention, as opposed to pushing empirical constraints into\\nexisting algorithms case-by-case. As a result, the state-of-the-art will\\nadvance from heuristic repairing, to proactive and theoretically supported\\nprevention. This is needed not only because law requires to protect vulnerable\\npeople. Penetration of big data initiatives will only increase, and computer\\nscience needs to provide solid explanations and accountability to the public,\\nbefore public concerns lead to unnecessarily restrictive regulations against\\nmachine learning.\\n</summary>\\n    <author>\\n      <name>Indre Zliobaite</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.00754v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.00754v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.03753v1</id>\\n    <updated>2018-08-11T05:05:26Z</updated>\\n    <published>2018-08-11T05:05:26Z</published>\\n    <title>MARVIN: An Open Machine Learning Corpus and Environment for Automated\\n  Machine Learning Primitive Annotation and Execution</title>\\n    <summary>  In this demo paper, we introduce the DARPA D3M program for automatic machine\\nlearning (ML) and JPL\\'s MARVIN tool that provides an environment to locate,\\nannotate, and execute machine learning primitives for use in ML pipelines.\\nMARVIN is a web-based application and associated back-end interface written in\\nPython that enables composition of ML pipelines from hundreds of primitives\\nfrom the world of Scikit-Learn, Keras, DL4J and other widely used libraries.\\nMARVIN allows for the creation of Docker containers that run on Kubernetes\\nclusters within DARPA to provide an execution environment for automated machine\\nlearning. MARVIN currently contains over 400 datasets and challenge problems\\nfrom a wide array of ML domains including routine classification and regression\\nto advanced video/image classification and remote sensing.\\n</summary>\\n    <author>\\n      <name>Chris A. Mattmann</name>\\n    </author>\\n    <author>\\n      <name>Sujen Shah</name>\\n    </author>\\n    <author>\\n      <name>Brian Wilson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.03753v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.03753v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.06128v1</id>\\n    <updated>2018-11-15T00:40:32Z</updated>\\n    <published>2018-11-15T00:40:32Z</published>\\n    <title>Machine Learning for Combinatorial Optimization: a Methodological Tour\\n  d\\'Horizon</title>\\n    <summary>  This paper surveys the recent attempts, both from the machine learning and\\noperations research communities, at leveraging machine learning to solve\\ncombinatorial optimization problems. Given the hard nature of these problems,\\nstate-of-the-art methodologies involve algorithmic decisions that either\\nrequire too much computing time or are not mathematically well defined. Thus,\\nmachine learning looks like a promising candidate to effectively deal with\\nthose decisions. We advocate for pushing further the integration of machine\\nlearning and combinatorial optimization and detail methodology to do so. A main\\npoint of the paper is seeing generic optimization problems as data points and\\ninquiring what is the relevant distribution of problems to use for learning on\\na given task.\\n</summary>\\n    <author>\\n      <name>Yoshua Bengio</name>\\n    </author>\\n    <author>\\n      <name>Andrea Lodi</name>\\n    </author>\\n    <author>\\n      <name>Antoine Prouvost</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.06128v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.06128v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.11477v1</id>\\n    <updated>2018-12-30T06:37:46Z</updated>\\n    <published>2018-12-30T06:37:46Z</published>\\n    <title>Machine learning in resting-state fMRI analysis</title>\\n    <summary>  Machine learning techniques have gained prominence for the analysis of\\nresting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, we\\npresent an overview of various unsupervised and supervised machine learning\\napplications to rs-fMRI. We present a methodical taxonomy of machine learning\\nmethods in resting-state fMRI. We identify three major divisions of\\nunsupervised learning methods with regard to their applications to rs-fMRI,\\nbased on whether they discover principal modes of variation across space, time\\nor population. Next, we survey the algorithms and rs-fMRI feature\\nrepresentations that have driven the success of supervised subject-level\\npredictions. The goal is to provide a high-level overview of the burgeoning\\nfield of rs-fMRI from the perspective of machine learning applications.\\n</summary>\\n    <author>\\n      <name>Meenakshi Khosla</name>\\n    </author>\\n    <author>\\n      <name>Keith Jamison</name>\\n    </author>\\n    <author>\\n      <name>Gia H. Ngo</name>\\n    </author>\\n    <author>\\n      <name>Amy Kuceyeski</name>\\n    </author>\\n    <author>\\n      <name>Mert R. Sabuncu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">51 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.11477v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.11477v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.01686v1</id>\\n    <updated>2019-01-07T07:27:11Z</updated>\\n    <published>2019-01-07T07:27:11Z</published>\\n    <title>Ten ways to fool the masses with machine learning</title>\\n    <summary>  If you want to tell people the truth, make them laugh, otherwise they\\'ll kill\\nyou. (source unclear)\\n  Machine learning and deep learning are the technologies of the day for\\ndeveloping intelligent automatic systems. However, a key hurdle for progress in\\nthe field is the literature itself: we often encounter papers that report\\nresults that are difficult to reconstruct or reproduce, results that\\nmis-represent the performance of the system, or contain other biases that limit\\ntheir validity. In this semi-humorous article, we discuss issues that arise in\\nrunning and reporting results of machine learning experiments. The purpose of\\nthe article is to provide a list of watch out points for researchers to be\\naware of when developing machine learning models or writing and reviewing\\nmachine learning papers.\\n</summary>\\n    <author>\\n      <name>Fayyaz Minhas</name>\\n    </author>\\n    <author>\\n      <name>Amina Asif</name>\\n    </author>\\n    <author>\\n      <name>Asa Ben-Hur</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.01686v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.01686v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.00577v1</id>\\n    <updated>2019-04-01T06:21:31Z</updated>\\n    <published>2019-04-01T06:21:31Z</published>\\n    <title>Adaptive Bayesian Linear Regression for Automated Machine Learning</title>\\n    <summary>  To solve a machine learning problem, one typically needs to perform data\\npreprocessing, modeling, and hyperparameter tuning, which is known as model\\nselection and hyperparameter optimization.The goal of automated machine\\nlearning (AutoML) is to design methods that can automatically perform model\\nselection and hyperparameter optimization without human interventions for a\\ngiven dataset. In this paper, we propose a meta-learning method that can search\\nfor a high-performance machine learning pipeline from the predefined set of\\ncandidate pipelines for supervised classification datasets in an efficient way\\nby leveraging meta-data collected from previous experiments. More specifically,\\nour method combines an adaptive Bayesian regression model with a neural network\\nbasis function and the acquisition function from Bayesian optimization. The\\nadaptive Bayesian regression model is able to capture knowledge from previous\\nmeta-data and thus make predictions of the performances of machine learning\\npipelines on a new dataset. The acquisition function is then used to guide the\\nsearch of possible pipelines based on the predictions.The experiments\\ndemonstrate that our approach can quickly identify high-performance pipelines\\nfor a range of test datasets and outperforms the baseline methods.\\n</summary>\\n    <author>\\n      <name>Weilin Zhou</name>\\n    </author>\\n    <author>\\n      <name>Frederic Precioso</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.00577v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.00577v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.04640v1</id>\\n    <updated>2018-06-12T16:48:52Z</updated>\\n    <published>2018-06-12T16:48:52Z</published>\\n    <title>Unsupervised Meta-Learning for Reinforcement Learning</title>\\n    <summary>  Meta-learning is a powerful tool that builds on multi-task learning to learn\\nhow to quickly adapt a model to new tasks. In the context of reinforcement\\nlearning, meta-learning algorithms can acquire reinforcement learning\\nprocedures to solve new problems more efficiently by meta-learning prior tasks.\\nThe performance of meta-learning algorithms critically depends on the tasks\\navailable for meta-training: in the same way that supervised learning\\nalgorithms generalize best to test points drawn from the same distribution as\\nthe training points, meta-learning methods generalize best to tasks from the\\nsame distribution as the meta-training tasks. In effect, meta-reinforcement\\nlearning offloads the design burden from algorithm design to task design. If we\\ncan automate the process of task design as well, we can devise a meta-learning\\nalgorithm that is truly automated. In this work, we take a step in this\\ndirection, proposing a family of unsupervised meta-learning algorithms for\\nreinforcement learning. We describe a general recipe for unsupervised\\nmeta-reinforcement learning, and describe an effective instantiation of this\\napproach based on a recently proposed unsupervised exploration technique and\\nmodel-agnostic meta-learning. We also discuss practical and conceptual\\nconsiderations for developing unsupervised meta-learning methods. Our\\nexperimental results demonstrate that unsupervised meta-reinforcement learning\\neffectively acquires accelerated reinforcement learning procedures without the\\nneed for manual task design, significantly exceeds the performance of learning\\nfrom scratch, and even matches performance of meta-learning methods that use\\nhand-specified task distributions.\\n</summary>\\n    <author>\\n      <name>Abhishek Gupta</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Eysenbach</name>\\n    </author>\\n    <author>\\n      <name>Chelsea Finn</name>\\n    </author>\\n    <author>\\n      <name>Sergey Levine</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.04640v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.04640v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.03522v1</id>\\n    <updated>2018-02-10T05:35:16Z</updated>\\n    <published>2018-02-10T05:35:16Z</published>\\n    <title>Enhanced version of AdaBoostM1 with J48 Tree learning method</title>\\n    <summary>  Machine Learning focuses on the construction and study of systems that can\\nlearn from data. This is connected with the classification problem, which\\nusually is what Machine Learning algorithms are designed to solve. When a\\nmachine learning method is used by people with no special expertise in machine\\nlearning, it is important that the method be robust in classification, in the\\nsense that reasonable performance is obtained with minimal tuning of the\\nproblem at hand. Algorithms are evaluated based on how robust they can classify\\nthe given data. In this paper, we propose a quantifiable measure of robustness,\\nand describe a particular learning method that is robust according to this\\nmeasure in the context of classification problem. We proposed Adaptive Boosting\\n(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold\\n(P) and number of iterations (I) for boosting algorithm. To benchmark the\\nperformance, we used the baseline classifier, AdaBoostM1 with Decision Stump as\\nbase learner without tuning parameters. By tuning parameters and using J48 as\\nbase learner, we are able to reduce the overall average error rate ratio\\n(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2\\nfor evaluation sets of data.\\n</summary>\\n    <author>\\n      <name>Kyongche Kang</name>\\n    </author>\\n    <author>\\n      <name>Jack Michalak</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.03522v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.03522v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.06865v1</id>\\n    <updated>2018-08-21T12:17:08Z</updated>\\n    <published>2018-08-21T12:17:08Z</published>\\n    <title>Machine Learning for Spatiotemporal Sequence Forecasting: A Survey</title>\\n    <summary>  Spatiotemporal systems are common in the real-world. Forecasting the\\nmulti-step future of these spatiotemporal systems based on the past\\nobservations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant\\nand challenging problem. Although lots of real-world problems can be viewed as\\nSTSF and many research works have proposed machine learning based methods for\\nthem, no existing work has summarized and compared these methods from a unified\\nperspective. This survey aims to provide a systematic review of machine\\nlearning for STSF. In this survey, we define the STSF problem and classify it\\ninto three subcategories: Trajectory Forecasting of Moving Point Cloud\\n(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).\\nWe then introduce the two major challenges of STSF: 1) how to learn a model for\\nmulti-step forecasting and 2) how to adequately model the spatial and temporal\\nstructures. After that, we review the existing works for solving these\\nchallenges, including the general learning strategies for multi-step\\nforecasting, the classical machine learning based methods for STSF, and the\\ndeep learning based methods for STSF. We also compare these methods and point\\nout some potential research directions.\\n</summary>\\n    <author>\\n      <name>Xingjian Shi</name>\\n    </author>\\n    <author>\\n      <name>Dit-Yan Yeung</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.06865v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.06865v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.07339v2</id>\\n    <updated>2018-10-23T03:13:05Z</updated>\\n    <published>2018-10-16T09:06:26Z</published>\\n    <title>Security Matters: A Survey on Adversarial Machine Learning</title>\\n    <summary>  Adversarial machine learning is a fast growing research area, which considers\\nthe scenarios when machine learning systems may face potential adversarial\\nattackers, who intentionally synthesize input data to make a well-trained model\\nto make mistake. It always involves a defending side, usually a classifier, and\\nan attacking side that aims to cause incorrect output. The earliest studies on\\nthe adversarial examples for machine learning algorithms start from the\\ninformation security area, which considers a much wider varieties of attacking\\nmethods. But recent research focus that popularized by the deep learning\\ncommunity places strong emphasis on how the \"imperceivable\" perturbations on\\nthe normal inputs may cause dramatic mistakes by the deep learning with\\nsupposed super-human accuracy. This paper serves to give a comprehensive\\nintroduction to a range of aspects of the adversarial deep learning topic,\\nincluding its foundations, typical attacking and defending strategies, and some\\nextended studies.\\n</summary>\\n    <author>\\n      <name>Guofu Li</name>\\n    </author>\\n    <author>\\n      <name>Pengjia Zhu</name>\\n    </author>\\n    <author>\\n      <name>Jin Li</name>\\n    </author>\\n    <author>\\n      <name>Zhemin Yang</name>\\n    </author>\\n    <author>\\n      <name>Ning Cao</name>\\n    </author>\\n    <author>\\n      <name>Zhiyi Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.07339v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.07339v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.11400v2</id>\\n    <updated>2018-12-03T02:08:28Z</updated>\\n    <published>2018-11-28T06:06:38Z</published>\\n    <title>FADL:Federated-Autonomous Deep Learning for Distributed Electronic\\n  Health Record</title>\\n    <summary>  Electronic health record (EHR) data is collected by individual institutions\\nand often stored across locations in silos. Getting access to these data is\\ndifficult and slow due to security, privacy, regulatory, and operational\\nissues. We show, using ICU data from 58 different hospitals, that machine\\nlearning models to predict patient mortality can be trained efficiently without\\nmoving health data out of their silos using a distributed machine learning\\nstrategy. We propose a new method, called Federated-Autonomous Deep Learning\\n(FADL) that trains part of the model using all data sources in a distributed\\nmanner and other parts using data from specific data sources. We observed that\\nFADL outperforms traditional federated learning strategy and conclude that\\nbalance between global and local training is an important factor to consider\\nwhen design distributed machine learning methods , especially in healthcare.\\n</summary>\\n    <author>\\n      <name>Dianbo Liu</name>\\n    </author>\\n    <author>\\n      <name>Timothy Miller</name>\\n    </author>\\n    <author>\\n      <name>Raheel Sayeed</name>\\n    </author>\\n    <author>\\n      <name>Kenneth D. Mandl</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  arXiv:cs/0101200</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1811.11400v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.11400v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1602.00203v1</id>\\n    <updated>2016-01-31T06:12:58Z</updated>\\n    <published>2016-01-31T06:12:58Z</published>\\n    <title>Greedy Deep Dictionary Learning</title>\\n    <summary>  In this work we propose a new deep learning tool called deep dictionary\\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\\na time. This requires solving a simple (shallow) dictionary learning problem,\\nthe solution to this is well known. We apply the proposed technique on some\\nbenchmark deep learning datasets. We compare our results with other deep\\nlearning tools like stacked autoencoder and deep belief network; and state of\\nthe art supervised dictionary learning tools like discriminative KSVD and label\\nconsistent KSVD. Our method yields better results than all.\\n</summary>\\n    <author>\\n      <name>Snigdha Tariyal</name>\\n    </author>\\n    <author>\\n      <name>Angshul Majumdar</name>\\n    </author>\\n    <author>\\n      <name>Richa Singh</name>\\n    </author>\\n    <author>\\n      <name>Mayank Vatsa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1602.00203v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.00203v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.00994v1</id>\\n    <updated>2017-03-03T00:03:14Z</updated>\\n    <published>2017-03-03T00:03:14Z</published>\\n    <title>Co-Clustering for Multitask Learning</title>\\n    <summary>  This paper presents a new multitask learning framework that learns a shared\\nrepresentation among the tasks, incorporating both task and feature clusters.\\nThe jointly-induced clusters yield a shared latent subspace where task\\nrelationships are learned more effectively and more generally than in\\nstate-of-the-art multitask learning methods. The proposed general framework\\nenables the derivation of more specific or restricted state-of-the-art\\nmultitask methods. The paper also proposes a highly-scalable multitask learning\\nalgorithm, based on the new framework, using conjugate gradient descent and\\ngeneralized \\\\textit{Sylvester equations}. Experimental results on synthetic and\\nbenchmark datasets show that the proposed method systematically outperforms\\nseveral state-of-the-art multitask learning methods.\\n</summary>\\n    <author>\\n      <name>Keerthiram Murugesan</name>\\n    </author>\\n    <author>\\n      <name>Jaime Carbonell</name>\\n    </author>\\n    <author>\\n      <name>Yiming Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.00994v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.00994v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.07991v1</id>\\n    <updated>2017-10-22T18:13:24Z</updated>\\n    <published>2017-10-22T18:13:24Z</published>\\n    <title>Rethinking Convolutional Semantic Segmentation Learning</title>\\n    <summary>  Deep convolutional semantic segmentation (DCSS) learning doesn\\'t converge to\\nan optimal local minimum with random parameters initializations; a pre-trained\\nmodel on the same domain becomes necessary to achieve convergence.In this work,\\nwe propose a joint cooperative end-to-end learning method for DCSS. It\\naddresses many drawbacks with existing deep semantic segmentation learning; the\\nproposed approach simultaneously learn both segmentation and classification;\\ntaking away the essential need of the pre-trained model for learning\\nconvergence. We present an improved inception based architecture with partial\\nattention gating (PAG) over encoder information. The PAG also adds to achieve\\nfaster convergence and better accuracy for segmentation task. We will show the\\neffectiveness of this learning on a diabetic retinopathy classification and\\nsegmentation dataset.\\n</summary>\\n    <author>\\n      <name>Mrinal Haloi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1710.07991v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.07991v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T45\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.07654v1</id>\\n    <updated>2018-01-23T16:47:32Z</updated>\\n    <published>2018-01-23T16:47:32Z</published>\\n    <title>Expectation Learning for Adaptive Crossmodal Stimuli Association</title>\\n    <summary>  The human brain is able to learn, generalize, and predict crossmodal stimuli.\\nLearning by expectation fine-tunes crossmodal processing at different levels,\\nthus enhancing our power of generalization and adaptation in highly dynamic\\nenvironments. In this paper, we propose a deep neural architecture trained by\\nusing expectation learning accounting for unsupervised learning tasks. Our\\nlearning model exhibits a self-adaptable behavior, setting the first steps\\ntowards the development of deep learning architectures for crossmodal stimuli\\nassociation.\\n</summary>\\n    <author>\\n      <name>Pablo Barros</name>\\n    </author>\\n    <author>\\n      <name>German I. Parisi</name>\\n    </author>\\n    <author>\\n      <name>Di Fu</name>\\n    </author>\\n    <author>\\n      <name>Xun Liu</name>\\n    </author>\\n    <author>\\n      <name>Stefan Wermter</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages 2017 EUCog meeting abstract</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.07654v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.07654v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08180v2</id>\\n    <updated>2019-03-08T17:52:47Z</updated>\\n    <published>2018-05-21T17:02:53Z</published>\\n    <title>Hierarchical Reinforcement Learning with Hindsight</title>\\n    <summary>  Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency\\nwhen rewards are delayed and sparse. We introduce a solution that enables\\nagents to learn temporally extended actions at multiple levels of abstraction\\nin a sample efficient and automated fashion. Our approach combines universal\\nvalue functions and hindsight learning, allowing agents to learn policies\\nbelonging to different time scales in parallel. We show that our method\\nsignificantly accelerates learning in a variety of discrete and continuous\\ntasks.\\n</summary>\\n    <author>\\n      <name>Andrew Levy</name>\\n    </author>\\n    <author>\\n      <name>Robert Platt</name>\\n    </author>\\n    <author>\\n      <name>Kate Saenko</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Duplicate. See arXiv:1712.00948 \"Learning Multi-Level Hierarchies\\n  with Hindsight\" for latest version</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.08180v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08180v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.12369v1</id>\\n    <updated>2018-05-31T08:11:12Z</updated>\\n    <published>2018-05-31T08:11:12Z</published>\\n    <title>Reinforced Continual Learning</title>\\n    <summary>  Most artificial intelligence models have limiting ability to solve new tasks\\nfaster, without forgetting previously acquired knowledge. The recently emerging\\nparadigm of continual learning aims to solve this issue, in which the model\\nlearns various tasks in a sequential fashion. In this work, a novel approach\\nfor continual learning is proposed, which searches for the best neural\\narchitecture for each coming task via sophisticatedly designed reinforcement\\nlearning strategies. We name it as Reinforced Continual Learning. Our method\\nnot only has good performance on preventing catastrophic forgetting but also\\nfits new tasks well. The experiments on sequential classification tasks for\\nvariants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach\\noutperforms existing continual learning alternatives for deep networks.\\n</summary>\\n    <author>\\n      <name>Ju Xu</name>\\n    </author>\\n    <author>\\n      <name>Zhanxing Zhu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.12369v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.12369v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.00512v1</id>\\n    <updated>2018-11-01T17:31:10Z</updated>\\n    <published>2018-11-01T17:31:10Z</published>\\n    <title>Learning Beam Search Policies via Imitation Learning</title>\\n    <summary>  Beam search is widely used for approximate decoding in structured prediction\\nproblems. Models often use a beam at test time but ignore its existence at\\ntrain time, and therefore do not explicitly learn how to use the beam. We\\ndevelop an unifying meta-algorithm for learning beam search policies using\\nimitation learning. In our setting, the beam is part of the model, and not just\\nan artifact of approximate decoding. Our meta-algorithm captures existing\\nlearning algorithms and suggests new ones. It also lets us show novel no-regret\\nguarantees for learning beam search policies.\\n</summary>\\n    <author>\\n      <name>Renato Negrinho</name>\\n    </author>\\n    <author>\\n      <name>Matthew R. Gormley</name>\\n    </author>\\n    <author>\\n      <name>Geoffrey J. Gordon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in NIPS 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.00512v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.00512v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00564v1</id>\\n    <updated>2018-12-03T05:43:20Z</updated>\\n    <published>2018-12-03T05:43:20Z</published>\\n    <title>Split learning for health: Distributed deep learning without sharing raw\\n  patient data</title>\\n    <summary>  Can health entities collaboratively train deep learning models without\\nsharing sensitive raw data? This paper proposes several configurations of a\\ndistributed deep learning method called SplitNN to facilitate such\\ncollaborations. SplitNN does not share raw data or model details with\\ncollaborating institutions. The proposed configurations of splitNN cater to\\npractical settings of i) entities holding different modalities of patient data,\\nii) centralized and local health entities collaborating on multiple tasks and\\niii) learning without sharing labels. We compare performance and resource\\nefficiency trade-offs of splitNN and other distributed deep learning methods\\nlike federated learning, large batch synchronous stochastic gradient descent\\nand show highly encouraging results for splitNN.\\n</summary>\\n    <author>\\n      <name>Praneeth Vepakomma</name>\\n    </author>\\n    <author>\\n      <name>Otkrist Gupta</name>\\n    </author>\\n    <author>\\n      <name>Tristan Swedish</name>\\n    </author>\\n    <author>\\n      <name>Ramesh Raskar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.00564v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00564v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.06497v2</id>\\n    <updated>2018-12-13T17:00:18Z</updated>\\n    <published>2017-04-21T11:56:00Z</published>\\n    <title>Bandit Structured Prediction for Neural Sequence-to-Sequence Learning</title>\\n    <summary>  Bandit structured prediction describes a stochastic optimization framework\\nwhere learning is performed from partial feedback. This feedback is received in\\nthe form of a task loss evaluation to a predicted output structure, without\\nhaving access to gold standard structures. We advance this framework by lifting\\nlinear bandit learning to neural sequence-to-sequence learning problems using\\nattention-based recurrent neural networks. Furthermore, we show how to\\nincorporate control variates into our learning algorithms for variance\\nreduction and improved generalization. We present an evaluation on a neural\\nmachine translation task that shows improvements of up to 5.89 BLEU points for\\ndomain adaptation from simulated bandit feedback.\\n</summary>\\n    <author>\\n      <name>Julia Kreutzer</name>\\n    </author>\\n    <author>\\n      <name>Artem Sokolov</name>\\n    </author>\\n    <author>\\n      <name>Stefan Riezler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ACL 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1704.06497v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.06497v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1112.2738v1</id>\\n    <updated>2011-12-12T22:33:55Z</updated>\\n    <published>2011-12-12T22:33:55Z</published>\\n    <title>Robust Learning via Cause-Effect Models</title>\\n    <summary>  We consider the problem of function estimation in the case where the data\\ndistribution may shift between training and test time, and additional\\ninformation about it may be available at test time. This relates to popular\\nscenarios such as covariate shift, concept drift, transfer learning and\\nsemi-supervised learning. This working paper discusses how these tasks could be\\ntackled depending on the kind of changes of the distributions. It argues that\\nknowledge of an underlying causal direction can facilitate several of these\\ntasks.\\n</summary>\\n    <author>\\n      <name>Bernhard Sch\\xc3\\xb6lkopf</name>\\n    </author>\\n    <author>\\n      <name>Dominik Janzing</name>\\n    </author>\\n    <author>\\n      <name>Jonas Peters</name>\\n    </author>\\n    <author>\\n      <name>Kun Zhang</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A version of this paper has been published as \"On Causal and\\n  Anticausal Learning\" in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1112.2738v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1112.2738v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.06434v1</id>\\n    <updated>2016-10-20T14:37:46Z</updated>\\n    <published>2016-10-20T14:37:46Z</published>\\n    <title>Kernel Alignment for Unsupervised Transfer Learning</title>\\n    <summary>  The ability of a human being to extrapolate previously gained knowledge to\\nother domains inspired a new family of methods in machine learning called\\ntransfer learning. Transfer learning is often based on the assumption that\\nobjects in both target and source domains share some common feature and/or data\\nspace. In this paper, we propose a simple and intuitive approach that minimizes\\niteratively the distance between source and target task distributions by\\noptimizing the kernel target alignment (KTA). We show that this procedure is\\nsuitable for transfer learning by relating it to Hilbert-Schmidt Independence\\nCriterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run\\nour method on benchmark computer vision data sets and show that it can\\noutperform some state-of-art methods.\\n</summary>\\n    <author>\\n      <name>Ievgen Redko</name>\\n    </author>\\n    <author>\\n      <name>Youn\\xc3\\xa8s Bennani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1610.06434v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.06434v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.09369v1</id>\\n    <updated>2016-10-28T11:57:26Z</updated>\\n    <published>2016-10-28T11:57:26Z</published>\\n    <title>Discriminative Gaifman Models</title>\\n    <summary>  We present discriminative Gaifman models, a novel family of relational\\nmachine learning models. Gaifman models learn feature representations bottom up\\nfrom representations of locally connected and bounded-size regions of knowledge\\nbases (KBs). Considering local and bounded-size neighborhoods of knowledge\\nbases renders logical inference and learning tractable, mitigates the problem\\nof overfitting, and facilitates weight sharing. Gaifman models sample\\nneighborhoods of knowledge bases so as to make the learned relational models\\nmore robust to missing objects and relations which is a common situation in\\nopen-world KBs. We present the core ideas of Gaifman models and apply them to\\nlarge-scale relational learning problems. We also discuss the ways in which\\nGaifman models relate to some existing relational machine learning approaches.\\n</summary>\\n    <author>\\n      <name>Mathias Niepert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.09369v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.09369v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.10467v2</id>\\n    <updated>2018-02-27T07:29:26Z</updated>\\n    <published>2017-05-30T06:20:31Z</published>\\n    <title>Federated Multi-Task Learning</title>\\n    <summary>  Federated learning poses new statistical and systems challenges in training\\nmachine learning models over distributed networks of devices. In this work, we\\nshow that multi-task learning is naturally suited to handle the statistical\\nchallenges of this setting, and propose a novel systems-aware optimization\\nmethod, MOCHA, that is robust to practical systems issues. Our method and\\ntheory for the first time consider issues of high communication cost,\\nstragglers, and fault tolerance for distributed multi-task learning. The\\nresulting method achieves significant speedups compared to alternatives in the\\nfederated setting, as we demonstrate through simulations on real-world\\nfederated datasets.\\n</summary>\\n    <author>\\n      <name>Virginia Smith</name>\\n    </author>\\n    <author>\\n      <name>Chao-Kai Chiang</name>\\n    </author>\\n    <author>\\n      <name>Maziar Sanjabi</name>\\n    </author>\\n    <author>\\n      <name>Ameet Talwalkar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.10467v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.10467v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.02527v1</id>\\n    <updated>2018-04-07T07:52:04Z</updated>\\n    <published>2018-04-07T07:52:04Z</published>\\n    <title>Visual Analytics for Explainable Deep Learning</title>\\n    <summary>  Recently, deep learning has been advancing the state of the art in artificial\\nintelligence to a new level, and humans rely on artificial intelligence\\ntechniques more than ever. However, even with such unprecedented advancements,\\nthe lack of explanation regarding the decisions made by deep learning models\\nand absence of control over their internal processes act as major drawbacks in\\ncritical decision-making processes, such as precision medicine and law\\nenforcement. In response, efforts are being made to make deep learning\\ninterpretable and controllable by humans. In this paper, we review visual\\nanalytics, information visualization, and machine learning perspectives\\nrelevant to this aim, and discuss potential challenges and future research\\ndirections.\\n</summary>\\n    <author>\\n      <name>Jaegul Choo</name>\\n    </author>\\n    <author>\\n      <name>Shixia Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Computer Graphics and Applications, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1804.02527v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.02527v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.6.9.c\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.05806v1</id>\\n    <updated>2018-04-16T17:25:24Z</updated>\\n    <published>2018-04-16T17:25:24Z</published>\\n    <title>Deep Embedding Kernel</title>\\n    <summary>  In this paper, we propose a novel supervised learning method that is called\\nDeep Embedding Kernel (DEK). DEK combines the advantages of deep learning and\\nkernel methods in a unified framework. More specifically, DEK is a learnable\\nkernel represented by a newly designed deep architecture. Compared with\\npre-defined kernels, this kernel can be explicitly trained to map data to an\\noptimized high-level feature space where data may have favorable features\\ntoward the application. Compared with typical deep learning using SoftMax or\\nlogistic regression as the top layer, DEK is expected to be more generalizable\\nto new data. Experimental results show that DEK has superior performance than\\ntypical machine learning methods in identity detection, classification,\\nregression, dimension reduction, and transfer learning.\\n</summary>\\n    <author>\\n      <name>Linh Le</name>\\n    </author>\\n    <author>\\n      <name>Ying Xie</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.05806v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.05806v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.05454v1</id>\\n    <updated>2018-06-14T10:35:21Z</updated>\\n    <published>2018-06-14T10:35:21Z</published>\\n    <title>Low-rank geometric mean metric learning</title>\\n    <summary>  We propose a low-rank approach to learning a Mahalanobis metric from data.\\nInspired by the recent geometric mean metric learning (GMML) algorithm, we\\npropose a low-rank variant of the algorithm. This allows to jointly learn a\\nlow-dimensional subspace where the data reside and the Mahalanobis metric that\\nappropriately fits the data. Our results show that we compete effectively with\\nGMML at lower ranks.\\n</summary>\\n    <author>\\n      <name>Mukul Bhutani</name>\\n    </author>\\n    <author>\\n      <name>Pratik Jawanpuria</name>\\n    </author>\\n    <author>\\n      <name>Hiroyuki Kasai</name>\\n    </author>\\n    <author>\\n      <name>Bamdev Mishra</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to the geometry in machine learning (GiMLi) workshop at ICML\\n  2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.05454v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.05454v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.07987v2</id>\\n    <updated>2018-08-03T11:27:28Z</updated>\\n    <published>2018-07-20T18:20:34Z</published>\\n    <title>Deep Learning</title>\\n    <summary>  Deep learning (DL) is a high dimensional data reduction technique for\\nconstructing high-dimensional predictors in input-output models. DL is a form\\nof machine learning that uses hierarchical layers of latent features. In this\\narticle, we review the state-of-the-art of deep learning from a modeling and\\nalgorithmic perspective. We provide a list of successful areas of applications\\nin Artificial Intelligence (AI), Image Processing, Robotics and Automation.\\nDeep learning is predictive in its nature rather then inferential and can be\\nviewed as a black-box methodology for high-dimensional function estimation.\\n</summary>\\n    <author>\\n      <name>Nicholas G. Polson</name>\\n    </author>\\n    <author>\\n      <name>Vadim O. Sokolov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1602.06561</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.07987v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.07987v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.10110v2</id>\\n    <updated>2018-11-30T14:12:22Z</updated>\\n    <published>2018-07-26T13:27:35Z</published>\\n    <title>ToriLLE: Learning Environment for Hand-to-Hand Combat</title>\\n    <summary>  We present Toribash Learning Environment (ToriLLE), a learning environment\\nfor machine learning agents based on the video game Toribash. Toribash is a\\nMuJoCo-like environment of two humanoid character fighting each other\\nhand-to-hand, controlled by changing actuation modes of the joints. Competitive\\nnature of Toribash as well its focused domain provide a platform for evaluating\\nself-play methods, and evaluating machine learning agents against human\\nplayers. In this paper we describe the environment with ToriLLE\\'s capabilities\\nand limitations, and experimentally show its applicability as a learning\\nenvironment. The source code of the environment and conducted experiments can\\nbe found at https://github.com/Miffyli/ToriLLE.\\n</summary>\\n    <author>\\n      <name>Anssi Kanervisto</name>\\n    </author>\\n    <author>\\n      <name>Ville Hautam\\xc3\\xa4ki</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">https://github.com/Miffyli/ToriLLE</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.10110v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.10110v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.04114v2</id>\\n    <updated>2019-04-02T10:53:33Z</updated>\\n    <published>2018-10-09T16:40:02Z</published>\\n    <title>Discovering General-Purpose Active Learning Strategies</title>\\n    <summary>  We propose a general-purpose approach to discovering active learning (AL)\\nstrategies from data. These strategies are transferable from one domain to\\nanother and can be used in conjunction with many machine learning models. To\\nthis end, we formalize the annotation process as a Markov decision process,\\ndesign universal state and action spaces and introduce a new reward function\\nthat precisely model the AL objective of minimizing the annotation cost. We\\nseek to find an optimal (non-myopic) AL strategy using reinforcement learning.\\nWe evaluate the learned strategies on multiple unrelated domains and show that\\nthey consistently outperform state-of-the-art baselines.\\n</summary>\\n    <author>\\n      <name>Ksenia Konyushkova</name>\\n    </author>\\n    <author>\\n      <name>Raphael Sznitman</name>\\n    </author>\\n    <author>\\n      <name>Pascal Fua</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.04114v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.04114v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.04820v1</id>\\n    <updated>2018-11-12T16:00:36Z</updated>\\n    <published>2018-11-12T16:00:36Z</published>\\n    <title>Learning From Positive and Unlabeled Data: A Survey</title>\\n    <summary>  Learning from positive and unlabeled data or PU learning is the setting where\\na learner only has access to positive examples and unlabeled data. The\\nassumption is that the unlabeled data can contain both positive and negative\\nexamples. This setting has attracted increasing interest within the machine\\nlearning literature as this type of data naturally arises in applications such\\nas medical diagnosis and knowledge base completion. This article provides a\\nsurvey of the current state of the art in PU learning. It proposes seven key\\nresearch questions that commonly arise in this field and provides a broad\\noverview of how the field has tried to address them.\\n</summary>\\n    <author>\\n      <name>Jessa Bekker</name>\\n    </author>\\n    <author>\\n      <name>Jesse Davis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.04820v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.04820v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00975v1</id>\\n    <updated>2018-12-03T18:49:31Z</updated>\\n    <published>2018-12-03T18:49:31Z</published>\\n    <title>Structure Learning Using Forced Pruning</title>\\n    <summary>  Markov networks are widely used in many Machine Learning applications\\nincluding natural language processing, computer vision, and bioinformatics .\\nLearning Markov networks have many complications ranging from intractable\\ncomputations involved to the possibility of learning a model with a huge number\\nof parameters. In this report, we provide a computationally tractable greedy\\nheuristic for learning Markov networks structure. The proposed heuristic\\nresults in a model with a limited predefined number of parameters. We ran our\\nmethod on 3 fully-observed real datasets, and we observed that our method is\\ndoing comparably good to the state of the art methods.\\n</summary>\\n    <author>\\n      <name>Ahmed Abdelatty</name>\\n    </author>\\n    <author>\\n      <name>Pracheta Sahoo</name>\\n    </author>\\n    <author>\\n      <name>Chiradeep Roy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.00975v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00975v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.02903v1</id>\\n    <updated>2018-12-07T04:18:12Z</updated>\\n    <published>2018-12-07T04:18:12Z</published>\\n    <title>Applied Federated Learning: Improving Google Keyboard Query Suggestions</title>\\n    <summary>  Federated learning is a distributed form of machine learning where both the\\ntraining data and model training are decentralized. In this paper, we use\\nfederated learning in a commercial, global-scale setting to train, evaluate and\\ndeploy a model to improve virtual keyboard search suggestion quality without\\ndirect access to the underlying user data. We describe our observations in\\nfederated training, compare metrics to live deployments, and present resulting\\nquality increases. In whole, we demonstrate how federated learning can be\\napplied end-to-end to both improve user experiences and enhance user privacy.\\n</summary>\\n    <author>\\n      <name>Timothy Yang</name>\\n    </author>\\n    <author>\\n      <name>Galen Andrew</name>\\n    </author>\\n    <author>\\n      <name>Hubert Eichner</name>\\n    </author>\\n    <author>\\n      <name>Haicheng Sun</name>\\n    </author>\\n    <author>\\n      <name>Wei Li</name>\\n    </author>\\n    <author>\\n      <name>Nicholas Kong</name>\\n    </author>\\n    <author>\\n      <name>Daniel Ramage</name>\\n    </author>\\n    <author>\\n      <name>Fran\\xc3\\xa7oise Beaufays</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.02903v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.02903v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.10310v1</id>\\n    <updated>2019-01-29T14:33:42Z</updated>\\n    <published>2019-01-29T14:33:42Z</published>\\n    <title>Robust Learning from Untrusted Sources</title>\\n    <summary>  Modern machine learning methods often require more data for training than a\\nsingle expert can provide. Therefore, it has become a standard procedure to\\ncollect data from external sources, e.g. via crowdsourcing. Unfortunately, the\\nquality of these sources is not always guaranteed. As additional complications,\\nthe data might be stored in a distributed way, or might even have to remain\\nprivate. In this work, we address the question of how to learn robustly in such\\nscenarios. Studying the problem through the lens of statistical learning\\ntheory, we derive a procedure that allows for learning from all available\\nsources, yet automatically suppresses irrelevant or corrupted data. We show by\\nextensive experiments that our method provides significant improvements over\\nalternative approaches from robust statistics and distributed optimization.\\n</summary>\\n    <author>\\n      <name>Nikola Konstantinov</name>\\n    </author>\\n    <author>\\n      <name>Christoph Lampert</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.10310v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.10310v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.04247v2</id>\\n    <updated>2019-02-13T11:41:49Z</updated>\\n    <published>2019-02-12T05:49:34Z</published>\\n    <title>PAC-Bayes Analysis of Sentence Representation</title>\\n    <summary>  Learning sentence vectors from an unlabeled corpus has attracted attention\\nbecause such vectors can represent sentences in a lower dimensional and\\ncontinuous space. Simple heuristics using pre-trained word vectors are widely\\napplied to machine learning tasks. However, they are not well understood from a\\ntheoretical perspective. We analyze learning sentence vectors from a transfer\\nlearning perspective by using a PAC-Bayes bound that enables us to understand\\nexisting heuristics. We show that simple heuristics such as averaging and\\ninverse document frequency weighted averaging are derived by our formulation.\\nMoreover, we propose novel sentence vector learning algorithms on the basis of\\nour PAC-Bayes analysis.\\n</summary>\\n    <author>\\n      <name>Kento Nozawa</name>\\n    </author>\\n    <author>\\n      <name>Issei Sato</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">fix styles</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.04247v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.04247v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.11175v2</id>\\n    <updated>2019-03-05T20:33:39Z</updated>\\n    <published>2019-02-28T15:55:18Z</published>\\n    <title>One-Shot Federated Learning</title>\\n    <summary>  We present one-shot federated learning, where a central server learns a\\nglobal model over a network of federated devices in a single round of\\ncommunication. Our approach - drawing on ensemble learning and knowledge\\naggregation - achieves an average relative gain of 51.5% in AUC over local\\nbaselines and comes within 90.1% of the (unattainable) global ideal. We discuss\\nthese methods and identify several promising directions of future work.\\n</summary>\\n    <author>\\n      <name>Neel Guha</name>\\n    </author>\\n    <author>\\n      <name>Ameet Talwalkar</name>\\n    </author>\\n    <author>\\n      <name>Virginia Smith</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 3 figures, 1 table. 2nd Workshop on Machine Learning on the\\n  Phone and other Consumer Devices, NeurIPs 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.11175v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.11175v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.05196v2</id>\\n    <updated>2019-03-15T17:06:11Z</updated>\\n    <published>2019-03-12T20:38:54Z</published>\\n    <title>A Review of Reinforcement Learning for Autonomous Building Energy\\n  Management</title>\\n    <summary>  The area of building energy management has received a significant amount of\\ninterest in recent years. This area is concerned with combining advancements in\\nsensor technologies, communications and advanced control algorithms to optimize\\nenergy utilization. Reinforcement learning is one of the most prominent machine\\nlearning algorithms used for control problems and has had many successful\\napplications in the area of building energy management. This research gives a\\ncomprehensive review of the literature relating to the application of\\nreinforcement learning to developing autonomous building energy management\\nsystems. The main direction for future research and challenges in reinforcement\\nlearning are also outlined.\\n</summary>\\n    <author>\\n      <name>Karl Mason</name>\\n    </author>\\n    <author>\\n      <name>Santiago Grijalva</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.05196v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.05196v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.07378v1</id>\\n    <updated>2019-03-18T12:09:36Z</updated>\\n    <published>2019-03-18T12:09:36Z</published>\\n    <title>On-line learning dynamics of ReLU neural networks using statistical\\n  physics techniques</title>\\n    <summary>  We introduce exact macroscopic on-line learning dynamics of two-layer neural\\nnetworks with ReLU units in the form of a system of differential equations,\\nusing techniques borrowed from statistical physics. For the first experiments,\\nnumerical solutions reveal similar behavior compared to sigmoidal activation\\nresearched in earlier work. In these experiments the theoretical results show\\ngood correspondence with simulations. In ove-rrealizable and unrealizable\\nlearning scenarios, the learning behavior of ReLU networks shows distinctive\\ncharacteristics compared to sigmoidal networks.\\n</summary>\\n    <author>\\n      <name>Michiel Straat</name>\\n    </author>\\n    <author>\\n      <name>Michael Biehl</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted contribution: ESANN 2019, 6 pages European Symposium on\\n  Artificial Neural Networks, Computational Intelligence and Machine Learning\\n  2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.07378v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.07378v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1102.2467v1</id>\\n    <updated>2011-02-12T01:34:52Z</updated>\\n    <published>2011-02-12T01:34:52Z</published>\\n    <title>Universal Learning Theory</title>\\n    <summary>  This encyclopedic article gives a mini-introduction into the theory of\\nuniversal learning, founded by Ray Solomonoff in the 1960s and significantly\\ndeveloped and extended in the last decade. It explains the spirit of universal\\nlearning, but necessarily glosses over technical subtleties.\\n</summary>\\n    <author>\\n      <name>Marcus Hutter</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 LaTeX pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Encyclopedia of Machine Learning (2011) pages 1001-1008</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1102.2467v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1102.2467v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.5976v1</id>\\n    <updated>2013-03-24T18:32:38Z</updated>\\n    <published>2013-03-24T18:32:38Z</published>\\n    <title>On Learnability, Complexity and Stability</title>\\n    <summary>  We consider the fundamental question of learnability of a hypotheses class in\\nthe supervised learning setting and in the general learning setting introduced\\nby Vladimir Vapnik. We survey classic results characterizing learnability in\\nterm of suitable notions of complexity, as well as more recent results that\\nestablish the connection between learnability and stability of a learning\\nalgorithm.\\n</summary>\\n    <author>\\n      <name>Silvia Villa</name>\\n    </author>\\n    <author>\\n      <name>Lorenzo Rosasco</name>\\n    </author>\\n    <author>\\n      <name>Tomaso Poggio</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1303.5976v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.5976v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.0440v1</id>\\n    <updated>2014-10-02T02:28:04Z</updated>\\n    <published>2014-10-02T02:28:04Z</published>\\n    <title>Scalable Nonlinear Learning with Adaptive Polynomial Expansions</title>\\n    <summary>  Can we effectively learn a nonlinear representation in time comparable to\\nlinear learning? We describe a new algorithm that explicitly and adaptively\\nexpands higher-order interaction features over base linear representations. The\\nalgorithm is designed for extreme computational efficiency, and an extensive\\nexperimental study shows that its computation/prediction tradeoff ability\\ncompares very favorably against strong baselines.\\n</summary>\\n    <author>\\n      <name>Alekh Agarwal</name>\\n    </author>\\n    <author>\\n      <name>Alina Beygelzimer</name>\\n    </author>\\n    <author>\\n      <name>Daniel Hsu</name>\\n    </author>\\n    <author>\\n      <name>John Langford</name>\\n    </author>\\n    <author>\\n      <name>Matus Telgarsky</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in NIPS 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.0440v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.0440v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1504.08215v1</id>\\n    <updated>2015-04-30T13:26:46Z</updated>\\n    <published>2015-04-30T13:26:46Z</published>\\n    <title>Lateral Connections in Denoising Autoencoders Support Supervised\\n  Learning</title>\\n    <summary>  We show how a deep denoising autoencoder with lateral connections can be used\\nas an auxiliary unsupervised learning task to support supervised learning. The\\nproposed model is trained to minimize simultaneously the sum of supervised and\\nunsupervised cost functions by back-propagation, avoiding the need for\\nlayer-wise pretraining. It improves the state of the art significantly in the\\npermutation-invariant MNIST classification task.\\n</summary>\\n    <author>\\n      <name>Antti Rasmus</name>\\n    </author>\\n    <author>\\n      <name>Harri Valpola</name>\\n    </author>\\n    <author>\\n      <name>Tapani Raiko</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1504.08215v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1504.08215v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.04882v1</id>\\n    <updated>2016-03-15T20:46:46Z</updated>\\n    <published>2016-03-15T20:46:46Z</published>\\n    <title>Bias Correction for Regularized Regression and its Application in\\n  Learning with Streaming Data</title>\\n    <summary>  We propose an approach to reduce the bias of ridge regression and\\nregularization kernel network. When applied to a single data set the new\\nalgorithms have comparable learning performance with the original ones. When\\napplied to incremental learning with block wise streaming data the new\\nalgorithms are more efficient due to bias reduction. Both theoretical\\ncharacterizations and simulation studies are used to verify the effectiveness\\nof these new algorithms.\\n</summary>\\n    <author>\\n      <name>Qiang Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1603.04882v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.04882v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.02510v1</id>\\n    <updated>2015-06-08T14:00:32Z</updated>\\n    <published>2015-06-08T14:00:32Z</published>\\n    <title>Learning Mixtures of Ising Models using Pseudolikelihood</title>\\n    <summary>  Maximum pseudolikelihood method has been among the most important methods for\\nlearning parameters of statistical physics models, such as Ising models. In\\nthis paper, we study how pseudolikelihood can be derived for learning\\nparameters of a mixture of Ising models. The performance of the proposed\\napproach is demonstrated for Ising and Potts models on both synthetic and real\\ndata.\\n</summary>\\n    <author>\\n      <name>Onur Dikmen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1506.02510v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.02510v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.00251v1</id>\\n    <updated>2016-05-01T13:19:57Z</updated>\\n    <published>2016-05-01T13:19:57Z</published>\\n    <title>A vector-contraction inequality for Rademacher complexities</title>\\n    <summary>  The contraction inequality for Rademacher averages is extended to Lipschitz\\nfunctions with vector-valued domains, and it is also shown that in the bounding\\nexpression the Rademacher variables can be replaced by arbitrary iid symmetric\\nand sub-gaussian variables. Example applications are given for multi-category\\nlearning, K-means clustering and learning-to-learn.\\n</summary>\\n    <author>\\n      <name>Andreas Maurer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1605.00251v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.00251v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.07993v1</id>\\n    <updated>2016-12-23T15:02:54Z</updated>\\n    <published>2016-12-23T15:02:54Z</published>\\n    <title>RSSL: Semi-supervised Learning in R</title>\\n    <summary>  In this paper, we introduce a package for semi-supervised learning research\\nin the R programming language called RSSL. We cover the purpose of the package,\\nthe methods it includes and comment on their use and implementation. We then\\nshow, using several code examples, how the package can be used to replicate\\nwell-known results from the semi-supervised learning literature.\\n</summary>\\n    <author>\\n      <name>Jesse H. Krijthe</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at RRPR 2016: 1st Workshop on Reproducible Research in\\n  Pattern Recognition</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1612.07993v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.07993v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.05487v1</id>\\n    <updated>2017-01-19T15:48:11Z</updated>\\n    <published>2017-01-19T15:48:11Z</published>\\n    <title>Learning first-order definable concepts over structures of small degree</title>\\n    <summary>  We consider a declarative framework for machine learning where concepts and\\nhypotheses are defined by formulas of a logic over some background structure.\\nWe show that within this framework, concepts defined by first-order formulas\\nover a background structure of at most polylogarithmic degree can be learned in\\npolylogarithmic time in the \"probably approximately correct\" learning sense.\\n</summary>\\n    <author>\\n      <name>Martin Grohe</name>\\n    </author>\\n    <author>\\n      <name>Martin Ritzert</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1701.05487v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.05487v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.08553v2</id>\\n    <updated>2017-06-09T00:39:57Z</updated>\\n    <published>2017-02-27T21:59:24Z</published>\\n    <title>Diameter-Based Active Learning</title>\\n    <summary>  To date, the tightest upper and lower-bounds for the active learning of\\ngeneral concept classes have been in terms of a parameter of the learning\\nproblem called the splitting index. We provide, for the first time, an\\nefficient algorithm that is able to realize this upper bound, and we\\nempirically demonstrate its good performance.\\n</summary>\\n    <author>\\n      <name>Christopher Tosh</name>\\n    </author>\\n    <author>\\n      <name>Sanjoy Dasgupta</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1702.08553v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.08553v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.01907v2</id>\\n    <updated>2018-06-21T16:57:32Z</updated>\\n    <published>2018-05-04T18:07:21Z</published>\\n    <title>Exploration by Distributional Reinforcement Learning</title>\\n    <summary>  We propose a framework based on distributional reinforcement learning and\\nrecent attempts to combine Bayesian parameter updates with deep reinforcement\\nlearning. We show that our proposed framework conceptually unifies multiple\\nprevious methods in exploration. We also derive a practical algorithm that\\nachieves efficient exploration on challenging control tasks.\\n</summary>\\n    <author>\\n      <name>Yunhao Tang</name>\\n    </author>\\n    <author>\\n      <name>Shipra Agrawal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJCAI 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.01907v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.01907v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.06798v2</id>\\n    <updated>2019-02-03T16:26:40Z</updated>\\n    <published>2018-06-10T08:24:36Z</published>\\n    <title>Implicit Policy for Reinforcement Learning</title>\\n    <summary>  We introduce Implicit Policy, a general class of expressive policies that can\\nflexibly represent complex action distributions in reinforcement learning, with\\nefficient algorithms to compute entropy regularized policy gradients. We\\nempirically show that, despite its simplicity in implementation, entropy\\nregularization combined with a rich policy class can attain desirable\\nproperties displayed under maximum entropy reinforcement learning framework,\\nsuch as robustness and multi-modality.\\n</summary>\\n    <author>\\n      <name>Yunhao Tang</name>\\n    </author>\\n    <author>\\n      <name>Shipra Agrawal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.06798v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.06798v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.06540v1</id>\\n    <updated>2018-07-17T16:35:51Z</updated>\\n    <published>2018-07-17T16:35:51Z</published>\\n    <title>Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try\\n  After Deep Learning</title>\\n    <summary>  We found an easy and quick post-learning method named \"Icing on the Cake\" to\\nenhance a classification performance in deep learning. The method is that we\\ntrain only the final classifier again after an ordinary training is done.\\n</summary>\\n    <author>\\n      <name>Tomohiko Konno</name>\\n    </author>\\n    <author>\\n      <name>Michiaki Iwazume</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.06540v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.06540v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00423v1</id>\\n    <updated>2018-08-01T17:04:48Z</updated>\\n    <published>2018-08-01T17:04:48Z</published>\\n    <title>Seq2Seq and Multi-Task Learning for joint intent and content extraction\\n  for domain specific interpreters</title>\\n    <summary>  This study evaluates the performances of an LSTM network for detecting and\\nextracting the intent and content of com- mands for a financial chatbot. It\\npresents two techniques, sequence to sequence learning and Multi-Task Learning,\\nwhich might improve on the previous task.\\n</summary>\\n    <author>\\n      <name>Marc Velay</name>\\n    </author>\\n    <author>\\n      <name>Fabrice Daniel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.00423v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00423v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.00200v1</id>\\n    <updated>2018-11-01T03:12:26Z</updated>\\n    <published>2018-11-01T03:12:26Z</published>\\n    <title>Online Learning Algorithms for Statistical Arbitrage</title>\\n    <summary>  Statistical arbitrage is a class of financial trading strategies using mean\\nreversion models. The corresponding techniques rely on a number of assumptions\\nwhich may not hold for general non-stationary stochastic processes. This paper\\npresents an alternative technique for statistical arbitrage based on online\\nlearning which does not require such assumptions and which benefits from strong\\nlearning guarantees.\\n</summary>\\n    <author>\\n      <name>Christopher Mohri</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.00200v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.00200v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.04217v1</id>\\n    <updated>2019-02-12T02:23:15Z</updated>\\n    <published>2019-02-12T02:23:15Z</published>\\n    <title>VC Classes are Adversarially Robustly Learnable, but Only Improperly</title>\\n    <summary>  We study the question of learning an adversarially robust predictor. We show\\nthat any hypothesis class $\\\\mathcal{H}$ with finite VC dimension is robustly\\nPAC learnable with an improper learning rule. The requirement of being improper\\nis necessary as we exhibit examples of hypothesis classes $\\\\mathcal{H}$ with\\nfinite VC dimension that are not robustly PAC learnable with any proper\\nlearning rule.\\n</summary>\\n    <author>\\n      <name>Omar Montasser</name>\\n    </author>\\n    <author>\\n      <name>Steve Hanneke</name>\\n    </author>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.04217v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.04217v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.05017v1</id>\\n    <updated>2019-02-13T17:24:35Z</updated>\\n    <published>2019-02-13T17:24:35Z</published>\\n    <title>Differentially Private Learning of Geometric Concepts</title>\\n    <summary>  We present differentially private efficient algorithms for learning union of\\npolygons in the plane (which are not necessarily convex). Our algorithms\\nachieve $(\\\\alpha,\\\\beta)$-PAC learning and $(\\\\epsilon,\\\\delta)$-differential\\nprivacy using a sample of size $\\\\tilde{O}\\\\left(\\\\frac{1}{\\\\alpha\\\\epsilon}k\\\\log\\nd\\\\right)$, where the domain is $[d]\\\\times[d]$ and $k$ is the number of edges in\\nthe union of polygons.\\n</summary>\\n    <author>\\n      <name>Haim Kaplan</name>\\n    </author>\\n    <author>\\n      <name>Yishay Mansour</name>\\n    </author>\\n    <author>\\n      <name>Yossi Matias</name>\\n    </author>\\n    <author>\\n      <name>Uri Stemmer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.05017v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.05017v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.3831v1</id>\\n    <updated>2014-10-14T20:00:09Z</updated>\\n    <published>2014-10-14T20:00:09Z</published>\\n    <title>An exact mapping between the Variational Renormalization Group and Deep\\n  Learning</title>\\n    <summary>  Deep learning is a broad set of techniques that uses multiple layers of\\nrepresentation to automatically learn relevant features directly from\\nstructured data. Recently, such techniques have yielded record-breaking results\\non a diverse set of difficult machine learning tasks in computer vision, speech\\nrecognition, and natural language processing. Despite the enormous success of\\ndeep learning, relatively little is understood theoretically about why these\\ntechniques are so successful at feature learning and compression. Here, we show\\nthat deep learning is intimately related to one of the most important and\\nsuccessful techniques in theoretical physics, the renormalization group (RG).\\nRG is an iterative coarse-graining scheme that allows for the extraction of\\nrelevant features (i.e. operators) as a physical system is examined at\\ndifferent length scales. We construct an exact mapping from the variational\\nrenormalization group, first introduced by Kadanoff, and deep learning\\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\\nOur results suggests that deep learning algorithms may be employing a\\ngeneralized RG-like scheme to learn relevant features from data.\\n</summary>\\n    <author>\\n      <name>Pankaj Mehta</name>\\n    </author>\\n    <author>\\n      <name>David J. Schwab</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.3831v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.3831v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.2505v1</id>\\n    <updated>2013-05-11T13:52:37Z</updated>\\n    <published>2013-05-11T13:52:37Z</published>\\n    <title>On the Generalization Ability of Online Learning Algorithms for Pairwise\\n  Loss Functions</title>\\n    <summary>  In this paper, we study the generalization properties of online learning\\nbased stochastic methods for supervised learning problems where the loss\\nfunction is dependent on more than one training sample (e.g., metric learning,\\nranking). We present a generic decoupling technique that enables us to provide\\nRademacher complexity-based generalization error bounds. Our bounds are in\\ngeneral tighter than those obtained by Wang et al (COLT 2012) for the same\\nproblem. Using our decoupling technique, we are further able to obtain fast\\nconvergence rates for strongly convex pairwise loss functions. We are also able\\nto analyze a class of memory efficient online learning algorithms for pairwise\\nlearning problems that use only a bounded subset of past training samples to\\nupdate the hypothesis at each step. Finally, in order to complement our\\ngeneralization bounds, we propose a novel memory efficient online learning\\nalgorithm for higher order learning problems with bounded regret guarantees.\\n</summary>\\n    <author>\\n      <name>Purushottam Kar</name>\\n    </author>\\n    <author>\\n      <name>Bharath K Sriperumbudur</name>\\n    </author>\\n    <author>\\n      <name>Prateek Jain</name>\\n    </author>\\n    <author>\\n      <name>Harish C Karnick</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in proceedings of the 30th International Conference on\\n  Machine Learning (ICML 2013)</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research, W&amp;CP 28(3) (2013)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1305.2505v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.2505v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.10451v2</id>\\n    <updated>2018-05-31T00:30:35Z</updated>\\n    <published>2018-05-26T09:15:53Z</published>\\n    <title>Geometric Understanding of Deep Learning</title>\\n    <summary>  Deep learning is the mainstream technique for many machine learning tasks,\\nincluding image recognition, machine translation, speech recognition, and so\\non. It has outperformed conventional methods in various fields and achieved\\ngreat successes. Unfortunately, the understanding on how it works remains\\nunclear. It has the central importance to lay down the theoretic foundation for\\ndeep learning.\\n  In this work, we give a geometric view to understand deep learning: we show\\nthat the fundamental principle attributing to the success is the manifold\\nstructure in data, namely natural high dimensional data concentrates close to a\\nlow-dimensional manifold, deep learning learns the manifold and the probability\\ndistribution on it.\\n  We further introduce the concepts of rectified linear complexity for deep\\nneural network measuring its learning capability, rectified linear complexity\\nof an embedding manifold describing the difficulty to be learned. Then we show\\nfor any deep neural network with fixed architecture, there exists a manifold\\nthat cannot be learned by the network. Finally, we propose to apply optimal\\nmass transportation theory to control the probability distribution in the\\nlatent space.\\n</summary>\\n    <author>\\n      <name>Na Lei</name>\\n    </author>\\n    <author>\\n      <name>Zhongxuan Luo</name>\\n    </author>\\n    <author>\\n      <name>Shing-Tung Yau</name>\\n    </author>\\n    <author>\\n      <name>David Xianfeng Gu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.10451v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.10451v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.08701v2</id>\\n    <updated>2019-02-10T07:31:43Z</updated>\\n    <published>2018-02-23T19:11:25Z</published>\\n    <title>Machine learning based hyperspectral image analysis: A survey</title>\\n    <summary>  Hyperspectral sensors enable the study of the chemical properties of scene\\nmaterials remotely for the purpose of identification, detection, and chemical\\ncomposition analysis of objects in the environment. Hence, hyperspectral images\\ncaptured from earth observing satellites and aircraft have been increasingly\\nimportant in agriculture, environmental monitoring, urban planning, mining, and\\ndefense. Machine learning algorithms due to their outstanding predictive power\\nhave become a key tool for modern hyperspectral image analysis. Therefore, a\\nsolid understanding of machine learning techniques have become essential for\\nremote sensing researchers and practitioners. This paper reviews and compares\\nrecent machine learning-based hyperspectral image analysis methods published in\\nliterature. We organize the methods by the image analysis task and by the type\\nof machine learning algorithm, and present a two-way mapping between the image\\nanalysis tasks and the types of machine learning algorithms that can be applied\\nto them. The paper is comprehensive in coverage of both hyperspectral image\\nanalysis tasks and machine learning algorithms. The image analysis tasks\\nconsidered are land cover classification, target detection, unmixing, and\\nphysical parameter estimation. The machine learning algorithms covered are\\nGaussian models, linear regression, logistic regression, support vector\\nmachines, Gaussian mixture model, latent linear models, sparse linear models,\\nGaussian mixture models, ensemble learning, directed graphical models,\\nundirected graphical models, clustering, Gaussian processes, Dirichlet\\nprocesses, and deep learning. We also discuss the open challenges in the field\\nof hyperspectral image analysis and explore possible future directions.\\n</summary>\\n    <author>\\n      <name>Utsav B. Gewali</name>\\n    </author>\\n    <author>\\n      <name>Sildomar T. Monteiro</name>\\n    </author>\\n    <author>\\n      <name>Eli Saber</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.08701v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.08701v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.08305v1</id>\\n    <updated>2017-04-26T19:12:37Z</updated>\\n    <published>2017-04-26T19:12:37Z</published>\\n    <title>Limits of End-to-End Learning</title>\\n    <summary>  End-to-end learning refers to training a possibly complex learning system by\\napplying gradient-based learning to the system as a whole. End-to-end learning\\nsystem is specifically designed so that all modules are differentiable. In\\neffect, not only a central learning machine, but also all \"peripheral\" modules\\nlike representation learning and memory formation are covered by a holistic\\nlearning process. The power of end-to-end learning has been demonstrated on\\nmany tasks, like playing a whole array of Atari video games with a single\\narchitecture. While pushing for solutions to more challenging tasks, network\\narchitectures keep growing more and more complex.\\n  In this paper we ask the question whether and to what extent end-to-end\\nlearning is a future-proof technique in the sense of scaling to complex and\\ndiverse data processing architectures. We point out potential inefficiencies,\\nand we argue in particular that end-to-end learning does not make optimal use\\nof the modular design of present neural networks. Our surprisingly simple\\nexperiments demonstrate these inefficiencies, up to the complete breakdown of\\nlearning.\\n</summary>\\n    <author>\\n      <name>Tobias Glasmachers</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1704.08305v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.08305v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.06885v1</id>\\n    <updated>2017-04-23T05:48:18Z</updated>\\n    <published>2017-04-23T05:48:18Z</published>\\n    <title>A General Theory for Training Learning Machine</title>\\n    <summary>  Though the deep learning is pushing the machine learning to a new stage,\\nbasic theories of machine learning are still limited. The principle of\\nlearning, the role of the a prior knowledge, the role of neuron bias, and the\\nbasis for choosing neural transfer function and cost function, etc., are still\\nfar from clear. In this paper, we present a general theoretical framework for\\nmachine learning. We classify the prior knowledge into common and\\nproblem-dependent parts, and consider that the aim of learning is to maximally\\nincorporate them. The principle we suggested for maximizing the former is the\\ndesign risk minimization principle, while the neural transfer function, the\\ncost function, as well as pretreatment of samples, are endowed with the role\\nfor maximizing the latter. The role of the neuron bias is explained from a\\ndifferent angle. We develop a Monte Carlo algorithm to establish the\\ninput-output responses, and we control the input-output sensitivity of a\\nlearning machine by controlling that of individual neurons. Applications of\\nfunction approaching and smoothing, pattern recognition and classification, are\\nprovided to illustrate how to train general learning machines based on our\\ntheory and algorithm. Our method may in addition induce new applications, such\\nas the transductive inference.\\n</summary>\\n    <author>\\n      <name>Hong Zhao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">55 pages, 18 figures. arXiv admin note: substantial text overlap with\\n  arXiv:1602.03950</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1704.06885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.06885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.08561v3</id>\\n    <updated>2018-02-13T19:20:03Z</updated>\\n    <published>2017-07-26T17:48:25Z</published>\\n    <title>Quantum machine learning: a classical perspective</title>\\n    <summary>  Recently, increased computational power and data availability, as well as\\nalgorithmic advances, have led machine learning techniques to impressive\\nresults in regression, classification, data-generation and reinforcement\\nlearning tasks. Despite these successes, the proximity to the physical limits\\nof chip fabrication alongside the increasing size of datasets are motivating a\\ngrowing number of researchers to explore the possibility of harnessing the\\npower of quantum computation to speed-up classical machine learning algorithms.\\nHere we review the literature in quantum machine learning and discuss\\nperspectives for a mixed readership of classical machine learning and quantum\\ncomputation experts. Particular emphasis will be placed on clarifying the\\nlimitations of quantum algorithms, how they compare with their best classical\\ncounterparts and why quantum resources are expected to provide advantages for\\nlearning problems. Learning in the presence of noise and certain\\ncomputationally hard problems in machine learning are identified as promising\\ndirections for the field. Practical questions, like how to upload classical\\ndata into quantum form, will also be addressed.\\n</summary>\\n    <author>\\n      <name>Carlo Ciliberto</name>\\n    </author>\\n    <author>\\n      <name>Mark Herbster</name>\\n    </author>\\n    <author>\\n      <name>Alessandro Davide Ialongo</name>\\n    </author>\\n    <author>\\n      <name>Massimiliano Pontil</name>\\n    </author>\\n    <author>\\n      <name>Andrea Rocchetto</name>\\n    </author>\\n    <author>\\n      <name>Simone Severini</name>\\n    </author>\\n    <author>\\n      <name>Leonard Wossnig</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1098/rspa.2017.0551</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1098/rspa.2017.0551\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">v3 33 pages; typos corrected and references added</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. R. Soc. A, vol. 474, no. 2209, p. 20170551. The Royal\\n  Society, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1707.08561v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.08561v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.01946v1</id>\\n    <updated>2019-01-24T03:52:53Z</updated>\\n    <published>2019-01-24T03:52:53Z</published>\\n    <title>Thirty Years of Machine Learning:The Road to Pareto-Optimal\\n  Next-Generation Wireless Networks</title>\\n    <summary>  Next-generation wireless networks (NGWN) have a substantial potential in\\nterms of supporting a broad range of complex compelling applications both in\\nmilitary and civilian fields, where the users are able to enjoy high-rate,\\nlow-latency, low-cost and reliable information services. Achieving this\\nambitious goal requires new radio techniques for adaptive learning and\\nintelligent decision making because of the complex heterogeneous nature of the\\nnetwork structures and wireless services. Machine learning algorithms have\\ngreat success in supporting big data analytics, efficient parameter estimation\\nand interactive decision making. Hence, in this article, we review the\\nthirty-year history of machine learning by elaborating on supervised learning,\\nunsupervised learning, reinforcement learning and deep learning, respectively.\\nFurthermore, we investigate their employment in the compelling applications of\\nNGWNs, including heterogeneous networks (HetNets), cognitive radios (CR),\\nInternet of things (IoT), machine to machine networks (M2M), and so on. This\\narticle aims for assisting the readers in clarifying the motivation and\\nmethodology of the various machine learning algorithms, so as to invoke them\\nfor hitherto unexplored services as well as scenarios of future wireless\\nnetworks.\\n</summary>\\n    <author>\\n      <name>Jingjing Wang</name>\\n    </author>\\n    <author>\\n      <name>Chunxiao Jiang</name>\\n    </author>\\n    <author>\\n      <name>Haijun Zhang</name>\\n    </author>\\n    <author>\\n      <name>Yong Ren</name>\\n    </author>\\n    <author>\\n      <name>Kwang-Cheng Chen</name>\\n    </author>\\n    <author>\\n      <name>Lajos Hanzo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">75 pages, 20 figs</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.01946v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.01946v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6418v1</id>\\n    <updated>2012-06-27T19:59:59Z</updated>\\n    <published>2012-06-27T19:59:59Z</published>\\n    <title>Learning Invariant Representations with Local Transformations</title>\\n    <summary>  Learning invariant representations is an important problem in machine\\nlearning and pattern recognition. In this paper, we present a novel framework\\nof transformation-invariant feature learning by incorporating linear\\ntransformations into the feature learning algorithms. For example, we present\\nthe transformation-invariant restricted Boltzmann machine that compactly\\nrepresents data by its weights and their transformations, which achieves\\ninvariance of the feature representation via probabilistic max pooling. In\\naddition, we show that our transformation-invariant feature learning framework\\ncan also be extended to other unsupervised learning methods, such as\\nautoencoders or sparse coding. We evaluate our method on several image\\nclassification benchmark datasets, such as MNIST variations, CIFAR-10, and\\nSTL-10, and show competitive or superior classification performance when\\ncompared to the state-of-the-art. Furthermore, our method achieves\\nstate-of-the-art performance on phone classification tasks with the TIMIT\\ndataset, which demonstrates wide applicability of our proposed algorithms to\\nother domains.\\n</summary>\\n    <author>\\n      <name>Kihyuk Sohn</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Michigan</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Honglak Lee</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Michigan</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6418v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6418v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.01993v2</id>\\n    <updated>2018-07-04T09:32:57Z</updated>\\n    <published>2015-08-09T07:39:24Z</published>\\n    <title>Improving Decision Analytics with Deep Learning: The Case of Financial\\n  Disclosures</title>\\n    <summary>  Decision analytics commonly focuses on the text mining of financial news\\nsources in order to provide managerial decision support and to predict stock\\nmarket movements. Existing predictive frameworks almost exclusively apply\\ntraditional machine learning methods, whereas recent research indicates that\\ntraditional machine learning methods are not sufficiently capable of extracting\\nsuitable features and capturing the non-linear nature of complex tasks. As a\\nremedy, novel deep learning models aim to overcome this issue by extending\\ntraditional neural network models with additional hidden layers. Indeed, deep\\nlearning has been shown to outperform traditional methods in terms of\\npredictive performance. In this paper, we adapt the novel deep learning\\ntechnique to financial decision support. In this instance, we aim to predict\\nthe direction of stock movements following financial disclosures. As a result,\\nwe show how deep learning can outperform the accuracy of random forests as a\\nbenchmark for machine learning by 5.66%.\\n</summary>\\n    <author>\\n      <name>Stefan Feuerriegel</name>\\n    </author>\\n    <author>\\n      <name>Ralph Fehrer</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Twenty-Fourth European Conference on Information Systems (ECIS\\n  2016), Istanbul, Turkey, 2016</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1508.01993v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.01993v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.06324v1</id>\\n    <updated>2018-08-20T06:41:01Z</updated>\\n    <published>2018-08-20T06:41:01Z</published>\\n    <title>PAC-learning is Undecidable</title>\\n    <summary>  The problem of attempting to learn the mapping between data and labels is the\\ncrux of any machine learning task. It is, therefore, of interest to the machine\\nlearning community on practical as well as theoretical counts to consider the\\nexistence of a test or criterion for deciding the feasibility of attempting to\\nlearn. We investigate the existence of such a criterion in the setting of\\nPAC-learning, basing the feasibility solely on whether the mapping to be learnt\\nlends itself to approximation by a given class of hypothesis functions. We show\\nthat no such criterion exists, exposing a fundamental limitation in the\\ndecidability of learning. In other words, we prove that testing for\\nPAC-learnability is undecidable in the Turing sense. We also briefly discuss\\nsome of the probable implications of this result to the current practice of\\nmachine learning.\\n</summary>\\n    <author>\\n      <name>Sairaam Venkatraman</name>\\n    </author>\\n    <author>\\n      <name>S Balasubramanian</name>\\n    </author>\\n    <author>\\n      <name>R Raghunatha Sarma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.06324v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.06324v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.07592v2</id>\\n    <updated>2019-01-24T01:29:07Z</updated>\\n    <published>2019-01-22T19:41:44Z</published>\\n    <title>What Can Machine Learning Teach Us about Communications?</title>\\n    <summary>  Rapid improvements in machine learning over the past decade are beginning to\\nhave far-reaching effects. For communications, engineers with limited domain\\nexpertise can now use off-the-shelf learning packages to design\\nhigh-performance systems based on simulations. Prior to the current revolution\\nin machine learning, the majority of communication engineers were quite aware\\nthat system parameters (such as filter coefficients) could be learned using\\nstochastic gradient descent. It was not at all clear, however, that more\\ncomplicated parts of the system architecture could be learned as well. In this\\npaper, we discuss the application of machine-learning techniques to two\\ncommunications problems and focus on what can be learned from the resulting\\nsystems. We were pleasantly surprised that the observed gains in one example\\nhave a simple explanation that only became clear in hindsight. In essence, deep\\nlearning discovered a simple and effective strategy that had not been\\nconsidered earlier.\\n</summary>\\n    <author>\\n      <name>Mengke Lian</name>\\n    </author>\\n    <author>\\n      <name>Christian H\\xc3\\xa4ger</name>\\n    </author>\\n    <author>\\n      <name>Henry D. Pfister</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 4 figures, paper presented at ITW 2018, corrected version\\n  and updated reference list</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.07592v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.07592v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.11233v1</id>\\n    <updated>2019-02-28T17:30:46Z</updated>\\n    <published>2019-02-28T17:30:46Z</published>\\n    <title>The principles of adaptation in organisms and machines I: machine\\n  learning, information theory, and thermodynamics</title>\\n    <summary>  How do organisms recognize their environment by acquiring knowledge about the\\nworld, and what actions do they take based on this knowledge? This article\\nexamines hypotheses about organisms\\' adaptation to the environment from machine\\nlearning, information-theoretic, and thermodynamic perspectives. We start with\\nconstructing a hierarchical model of the world as an internal model in the\\nbrain, and review standard machine learning methods to infer causes by\\napproximately learning the model under the maximum likelihood principle. This\\nin turn provides an overview of the free energy principle for an organism, a\\nhypothesis to explain perception and action from the principle of least\\nsurprise. Treating this statistical learning as communication between the world\\nand brain, learning is interpreted as a process to maximize information about\\nthe world. We investigate how the classical theories of perception such as the\\ninfomax principle relates to learning the hierarchical model. We then present\\nan approach to the recognition and learning based on thermodynamics, showing\\nthat adaptation by causal learning results in the second law of thermodynamics\\nwhereas inference dynamics that fuses observation with prior knowledge forms a\\nthermodynamic process. These provide a unified view on the adaptation of\\norganisms to the environment.\\n</summary>\\n    <author>\\n      <name>Hideaki Shimazaki</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.11233v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.11233v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0201009v1</id>\\n    <updated>2002-01-14T18:38:55Z</updated>\\n    <published>2002-01-14T18:38:55Z</published>\\n    <title>The performance of the batch learner algorithm</title>\\n    <summary>  We analyze completely the convergence speed of the \\\\emph{batch learning\\nalgorithm}, and compare its speed to that of the memoryless learning algorithm\\nand of learning with memory. We show that the batch learning algorithm is never\\nworse than the memoryless learning algorithm (at least asymptotically). Its\\nperformance \\\\emph{vis-a-vis} learning with full memory is less clearcut, and\\ndepends on certain probabilistic assumptions.\\n</summary>\\n    <author>\\n      <name>Igor Rivin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Supercedes a part of cs.LG/0107033</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0201009v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0201009v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.01756v1</id>\\n    <updated>2018-06-05T15:50:30Z</updated>\\n    <published>2018-06-05T15:50:30Z</published>\\n    <title>Concept-Oriented Deep Learning</title>\\n    <summary>  Concepts are the foundation of human deep learning, understanding, and\\nknowledge integration and transfer. We propose concept-oriented deep learning\\n(CODL) which extends (machine) deep learning with concept representations and\\nconceptual understanding capability. CODL addresses some of the major\\nlimitations of deep learning: interpretability, transferability, contextual\\nadaptation, and requirement for lots of labeled training data. We discuss the\\nmajor aspects of CODL including concept graph, concept representations, concept\\nexemplars, and concept representation learning systems supporting incremental\\nand continual learning.\\n</summary>\\n    <author>\\n      <name>Daniel T Chang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.01756v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.01756v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0205070v1</id>\\n    <updated>2002-05-28T02:01:55Z</updated>\\n    <published>2002-05-28T02:01:55Z</published>\\n    <title>Thumbs up? Sentiment Classification using Machine Learning Techniques</title>\\n    <summary>  We consider the problem of classifying documents not by topic, but by overall\\nsentiment, e.g., determining whether a review is positive or negative. Using\\nmovie reviews as data, we find that standard machine learning techniques\\ndefinitively outperform human-produced baselines. However, the three machine\\nlearning methods we employed (Naive Bayes, maximum entropy classification, and\\nsupport vector machines) do not perform as well on sentiment classification as\\non traditional topic-based categorization. We conclude by examining factors\\nthat make the sentiment classification problem more challenging.\\n</summary>\\n    <author>\\n      <name>Bo Pang</name>\\n    </author>\\n    <author>\\n      <name>Lillian Lee</name>\\n    </author>\\n    <author>\\n      <name>Shivakumar Vaithyanathan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in EMNLP-2002</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0205070v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0205070v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.7; I.2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.2514v1</id>\\n    <updated>2012-10-19T15:08:24Z</updated>\\n    <published>2012-10-19T15:08:24Z</published>\\n    <title>Boltzmann Machine Learning with the Latent Maximum Entropy Principle</title>\\n    <summary>  We present a new statistical learning paradigm for Boltzmann machines based\\non a new inference principle we have proposed: the latent maximum entropy\\nprinciple (LME). LME is different both from Jaynes maximum entropy principle\\nand from standard maximum likelihood estimation.We demonstrate the LME\\nprinciple BY deriving new algorithms for Boltzmann machine parameter\\nestimation, and show how robust and fast new variant of the EM algorithm can be\\ndeveloped.Our experiments show that estimation based on LME generally yields\\nbetter results than maximum likelihood estimation, particularly when inferring\\nhidden units from small amounts of data.\\n</summary>\\n    <author>\\n      <name>Shaojun Wang</name>\\n    </author>\\n    <author>\\n      <name>Dale Schuurmans</name>\\n    </author>\\n    <author>\\n      <name>Fuchun Peng</name>\\n    </author>\\n    <author>\\n      <name>Yunxin Zhao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Nineteenth Conference on Uncertainty in\\n  Artificial Intelligence (UAI2003)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1212.2514v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.2514v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.10600v1</id>\\n    <updated>2017-10-29T12:17:19Z</updated>\\n    <published>2017-10-29T12:17:19Z</published>\\n    <title>Regularization approaches for support vector machines with applications\\n  to biomedical data</title>\\n    <summary>  The support vector machine (SVM) is a widely used machine learning tool for\\nclassification based on statistical learning theory. Given a set of training\\ndata, the SVM finds a hyperplane that separates two different classes of data\\npoints by the largest distance. While the standard form of SVM uses L2-norm\\nregularization, other regularization approaches are particularly attractive for\\nbiomedical datasets where, for example, sparsity and interpretability of the\\nclassifier\\'s coefficient values are highly desired features. Therefore, in this\\npaper we consider different types of regularization approaches for SVMs, and\\nexplore them in both synthetic and real biomedical datasets.\\n</summary>\\n    <author>\\n      <name>Daniel Lopez-Martinez</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1710.10600v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.10600v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.03202v1</id>\\n    <updated>2019-02-17T15:04:35Z</updated>\\n    <published>2019-02-17T15:04:35Z</published>\\n    <title>Nowcasting Recessions using the SVM Machine Learning Algorithm</title>\\n    <summary>  We introduce a novel application of Support Vector Machines (SVM), an\\nimportant Machine Learning algorithm, to determine the beginning and end of\\nrecessions in real time. Nowcasting, \"forecasting\" a condition about the\\npresent time because the full information about it is not available until\\nlater, is key for recessions, which are only determined months after the fact.\\nWe show that SVM has excellent predictive performance for this task, and we\\nprovide implementation details to facilitate its use in similar problems in\\neconomics and finance.\\n</summary>\\n    <author>\\n      <name>Alexander James</name>\\n    </author>\\n    <author>\\n      <name>Yaser S. Abu-Mostafa</name>\\n    </author>\\n    <author>\\n      <name>Xiao Qiao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.03202v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.03202v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-fin.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"econ.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.EC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.01631v1</id>\\n    <updated>2019-03-24T00:18:21Z</updated>\\n    <published>2019-03-24T00:18:21Z</published>\\n    <title>TonY: An Orchestrator for Distributed Machine Learning Jobs</title>\\n    <summary>  Training machine learning (ML) models on large datasets requires considerable\\ncomputing power. To speed up training, it is typical to distribute training\\nacross several machines, often with specialized hardware like GPUs or TPUs.\\nManaging a distributed training job is complex and requires dealing with\\nresource contention, distributed configurations, monitoring, and fault\\ntolerance. In this paper, we describe TonY, an open-source orchestrator for\\ndistributed ML jobs built at LinkedIn to address these challenges.\\n</summary>\\n    <author>\\n      <name>Anthony Hsu</name>\\n    </author>\\n    <author>\\n      <name>Keqiu Hu</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Hung</name>\\n    </author>\\n    <author>\\n      <name>Arun Suresh</name>\\n    </author>\\n    <author>\\n      <name>Zhe Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, to be published in OpML \\'19</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.01631v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.01631v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.09835v2</id>\\n    <updated>2017-09-28T15:59:41Z</updated>\\n    <published>2017-07-31T13:08:11Z</published>\\n    <title>Meta-SGD: Learning to Learn Quickly for Few-Shot Learning</title>\\n    <summary>  Few-shot learning is challenging for learning algorithms that learn each task\\nin isolation and from scratch. In contrast, meta-learning learns from many\\nrelated tasks a meta-learner that can learn a new task more accurately and\\nfaster with fewer examples, where the choice of meta-learners is crucial. In\\nthis paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner\\nthat can initialize and adapt any differentiable learner in just one step, on\\nboth supervised learning and reinforcement learning. Compared to the popular\\nmeta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and\\ncan be learned more efficiently. Compared to the latest meta-learner MAML,\\nMeta-SGD has a much higher capacity by learning to learn not just the learner\\ninitialization, but also the learner update direction and learning rate, all in\\na single meta-learning process. Meta-SGD shows highly competitive performance\\nfor few-shot learning on regression, classification, and reinforcement\\nlearning.\\n</summary>\\n    <author>\\n      <name>Zhenguo Li</name>\\n    </author>\\n    <author>\\n      <name>Fengwei Zhou</name>\\n    </author>\\n    <author>\\n      <name>Fei Chen</name>\\n    </author>\\n    <author>\\n      <name>Hang Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">reinforcement learning included, 20-way classification on\\n  MiniImagenet included</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.09835v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.09835v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07507v1</id>\\n    <updated>2018-05-19T03:40:52Z</updated>\\n    <published>2018-05-19T03:40:52Z</published>\\n    <title>Reconciled Polynomial Machine: A Unified Representation of Shallow and\\n  Deep Learning Models</title>\\n    <summary>  In this paper, we aim at introducing a new machine learning model, namely\\nreconciled polynomial machine, which can provide a unified representation of\\nexisting shallow and deep machine learning models. Reconciled polynomial\\nmachine predicts the output by computing the inner product of the feature\\nkernel function and variable reconciling function. Analysis of several concrete\\nmodels, including Linear Models, FM, MVM, Perceptron, MLP and Deep Neural\\nNetworks, will be provided in this paper, which can all be reduced to the\\nreconciled polynomial machine representations. Detailed analysis of the\\nlearning error by these models will also be illustrated in this paper based on\\ntheir reduced representations from the function approximation perspective.\\n</summary>\\n    <author>\\n      <name>Jiawei Zhang</name>\\n    </author>\\n    <author>\\n      <name>Limeng Cui</name>\\n    </author>\\n    <author>\\n      <name>Fisher B. Gouza</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.07507v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07507v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1504.04646v1</id>\\n    <updated>2015-04-17T22:05:34Z</updated>\\n    <published>2015-04-17T22:05:34Z</published>\\n    <title>Performance Evaluation of Machine Learning Algorithms in Post-operative\\n  Life Expectancy in the Lung Cancer Patients</title>\\n    <summary>  The nature of clinical data makes it difficult to quickly select, tune and\\napply machine learning algorithms to clinical prognosis. As a result, a lot of\\ntime is spent searching for the most appropriate machine learning algorithms\\napplicable in clinical prognosis that contains either binary-valued or\\nmulti-valued attributes. The study set out to identify and evaluate the\\nperformance of machine learning classification schemes applied in clinical\\nprognosis of post-operative life expectancy in the lung cancer patients.\\nMultilayer Perceptron, J48, and the Naive Bayes algorithms were used to train\\nand test models on Thoracic Surgery datasets obtained from the University of\\nCalifornia Irvine machine learning repository. Stratified 10-fold\\ncross-validation was used to evaluate baseline performance accuracy of the\\nclassifiers. The comparative analysis shows that multilayer perceptron\\nperformed best with classification accuracy of 82.3%, J48 came out second with\\nclassification accuracy of 81.8%, and Naive Bayes came out the worst with\\nclassification accuracy of 74.4%. The quality and outcome of the chosen machine\\nlearning algorithms depends on the ingenuity of the clinical miner.\\n</summary>\\n    <author>\\n      <name>Kwetishe Joro Danjuma</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 3 figures, 2 tables, ISSN (Print): 1694-0814 | ISSN\\n  (Online): 1694-0784</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IJCSI International Journal of Computer Science Issues, Volume 12,\\n  Issue 2, March 2015</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1504.04646v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1504.04646v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.04838v3</id>\\n    <updated>2018-02-08T20:40:22Z</updated>\\n    <published>2016-06-15T16:15:53Z</published>\\n    <title>Optimization Methods for Large-Scale Machine Learning</title>\\n    <summary>  This paper provides a review and commentary on the past, present, and future\\nof numerical optimization algorithms in the context of machine learning\\napplications. Through case studies on text classification and the training of\\ndeep neural networks, we discuss how optimization problems arise in machine\\nlearning and what makes them challenging. A major theme of our study is that\\nlarge-scale machine learning represents a distinctive setting in which the\\nstochastic gradient (SG) method has traditionally played a central role while\\nconventional gradient-based nonlinear optimization techniques typically falter.\\nBased on this viewpoint, we present a comprehensive theory of a\\nstraightforward, yet versatile SG algorithm, discuss its practical behavior,\\nand highlight opportunities for designing algorithms with improved performance.\\nThis leads to a discussion about the next generation of optimization methods\\nfor large-scale machine learning, including an investigation of two main\\nstreams of research on techniques that diminish noise in the stochastic\\ndirections and methods that make use of second-order derivative approximations.\\n</summary>\\n    <author>\\n      <name>L\\xc3\\xa9on Bottou</name>\\n    </author>\\n    <author>\\n      <name>Frank E. Curtis</name>\\n    </author>\\n    <author>\\n      <name>Jorge Nocedal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1606.04838v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.04838v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.07292v1</id>\\n    <updated>2016-03-23T18:30:37Z</updated>\\n    <published>2016-03-23T18:30:37Z</published>\\n    <title>Debugging Machine Learning Tasks</title>\\n    <summary>  Unlike traditional programs (such as operating systems or word processors)\\nwhich have large amounts of code, machine learning tasks use programs with\\nrelatively small amounts of code (written in machine learning libraries), but\\nvoluminous amounts of data. Just like developers of traditional programs debug\\nerrors in their code, developers of machine learning tasks debug and fix errors\\nin their data. However, algorithms and tools for debugging and fixing errors in\\ndata are less common, when compared to their counterparts for detecting and\\nfixing errors in code. In this paper, we consider classification tasks where\\nerrors in training data lead to misclassifications in test points, and propose\\nan automated method to find the root causes of such misclassifications. Our\\nroot cause analysis is based on Pearl\\'s theory of causation, and uses Pearl\\'s\\nPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\\nencodes the computation of PS as a probabilistic program, and uses recent work\\non probabilistic programs and transformations on probabilistic programs (along\\nwith gray-box models of machine learning algorithms) to efficiently compute PS.\\nPsi is able to identify root causes of data errors in interesting data sets.\\n</summary>\\n    <author>\\n      <name>Aleksandar Chakarov</name>\\n    </author>\\n    <author>\\n      <name>Aditya Nori</name>\\n    </author>\\n    <author>\\n      <name>Sriram Rajamani</name>\\n    </author>\\n    <author>\\n      <name>Shayak Sen</name>\\n    </author>\\n    <author>\\n      <name>Deepak Vijaykeerthy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1603.07292v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.07292v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"D.2.5; I.2.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.06064v1</id>\\n    <updated>2015-02-21T04:29:41Z</updated>\\n    <published>2015-02-21T04:29:41Z</published>\\n    <title>MILJS : Brand New JavaScript Libraries for Matrix Calculation and\\n  Machine Learning</title>\\n    <summary>  MILJS is a collection of state-of-the-art, platform-independent, scalable,\\nfast JavaScript libraries for matrix calculation and machine learning. Our core\\nlibrary offering a matrix calculation is called Sushi, which exhibits far\\nbetter performance than any other leading machine learning libraries written in\\nJavaScript. Especially, our matrix multiplication is 177 times faster than the\\nfastest JavaScript benchmark. Based on Sushi, a machine learning library called\\nTempura is provided, which supports various algorithms widely used in machine\\nlearning research. We also provide Soba as a visualization library. The\\nimplementations of our libraries are clearly written, properly documented and\\nthus can are easy to get started with, as long as there is a web browser. These\\nlibraries are available from http://mil-tokyo.github.io/ under the MIT license.\\n</summary>\\n    <author>\\n      <name>Ken Miura</name>\\n    </author>\\n    <author>\\n      <name>Tetsuaki Mano</name>\\n    </author>\\n    <author>\\n      <name>Atsushi Kanehira</name>\\n    </author>\\n    <author>\\n      <name>Yuichiro Tsuchiya</name>\\n    </author>\\n    <author>\\n      <name>Tatsuya Harada</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.06064v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.06064v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.07096v1</id>\\n    <updated>2015-08-28T05:24:06Z</updated>\\n    <published>2015-08-28T05:24:06Z</published>\\n    <title>Partitioning Large Scale Deep Belief Networks Using Dropout</title>\\n    <summary>  Deep learning methods have shown great promise in many practical\\napplications, ranging from speech recognition, visual object recognition, to\\ntext processing. However, most of the current deep learning methods suffer from\\nscalability problems for large-scale applications, forcing researchers or users\\nto focus on small-scale problems with fewer parameters.\\n  In this paper, we consider a well-known machine learning model, deep belief\\nnetworks (DBNs) that have yielded impressive classification performance on a\\nlarge number of benchmark machine learning tasks. To scale up DBN, we propose\\nan approach that can use the computing clusters in a distributed environment to\\ntrain large models, while the dense matrix computations within a single machine\\nare sped up using graphics processors (GPU). When training a DBN, each machine\\nrandomly drops out a portion of neurons in each hidden layer, for each training\\ncase, making the remaining neurons only learn to detect features that are\\ngenerally helpful for producing the correct answer. Within our approach, we\\nhave developed four methods to combine outcomes from each machine to form a\\nunified model. Our preliminary experiment on the mnst handwritten digit\\ndatabase demonstrates that our approach outperforms the state of the art test\\nerror rate.\\n</summary>\\n    <author>\\n      <name>Yanping Huang</name>\\n    </author>\\n    <author>\\n      <name>Sai Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1207.0580 by other authors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1508.07096v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.07096v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.01293v2</id>\\n    <updated>2017-05-04T07:03:28Z</updated>\\n    <published>2017-01-05T12:33:19Z</published>\\n    <title>OpenML: An R Package to Connect to the Machine Learning Platform OpenML</title>\\n    <summary>  OpenML is an online machine learning platform where researchers can easily\\nshare data, machine learning tasks and experiments as well as organize them\\nonline to work and collaborate more efficiently. In this paper, we present an R\\npackage to interface with the OpenML platform and illustrate its usage in\\ncombination with the machine learning R package mlr. We show how the OpenML\\npackage allows R users to easily search, download and upload data sets and\\nmachine learning tasks. Furthermore, we also show how to upload results of\\nexperiments, share them with others and download results from other users.\\nBeyond ensuring reproducibility of results, the OpenML platform automates much\\nof the drudge work, speeds up research, facilitates collaboration and increases\\nthe users\\' visibility online.\\n</summary>\\n    <author>\\n      <name>Giuseppe Casalicchio</name>\\n    </author>\\n    <author>\\n      <name>Jakob Bossek</name>\\n    </author>\\n    <author>\\n      <name>Michel Lang</name>\\n    </author>\\n    <author>\\n      <name>Dominik Kirchhoff</name>\\n    </author>\\n    <author>\\n      <name>Pascal Kerschke</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Hofner</name>\\n    </author>\\n    <author>\\n      <name>Heidi Seibold</name>\\n    </author>\\n    <author>\\n      <name>Joaquin Vanschoren</name>\\n    </author>\\n    <author>\\n      <name>Bernd Bischl</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s00180-017-0742-2</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s00180-017-0742-2\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1701.01293v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.01293v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.04739v1</id>\\n    <updated>2017-01-17T15:59:17Z</updated>\\n    <published>2017-01-17T15:59:17Z</published>\\n    <title>Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning</title>\\n    <summary>  Governments and businesses increasingly rely on data analytics and machine\\nlearning (ML) for improving their competitive edge in areas such as consumer\\nsatisfaction, threat intelligence, decision making, and product efficiency.\\nHowever, by cleverly corrupting a subset of data used as input to a target\\'s ML\\nalgorithms, an adversary can perturb outcomes and compromise the effectiveness\\nof ML technology. While prior work in the field of adversarial machine learning\\nhas studied the impact of input manipulation on correct ML algorithms, we\\nconsider the exploitation of bugs in ML implementations. In this paper, we\\ncharacterize the attack surface of ML programs, and we show that malicious\\ninputs exploiting implementation bugs enable strictly more powerful attacks\\nthan the classic adversarial machine learning techniques. We propose a\\nsemi-automated technique, called steered fuzzing, for exploring this attack\\nsurface and for discovering exploitable bugs in machine learning programs, in\\norder to demonstrate the magnitude of this threat. As a result of our work, we\\nresponsibly disclosed five vulnerabilities, established three new CVE-IDs, and\\nilluminated a common insecure practice across many machine learning systems.\\nFinally, we outline several research directions for further understanding and\\nmitigating this threat.\\n</summary>\\n    <author>\\n      <name>Rock Stevens</name>\\n    </author>\\n    <author>\\n      <name>Octavian Suciu</name>\\n    </author>\\n    <author>\\n      <name>Andrew Ruef</name>\\n    </author>\\n    <author>\\n      <name>Sanghyun Hong</name>\\n    </author>\\n    <author>\\n      <name>Michael Hicks</name>\\n    </author>\\n    <author>\\n      <name>Tudor Dumitra\\xc5\\x9f</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1701.04739v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.04739v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.00564v2</id>\\n    <updated>2018-04-07T01:48:13Z</updated>\\n    <published>2017-05-01T15:47:55Z</published>\\n    <title>Attacking Machine Learning models as part of a cyber kill chain</title>\\n    <summary>  Machine learning is gaining popularity in the network security domain as many\\nmore network-enabled devices get connected, as malicious activities become\\nstealthier, and as new technologies like Software Defined Networking emerge.\\nCompromising machine learning model is a desirable goal. In fact, spammers have\\nbeen quite successful getting through machine learning enabled spam filters for\\nyears. While previous works have been done on adversarial machine learning,\\nnone has been considered within a defense-in-depth environment, in which\\ncorrect classification alone may not be good enough. For the first time, this\\npaper proposes a cyber kill-chain for attacking machine learning models\\ntogether with a proof of concept. The intention is to provide a high level\\nattack model that inspire more secure processes in\\nresearch/design/implementation of machine learning based security solutions.\\n</summary>\\n    <author>\\n      <name>Tam N. Nguyen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.00564v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.00564v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00198v1</id>\\n    <updated>2018-08-01T07:13:16Z</updated>\\n    <published>2018-08-01T07:13:16Z</published>\\n    <title>Towards Machine Learning on data from Professional Cyclists</title>\\n    <summary>  Professional sports are developing towards increasingly scientific training\\nmethods with increasing amounts of data being collected from laboratory tests,\\ntraining sessions and competitions. In cycling, it is standard to equip\\nbicycles with small computers recording data from sensors such as power-meters,\\nin addition to heart-rate, speed, altitude etc. Recently, machine learning\\ntechniques have provided huge success in a wide variety of areas where large\\namounts of data (big data) is available. In this paper, we perform a pilot\\nexperiment on machine learning to model physical response in elite cyclists. As\\na first experiment, we show that it is possible to train a LSTM machine\\nlearning algorithm to predict the heart-rate response of a cyclist during a\\ntraining session. This work is a promising first step towards developing more\\nelaborate models based on big data and machine learning to capture performance\\naspects of athletes.\\n</summary>\\n    <author>\\n      <name>Agrin Hilmkil</name>\\n    </author>\\n    <author>\\n      <name>Oscar Ivarsson</name>\\n    </author>\\n    <author>\\n      <name>Moa Johansson</name>\\n    </author>\\n    <author>\\n      <name>Dan Kuylenstierna</name>\\n    </author>\\n    <author>\\n      <name>Teun van Erp</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for the 12th World Congress on Performance Analysis of\\n  Sports, Opatija, Croatia, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.00198v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00198v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.01095v1</id>\\n    <updated>2018-08-03T06:02:46Z</updated>\\n    <published>2018-08-03T06:02:46Z</published>\\n    <title>Helix: Accelerating Human-in-the-loop Machine Learning</title>\\n    <summary>  Data application developers and data scientists spend an inordinate amount of\\ntime iterating on machine learning (ML) workflows -- by modifying the data\\npre-processing, model training, and post-processing steps -- via\\ntrial-and-error to achieve the desired model performance. Existing work on\\naccelerating machine learning focuses on speeding up one-shot execution of\\nworkflows, failing to address the incremental and dynamic nature of typical ML\\ndevelopment. We propose Helix, a declarative machine learning system that\\naccelerates iterative development by optimizing workflow execution end-to-end\\nand across iterations. Helix minimizes the runtime per iteration via program\\nanalysis and intelligent reuse of previous results, which are selectively\\nmaterialized -- trading off the cost of materialization for potential future\\nbenefits -- to speed up future iterations. Additionally, Helix offers a\\ngraphical interface to visualize workflow DAGs and compare versions to\\nfacilitate iterative development. Through two ML applications, in\\nclassification and in structured prediction, attendees will experience the\\nsuccinctness of Helix programming interface and the speed and ease of iterative\\ndevelopment using Helix. In our evaluations, Helix achieved up to an order of\\nmagnitude reduction in cumulative run time compared to state-of-the-art machine\\nlearning tools.\\n</summary>\\n    <author>\\n      <name>Doris Xin</name>\\n    </author>\\n    <author>\\n      <name>Litian Ma</name>\\n    </author>\\n    <author>\\n      <name>Jialin Liu</name>\\n    </author>\\n    <author>\\n      <name>Stephen Macke</name>\\n    </author>\\n    <author>\\n      <name>Shuchen Song</name>\\n    </author>\\n    <author>\\n      <name>Aditya Parameswaran</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.14778/3229863.3236234</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.14778/3229863.3236234\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1808.01095v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.01095v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.11674v1</id>\\n    <updated>2018-11-26T15:38:29Z</updated>\\n    <published>2018-11-26T15:38:29Z</published>\\n    <title>Interlacing Personal and Reference Genomes for Machine Learning\\n  Disease-Variant Detection</title>\\n    <summary>  DNA sequencing to identify genetic variants is becoming increasingly valuable\\nin clinical settings. Assessment of variants in such sequencing data is\\ncommonly implemented through Bayesian heuristic algorithms. Machine learning\\nhas shown great promise in improving on these variant calls, but the input for\\nthese is still a standardized \"pile-up\" image, which is not always best suited.\\nIn this paper, we present a novel method for generating images from DNA\\nsequencing data, which interlaces the human reference genome with personalized\\nsequencing output, to maximize usage of sequencing reads and improve machine\\nlearning algorithm performance. We demonstrate the success of this in improving\\nstandard germline variant calling. We also furthered this approach to include\\nsomatic variant calling across tumor/normal data with Siamese networks. These\\napproaches can be used in machine learning applications on sequencing data with\\nthe hope of improving clinical outcomes, and are freely available for\\nnoncommercial use at www.ccg.ai.\\n</summary>\\n    <author>\\n      <name>Luke R Harries</name>\\n    </author>\\n    <author>\\n      <name>Suyi Zhang</name>\\n    </author>\\n    <author>\\n      <name>Geoffroy Dubourg-Felonneau</name>\\n    </author>\\n    <author>\\n      <name>James H R Farmery</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Sinai</name>\\n    </author>\\n    <author>\\n      <name>Belle Taylor</name>\\n    </author>\\n    <author>\\n      <name>Nirmesh Patel</name>\\n    </author>\\n    <author>\\n      <name>John W Cassidy</name>\\n    </author>\\n    <author>\\n      <name>John Shawe-Taylor</name>\\n    </author>\\n    <author>\\n      <name>Harry W Clifford</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  arXiv:cs/0101200</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.11674v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.11674v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.00770v1</id>\\n    <updated>2019-01-03T14:37:04Z</updated>\\n    <published>2019-01-03T14:37:04Z</published>\\n    <title>Personalized explanation in machine learning</title>\\n    <summary>  Explanation in machine learning and related fields such as artificial\\nintelligence aims at making machine learning models and their decisions\\nunderstandable to humans. Existing work suggests that personalizing\\nexplanations might help to improve understandability. In this work, we derive a\\nconceptualization of personalized explanation by defining and structuring the\\nproblem based on prior work on machine learning explanation, personalization\\n(in machine learning) and concepts and techniques from other domains such as\\nprivacy and knowledge elicitation. We perform a categorization of explainee\\ninformation used in the process of personalization as well as describing means\\nto collect this information. We also identify three key explanation properties\\nthat are amendable to personalization: complexity, decision information and\\npresentation. We also enhance existing work on explanation by introducing\\nadditional desiderata and measures to quantify the quality of personalized\\nexplanations.\\n</summary>\\n    <author>\\n      <name>Johanes Schneider</name>\\n    </author>\\n    <author>\\n      <name>Joshua Handali</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In Submission</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.00770v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.00770v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.04109v1</id>\\n    <updated>2019-02-11T19:37:40Z</updated>\\n    <published>2019-02-11T19:37:40Z</published>\\n    <title>Applications of Machine Learning in Cryptography: A Survey</title>\\n    <summary>  Machine learning techniques have had a long list of applications in recent\\nyears. However, the use of machine learning in information and network security\\nis not new. Machine learning and cryptography have many things in common. The\\nmost apparent is the processing of large amounts of data and large search\\nspaces. In its varying techniques, machine learning has been an interesting\\nfield of study with massive potential for application. In the past three\\ndecades, machine learning techniques, whether supervised or unsupervised, have\\nbeen applied in cryptographic algorithms, cryptanalysis, steganography, among\\nother data-security-related applications. This paper presents an updated survey\\nof applications of machine learning techniques in cryptography and\\ncryptanalysis. The paper summarizes the research done in these areas and\\nprovides suggestions for future directions in research.\\n</summary>\\n    <author>\\n      <name>Mohammed M. Alani</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3309074.3309092</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3309074.3309092\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1902.04109v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.04109v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.10178v1</id>\\n    <updated>2019-02-26T19:25:11Z</updated>\\n    <published>2019-02-26T19:25:11Z</published>\\n    <title>Unmasking Clever Hans Predictors and Assessing What Machines Really\\n  Learn</title>\\n    <summary>  Current learning machines have successfully solved hard application problems,\\nreaching high accuracy and displaying seemingly \"intelligent\" behavior. Here we\\napply recent techniques for explaining decisions of state-of-the-art learning\\nmachines and analyze various tasks from computer vision and arcade games. This\\nshowcases a spectrum of problem-solving behaviors ranging from naive and\\nshort-sighted, to well-informed and strategic. We observe that standard\\nperformance evaluation metrics can be oblivious to distinguishing these diverse\\nproblem solving behaviors. Furthermore, we propose our semi-automated Spectral\\nRelevance Analysis that provides a practically effective way of characterizing\\nand validating the behavior of nonlinear learning machines. This helps to\\nassess whether a learned model indeed delivers reliably for the problem that it\\nwas conceived for. Furthermore, our work intends to add a voice of caution to\\nthe ongoing excitement about machine intelligence and pledges to evaluate and\\njudge some of these recent successes in a more nuanced manner.\\n</summary>\\n    <author>\\n      <name>Sebastian Lapuschkin</name>\\n    </author>\\n    <author>\\n      <name>Stephan W\\xc3\\xa4ldchen</name>\\n    </author>\\n    <author>\\n      <name>Alexander Binder</name>\\n    </author>\\n    <author>\\n      <name>Gr\\xc3\\xa9goire Montavon</name>\\n    </author>\\n    <author>\\n      <name>Wojciech Samek</name>\\n    </author>\\n    <author>\\n      <name>Klaus-Robert M\\xc3\\xbcller</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1038/s41467-019-08987-4</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1038/s41467-019-08987-4\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for publication in Nature Communications</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.10178v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.10178v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.09493v1</id>\\n    <updated>2019-03-22T13:24:41Z</updated>\\n    <published>2019-03-22T13:24:41Z</published>\\n    <title>The invisible power of fairness. How machine learning shapes democracy</title>\\n    <summary>  Many machine learning systems make extensive use of large amounts of data\\nregarding human behaviors. Several researchers have found various\\ndiscriminatory practices related to the use of human-related machine learning\\nsystems, for example in the field of criminal justice, credit scoring and\\nadvertising. Fair machine learning is therefore emerging as a new field of\\nstudy to mitigate biases that are inadvertently incorporated into algorithms.\\nData scientists and computer engineers are making various efforts to provide\\ndefinitions of fairness. In this paper, we provide an overview of the most\\nwidespread definitions of fairness in the field of machine learning, arguing\\nthat the ideas highlighting each formalization are closely related to different\\nideas of justice and to different interpretations of democracy embedded in our\\nculture. This work intends to analyze the definitions of fairness that have\\nbeen proposed to date to interpret the underlying criteria and to relate them\\nto different ideas of democracy.\\n</summary>\\n    <author>\\n      <name>Elena Beretta</name>\\n    </author>\\n    <author>\\n      <name>Antonio Santangelo</name>\\n    </author>\\n    <author>\\n      <name>Bruno Lepri</name>\\n    </author>\\n    <author>\\n      <name>Antonio Vetr\\xc3\\xb2</name>\\n    </author>\\n    <author>\\n      <name>Juan Carlos De Martin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 1 figure, preprint version, submitted to The 32nd Canadian\\n  Conference on Artificial Intelligence that will take place in Kingston,\\n  Ontario, May 28 to May 31, 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.09493v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.09493v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.12394v1</id>\\n    <updated>2019-03-29T08:37:40Z</updated>\\n    <published>2019-03-29T08:37:40Z</published>\\n    <title>Informed Machine Learning - Towards a Taxonomy of Explicit Integration\\n  of Knowledge into Machine Learning</title>\\n    <summary>  Despite the great successes of machine learning, it can have its limits when\\ndealing with insufficient training data.A potential solution is to incorporate\\nadditional knowledge into the training process which leads to the idea of\\ninformed machine learning. We present a research survey and structured overview\\nof various approaches in this field. We aim to establish a taxonomy which can\\nserve as a classification framework that considers the kind of additional\\nknowledge, its representation,and its integration into the machine learning\\npipeline. The evaluation of numerous papers on the bases of the taxonomy\\nuncovers key methods in this field.\\n</summary>\\n    <author>\\n      <name>Laura von Rueden</name>\\n    </author>\\n    <author>\\n      <name>Sebastian Mayer</name>\\n    </author>\\n    <author>\\n      <name>Jochen Garcke</name>\\n    </author>\\n    <author>\\n      <name>Christian Bauckhage</name>\\n    </author>\\n    <author>\\n      <name>Jannis Schuecker</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.12394v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.12394v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.1490v2</id>\\n    <updated>2014-12-04T22:59:04Z</updated>\\n    <published>2014-11-06T03:51:39Z</published>\\n    <title>Efficient Representations for Life-Long Learning and Autoencoding</title>\\n    <summary>  It has been a long-standing goal in machine learning, as well as in AI more\\ngenerally, to develop life-long learning systems that learn many different\\ntasks over time, and reuse insights from tasks learned, \"learning to learn\" as\\nthey do so. In this work we pose and provide efficient algorithms for several\\nnatural theoretical formulations of this goal. Specifically, we consider the\\nproblem of learning many different target functions over time, that share\\ncertain commonalities that are initially unknown to the learning algorithm. Our\\naim is to learn new internal representations as the algorithm learns new target\\nfunctions, that capture this commonality and allow subsequent learning tasks to\\nbe solved more efficiently and from less data. We develop efficient algorithms\\nfor two very different kinds of commonalities that target functions might\\nshare: one based on learning common low-dimensional and unions of\\nlow-dimensional subspaces and one based on learning nonlinear Boolean\\ncombinations of features. Our algorithms for learning Boolean feature\\ncombinations additionally have a dual interpretation, and can be viewed as\\ngiving an efficient procedure for constructing near-optimal sparse Boolean\\nautoencoders under a natural \"anchor-set\" assumption.\\n</summary>\\n    <author>\\n      <name>Maria-Florina Balcan</name>\\n    </author>\\n    <author>\\n      <name>Avrim Blum</name>\\n    </author>\\n    <author>\\n      <name>Santosh Vempala</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.1490v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.1490v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.09890v1</id>\\n    <updated>2019-01-26T01:55:33Z</updated>\\n    <published>2019-01-26T01:55:33Z</published>\\n    <title>Few-shot Learning with Meta Metric Learners</title>\\n    <summary>  Few-shot Learning aims to learn classifiers for new classes with only a few\\ntraining examples per class. Existing meta-learning or metric-learning based\\nfew-shot learning approaches are limited in handling diverse domains with\\nvarious number of labels. The meta-learning approaches train a meta learner to\\npredict weights of homogeneous-structured task-specific networks, requiring a\\nuniform number of classes across tasks. The metric-learning approaches learn\\none task-invariant metric for all the tasks, and they fail if the tasks\\ndiverge. We propose to deal with these limitations with meta metric learning.\\nOur meta metric learning approach consists of task-specific learners, that\\nexploit metric learning to handle flexible labels, and a meta learner, that\\ndiscovers good parameters and gradient decent to specify the metrics in\\ntask-specific learners. Thus the proposed model is able to handle unbalanced\\nclasses as well as to generate task-specific metrics. We test our approach in\\nthe `$k$-shot $N$-way\\' few-shot learning setting used in previous work and new\\nrealistic few-shot setting with diverse multi-domain tasks and flexible label\\nnumbers. Experiments show that our approach attains superior performances in\\nboth settings.\\n</summary>\\n    <author>\\n      <name>Yu Cheng</name>\\n    </author>\\n    <author>\\n      <name>Mo Yu</name>\\n    </author>\\n    <author>\\n      <name>Xiaoxiao Guo</name>\\n    </author>\\n    <author>\\n      <name>Bowen Zhou</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in NIPS 2017 workshop on Meta-Learning, arXiv version</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.09890v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.09890v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.07854v1</id>\\n    <updated>2019-03-19T06:16:56Z</updated>\\n    <published>2019-03-19T06:16:56Z</published>\\n    <title>Hindsight Generative Adversarial Imitation Learning</title>\\n    <summary>  Compared to reinforcement learning, imitation learning (IL) is a powerful\\nparadigm for training agents to learn control policies efficiently from expert\\ndemonstrations. However, in most cases, obtaining demonstration data is costly\\nand laborious, which poses a significant challenge in some scenarios. A\\npromising alternative is to train agent learning skills via imitation learning\\nwithout expert demonstrations, which, to some extent, would extremely expand\\nimitation learning areas. To achieve such expectation, in this paper, we\\npropose Hindsight Generative Adversarial Imitation Learning (HGAIL) algorithm,\\nwith the aim of achieving imitation learning satisfying no need of\\ndemonstrations. Combining hindsight idea with the generative adversarial\\nimitation learning (GAIL) framework, we realize implementing imitation learning\\nsuccessfully in cases of expert demonstration data are not available.\\nExperiments show that the proposed method can train policies showing comparable\\nperformance to current imitation learning methods. Further more, HGAIL\\nessentially endows curriculum learning mechanism which is critical for learning\\npolicies.\\n</summary>\\n    <author>\\n      <name>Naijun Liu</name>\\n    </author>\\n    <author>\\n      <name>Tao Lu</name>\\n    </author>\\n    <author>\\n      <name>Yinghao Cai</name>\\n    </author>\\n    <author>\\n      <name>Boyao Li</name>\\n    </author>\\n    <author>\\n      <name>Shuo Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.07854v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.07854v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.05664v1</id>\\n    <updated>2016-05-31T09:54:46Z</updated>\\n    <published>2016-05-31T09:54:46Z</published>\\n    <title>Linear Classification of data with Support Vector Machines and\\n  Generalized Support Vector Machines</title>\\n    <summary>  In this paper, we study the support vector machine and introduced the notion\\nof generalized support vector machine for classification of data. We show that\\nthe problem of generalized support vector machine is equivalent to the problem\\nof generalized variational inequality and establish various results for the\\nexistence of solutions. Moreover, we provide various examples to support our\\nresults.\\n</summary>\\n    <author>\\n      <name>Xiaomin Qi</name>\\n    </author>\\n    <author>\\n      <name>Sergei Silvestrov</name>\\n    </author>\\n    <author>\\n      <name>Talat Nazir</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.4972718</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.4972718\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">submitted</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.05664v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.05664v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1602.02823v1</id>\\n    <updated>2016-02-09T00:14:03Z</updated>\\n    <published>2016-02-09T00:14:03Z</published>\\n    <title>Poor starting points in machine learning</title>\\n    <summary>  Poor (even random) starting points for learning/training/optimization are\\ncommon in machine learning. In many settings, the method of Robbins and Monro\\n(online stochastic gradient descent) is known to be optimal for good starting\\npoints, but may not be optimal for poor starting points -- indeed, for poor\\nstarting points Nesterov acceleration can help during the initial iterations,\\neven though Nesterov methods not designed for stochastic approximation could\\nhurt during later iterations. The common practice of training with nontrivial\\nminibatches enhances the advantage of Nesterov acceleration.\\n</summary>\\n    <author>\\n      <name>Mark Tygert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 3 figures, 1 table; this initial version is literally\\n  identical to that circulated among a restricted audience over a month ago</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1602.02823v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.02823v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.4608v1</id>\\n    <updated>2012-06-18T14:44:28Z</updated>\\n    <published>2012-06-18T14:44:28Z</published>\\n    <title>A Hybrid Algorithm for Convex Semidefinite Optimization</title>\\n    <summary>  We present a hybrid algorithm for optimizing a convex, smooth function over\\nthe cone of positive semidefinite matrices. Our algorithm converges to the\\nglobal optimal solution and can be used to solve general large-scale\\nsemidefinite programs and hence can be readily applied to a variety of machine\\nlearning problems. We show experimental results on three machine learning\\nproblems (matrix completion, metric learning, and sparse PCA) . Our approach\\noutperforms state-of-the-art algorithms.\\n</summary>\\n    <author>\\n      <name>Soeren Laue</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Friedrich-Schiller-University</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.4608v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.4608v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.01605v1</id>\\n    <updated>2017-04-05T18:49:56Z</updated>\\n    <published>2017-04-05T18:49:56Z</published>\\n    <title>Nonnegative/binary matrix factorization with a D-Wave quantum annealer</title>\\n    <summary>  D-Wave quantum annealers represent a novel computational architecture and\\nhave attracted significant interest, but have been used for few real-world\\ncomputations. Machine learning has been identified as an area where quantum\\nannealing may be useful. Here, we show that the D-Wave 2X can be effectively\\nused as part of an unsupervised machine learning method. This method can be\\nused to analyze large datasets. The D-Wave only limits the number of features\\nthat can be extracted from the dataset. We apply this method to learn the\\nfeatures from a set of facial images.\\n</summary>\\n    <author>\\n      <name>Daniel O\\'Malley</name>\\n    </author>\\n    <author>\\n      <name>Velimir V. Vesselinov</name>\\n    </author>\\n    <author>\\n      <name>Boian S. Alexandrov</name>\\n    </author>\\n    <author>\\n      <name>Ludmil B. Alexandrov</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1371/journal.pone.0206653</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1371/journal.pone.0206653\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1704.01605v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.01605v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1510.02674v1</id>\\n    <updated>2015-10-09T14:00:48Z</updated>\\n    <published>2015-10-09T14:00:48Z</published>\\n    <title>Technical Report of Participation in Higgs Boson Machine Learning\\n  Challenge</title>\\n    <summary>  This report entails the detailed description of the approach and\\nmethodologies taken as part of competing in the Higgs Boson Machine Learning\\nCompetition hosted by Kaggle Inc. and organized by CERN et al. It briefly\\ndescribes the theoretical background of the problem and the motivation for\\ntaking part in the competition. Furthermore, the various machine learning\\nmodels and algorithms analyzed and implemented during the 4 month period of\\nparticipation are discussed and compared. Special attention is paid to the Deep\\nLearning techniques and architectures implemented from scratch using Python and\\nNumPy for this competition.\\n</summary>\\n    <author>\\n      <name>S. Raza Ahmad</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1510.02674v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1510.02674v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.03196v1</id>\\n    <updated>2017-06-10T07:41:22Z</updated>\\n    <published>2017-06-10T07:41:22Z</published>\\n    <title>Online Learning for Neural Machine Translation Post-editing</title>\\n    <summary>  Neural machine translation has meant a revolution of the field. Nevertheless,\\npost-editing the outputs of the system is mandatory for tasks requiring high\\ntranslation quality. Post-editing offers a unique opportunity for improving\\nneural machine translation systems, using online learning techniques and\\ntreating the post-edited translations as new, fresh training data. We review\\nclassical learning methods and propose a new optimization algorithm. We\\nthoroughly compare online learning algorithms in a post-editing scenario.\\nResults show significant improvements in translation quality and effort\\nreduction.\\n</summary>\\n    <author>\\n      <name>\\xc3\\x81lvaro Peris</name>\\n    </author>\\n    <author>\\n      <name>Luis Cebri\\xc3\\xa1n</name>\\n    </author>\\n    <author>\\n      <name>Francisco Casacuberta</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.03196v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.03196v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.00076v1</id>\\n    <updated>2017-11-30T20:56:17Z</updated>\\n    <published>2017-11-30T20:56:17Z</published>\\n    <title>Machine Learning and Manycore Systems Design: A Serendipitous Symbiosis</title>\\n    <summary>  Tight collaboration between experts of machine learning and manycore system\\ndesign is necessary to create a data-driven manycore design framework that\\nintegrates both learning and expert knowledge. Such a framework will be\\nnecessary to address the rising complexity of designing large-scale manycore\\nsystems and machine learning techniques.\\n</summary>\\n    <author>\\n      <name>Ryan Gary Kim</name>\\n    </author>\\n    <author>\\n      <name>Janardhan Rao Doppa</name>\\n    </author>\\n    <author>\\n      <name>Partha Pratim Pande</name>\\n    </author>\\n    <author>\\n      <name>Diana Marculescu</name>\\n    </author>\\n    <author>\\n      <name>Radu Marculescu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in a future publication of IEEE Computer</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.00076v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.00076v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.06091v1</id>\\n    <updated>2018-02-16T19:19:00Z</updated>\\n    <published>2018-02-16T19:19:00Z</published>\\n    <title>Bridging Cognitive Programs and Machine Learning</title>\\n    <summary>  While great advances are made in pattern recognition and machine learning,\\nthe successes of such fields remain restricted to narrow applications and seem\\nto break down when training data is scarce, a shift in domain occurs, or when\\nintelligent reasoning is required for rapid adaptation to new environments. In\\nthis work, we list several of the shortcomings of modern machine-learning\\nsolutions, specifically in the contexts of computer vision and in reinforcement\\nlearning and suggest directions to explore in order to try to ameliorate these\\nweaknesses.\\n</summary>\\n    <author>\\n      <name>Amir Rosenfeld</name>\\n    </author>\\n    <author>\\n      <name>John K. Tsotsos</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.06091v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.06091v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.09225v1</id>\\n    <updated>2018-02-26T09:56:04Z</updated>\\n    <published>2018-02-26T09:56:04Z</published>\\n    <title>Interpreting Complex Regression Models</title>\\n    <summary>  Interpretation of a machine learning induced models is critical for feature\\nengineering, debugging, and, arguably, compliance. Yet, best of breed machine\\nlearning models tend to be very complex. This paper presents a method for model\\ninterpretation which has the main benefit that the simple interpretations it\\nprovides are always grounded in actual sets of learning examples. The method is\\nvalidated on the task of interpreting a complex regression model in the context\\nof both an academic problem -- predicting the year in which a song was recorded\\nand an industrial one -- predicting mail user churn.\\n</summary>\\n    <author>\\n      <name>Noa Avigdor-Elgrabli</name>\\n    </author>\\n    <author>\\n      <name>Alex Libov</name>\\n    </author>\\n    <author>\\n      <name>Michael Viderman</name>\\n    </author>\\n    <author>\\n      <name>Ran Wolff</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.09225v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.09225v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.04640v3</id>\\n    <updated>2019-01-07T20:01:56Z</updated>\\n    <published>2018-04-12T17:34:41Z</published>\\n    <title>Fast Counting in Machine Learning Applications</title>\\n    <summary>  We propose scalable methods to execute counting queries in machine learning\\napplications. To achieve memory and computational efficiency, we abstract\\ncounting queries and their context such that the counts can be aggregated as a\\nstream. We demonstrate performance and scalability of the resulting approach on\\nrandom queries, and through extensive experimentation using Bayesian networks\\nlearning and association rule mining. Our methods significantly outperform\\ncommonly used ADtrees and hash tables, and are practical alternatives for\\nprocessing large-scale data.\\n</summary>\\n    <author>\\n      <name>Subhadeep Karan</name>\\n    </author>\\n    <author>\\n      <name>Matthew Eichhorn</name>\\n    </author>\\n    <author>\\n      <name>Blake Hurlburt</name>\\n    </author>\\n    <author>\\n      <name>Grant Iraci</name>\\n    </author>\\n    <author>\\n      <name>Jaroslaw Zola</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.04640v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.04640v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.07758v1</id>\\n    <updated>2018-04-20T06:29:05Z</updated>\\n    <published>2018-04-20T06:29:05Z</published>\\n    <title>Mapping Images to Psychological Similarity Spaces Using Neural Networks</title>\\n    <summary>  The cognitive framework of conceptual spaces bridges the gap between symbolic\\nand subsymbolic AI by proposing an intermediate conceptual layer where\\nknowledge is represented geometrically. There are two main approaches for\\nobtaining the dimensions of this conceptual similarity space: using similarity\\nratings from psychological experiments and using machine learning techniques.\\nIn this paper, we propose a combination of both approaches by using\\npsychologically derived similarity ratings to constrain the machine learning\\nprocess. This way, a mapping from stimuli to conceptual spaces can be learned\\nthat is both supported by psychological data and allows generalization to\\nunseen stimuli. The results of a first feasibility study support our proposed\\napproach.\\n</summary>\\n    <author>\\n      <name>Lucas Bechberger</name>\\n    </author>\\n    <author>\\n      <name>Elektra Kypridemou</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to AIC 2018 (http://aic2018.pa.icar.cnr.it/)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1804.07758v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.07758v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07483v2</id>\\n    <updated>2018-05-23T18:51:32Z</updated>\\n    <published>2018-05-19T00:36:04Z</published>\\n    <title>Tell Me Something New: A New Framework for Asynchronous Parallel\\n  Learning</title>\\n    <summary>  We present a novel approach for parallel computation in the context of\\nmachine learning that we call \"Tell Me Something New\" (TMSN). This approach\\ninvolves a set of independent workers that use broadcast to update each other\\nwhen they observe \"something new\". TMSN does not require synchronization or a\\nhead node and is highly resilient against failing machines or laggards. We\\ndemonstrate the utility of TMSN by applying it to learning boosted trees. We\\nshow that our implementation is 10 times faster than XGBoost and LightGBM on\\nthe splice-site prediction problem.\\n</summary>\\n    <author>\\n      <name>Julaiti Alafate</name>\\n    </author>\\n    <author>\\n      <name>Yoav Freund</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.07483v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07483v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.07846v1</id>\\n    <updated>2018-06-20T17:11:54Z</updated>\\n    <published>2018-06-20T17:11:54Z</published>\\n    <title>Rethinking Machine Learning Development and Deployment for Edge Devices</title>\\n    <summary>  Machine learning (ML), especially deep learning is made possible by the\\navailability of big data, enormous compute power and, often overlooked,\\ndevelopment tools or frameworks. As the algorithms become mature and efficient,\\nmore and more ML inference is moving out of datacenters/cloud and deployed on\\nedge devices. This model deployment process can be challenging as the\\ndeployment environment and requirements can be substantially different from\\nthose during model development. In this paper, we propose a new ML development\\nand deployment approach that is specially designed and optimized for\\ninference-only deployment on edge devices. We build a prototype and demonstrate\\nthat this approach can address all the deployment challenges and result in more\\nefficient and high-quality solutions.\\n</summary>\\n    <author>\\n      <name>Liangzhen Lai</name>\\n    </author>\\n    <author>\\n      <name>Naveen Suda</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.07846v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.07846v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.01961v2</id>\\n    <updated>2018-07-23T11:14:20Z</updated>\\n    <published>2018-07-05T12:33:31Z</published>\\n    <title>A Boo(n) for Evaluating Architecture Performance</title>\\n    <summary>  We point out important problems with the common practice of using the best\\nsingle model performance for comparing deep learning architectures, and we\\npropose a method that corrects these flaws. Each time a model is trained, one\\ngets a different result due to random factors in the training process, which\\ninclude random parameter initialization and random data shuffling. Reporting\\nthe best single model performance does not appropriately address this\\nstochasticity. We propose a normalized expected best-out-of-$n$ performance\\n($\\\\text{Boo}_n$) as a way to correct these problems.\\n</summary>\\n    <author>\\n      <name>Ondrej Bajgar</name>\\n    </author>\\n    <author>\\n      <name>Rudolf Kadlec</name>\\n    </author>\\n    <author>\\n      <name>Jan Kleindienst</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML 2018</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 35th International Conference on Machine\\n  Learning (ICML 2018). Volume 80 of the Proceedings of Machine Learning\\n  Research (PMLR)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1807.01961v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.01961v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.07609v2</id>\\n    <updated>2018-12-10T12:39:29Z</updated>\\n    <published>2018-09-20T13:26:09Z</published>\\n    <title>Machine Learning for semi linear PDEs</title>\\n    <summary>  Recent machine learning algorithms dedicated to solving semi-linear PDEs are\\nimproved by using different neural network architectures and different\\nparameterizations. These algorithms are compared to a new one that solves a\\nfixed point problem by using deep learning techniques. This new algorithm\\nappears to be competitive in terms of accuracy with the best existing\\nalgorithms.\\n</summary>\\n    <author>\\n      <name>Quentin Chan-Wai-Nam</name>\\n    </author>\\n    <author>\\n      <name>Joseph Mikael</name>\\n    </author>\\n    <author>\\n      <name>Xavier Warin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">38 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.07609v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.07609v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"65C05, 49L25, 65C99\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.05975v1</id>\\n    <updated>2018-11-14T13:43:39Z</updated>\\n    <published>2018-11-14T13:43:39Z</published>\\n    <title>Machine Learning Analysis of Heterogeneity in the Effect of Student\\n  Mindset Interventions</title>\\n    <summary>  We study heterogeneity in the effect of a mindset intervention on\\nstudent-level performance through an observational dataset from the National\\nStudy of Learning Mindsets (NSLM). Our analysis uses machine learning (ML) to\\naddress the following associated problems: assessing treatment group overlap\\nand covariate balance, imputing conditional average treatment effects, and\\ninterpreting imputed effects. By comparing several different model families we\\nillustrate the flexibility of both off-the-shelf and purpose-built estimators.\\nWe find that the mindset intervention has a positive average effect of 0.26,\\n95%-CI [0.22, 0.30], and that heterogeneity in the range of [0.1, 0.4] is\\nmoderated by school-level achievement level, poverty concentration, urbanicity,\\nand student prior expectations.\\n</summary>\\n    <author>\\n      <name>Fredrik D. Johansson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.05975v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.05975v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.09687v1</id>\\n    <updated>2018-12-23T10:57:57Z</updated>\\n    <published>2018-12-23T10:57:57Z</published>\\n    <title>Computations in Stochastic Acceptors</title>\\n    <summary>  Machine learning provides algorithms that can learn from data and make\\ninferences or predictions on data. Stochastic acceptors or probabilistic\\nautomata are stochastic automata without output that can model components in\\nmachine learning scenarios. In this paper, we provide dynamic programming\\nalgorithms for the computation of input marginals and the acceptance\\nprobabilities in stochastic acceptors. Furthermore, we specify an algorithm for\\nthe parameter estimation of the conditional probabilities using the\\nexpectation-maximization technique and a more efficient implementation related\\nto the Baum-Welch algorithm.\\n</summary>\\n    <author>\\n      <name>Karl-Heinz Zimmermann</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.09687v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.09687v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Q70, 68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.00140v2</id>\\n    <updated>2019-02-20T15:56:31Z</updated>\\n    <published>2019-02-01T00:18:59Z</published>\\n    <title>Advances of Machine Learning in Molecular Modeling and Simulation</title>\\n    <summary>  In this review, we highlight recent developments in the application of\\nmachine learning for molecular modeling and simulation. After giving a brief\\noverview of the foundations, components, and workflow of a typical supervised\\nlearning approach for chemical problems, we showcase areas and state-of-the-art\\nexamples of their deployment. In this context, we discuss how machine learning\\nrelates to, supports, and augments more traditional physics-based approaches in\\ncomputational research. We conclude by outlining challenges and future research\\ndirections that need to be addressed in order to make machine learning a\\nmainstream chemical engineering tool.\\n</summary>\\n    <author>\\n      <name>Mojtaba Haghighatlari</name>\\n    </author>\\n    <author>\\n      <name>Johannes Hachmann</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">review summary</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.00140v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.00140v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.00368v2</id>\\n    <updated>2019-04-02T09:24:18Z</updated>\\n    <published>2019-03-31T09:41:28Z</published>\\n    <title>Fourier Transform Approach to Machine Learning</title>\\n    <summary>  We propose a supervised learning algorithm for machine learning applications.\\nContrary to the model developing in the classical methods, which treat\\ntraining, validation, and test as separate steps, in the presented approach,\\nthere is a unified training and evaluating procedure based on an iterative band\\nfiltering by the use of a fast Fourier transform. The presented approach does\\nnot apply the method of least squares, thus, basically typical ill-conditioned\\nmatrices do not occur at all. The optimal model results from the convergence of\\nthe performance metric, which automatically prevents the usual underfitting and\\noverfitting problems. The algorithm capability is investigated for noisy data,\\nand the obtained result demonstrates a reliable and powerful machine learning\\napproach beyond the typical limits of the classical methods.\\n</summary>\\n    <author>\\n      <name>Soheil Mehrabkhani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.00368v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.00368v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.08586v1</id>\\n    <updated>2017-02-28T00:39:04Z</updated>\\n    <published>2017-02-28T00:39:04Z</published>\\n    <title>Can Boltzmann Machines Discover Cluster Updates ?</title>\\n    <summary>  Boltzmann machines are physics informed generative models with wide\\napplications in machine learning. They can learn the probability distribution\\nfrom an input dataset and generate new samples accordingly. Applying them back\\nto physics, the Boltzmann machines are ideal recommender systems to accelerate\\nMonte Carlo simulation of physical systems due to their flexibility and\\neffectiveness. More intriguingly, we show that the generative sampling of the\\nBoltzmann Machines can even discover unknown cluster Monte Carlo algorithms.\\nThe creative power comes from the latent representation of the Boltzmann\\nmachines, which learn to mediate complex interactions and identify clusters of\\nthe physical system. We demonstrate these findings with concrete examples of\\nthe classical Ising model with and without four spin plaquette interactions.\\nOur results endorse a fresh research paradigm where intelligent machines are\\ndesigned to create or inspire human discovery of innovative algorithms.\\n</summary>\\n    <author>\\n      <name>Lei Wang</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevE.96.051301</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevE.96.051301\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 4 figures, and half page appendix</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Phys. Rev. E 96, 051301 (2017)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1702.08586v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.08586v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.10437v1</id>\\n    <updated>2018-12-26T18:10:40Z</updated>\\n    <published>2018-12-26T18:10:40Z</published>\\n    <title>Structure Learning of Sparse GGMs over Multiple Access Networks</title>\\n    <summary>  A central machine is interested in estimating the underlying structure of a\\nsparse Gaussian Graphical Model (GGM) from datasets distributed across multiple\\nlocal machines. The local machines can communicate with the central machine\\nthrough a wireless multiple access channel. In this paper, we are interested in\\ndesigning effective strategies where reliable learning is feasible under power\\nand bandwidth limitations. Two approaches are proposed: Signs and Uncoded\\nmethods. In Signs method, the local machines quantize their data into binary\\nvectors and an optimal channel coding scheme is used to reliably send the\\nvectors to the central machine where the structure is learned from the received\\ndata. In Uncoded method, data symbols are scaled and transmitted through the\\nchannel. The central machine uses the received noisy symbols to recover the\\nstructure. Theoretical results show that both methods can recover the structure\\nwith high probability for large enough sample size. Experimental results\\nindicate the superiority of Signs method over Uncoded method under several\\ncircumstances.\\n</summary>\\n    <author>\\n      <name>Mostafa Tavassolipour</name>\\n    </author>\\n    <author>\\n      <name>Armin Karamzade</name>\\n    </author>\\n    <author>\\n      <name>Reza Mirzaeifard</name>\\n    </author>\\n    <author>\\n      <name>Seyed Abolfazl Motahari</name>\\n    </author>\\n    <author>\\n      <name>Mohammad-Taghi Manzuri Shalmani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.10437v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.10437v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.02620v2</id>\\n    <updated>2016-02-14T12:15:45Z</updated>\\n    <published>2015-06-08T19:12:24Z</published>\\n    <title>Distributed Training of Structured SVM</title>\\n    <summary>  Training structured prediction models is time-consuming. However, most\\nexisting approaches only use a single machine, thus, the advantage of computing\\npower and the capacity for larger data sets of multiple machines have not been\\nexploited. In this work, we propose an efficient algorithm for distributedly\\ntraining structured support vector machines based on a distributed\\nblock-coordinate descent method. Both theoretical and experimental results\\nindicate that our method is efficient.\\n</summary>\\n    <author>\\n      <name>Ching-pei Lee</name>\\n    </author>\\n    <author>\\n      <name>Kai-Wei Chang</name>\\n    </author>\\n    <author>\\n      <name>Shyam Upadhyay</name>\\n    </author>\\n    <author>\\n      <name>Dan Roth</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS Workshop on Optimization for Machine Learning, 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1506.02620v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.02620v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.01840v1</id>\\n    <updated>2016-09-07T05:17:30Z</updated>\\n    <published>2016-09-07T05:17:30Z</published>\\n    <title>Learning Boltzmann Machine with EM-like Method</title>\\n    <summary>  We propose an expectation-maximization-like(EMlike) method to train Boltzmann\\nmachine with unconstrained connectivity. It adopts Monte Carlo approximation in\\nthe E-step, and replaces the intractable likelihood objective with efficiently\\ncomputed objectives or directly approximates the gradient of likelihood\\nobjective in the M-step. The EM-like method is a modification of alternating\\nminimization. We prove that EM-like method will be the exactly same with\\ncontrastive divergence in restricted Boltzmann machine if the M-step of this\\nmethod adopts special approximation. We also propose a new measure to assess\\nthe performance of Boltzmann machine as generative models of data, and its\\ncomputational complexity is O(Rmn). Finally, we demonstrate the performance of\\nEM-like method using numerical experiments.\\n</summary>\\n    <author>\\n      <name>Jinmeng Song</name>\\n    </author>\\n    <author>\\n      <name>Chun Yuan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1609.01840v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.01840v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.6709v4</id>\\n    <updated>2014-02-12T07:45:11Z</updated>\\n    <published>2013-06-28T03:56:15Z</published>\\n    <title>A Survey on Metric Learning for Feature Vectors and Structured Data</title>\\n    <summary>  The need for appropriate ways to measure the distance or similarity between\\ndata is ubiquitous in machine learning, pattern recognition and data mining,\\nbut handcrafting such good metrics for specific problems is generally\\ndifficult. This has led to the emergence of metric learning, which aims at\\nautomatically learning a metric from data and has attracted a lot of interest\\nin machine learning and related fields for the past ten years. This survey\\npaper proposes a systematic review of the metric learning literature,\\nhighlighting the pros and cons of each approach. We pay particular attention to\\nMahalanobis distance metric learning, a well-studied and successful framework,\\nbut additionally present a wide range of methods that have recently emerged as\\npowerful alternatives, including nonlinear metric learning, similarity learning\\nand local metric learning. Recent trends and extensions, such as\\nsemi-supervised metric learning, metric learning for histogram data and the\\nderivation of generalization guarantees, are also covered. Finally, this survey\\naddresses metric learning for structured data, in particular edit distance\\nlearning, and attempts to give an overview of the remaining challenges in\\nmetric learning for the years to come.\\n</summary>\\n    <author>\\n      <name>Aur\\xc3\\xa9lien Bellet</name>\\n    </author>\\n    <author>\\n      <name>Amaury Habrard</name>\\n    </author>\\n    <author>\\n      <name>Marc Sebban</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Technical report, 59 pages. Changes in v2: fixed typos and improved\\n  presentation. Changes in v3: fixed typos. Changes in v4: fixed typos and new\\n  methods</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.6709v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.6709v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.7584v1</id>\\n    <updated>2014-12-24T01:51:06Z</updated>\\n    <published>2014-12-24T01:51:06Z</published>\\n    <title>Differential Privacy and Machine Learning: a Survey and Review</title>\\n    <summary>  The objective of machine learning is to extract useful information from data,\\nwhile privacy is preserved by concealing information. Thus it seems hard to\\nreconcile these competing interests. However, they frequently must be balanced\\nwhen mining sensitive data. For example, medical research represents an\\nimportant application where it is necessary both to extract useful information\\nand protect patient privacy. One way to resolve the conflict is to extract\\ngeneral characteristics of whole populations without disclosing the private\\ninformation of individuals.\\n  In this paper, we consider differential privacy, one of the most popular and\\npowerful definitions of privacy. We explore the interplay between machine\\nlearning and differential privacy, namely privacy-preserving machine learning\\nalgorithms and learning-based data release mechanisms. We also describe some\\ntheoretical results that address what can be learned differentially privately\\nand upper bounds of loss functions for differentially private algorithms.\\n  Finally, we present some open questions, including how to incorporate public\\ndata, how to deal with missing data in private datasets, and whether, as the\\nnumber of observed samples grows arbitrarily large, differentially private\\nmachine learning algorithms can be achieved at no cost to utility as compared\\nto corresponding non-differentially private algorithms.\\n</summary>\\n    <author>\\n      <name>Zhanglong Ji</name>\\n    </author>\\n    <author>\\n      <name>Zachary C. Lipton</name>\\n    </author>\\n    <author>\\n      <name>Charles Elkan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.7584v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.7584v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.01461v1</id>\\n    <updated>2015-07-06T13:50:26Z</updated>\\n    <published>2015-07-06T13:50:26Z</published>\\n    <title>Revisiting Large Scale Distributed Machine Learning</title>\\n    <summary>  Nowadays, with the widespread of smartphones and other portable gadgets\\nequipped with a variety of sensors, data is ubiquitous available and the focus\\nof machine learning has shifted from being able to infer from small training\\nsamples to dealing with large scale high-dimensional data. In domains such as\\npersonal healthcare applications, which motivates this survey, distributed\\nmachine learning is a promising line of research, both for scaling up learning\\nalgorithms, but mostly for dealing with data which is inherently produced at\\ndifferent locations. This report offers a thorough overview of and\\nstate-of-the-art algorithms for distributed machine learning, for both\\nsupervised and unsupervised learning, ranging from simple linear logistic\\nregression to graphical models and clustering. We propose future directions for\\nmost categories, specific to the potential personal healthcare applications.\\nWith this in mind, the report focuses on how security and low communication\\noverhead can be assured in the specific case of a strictly client-server\\narchitectural model. As particular directions we provides an exhaustive\\npresentation of an empirical clustering algorithm, k-windows, and proposed an\\nasynchronous distributed machine learning algorithm that would scale well and\\nalso would be computationally cheap and easy to implement.\\n</summary>\\n    <author>\\n      <name>Radu Cristian Ionescu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1507.01461v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.01461v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.03125v1</id>\\n    <updated>2015-07-11T16:46:37Z</updated>\\n    <published>2015-07-11T16:46:37Z</published>\\n    <title>A new boosting algorithm based on dual averaging scheme</title>\\n    <summary>  The fields of machine learning and mathematical optimization increasingly\\nintertwined. The special topic on supervised learning and convex optimization\\nexamines this interplay. The training part of most supervised learning\\nalgorithms can usually be reduced to an optimization problem that minimizes a\\nloss between model predictions and training data. While most optimization\\ntechniques focus on accuracy and speed of convergence, the qualities of good\\noptimization algorithm from the machine learning perspective can be quite\\ndifferent since machine learning is more than fitting the data. Better\\noptimization algorithms that minimize the training loss can possibly give very\\npoor generalization performance. In this paper, we examine a particular kind of\\nmachine learning algorithm, boosting, whose training process can be viewed as\\nfunctional coordinate descent on the exponential loss. We study the relation\\nbetween optimization techniques and machine learning by implementing a new\\nboosting algorithm. DABoost, based on dual-averaging scheme and study its\\ngeneralization performance. We show that DABoost, although slower in reducing\\nthe training error, in general enjoys a better generalization error than\\nAdaBoost.\\n</summary>\\n    <author>\\n      <name>Nan Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.03125v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.03125v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.09773v4</id>\\n    <updated>2018-03-13T00:56:59Z</updated>\\n    <published>2017-06-29T14:30:40Z</published>\\n    <title>Interpretability via Model Extraction</title>\\n    <summary>  The ability to interpret machine learning models has become increasingly\\nimportant now that machine learning is used to inform consequential decisions.\\nWe propose an approach called model extraction for interpreting complex,\\nblackbox models. Our approach approximates the complex model using a much more\\ninterpretable model; as long as the approximation quality is good, then\\nstatistical properties of the complex model are reflected in the interpretable\\nmodel. We show how model extraction can be used to understand and debug random\\nforests and neural nets trained on several datasets from the UCI Machine\\nLearning Repository, as well as control policies learned for several classical\\nreinforcement learning problems.\\n</summary>\\n    <author>\\n      <name>Osbert Bastani</name>\\n    </author>\\n    <author>\\n      <name>Carolyn Kim</name>\\n    </author>\\n    <author>\\n      <name>Hamsa Bastani</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented as a poster at the 2017 Workshop on Fairness,\\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.09773v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.09773v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.00158v1</id>\\n    <updated>2018-03-01T01:45:30Z</updated>\\n    <published>2018-03-01T01:45:30Z</published>\\n    <title>Modeling reverse thinking for machine learning</title>\\n    <summary>  Human inertial thinking schemes can be formed through learning, which are\\nthen applied to quickly solve similar problems later. However, when problems\\nare significantly different, inertial thinking generally presents the solutions\\nthat are definitely imperfect. In such cases, people will apply creative\\nthinking, such as reverse thinking, to solve problems. Similarly, machine\\nlearning methods also form inertial thinking schemes through learning the\\nknowledge from a large amount of data. However, when the testing data are\\nvastly difference, the formed inertial thinking schemes will inevitably\\ngenerate errors. This kind of inertial thinking is called illusion inertial\\nthinking. Because all machine learning methods do not consider illusion\\ninertial thinking, in this paper we propose a new method that uses reverse\\nthinking to correct illusion inertial thinking, which increases the\\ngeneralization ability of machine learning methods. Experimental results on\\nbenchmark datasets are used to validate the proposed method.\\n</summary>\\n    <author>\\n      <name>Li Huihui</name>\\n    </author>\\n    <author>\\n      <name>Wen Guihua</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.00158v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.00158v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.00338v2</id>\\n    <updated>2018-09-28T00:22:58Z</updated>\\n    <published>2018-04-01T20:28:18Z</published>\\n    <title>Towards Intelligent Vehicular Networks: A Machine Learning Framework</title>\\n    <summary>  As wireless networks evolve towards high mobility and providing better\\nsupport for connected vehicles, a number of new challenges arise due to the\\nresulting high dynamics in vehicular environments and thus motive rethinking of\\ntraditional wireless design methodologies. Future intelligent vehicles, which\\nare at the heart of high mobility networks, are increasingly equipped with\\nmultiple advanced onboard sensors and keep generating large volumes of data.\\nMachine learning, as an effective approach to artificial intelligence, can\\nprovide a rich set of tools to exploit such data for the benefit of the\\nnetworks. In this article, we first identify the distinctive characteristics of\\nhigh mobility vehicular networks and motivate the use of machine learning to\\naddress the resulting challenges. After a brief introduction of the major\\nconcepts of machine learning, we discuss its applications to learn the dynamics\\nof vehicular networks and make informed decisions to optimize network\\nperformance. In particular, we discuss in greater detail the application of\\nreinforcement learning in managing network resources as an alternative to the\\nprevalent optimization approach. Finally, some open issues worth further\\ninvestigation are highlighted.\\n</summary>\\n    <author>\\n      <name>Le Liang</name>\\n    </author>\\n    <author>\\n      <name>Hao Ye</name>\\n    </author>\\n    <author>\\n      <name>Geoffrey Ye Li</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/JIOT.2018.2872122</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/JIOT.2018.2872122\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by IEEE Internet of Things Journal</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1804.00338v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.00338v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.09710v1</id>\\n    <updated>2018-06-25T21:37:21Z</updated>\\n    <published>2018-06-25T21:37:21Z</published>\\n    <title>Why Interpretability in Machine Learning? An Answer Using Distributed\\n  Detection and Data Fusion Theory</title>\\n    <summary>  As artificial intelligence is increasingly affecting all parts of society and\\nlife, there is growing recognition that human interpretability of machine\\nlearning models is important. It is often argued that accuracy or other similar\\ngeneralization performance metrics must be sacrificed in order to gain\\ninterpretability. Such arguments, however, fail to acknowledge that the overall\\ndecision-making system is composed of two entities: the learned model and a\\nhuman who fuses together model outputs with his or her own information. As\\nsuch, the relevant performance criteria should be for the entire system, not\\njust for the machine learning component. In this work, we characterize the\\nperformance of such two-node tandem data fusion systems using the theory of\\ndistributed detection. In doing so, we work in the population setting and model\\ninterpretable learned models as multi-level quantizers. We prove that under our\\nabstraction, the overall system of a human with an interpretable classifier\\noutperforms one with a black box classifier.\\n</summary>\\n    <author>\\n      <name>Kush R. Varshney</name>\\n    </author>\\n    <author>\\n      <name>Prashant Khanduri</name>\\n    </author>\\n    <author>\\n      <name>Pranay Sharma</name>\\n    </author>\\n    <author>\\n      <name>Shan Zhang</name>\\n    </author>\\n    <author>\\n      <name>Pramod K. Varshney</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">presented at 2018 ICML Workshop on Human Interpretability in Machine\\n  Learning (WHI 2018), Stockholm, Sweden</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.09710v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.09710v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.02213v1</id>\\n    <updated>2018-08-07T05:09:13Z</updated>\\n    <published>2018-08-07T05:09:13Z</published>\\n    <title>Importance of the Mathematical Foundations of Machine Learning Methods\\n  for Scientific and Engineering Applications</title>\\n    <summary>  There has been a lot of recent interest in adopting machine learning methods\\nfor scientific and engineering applications. This has in large part been\\ninspired by recent successes and advances in the domains of Natural Language\\nProcessing (NLP) and Image Classification (IC). However, scientific and\\nengineering problems have their own unique characteristics and requirements\\nraising new challenges for effective design and deployment of machine learning\\napproaches. There is a strong need for further mathematical developments on the\\nfoundations of machine learning methods to increase the level of rigor of\\nemployed methods and to ensure more reliable and interpretable results. Also as\\nreported in the recent literature on state-of-the-art results and indicated by\\nthe No Free Lunch Theorems of statistical learning theory incorporating some\\nform of inductive bias and domain knowledge is essential to success.\\nConsequently, even for existing and widely used methods there is a strong need\\nfor further mathematical work to facilitate ways to incorporate prior\\nscientific knowledge and related inductive biases into learning frameworks and\\nalgorithms. We briefly discuss these topics and discuss some ideas proceeding\\nin this direction.\\n</summary>\\n    <author>\\n      <name>Paul J. Atzberger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Position Paper at SciML2018 Workshop, US Department of Energy,\\n  January 2018, (two-page limit)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.02213v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.02213v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.00246v2</id>\\n    <updated>2019-01-18T16:35:42Z</updated>\\n    <published>2019-01-02T03:00:21Z</published>\\n    <title>Natively Interpretable Machine Learning and Artificial Intelligence:\\n  Preliminary Results and Future Directions</title>\\n    <summary>  Machine learning models have become more and more complex in order to better\\napproximate complex functions. Although fruitful in many domains, the added\\ncomplexity has come at the cost of model interpretability. The once popular\\nk-nearest neighbors (kNN) approach, which finds and uses the most similar data\\nfor reasoning, has received much less attention in recent decades due to\\nnumerous problems when compared to other techniques. We show that many of these\\nhistorical problems with kNN can be overcome, and our contribution has\\napplications not only in machine learning but also in online learning, data\\nsynthesis, anomaly detection, model compression, and reinforcement learning,\\nwithout sacrificing interpretability. We introduce a synthesis between kNN and\\ninformation theory that we hope will provide a clear path towards models that\\nare innately interpretable and auditable. Through this work we hope to gather\\ninterest in combining kNN with information theory as a promising path to fully\\nauditable machine learning and artificial intelligence.\\n</summary>\\n    <author>\\n      <name>Christopher J. Hazard</name>\\n    </author>\\n    <author>\\n      <name>Christopher Fusting</name>\\n    </author>\\n    <author>\\n      <name>Michael Resnick</name>\\n    </author>\\n    <author>\\n      <name>Michael Auerbach</name>\\n    </author>\\n    <author>\\n      <name>Michael Meehan</name>\\n    </author>\\n    <author>\\n      <name>Valeri Korobov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.00246v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.00246v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.06339v1</id>\\n    <updated>2018-10-15T13:20:56Z</updated>\\n    <published>2018-10-15T13:20:56Z</published>\\n    <title>Deep Reinforcement Learning</title>\\n    <summary>  We discuss deep reinforcement learning in an overview style. We draw a big\\npicture, filled with details. We discuss six core elements, six important\\nmechanisms, and twelve applications, focusing on contemporary work, and in\\nhistorical contexts. We start with background of artificial intelligence,\\nmachine learning, deep learning, and reinforcement learning (RL), with\\nresources. Next we discuss RL core elements, including value function, policy,\\nreward, model, exploration vs. exploitation, and representation. Then we\\ndiscuss important mechanisms for RL, including attention and memory,\\nunsupervised learning, hierarchical RL, multi-agent RL, relational RL, and\\nlearning to learn. After that, we discuss RL applications, including games,\\nrobotics, natural language processing (NLP), computer vision, finance, business\\nmanagement, healthcare, education, energy, transportation, computer systems,\\nand, science, engineering, and art. Finally we summarize briefly, discuss\\nchallenges and opportunities, and close with an epilogue.\\n</summary>\\n    <author>\\n      <name>Yuxi Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Under review for Morgan &amp; Claypool: Synthesis Lectures in Artificial\\n  Intelligence and Machine Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.06339v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.06339v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.11295v1</id>\\n    <updated>2018-10-26T12:50:56Z</updated>\\n    <published>2018-10-26T12:50:56Z</published>\\n    <title>Real-time Context-aware Learning System for IoT Applications</title>\\n    <summary>  We propose a real-time context-aware learning system along with the\\narchitecture that runs on the mobile devices, provide services to the user and\\nmanage the IoT devices. In this system, an application running on mobile\\ndevices collected data from the sensors, learned about the user-defined\\ncontext, made predictions in real-time and manage IoT devices accordingly.\\nHowever, the computational power of the mobile devices makes it challenging to\\nrun machine learning algorithms with acceptable accuracy. To solve this issue,\\nsome authors have run machine learning algorithms on the server and transmitted\\nthe results to the mobile devices. Although the context-aware predictions made\\nby the server are more accurate than their mobile counterpart, it heavily\\ndepends on the network connection for the delivery of the results to the\\ndevices, which negatively affects real-time context-learning. Therefore, in\\nthis work, we describe a context-learning algorithm for mobile devices which is\\nless demanding on the computational resources and maintains the accuracy of the\\nprediction by updating itself from the learning parameters obtained from the\\nserver periodically. Experimental results show that the proposed light-weight\\ncontext-learning algorithm can achieve mean accuracy up to 97.51% while mean\\nexecution time requires only 11ms.\\n</summary>\\n    <author>\\n      <name>Bhaskar Das</name>\\n    </author>\\n    <author>\\n      <name>Jalal Almhana</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">34 pages, 12 figures, Journal article</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.11295v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.11295v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.05907v1</id>\\n    <updated>2017-05-16T20:42:25Z</updated>\\n    <published>2017-05-16T20:42:25Z</published>\\n    <title>Machine Learning Molecular Dynamics for the Simulation of Infrared\\n  Spectra</title>\\n    <summary>  Machine learning has emerged as an invaluable tool in many research areas. In\\nthe present work, we harness this power to predict highly accurate molecular\\ninfrared spectra with unprecedented computational efficiency. To account for\\nvibrational anharmonic and dynamical effects -- typically neglected by\\nconventional quantum chemistry approaches -- we base our machine learning\\nstrategy on ab initio molecular dynamics simulations. While these simulations\\nare usually extremely time consuming even for small molecules, we overcome\\nthese limitations by leveraging the power of a variety of machine learning\\ntechniques, not only accelerating simulations by several orders of magnitude,\\nbut also greatly extending the size of systems that can be treated. To this\\nend, we develop a molecular dipole moment model based on environment dependent\\nneural network charges and combine it with the neural network potentials of\\nBehler and Parrinello. Contrary to the prevalent big data philosophy, we are\\nable to obtain very accurate machine learning models for the prediction of\\ninfrared spectra based on only a few hundreds of electronic structure reference\\npoints. This is made possible through the introduction of a fully automated\\nsampling scheme and the use of molecular forces during neural network potential\\ntraining. We demonstrate the power of our machine learning approach by applying\\nit to model the infrared spectra of a methanol molecule, n-alkanes containing\\nup to 200 atoms and the protonated alanine tripeptide, which at the same time\\nrepresents the first application of machine learning techniques to simulate the\\ndynamics of a peptide. In all these case studies we find excellent agreement\\nbetween the infrared spectra predicted via machine learning models and the\\nrespective theoretical and experimental spectra.\\n</summary>\\n    <author>\\n      <name>Michael Gastegger</name>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xb6rg Behler</name>\\n    </author>\\n    <author>\\n      <name>Philipp Marquetand</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.05907v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.05907v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.bio-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.02852v1</id>\\n    <updated>2018-12-06T23:22:15Z</updated>\\n    <published>2018-12-06T23:22:15Z</published>\\n    <title>Automatically Explaining Machine Learning Prediction Results: A\\n  Demonstration on Type 2 Diabetes Risk Prediction</title>\\n    <summary>  Background: Predictive modeling is a key component of solutions to many\\nhealthcare problems. Among all predictive modeling approaches, machine learning\\nmethods often achieve the highest prediction accuracy, but suffer from a\\nlong-standing open problem precluding their widespread use in healthcare. Most\\nmachine learning models give no explanation for their prediction results,\\nwhereas interpretability is essential for a predictive model to be adopted in\\ntypical healthcare settings. Methods: This paper presents the first complete\\nmethod for automatically explaining results for any machine learning predictive\\nmodel without degrading accuracy. We did a computer coding implementation of\\nthe method. Using the electronic medical record data set from the Practice\\nFusion diabetes classification competition containing patient records from all\\n50 states in the United States, we demonstrated the method on predicting type 2\\ndiabetes diagnosis within the next year. Results: For the champion machine\\nlearning model of the competition, our method explained prediction results for\\n87.4% of patients who were correctly predicted by the model to have type 2\\ndiabetes diagnosis within the next year. Conclusions: Our demonstration showed\\nthe feasibility of automatically explaining results for any machine learning\\npredictive model without degrading accuracy.\\n</summary>\\n    <author>\\n      <name>Gang Luo</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1186/s13755-016-0015-4</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1186/s13755-016-0015-4\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Gang Luo. Automatically Explaining Machine Learning Prediction\\n  Results: A Demonstration on Type 2 Diabetes Risk Prediction. Health\\n  Information Science and Systems, Vol. 4, No. 2, Mar. 2016</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1812.02852v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.02852v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0907.4643v2</id>\\n    <updated>2009-10-09T11:31:24Z</updated>\\n    <published>2009-07-27T14:43:22Z</published>\\n    <title>Mean-Field Theory of Meta-Learning</title>\\n    <summary>  We discuss here the mean-field theory for a cellular automata model of\\nmeta-learning. The meta-learning is the process of combining outcomes of\\nindividual learning procedures in order to determine the final decision with\\nhigher accuracy than any single learning method. Our method is constructed from\\nan ensemble of interacting, learning agents, that acquire and process incoming\\ninformation using various types, or different versions of machine learning\\nalgorithms. The abstract learning space, where all agents are located, is\\nconstructed here using a fully connected model that couples all agents with\\nrandom strength values. The cellular automata network simulates the higher\\nlevel integration of information acquired from the independent learning trials.\\nThe final classification of incoming input data is therefore defined as the\\nstationary state of the meta-learning system using simple majority rule, yet\\nthe minority clusters that share opposite classification outcome can be\\nobserved in the system. Therefore, the probability of selecting proper class\\nfor a given input data, can be estimated even without the prior knowledge of\\nits affiliation. The fuzzy logic can be easily introduced into the system, even\\nif learning agents are build from simple binary classification machine learning\\nalgorithms by calculating the percentage of agreeing agents.\\n</summary>\\n    <author>\\n      <name>Dariusz Plewczynski</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1088/1742-5468/2009/11/P11003</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1088/1742-5468/2009/11/P11003\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">23 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0907.4643v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.4643v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.02254v1</id>\\n    <updated>2015-11-06T22:30:46Z</updated>\\n    <published>2015-11-06T22:30:46Z</published>\\n    <title>Active Perceptual Similarity Modeling with Auxiliary Information</title>\\n    <summary>  Learning a model of perceptual similarity from a collection of objects is a\\nfundamental task in machine learning underlying numerous applications. A common\\nway to learn such a model is from relative comparisons in the form of triplets:\\nresponses to queries of the form \"Is object a more similar to b than it is to\\nc?\". If no consideration is made in the determination of which queries to ask,\\nexisting similarity learning methods can require a prohibitively large number\\nof responses. In this work, we consider the problem of actively learning from\\ntriplets -finding which queries are most useful for learning. Different from\\nprevious active triplet learning approaches, we incorporate auxiliary\\ninformation into our similarity model and introduce an active learning scheme\\nto find queries that are informative for quickly learning both the relevant\\naspects of auxiliary data and the directly-learned similarity components.\\nCompared to prior approaches, we show that we can learn just as effectively\\nwith much fewer queries. For evaluation, we introduce a new dataset of\\nexhaustive triplet comparisons obtained from humans and demonstrate improved\\nperformance for different types of auxiliary information.\\n</summary>\\n    <author>\\n      <name>Eric Heim</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Pittsburgh</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Matthew Berger</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Air Force Research Laboratory, Information Directorate</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Lee Seversky</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Air Force Research Laboratory, Information Directorate</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Milos Hauskrecht</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Pittsburgh</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1511.02254v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.02254v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.06617v1</id>\\n    <updated>2017-06-20T18:44:49Z</updated>\\n    <published>2017-06-20T18:44:49Z</published>\\n    <title>Observational Learning by Reinforcement Learning</title>\\n    <summary>  Observational learning is a type of learning that occurs as a function of\\nobserving, retaining and possibly replicating or imitating the behaviour of\\nanother agent. It is a core mechanism appearing in various instances of social\\nlearning and has been found to be employed in several intelligent species,\\nincluding humans. In this paper, we investigate to what extent the explicit\\nmodelling of other agents is necessary to achieve observational learning\\nthrough machine learning. Especially, we argue that observational learning can\\nemerge from pure Reinforcement Learning (RL), potentially coupled with memory.\\nThrough simple scenarios, we demonstrate that an RL agent can leverage the\\ninformation provided by the observations of an other agent performing a task in\\na shared environment. The other agent is only observed through the effect of\\nits actions on the environment and never explicitly modeled. Two key aspects\\nare borrowed from observational learning: i) the observer behaviour needs to\\nchange as a result of viewing a \\'teacher\\' (another agent) and ii) the observer\\nneeds to be motivated somehow to engage in making use of the other agent\\'s\\nbehaviour. The later is naturally modeled by RL, by correlating the learning\\nagent\\'s reward with the teacher agent\\'s behaviour.\\n</summary>\\n    <author>\\n      <name>Diana Borsa</name>\\n    </author>\\n    <author>\\n      <name>Bilal Piot</name>\\n    </author>\\n    <author>\\n      <name>R\\xc3\\xa9mi Munos</name>\\n    </author>\\n    <author>\\n      <name>Olivier Pietquin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.06617v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.06617v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.05561v1</id>\\n    <updated>2017-03-16T11:15:28Z</updated>\\n    <published>2017-03-16T11:15:28Z</published>\\n    <title>Fraternal Twins: Unifying Attacks on Machine Learning and Digital\\n  Watermarking</title>\\n    <summary>  Machine learning is increasingly used in security-critical applications, such\\nas autonomous driving, face recognition and malware detection. Most learning\\nmethods, however, have not been designed with security in mind and thus are\\nvulnerable to different types of attacks. This problem has motivated the\\nresearch field of adversarial machine learning that is concerned with attacking\\nand defending learning methods. Concurrently, a different line of research has\\ntackled a very similar problem: In digital watermarking information are\\nembedded in a signal in the presence of an adversary. As a consequence, this\\nresearch field has also extensively studied techniques for attacking and\\ndefending watermarking methods.\\n  The two research communities have worked in parallel so far, unnoticeably\\ndeveloping similar attack and defense strategies. This paper is a first effort\\nto bring these communities together. To this end, we present a unified notation\\nof black-box attacks against machine learning and watermarking that reveals the\\nsimilarity of both settings. To demonstrate the efficacy of this unified view,\\nwe apply concepts from watermarking to machine learning and vice versa. We show\\nthat countermeasures from watermarking can mitigate recent model-extraction\\nattacks and, similarly, that techniques for hardening machine learning can fend\\noff oracle attacks against watermarks. Our work provides a conceptual link\\nbetween two research fields and thereby opens novel directions for improving\\nthe security of both, machine learning and digital watermarking.\\n</summary>\\n    <author>\\n      <name>Erwin Quiring</name>\\n    </author>\\n    <author>\\n      <name>Daniel Arp</name>\\n    </author>\\n    <author>\\n      <name>Konrad Rieck</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.05561v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.05561v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.13306v3</id>\\n    <updated>2019-01-17T10:47:18Z</updated>\\n    <published>2018-10-31T14:35:38Z</published>\\n    <title>Taking Human out of Learning Applications: A Survey on Automated Machine\\n  Learning</title>\\n    <summary>  Machine learning techniques have deeply rooted in our everyday life. However,\\nsince it is knowledge- and labor-intensive to pursue good learning performance,\\nhuman experts are heavily involved in every aspect of machine learning. In\\norder to make machine learning techniques easier to apply and reduce the demand\\nfor experienced human experts, automated machine learning (AutoML) has emerged\\nas a hot topic with both industrial and academic interest. In this paper, we\\nprovide an up to date survey on AutoML. First, we introduce and define the\\nAutoML problem, with inspiration from both realms of automation and machine\\nlearning. Then, we propose a general AutoML framework that not only covers most\\nexisting approaches to date but also can guide the design for new methods.\\nSubsequently, we categorize and review the existing works from two aspects,\\ni.e., the problem setup and the employed techniques. Finally, we provide a\\ndetailed analysis of AutoML approaches and explain the reasons underneath their\\nsuccessful applications. We hope this survey can serve as not only an\\ninsightful guideline for AutoML beginners but also an inspiration for future\\nresearch.\\n</summary>\\n    <author>\\n      <name>Quanming Yao</name>\\n    </author>\\n    <author>\\n      <name>Mengshuo Wang</name>\\n    </author>\\n    <author>\\n      <name>Yuqiang Chen</name>\\n    </author>\\n    <author>\\n      <name>Wenyuan Dai</name>\\n    </author>\\n    <author>\\n      <name>Hu Yi-Qi</name>\\n    </author>\\n    <author>\\n      <name>Li Yu-Feng</name>\\n    </author>\\n    <author>\\n      <name>Tu Wei-Wei</name>\\n    </author>\\n    <author>\\n      <name>Yang Qiang</name>\\n    </author>\\n    <author>\\n      <name>Yu Yang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This is a preliminary and will be kept updated</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.13306v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.13306v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.07901v4</id>\\n    <updated>2019-01-08T21:15:07Z</updated>\\n    <published>2018-11-19T19:00:01Z</published>\\n    <title>On Human Predictions with Explanations and Predictions of Machine\\n  Learning Models: A Case Study on Deception Detection</title>\\n    <summary>  Humans are the final decision makers in critical tasks that involve ethical\\nand legal concerns, ranging from recidivism prediction, to medical diagnosis,\\nto fighting against fake news. Although machine learning models can sometimes\\nachieve impressive performance in these tasks, these tasks are not amenable to\\nfull automation. To realize the potential of machine learning for improving\\nhuman decisions, it is important to understand how assistance from machine\\nlearning models affects human performance and human agency.\\n  In this paper, we use deception detection as a testbed and investigate how we\\ncan harness explanations and predictions of machine learning models to improve\\nhuman performance while retaining human agency. We propose a spectrum between\\nfull human agency and full automation, and develop varying levels of machine\\nassistance along the spectrum that gradually increase the influence of machine\\npredictions. We find that without showing predicted labels, explanations alone\\nslightly improve human performance in the end task. In comparison, human\\nperformance is greatly improved by showing predicted labels (&gt;20% relative\\nimprovement) and can be further improved by explicitly suggesting strong\\nmachine performance. Interestingly, when predicted labels are shown,\\nexplanations of machine predictions induce a similar level of accuracy as an\\nexplicit statement of strong machine performance. Our results demonstrate a\\ntradeoff between human performance and human agency and show that explanations\\nof machine predictions can moderate this tradeoff.\\n</summary>\\n    <author>\\n      <name>Vivian Lai</name>\\n    </author>\\n    <author>\\n      <name>Chenhao Tan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3287560.3287590</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3287560.3287590\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 19 figures, in Proceedings of ACM FAT* 2019, dataset &amp; demo\\n  available at https://deception.machineintheloop.com</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.07901v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.07901v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.soc-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1006.1138v3</id>\\n    <updated>2014-08-12T16:44:00Z</updated>\\n    <published>2010-06-06T21:05:27Z</published>\\n    <title>Online Learning via Sequential Complexities</title>\\n    <summary>  We consider the problem of sequential prediction and provide tools to study\\nthe minimax value of the associated game. Classical statistical learning theory\\nprovides several useful complexity measures to study learning with i.i.d. data.\\nOur proposed sequential complexities can be seen as extensions of these\\nmeasures to the sequential setting. The developed theory is shown to yield\\nprecise learning guarantees for the problem of sequential prediction. In\\nparticular, we show necessary and sufficient conditions for online learnability\\nin the setting of supervised learning. Several examples show the utility of our\\nframework: we can establish learnability without having to exhibit an explicit\\nonline learning algorithm.\\n</summary>\\n    <author>\\n      <name>Alexander Rakhlin</name>\\n    </author>\\n    <author>\\n      <name>Karthik Sridharan</name>\\n    </author>\\n    <author>\\n      <name>Ambuj Tewari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1006.1138v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1006.1138v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6177v3</id>\\n    <updated>2015-03-31T19:22:18Z</updated>\\n    <published>2014-12-18T23:25:22Z</published>\\n    <title>Example Selection For Dictionary Learning</title>\\n    <summary>  In unsupervised learning, an unbiased uniform sampling strategy is typically\\nused, in order that the learned features faithfully encode the statistical\\nstructure of the training data. In this work, we explore whether active example\\nselection strategies - algorithms that select which examples to use, based on\\nthe current estimate of the features - can accelerate learning. Specifically,\\nwe investigate effects of heuristic and saliency-inspired selection algorithms\\non the dictionary learning task with sparse activations. We show that some\\nselection algorithms do improve the speed of learning, and we speculate on why\\nthey might work.\\n</summary>\\n    <author>\\n      <name>Tomoki Tsuchida</name>\\n    </author>\\n    <author>\\n      <name>Garrison W. Cottrell</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.6177v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6177v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.1106v2</id>\\n    <updated>2013-02-18T16:09:50Z</updated>\\n    <published>2012-06-06T02:06:57Z</published>\\n    <title>No More Pesky Learning Rates</title>\\n    <summary>  The performance of stochastic gradient descent (SGD) depends critically on\\nhow learning rates are tuned and decreased over time. We propose a method to\\nautomatically adjust multiple learning rates so as to minimize the expected\\nerror at any one time. The method relies on local gradient variations across\\nsamples. In our approach, learning rates can increase as well as decrease,\\nmaking it suitable for non-stationary problems. Using a number of convex and\\nnon-convex learning tasks, we show that the resulting algorithm matches the\\nperformance of SGD or other adaptive approaches with their best settings\\nobtained through systematic search, and effectively removes the need for\\nlearning rate tuning.\\n</summary>\\n    <author>\\n      <name>Tom Schaul</name>\\n    </author>\\n    <author>\\n      <name>Sixin Zhang</name>\\n    </author>\\n    <author>\\n      <name>Yann LeCun</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1206.1106v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.1106v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.4630v1</id>\\n    <updated>2012-06-18T15:08:38Z</updated>\\n    <published>2012-06-18T15:08:38Z</published>\\n    <title>Efficient Decomposed Learning for Structured Prediction</title>\\n    <summary>  Structured prediction is the cornerstone of several machine learning\\napplications. Unfortunately, in structured prediction settings with expressive\\ninter-variable interactions, exact inference-based learning algorithms, e.g.\\nStructural SVM, are often intractable. We present a new way, Decomposed\\nLearning (DecL), which performs efficient learning by restricting the inference\\nstep to a limited part of the structured spaces. We provide characterizations\\nbased on the structure, target parameters, and gold labels, under which DecL is\\nequivalent to exact learning. We then show that in real world settings, where\\nour theoretical assumptions may not completely hold, DecL-based algorithms are\\nsignificantly more efficient and as accurate as exact learning.\\n</summary>\\n    <author>\\n      <name>Rajhans Samdani</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Illinois, U-C</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Dan Roth</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Illinois, U-C</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.4630v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.4630v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.07956v5</id>\\n    <updated>2017-11-15T21:50:12Z</updated>\\n    <published>2017-02-25T22:45:20Z</published>\\n    <title>Generative Adversarial Active Learning</title>\\n    <summary>  We propose a new active learning by query synthesis approach using Generative\\nAdversarial Networks (GAN). Different from regular active learning, the\\nresulting algorithm adaptively synthesizes training instances for querying to\\nincrease learning speed. We generate queries according to the uncertainty\\nprinciple, but our idea can work with other active learning principles. We\\nreport results from various numerical experiments to demonstrate the\\neffectiveness the proposed approach. In some settings, the proposed algorithm\\noutperforms traditional pool-based approaches. To the best our knowledge, this\\nis the first active learning work using GAN.\\n</summary>\\n    <author>\\n      <name>Jia-Jie Zhu</name>\\n    </author>\\n    <author>\\n      <name>Jos\\xc3\\xa9 Bento</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1702.07956v5\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.07956v5\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.10628v3</id>\\n    <updated>2018-05-20T14:51:43Z</updated>\\n    <published>2017-10-29T15:30:58Z</published>\\n    <title>Variational Continual Learning</title>\\n    <summary>  This paper develops variational continual learning (VCL), a simple but\\ngeneral framework for continual learning that fuses online variational\\ninference (VI) and recent advances in Monte Carlo VI for neural networks. The\\nframework can successfully train both deep discriminative models and deep\\ngenerative models in complex continual learning settings where existing tasks\\nevolve over time and entirely new tasks emerge. Experimental results show that\\nVCL outperforms state-of-the-art continual learning methods on a variety of\\ntasks, avoiding catastrophic forgetting in a fully automatic way.\\n</summary>\\n    <author>\\n      <name>Cuong V. Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Yingzhen Li</name>\\n    </author>\\n    <author>\\n      <name>Thang D. Bui</name>\\n    </author>\\n    <author>\\n      <name>Richard E. Turner</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published at International Conference on Learning Representations\\n  (ICLR) 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1710.10628v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.10628v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.03343v1</id>\\n    <updated>2017-11-09T12:10:27Z</updated>\\n    <published>2017-11-09T12:10:27Z</published>\\n    <title>Analysis of Dropout in Online Learning</title>\\n    <summary>  Deep learning is the state-of-the-art in fields such as visual object\\nrecognition and speech recognition. This learning uses a large number of layers\\nand a huge number of units and connections. Therefore, overfitting is a serious\\nproblem with it, and the dropout which is a kind of regularization tool is\\nused. However, in online learning, the effect of dropout is not well known.\\nThis paper presents our investigation on the effect of dropout in online\\nlearning. We analyzed the effect of dropout on convergence speed near the\\nsingular point. Our results indicated that dropout is effective in online\\nlearning. Dropout tends to avoid the singular point for convergence speed near\\nthat point.\\n</summary>\\n    <author>\\n      <name>Kazuyuki Hara</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 6 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEICE Technical Report IBIS2017-61</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1711.03343v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.03343v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.09136v2</id>\\n    <updated>2018-04-08T15:17:02Z</updated>\\n    <published>2018-01-27T20:39:24Z</published>\\n    <title>Gradient descent revisited via an adaptive online learning rate</title>\\n    <summary>  Any gradient descent optimization requires to choose a learning rate. With\\ndeeper and deeper models, tuning that learning rate can easily become tedious\\nand does not necessarily lead to an ideal convergence. We propose a variation\\nof the gradient descent algorithm in the which the learning rate is not fixed.\\nInstead, we learn the learning rate itself, either by another gradient descent\\n(first-order method), or by Newton\\'s method (second-order). This way, gradient\\ndescent for any machine learning algorithm can be optimized.\\n</summary>\\n    <author>\\n      <name>Mathieu Ravaut</name>\\n    </author>\\n    <author>\\n      <name>Satya Gorti</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1801.09136v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.09136v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.09733v2</id>\\n    <updated>2018-11-06T16:37:46Z</updated>\\n    <published>2018-05-24T15:38:07Z</published>\\n    <title>Towards Robust Evaluations of Continual Learning</title>\\n    <summary>  The experiments used in current continual learning research do not faithfully\\nassess fundamental challenges of learning continually. We examine standard\\nevaluations and show why these evaluations make some types of continual\\nlearning approaches look better than they are. In particular, current\\nevaluations are biased towards continual learning approaches that treat\\nprevious models as a prior (e.g., EWC, VCL). We introduce desiderata for\\ncontinual learning evaluations and explain why their absence creates misleading\\ncomparisons. Our analysis calls for a reprioritization of research effort by\\nthe community.\\n</summary>\\n    <author>\\n      <name>Sebastian Farquhar</name>\\n    </author>\\n    <author>\\n      <name>Yarin Gal</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.09733v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.09733v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.04439v1</id>\\n    <updated>2018-07-12T06:43:12Z</updated>\\n    <published>2018-07-12T06:43:12Z</published>\\n    <title>Will it Blend? Composing Value Functions in Reinforcement Learning</title>\\n    <summary>  An important property for lifelong-learning agents is the ability to combine\\nexisting skills to solve unseen tasks. In general, however, it is unclear how\\nto compose skills in a principled way. We provide a \"recipe\" for optimal value\\nfunction composition in entropy-regularised reinforcement learning (RL) and\\nthen extend this to the standard RL setting. Composition is demonstrated in a\\nvideo game environment, where an agent with an existing library of policies is\\nable to solve new tasks without the need for further learning.\\n</summary>\\n    <author>\\n      <name>Benjamin van Niekerk</name>\\n    </author>\\n    <author>\\n      <name>Steven James</name>\\n    </author>\\n    <author>\\n      <name>Adam Earle</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Rosman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 2nd Lifelong Learning: A Reinforcement Learning Approach (LLARLA)\\n  Workshop, Stockholm, Sweden, FAIM 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.04439v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.04439v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.05443v1</id>\\n    <updated>2018-08-16T12:28:11Z</updated>\\n    <published>2018-08-16T12:28:11Z</published>\\n    <title>Transfer Learning and Organic Computing for Autonomous Vehicles</title>\\n    <summary>  Autonomous Vehicles(AV) are one of the brightest promises of the future which\\nwould help cut down fatalities and improve travel time while working in\\nharmony. Autonomous vehicles will face with challenging situations and\\nexperiences not seen before. These experiences should be converted to knowledge\\nand help the vehicle prepare better in the future. Online Transfer Learning\\nwill help transferring prior knowledge to a new task and also keep the\\nknowledge updated as the task evolves. This paper presents the different\\nmethods of transfer learning, online transfer learning and organic computing\\nthat could be adapted to the domain of autonomous vehicles.\\n</summary>\\n    <author>\\n      <name>Christofer Fellicious</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 2 figures, survey of papers and methods in transfer\\n  learning, organic computing and online transfer learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.05443v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.05443v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.08323v1</id>\\n    <updated>2018-10-19T00:56:42Z</updated>\\n    <published>2018-10-19T00:56:42Z</published>\\n    <title>Learning Multi-Layer Transform Models</title>\\n    <summary>  Learned data models based on sparsity are widely used in signal processing\\nand imaging applications. A variety of methods for learning synthesis\\ndictionaries, sparsifying transforms, etc., have been proposed in recent years,\\noften imposing useful structures or properties on the models. In this work, we\\nfocus on sparsifying transform learning, which enjoys a number of advantages.\\nWe consider multi-layer or nested extensions of the transform model, and\\npropose efficient learning algorithms. Numerical experiments with image data\\nillustrate the behavior of the multi-layer transform learning algorithm and its\\nusefulness for image denoising. Multi-layer models provide better denoising\\nquality than single layer schemes.\\n</summary>\\n    <author>\\n      <name>Saiprasad Ravishankar</name>\\n    </author>\\n    <author>\\n      <name>Brendt Wohlberg</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In Proceedings of the Annual Allerton Conference on Communication,\\n  Control, and Computing, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.08323v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.08323v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.07579v1</id>\\n    <updated>2018-11-19T09:45:20Z</updated>\\n    <published>2018-11-19T09:45:20Z</published>\\n    <title>Deep Active Learning with a Neural Architecture Search</title>\\n    <summary>  We consider active learning of deep neural networks. Most active learning\\nworks in this context have focused on studying effective querying mechanisms\\nand assumed that an appropriate network architecture is a priori known for the\\nproblem at hand. We challenge this assumption and propose a novel active\\nstrategy whereby the learning algorithm searches for effective architectures on\\nthe fly, while actively learning. We apply our strategy using three known\\nquerying techniques (softmax response, MC-dropout, and coresets) and show that\\nthe proposed approach overwhelmingly outperforms active learning using fixed\\narchitectures.\\n</summary>\\n    <author>\\n      <name>Yonatan Geifman</name>\\n    </author>\\n    <author>\\n      <name>Ran El-Yaniv</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.07579v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.07579v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.12273v1</id>\\n    <updated>2018-11-29T16:06:57Z</updated>\\n    <published>2018-11-29T16:06:57Z</published>\\n    <title>On the Transferability of Representations in Neural Networks Between\\n  Datasets and Tasks</title>\\n    <summary>  Deep networks, composed of multiple layers of hierarchical distributed\\nrepresentations, tend to learn low-level features in initial layers and\\ntransition to high-level features towards final layers. Paradigms such as\\ntransfer learning, multi-task learning, and continual learning leverage this\\nnotion of generic hierarchical distributed representations to share knowledge\\nacross datasets and tasks. Herein, we study the layer-wise transferability of\\nrepresentations in deep networks across a few datasets and tasks and note some\\ninteresting empirical observations.\\n</summary>\\n    <author>\\n      <name>Haytham M. Fayek</name>\\n    </author>\\n    <author>\\n      <name>Lawrence Cavedon</name>\\n    </author>\\n    <author>\\n      <name>Hong Ren Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted Paper in the Continual Learning Workshop, NeurIPS 2018</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Continual Learning Workshop, 32nd Neural Information Processing\\n  Systems (NeurIPS 2018), Montreal, Canada</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1811.12273v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.12273v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.01449v1</id>\\n    <updated>2019-02-04T20:10:31Z</updated>\\n    <published>2019-02-04T20:10:31Z</published>\\n    <title>Generalization Bounds For Unsupervised and Semi-Supervised Learning With\\n  Autoencoders</title>\\n    <summary>  Autoencoders are widely used for unsupervised learning and as a\\nregularization scheme in semi-supervised learning. However, theoretical\\nunderstanding of their generalization properties and of the manner in which\\nthey can assist supervised learning has been lacking. We utilize recent\\nadvances in the theory of deep learning generalization, together with a novel\\nreconstruction loss, to provide generalization bounds for autoencoders. To the\\nbest of our knowledge, this is the first such bound. We further show that,\\nunder appropriate assumptions, an autoencoder with good generalization\\nproperties can improve any semi-supervised learning scheme. We support our\\ntheoretical results with empirical demonstrations.\\n</summary>\\n    <author>\\n      <name>Baruch Epstein</name>\\n    </author>\\n    <author>\\n      <name>Ron Meir</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to COLT 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.01449v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.01449v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.03234v1</id>\\n    <updated>2019-03-08T00:54:49Z</updated>\\n    <published>2019-03-08T00:54:49Z</published>\\n    <title>Dyna-AIL : Adversarial Imitation Learning by Planning</title>\\n    <summary>  Adversarial methods for imitation learning have been shown to perform well on\\nvarious control tasks. However, they require a large number of environment\\ninteractions for convergence. In this paper, we propose an end-to-end\\ndifferentiable adversarial imitation learning algorithm in a Dyna-like\\nframework for switching between model-based planning and model-free learning\\nfrom expert data. Our results on both discrete and continuous environments show\\nthat our approach of using model-based planning along with model-free learning\\nconverges to an optimal policy with fewer number of environment interactions in\\ncomparison to the state-of-the-art learning methods.\\n</summary>\\n    <author>\\n      <name>Vaibhav Saxena</name>\\n    </author>\\n    <author>\\n      <name>Srinivasan Sivanandan</name>\\n    </author>\\n    <author>\\n      <name>Pulkit Mathur</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 6 figures, pre-print</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.03234v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.03234v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.01777v4</id>\\n    <updated>2018-06-13T00:56:57Z</updated>\\n    <published>2018-01-03T23:47:52Z</published>\\n    <title>Deep Learning for Forecasting Stock Returns in the Cross-Section</title>\\n    <summary>  Many studies have been undertaken by using machine learning techniques,\\nincluding neural networks, to predict stock returns. Recently, a method known\\nas deep learning, which achieves high performance mainly in image recognition\\nand speech recognition, has attracted attention in the machine learning field.\\nThis paper implements deep learning to predict one-month-ahead stock returns in\\nthe cross-section in the Japanese stock market and investigates the performance\\nof the method. Our results show that deep neural networks generally outperform\\nshallow neural networks, and the best networks also outperform representative\\nmachine learning models. These results indicate that deep learning shows\\npromise as a skillful machine learning method to predict stock returns in the\\ncross-section.\\n</summary>\\n    <author>\\n      <name>Masaya Abe</name>\\n    </author>\\n    <author>\\n      <name>Hideki Nakayama</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 2 figures, 8 tables, accepted at PAKDD 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.01777v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.01777v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.04380v1</id>\\n    <updated>2018-11-11T09:45:41Z</updated>\\n    <published>2018-11-11T09:45:41Z</published>\\n    <title>ReSet: Learning Recurrent Dynamic Routing in ResNet-like Neural Networks</title>\\n    <summary>  Neural Network is a powerful Machine Learning tool that shows outstanding\\nperformance in Computer Vision, Natural Language Processing, and Artificial\\nIntelligence. In particular, recently proposed ResNet architecture and its\\nmodifications produce state-of-the-art results in image classification\\nproblems. ResNet and most of the previously proposed architectures have a fixed\\nstructure and apply the same transformation to all input images. In this work,\\nwe develop a ResNet-based model that dynamically selects Computational Units\\n(CU) for each input object from a learned set of transformations. Dynamic\\nselection allows the network to learn a sequence of useful transformations and\\napply only required units to predict the image label. We compare our model to\\nResNet-38 architecture and achieve better results than the original ResNet on\\nCIFAR-10.1 test set. While examining the produced paths, we discovered that the\\nnetwork learned different routes for images from different classes and similar\\nroutes for similar images.\\n</summary>\\n    <author>\\n      <name>Iurii Kemaev</name>\\n    </author>\\n    <author>\\n      <name>Daniil Polykovskiy</name>\\n    </author>\\n    <author>\\n      <name>Dmitry Vetrov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in Proceedings of The 10th Asian Conference on Machine\\n  Learning, http://proceedings.mlr.press/v95/kemaev18a.html</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of The 10th Asian Conference on Machine Learning, PMLR\\n  95:422-437, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1811.04380v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.04380v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.12560v2</id>\\n    <updated>2018-12-03T09:10:53Z</updated>\\n    <published>2018-11-30T00:57:30Z</published>\\n    <title>An Introduction to Deep Reinforcement Learning</title>\\n    <summary>  Deep reinforcement learning is the combination of reinforcement learning (RL)\\nand deep learning. This field of research has been able to solve a wide range\\nof complex decision-making tasks that were previously out of reach for a\\nmachine. Thus, deep RL opens up many new applications in domains such as\\nhealthcare, robotics, smart grids, finance, and many more. This manuscript\\nprovides an introduction to deep reinforcement learning models, algorithms and\\ntechniques. Particular focus is on the aspects related to generalization and\\nhow deep RL can be used for practical applications. We assume the reader is\\nfamiliar with basic machine learning concepts.\\n</summary>\\n    <author>\\n      <name>Vincent Francois-Lavet</name>\\n    </author>\\n    <author>\\n      <name>Peter Henderson</name>\\n    </author>\\n    <author>\\n      <name>Riashat Islam</name>\\n    </author>\\n    <author>\\n      <name>Marc G. Bellemare</name>\\n    </author>\\n    <author>\\n      <name>Joelle Pineau</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1561/2200000071</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1561/2200000071\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Foundations and Trends in Machine Learning: Vol. 11, No. 3-4, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1811.12560v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.12560v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00524v1</id>\\n    <updated>2018-12-03T02:26:35Z</updated>\\n    <published>2018-12-03T02:26:35Z</published>\\n    <title>Generalization in anti-causal learning</title>\\n    <summary>  The ability to learn and act in novel situations is still a prerogative of\\nanimate intelligence, as current machine learning methods mostly fail when\\nmoving beyond the standard i.i.d. setting. What is the reason for this\\ndiscrepancy? Most machine learning tasks are anti-causal, i.e., we infer causes\\n(labels) from effects (observations). Typically, in supervised learning we\\nbuild systems that try to directly invert causal mechanisms. Instead, in this\\npaper we argue that strong generalization capabilities crucially hinge on\\nsearching and validating meaningful hypotheses, requiring access to a causal\\nmodel. In such a framework, we want to find a cause that leads to the observed\\neffect. Anti-causal models are used to drive this search, but a causal model is\\nrequired for validation. We investigate the fundamental differences between\\ncausal and anti-causal tasks, discuss implications for topics ranging from\\nadversarial attacks to disentangling factors of variation, and provide\\nextensive evidence from the literature to substantiate our view. We advocate\\nfor incorporating causal models in supervised learning to shift the paradigm\\nfrom inference only, to search and validation.\\n</summary>\\n    <author>\\n      <name>Niki Kilbertus</name>\\n    </author>\\n    <author>\\n      <name>Giambattista Parascandolo</name>\\n    </author>\\n    <author>\\n      <name>Bernhard Sch\\xc3\\xb6lkopf</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A shorter version of this paper appeared at the workshop on\\n  `Critiquing and correcting trends in machine learning` at NeurIPS 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.00524v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00524v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.01934v1</id>\\n    <updated>2019-04-03T11:54:22Z</updated>\\n    <published>2019-04-03T11:54:22Z</published>\\n    <title>Optimization under Uncertainty in the Era of Big Data and Deep Learning:\\n  When Machine Learning Meets Mathematical Programming</title>\\n    <summary>  This paper reviews recent advances in the field of optimization under\\nuncertainty via a modern data lens, highlights key research challenges and\\npromise of data-driven optimization that organically integrates machine\\nlearning and mathematical programming for decision-making under uncertainty,\\nand identifies potential research opportunities. A brief review of classical\\nmathematical programming techniques for hedging against uncertainty is first\\npresented, along with their wide spectrum of applications in Process Systems\\nEngineering. A comprehensive review and classification of the relevant\\npublications on data-driven distributionally robust optimization, data-driven\\nchance constrained program, data-driven robust optimization, and data-driven\\nscenario-based optimization is then presented. This paper also identifies\\nfertile avenues for future research that focuses on a closed-loop data-driven\\noptimization framework, which allows the feedback from mathematical programming\\nto machine learning, as well as scenario-based optimization leveraging the\\npower of deep learning techniques. Perspectives on online learning-based\\ndata-driven multistage optimization with a learning-while-optimizing scheme is\\npresented.\\n</summary>\\n    <author>\\n      <name>Chao Ning</name>\\n    </author>\\n    <author>\\n      <name>Fengqi You</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.compchemeng.2019.03.034</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.compchemeng.2019.03.034\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1904.01934v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.01934v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cond-mat/0010423v1</id>\\n    <updated>2000-10-26T15:11:01Z</updated>\\n    <published>2000-10-26T15:11:01Z</published>\\n    <title>Hierarchical learning in polynomial Support Vector Machines</title>\\n    <summary>  We study the typical properties of polynomial Support Vector Machines within\\na Statistical Mechanics approach that allows us to analyze the effect of\\ndifferent normalizations of the features. If the normalization is adecuately\\nchosen, there is a hierarchical learning of features of increasing order as a\\nfunction of the training set size.\\n</summary>\\n    <author>\\n      <name>Sebastian Risau-Gusman</name>\\n    </author>\\n    <author>\\n      <name>Mirta B. Gordon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 7 figures, submitted to Machine Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cond-mat/0010423v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cond-mat/0010423v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1110.0593v1</id>\\n    <updated>2011-10-04T07:34:13Z</updated>\\n    <published>2011-10-04T07:34:13Z</published>\\n    <title>Two Projection Pursuit Algorithms for Machine Learning under\\n  Non-Stationarity</title>\\n    <summary>  This thesis derives, tests and applies two linear projection algorithms for\\nmachine learning under non-stationarity. The first finds a direction in a\\nlinear space upon which a data set is maximally non-stationary. The second aims\\nto robustify two-way classification against non-stationarity. The algorithm is\\ntested on a key application scenario, namely Brain Computer Interfacing.\\n</summary>\\n    <author>\\n      <name>Duncan A. J. Blythe</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1110.0593v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1110.0593v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.3078v1</id>\\n    <updated>2013-01-14T17:57:18Z</updated>\\n    <published>2013-01-14T17:57:18Z</published>\\n    <title>Fano schemes of generic intersections and machine learning</title>\\n    <summary>  We investigate Fano schemes of conditionally generic intersections, i.e. of\\nhypersurfaces in projective space chosen generically up to additional\\nconditions. Via a correspondence between generic properties of algebraic\\nvarieties and events in probability spaces that occur with probability one, we\\nuse the obtained results on Fano schemes to solve a problem in machine\\nlearning.\\n</summary>\\n    <author>\\n      <name>Franz Kir\\xc3\\xa1ly</name>\\n    </author>\\n    <author>\\n      <name>Paul Larsen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, comments very welcome!</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.3078v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.3078v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.AG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"14J40, 14J70, 14M15, 62M10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1204.6509v1</id>\\n    <updated>2012-04-29T19:31:15Z</updated>\\n    <published>2012-04-29T19:31:15Z</published>\\n    <title>Dissimilarity Clustering by Hierarchical Multi-Level Refinement</title>\\n    <summary>  We introduce in this paper a new way of optimizing the natural extension of\\nthe quantization error using in k-means clustering to dissimilarity data. The\\nproposed method is based on hierarchical clustering analysis combined with\\nmulti-level heuristic refinement. The method is computationally efficient and\\nachieves better quantization errors than the\\n</summary>\\n    <author>\\n      <name>Brieuc Conan-Guez</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LITA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fabrice Rossi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">20-th European Symposium on Artificial Neural Networks, Computational\\n  Intelligence and Machine Learning (ESANN 2012), Bruges : Belgium (2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1204.6509v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1204.6509v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6446v1</id>\\n    <updated>2012-06-27T19:59:59Z</updated>\\n    <published>2012-06-27T19:59:59Z</published>\\n    <title>Agglomerative Bregman Clustering</title>\\n    <summary>  This manuscript develops the theory of agglomerative clustering with Bregman\\ndivergences. Geometric smoothing techniques are developed to deal with\\ndegenerate clusters. To allow for cluster models based on exponential families\\nwith overcomplete representations, Bregman divergences are developed for\\nnondifferentiable convex functions.\\n</summary>\\n    <author>\\n      <name>Matus Telgarsky</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">UCSD</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Sanjoy Dasgupta</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">UCSD</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6446v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6446v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.06410v1</id>\\n    <updated>2015-03-22T11:32:34Z</updated>\\n    <published>2015-03-22T11:32:34Z</published>\\n    <title>What the F-measure doesn\\'t measure: Features, Flaws, Fallacies and Fixes</title>\\n    <summary>  The F-measure or F-score is one of the most commonly used single number\\nmeasures in Information Retrieval, Natural Language Processing and Machine\\nLearning, but it is based on a mistake, and the flawed assumptions render it\\nunsuitable for use in most contexts! Fortunately, there are better\\nalternatives.\\n</summary>\\n    <author>\\n      <name>David M. W. Powers</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.06410v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.06410v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.6013v1</id>\\n    <updated>2014-02-24T23:12:42Z</updated>\\n    <published>2014-02-24T23:12:42Z</published>\\n    <title>Open science in machine learning</title>\\n    <summary>  We present OpenML and mldata, open science platforms that provides easy\\naccess to machine learning data, software and results to encourage further\\nstudy and application. They go beyond the more traditional repositories for\\ndata sets and software packages in that they allow researchers to also easily\\nshare the results they obtained in experiments and to compare their solutions\\nwith those of others.\\n</summary>\\n    <author>\\n      <name>Joaquin Vanschoren</name>\\n    </author>\\n    <author>\\n      <name>Mikio L. Braun</name>\\n    </author>\\n    <author>\\n      <name>Cheng Soon Ong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1402.6013v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.6013v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.05264v1</id>\\n    <updated>2017-05-15T14:25:15Z</updated>\\n    <published>2017-05-15T14:25:15Z</published>\\n    <title>Extending Defensive Distillation</title>\\n    <summary>  Machine learning is vulnerable to adversarial examples: inputs carefully\\nmodified to force misclassification. Designing defenses against such inputs\\nremains largely an open problem. In this work, we revisit defensive\\ndistillation---which is one of the mechanisms proposed to mitigate adversarial\\nexamples---to address its limitations. We view our results not only as an\\neffective way of addressing some of the recently discovered attacks but also as\\nreinforcing the importance of improved training techniques.\\n</summary>\\n    <author>\\n      <name>Nicolas Papernot</name>\\n    </author>\\n    <author>\\n      <name>Patrick McDaniel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.05264v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.05264v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.01666v1</id>\\n    <updated>2017-09-06T04:07:39Z</updated>\\n    <published>2017-09-06T04:07:39Z</published>\\n    <title>Descriptors for Machine Learning of Materials Data</title>\\n    <summary>  Descriptors, which are representations of compounds, play an essential role\\nin machine learning of materials data. Although many representations of\\nelements and structures of compounds are known, these representations are\\ndifficult to use as descriptors in their unchanged forms. This chapter shows\\nhow compounds in a dataset can be represented as descriptors and applied to\\nmachine-learning models for materials datasets.\\n</summary>\\n    <author>\\n      <name>Atsuto Seko</name>\\n    </author>\\n    <author>\\n      <name>Atsushi Togo</name>\\n    </author>\\n    <author>\\n      <name>Isao Tanaka</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 10 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1709.01666v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.01666v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.09889v3</id>\\n    <updated>2017-12-12T02:07:47Z</updated>\\n    <published>2017-11-27T03:33:55Z</published>\\n    <title>Proceedings of NIPS 2017 Symposium on Interpretable Machine Learning</title>\\n    <summary>  This is the Proceedings of NIPS 2017 Symposium on Interpretable Machine\\nLearning, held in Long Beach, California, USA on December 7, 2017\\n</summary>\\n    <author>\\n      <name>Andrew Gordon Wilson</name>\\n    </author>\\n    <author>\\n      <name>Jason Yosinski</name>\\n    </author>\\n    <author>\\n      <name>Patrice Simard</name>\\n    </author>\\n    <author>\\n      <name>Rich Caruana</name>\\n    </author>\\n    <author>\\n      <name>William Herlands</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">25 papers</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.09889v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.09889v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.08101v1</id>\\n    <updated>2018-03-21T19:38:27Z</updated>\\n    <published>2018-03-21T19:38:27Z</published>\\n    <title>Clustering to Reduce Spatial Data Set Size</title>\\n    <summary>  Traditionally it had been a problem that researchers did not have access to\\nenough spatial data to answer pressing research questions or build compelling\\nvisualizations. Today, however, the problem is often that we have too much\\ndata. Spatially redundant or approximately redundant points may refer to a\\nsingle feature (plus noise) rather than many distinct spatial features. We use\\na machine learning approach with density-based clustering to compress such\\nspatial data into a set of representative features.\\n</summary>\\n    <author>\\n      <name>Geoff Boeing</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.08101v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.08101v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.08049v1</id>\\n    <updated>2018-06-21T02:33:44Z</updated>\\n    <published>2018-06-21T02:33:44Z</published>\\n    <title>On the Robustness of Interpretability Methods</title>\\n    <summary>  We argue that robustness of explanations---i.e., that similar inputs should\\ngive rise to similar explanations---is a key desideratum for interpretability.\\nWe introduce metrics to quantify robustness and demonstrate that current\\nmethods do not perform well according to these metrics. Finally, we propose\\nways that robustness can be enforced on existing interpretability approaches.\\n</summary>\\n    <author>\\n      <name>David Alvarez-Melis</name>\\n    </author>\\n    <author>\\n      <name>Tommi S. Jaakkola</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">presented at 2018 ICML Workshop on Human Interpretability in Machine\\n  Learning (WHI 2018), Stockholm, Sweden</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.08049v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.08049v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.01334v1</id>\\n    <updated>2018-07-03T18:13:55Z</updated>\\n    <published>2018-07-03T18:13:55Z</published>\\n    <title>Breast Cancer Diagnosis via Classification Algorithms</title>\\n    <summary>  In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using\\nMachine Learning classification techniques, such as the SVM, Bayesian Logistic\\nRegression (Variational Approximation), and K-Nearest-Neighbors. We describe\\neach model, and compare their performance through different measures. We\\nconclude that SVM has the best performance among all other classifiers, while\\nit competes closely with the Bayesian Logistic Regression that is ranked second\\nbest method for this dataset.\\n</summary>\\n    <author>\\n      <name>Reihaneh Entezari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.01334v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.01334v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.06275v1</id>\\n    <updated>2018-08-20T00:24:41Z</updated>\\n    <published>2018-08-20T00:24:41Z</published>\\n    <title>Applying Machine Learning To Maize Traits Prediction</title>\\n    <summary>  Heterosis is the improved or increased function of any biological quality in\\na hybrid offspring. We have studied yet the largest maize SNP dataset for\\ntraits prediction. We develop linear and non-linear models which consider\\nrelationships between different hybrids as well as other effect. Specially\\ndesigned model proved to be efficient and robust in prediction maize\\'s traits.\\n</summary>\\n    <author>\\n      <name>Binbin Shi</name>\\n    </author>\\n    <author>\\n      <name>Xupeng Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.06275v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.06275v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.00593v1</id>\\n    <updated>2018-09-03T13:21:28Z</updated>\\n    <published>2018-09-03T13:21:28Z</published>\\n    <title>IoU is not submodular</title>\\n    <summary>  This short article aims at demonstrate that the Intersection over Union (or\\nJaccard index) is not a submodular function. This mistake has been made in an\\narticle which is cited and used as a foundation in another article. The\\nIntersection of Union is widely used in machine learning as a cost function\\nespecially for imbalance data and semantic segmentation.\\n</summary>\\n    <author>\\n      <name>Tanguy Kerdoncuff</name>\\n    </author>\\n    <author>\\n      <name>R\\xc3\\xa9mi Emonet</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1809.00593v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.00593v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.04684v1</id>\\n    <updated>2018-09-12T21:29:20Z</updated>\\n    <published>2018-09-12T21:29:20Z</published>\\n    <title>Fair lending needs explainable models for responsible recommendation</title>\\n    <summary>  The financial services industry has unique explainability and fairness\\nchallenges arising from compliance and ethical considerations in credit\\ndecisioning. These challenges complicate the use of model machine learning and\\nartificial intelligence methods in business decision processes.\\n</summary>\\n    <author>\\n      <name>Jiahao Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, position paper accepted for FATREC 2018 conference at ACM\\n  RecSys</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.04684v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.04684v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"91G40, 68T01\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"J.1; I.5.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.09623v1</id>\\n    <updated>2018-11-23T06:25:20Z</updated>\\n    <published>2018-11-23T06:25:20Z</published>\\n    <title>Nonlinear Regression without i.i.d. Assumption</title>\\n    <summary>  In this paper, we consider a class of nonlinear regression problems without\\nthe assumption of being independent and identically distributed. We propose a\\ncorrespondent mini-max problem for nonlinear regression and outline a numerical\\nalgorithm. Such an algorithm can be applied in regression and machine learning\\nproblems, and yield better results than traditional regression and machine\\nlearning methods.\\n</summary>\\n    <author>\\n      <name>Qing Xu</name>\\n    </author>\\n    <author>\\n      <name>Xiaohua Xuan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.09623v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.09623v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.07825v1</id>\\n    <updated>2019-03-19T04:28:18Z</updated>\\n    <published>2019-03-19T04:28:18Z</published>\\n    <title>Machine Learning for removing EEG artifacts: Setting the benchmark</title>\\n    <summary>  Electroencephalograms (EEG) are often contaminated by artifacts which make\\ninterpreting them more challenging for clinicians. Hence, automated artifact\\nrecognition systems have the potential to aid the clinical workflow. In this\\nabstract, we share the first results on applying various machine learning\\nalgorithms to the recently released world\\'s largest open-source artifact\\nrecognition dataset. We envision that these results will serve as a benchmark\\nfor researchers who might work with this dataset in future.\\n</summary>\\n    <author>\\n      <name>Subhrajit Roy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.07825v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.07825v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.01517v1</id>\\n    <updated>2019-04-02T15:55:32Z</updated>\\n    <published>2019-04-02T15:55:32Z</published>\\n    <title>Convergence rates for the stochastic gradient descent method for\\n  non-convex objective functions</title>\\n    <summary>  We prove the local convergence to minima and estimates on the rate of\\nconvergence for the stochastic gradient descent method in the case of not\\nnecessarily globally convex nor contracting objective functions. In particular,\\nthe results are applicable to simple objective functions arising in machine\\nlearning.\\n</summary>\\n    <author>\\n      <name>Benjamin Fehrman</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Gess</name>\\n    </author>\\n    <author>\\n      <name>Arnulf Jentzen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">52 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.01517v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.01517v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.10052v2</id>\\n    <updated>2018-12-16T09:58:09Z</updated>\\n    <published>2018-11-25T16:40:42Z</published>\\n    <title>An overview of deep learning in medical imaging focusing on MRI</title>\\n    <summary>  What has happened in machine learning lately, and what does it mean for the\\nfuture of medical image analysis? Machine learning has witnessed a tremendous\\namount of attention over the last few years. The current boom started around\\n2009 when so-called deep artificial neural networks began outperforming other\\nestablished models on a number of important benchmarks. Deep neural networks\\nare now the state-of-the-art machine learning models across a variety of areas,\\nfrom image analysis to natural language processing, and widely deployed in\\nacademia and industry. These developments have a huge potential for medical\\nimaging technology, medical data analysis, medical diagnostics and healthcare\\nin general, slowly being realized. We provide a short overview of recent\\nadvances and some associated challenges in machine learning applied to medical\\nimage processing and image analysis. As this has become a very broad and fast\\nexpanding field we will not survey the entire landscape of applications, but\\nput particular focus on deep learning in MRI.\\n  Our aim is threefold: (i) give a brief introduction to deep learning with\\npointers to core references; (ii) indicate how deep learning has been applied\\nto the entire MRI processing chain, from acquisition to image retrieval, from\\nsegmentation to disease prediction; (iii) provide a starting point for people\\ninterested in experimenting and perhaps contributing to the field of machine\\nlearning for medical imaging by pointing out good educational resources,\\nstate-of-the-art open-source code, and interesting sources of data and problems\\nrelated medical imaging.\\n</summary>\\n    <author>\\n      <name>Alexander Selvikv\\xc3\\xa5g Lundervold</name>\\n    </author>\\n    <author>\\n      <name>Arvid Lundervold</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.zemedi.2018.11.002</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.zemedi.2018.11.002\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Minor updates. Close to the version published in Zeitschrift f\\\\\"ur\\n  Medizinische Physik (Available online 13 December 2018)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.10052v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.10052v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0810.4752v1</id>\\n    <updated>2008-10-27T08:25:20Z</updated>\\n    <published>2008-10-27T08:25:20Z</published>\\n    <title>Statistical Learning Theory: Models, Concepts, and Results</title>\\n    <summary>  Statistical learning theory provides the theoretical basis for many of\\ntoday\\'s machine learning algorithms. In this article we attempt to give a\\ngentle, non-technical overview over the key ideas and insights of statistical\\nlearning theory. We target at a broad audience, not necessarily machine\\nlearning researchers. This paper can serve as a starting point for people who\\nwant to get an overview on the field before diving into technical details.\\n</summary>\\n    <author>\\n      <name>Ulrike von Luxburg</name>\\n    </author>\\n    <author>\\n      <name>Bernhard Schoelkopf</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0810.4752v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0810.4752v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1101.4439v2</id>\\n    <updated>2011-01-27T14:45:29Z</updated>\\n    <published>2011-01-24T03:39:57Z</published>\\n    <title>Reproducing Kernel Banach Spaces with the l1 Norm II: Error Analysis for\\n  Regularized Least Square Regression</title>\\n    <summary>  A typical approach in estimating the learning rate of a regularized learning\\nscheme is to bound the approximation error by the sum of the sampling error,\\nthe hypothesis error and the regularization error. Using a reproducing kernel\\nspace that satisfies the linear representer theorem brings the advantage of\\ndiscarding the hypothesis error from the sum automatically. Following this\\ndirection, we illustrate how reproducing kernel Banach spaces with the l1 norm\\ncan be applied to improve the learning rate estimate of l1-regularization in\\nmachine learning.\\n</summary>\\n    <author>\\n      <name>Guohui Song</name>\\n    </author>\\n    <author>\\n      <name>Haizhang Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1101.4439v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1101.4439v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.FA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.0393v3</id>\\n    <updated>2017-02-18T00:34:19Z</updated>\\n    <published>2013-06-03T13:10:35Z</published>\\n    <title>Learning from networked examples in a k-partite graph</title>\\n    <summary>  Many machine learning algorithms are based on the assumption that training\\nexamples are drawn independently. However, this assumption does not hold\\nanymore when learning from a networked sample where two or more training\\nexamples may share common features. We propose an efficient weighting method\\nfor learning from networked examples and show the sample error bound which is\\nbetter than previous work.\\n</summary>\\n    <author>\\n      <name>Yuyi Wang</name>\\n    </author>\\n    <author>\\n      <name>Jan Ramon</name>\\n    </author>\\n    <author>\\n      <name>Zheng-Chu Guo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">a special case</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.0393v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.0393v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.2115v1</id>\\n    <updated>2013-01-10T13:29:17Z</updated>\\n    <published>2013-01-10T13:29:17Z</published>\\n    <title>Domain Generalization via Invariant Feature Representation</title>\\n    <summary>  This paper investigates domain generalization: How to take knowledge acquired\\nfrom an arbitrary number of related domains and apply it to previously unseen\\ndomains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based\\noptimization algorithm that learns an invariant transformation by minimizing\\nthe dissimilarity across domains, whilst preserving the functional relationship\\nbetween input and output variables. A learning-theoretic analysis shows that\\nreducing dissimilarity improves the expected generalization ability of\\nclassifiers on new domains, motivating the proposed algorithm. Experimental\\nresults on synthetic and real-world datasets demonstrate that DICA successfully\\nlearns invariant features and improves classifier performance in practice.\\n</summary>\\n    <author>\\n      <name>Krikamol Muandet</name>\\n    </author>\\n    <author>\\n      <name>David Balduzzi</name>\\n    </author>\\n    <author>\\n      <name>Bernhard Sch\\xc3\\xb6lkopf</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The 30th International Conference on Machine Learning (ICML 2013)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.2115v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.2115v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1601.07482v1</id>\\n    <updated>2016-01-27T18:19:32Z</updated>\\n    <published>2016-01-27T18:19:32Z</published>\\n    <title>Unsupervised Learning in Neuromemristive Systems</title>\\n    <summary>  Neuromemristive systems (NMSs) currently represent the most promising\\nplatform to achieve energy efficient neuro-inspired computation. However, since\\nthe research field is less than a decade old, there are still countless\\nalgorithms and design paradigms to be explored within these systems. One\\nparticular domain that remains to be fully investigated within NMSs is\\nunsupervised learning. In this work, we explore the design of an NMS for\\nunsupervised clustering, which is a critical element of several machine\\nlearning algorithms. Using a simple memristor crossbar architecture and\\nlearning rule, we are able to achieve performance which is on par with MATLAB\\'s\\nk-means clustering.\\n</summary>\\n    <author>\\n      <name>Cory Merkel</name>\\n    </author>\\n    <author>\\n      <name>Dhireesha Kudithipudi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in the proceedings of the National Aerospace &amp; Electronics\\n  Conference &amp; Ohio Innovation Summit (NAECON-OIS\\'15)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1601.07482v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1601.07482v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.01176v1</id>\\n    <updated>2015-02-04T12:27:04Z</updated>\\n    <published>2015-02-04T12:27:04Z</published>\\n    <title>Learning Local Invariant Mahalanobis Distances</title>\\n    <summary>  For many tasks and data types, there are natural transformations to which the\\ndata should be invariant or insensitive. For instance, in visual recognition,\\nnatural images should be insensitive to rotation and translation. This\\nrequirement and its implications have been important in many machine learning\\napplications, and tolerance for image transformations was primarily achieved by\\nusing robust feature vectors. In this paper we propose a novel and\\ncomputationally efficient way to learn a local Mahalanobis metric per datum,\\nand show how we can learn a local invariant metric to any transformation in\\norder to improve performance.\\n</summary>\\n    <author>\\n      <name>Ethan Fetaya</name>\\n    </author>\\n    <author>\\n      <name>Shimon Ullman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.01176v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.01176v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.00473v4</id>\\n    <updated>2016-02-07T16:35:39Z</updated>\\n    <published>2015-07-02T08:43:21Z</published>\\n    <title>The Optimal Sample Complexity of PAC Learning</title>\\n    <summary>  This work establishes a new upper bound on the number of samples sufficient\\nfor PAC learning in the realizable case. The bound matches known lower bounds\\nup to numerical constant factors. This solves a long-standing open problem on\\nthe sample complexity of PAC learning. The technique and analysis build on a\\nrecent breakthrough by Hans Simon.\\n</summary>\\n    <author>\\n      <name>Steve Hanneke</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research, Vol. 17 (2016), No. 38, pp.\\n  1-15</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1507.00473v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.00473v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.05536v1</id>\\n    <updated>2016-09-18T19:58:48Z</updated>\\n    <published>2016-09-18T19:58:48Z</published>\\n    <title>Learning Personalized Optimal Control for Repeatedly Operated Systems</title>\\n    <summary>  We consider the problem of online learning of optimal control for repeatedly\\noperated systems in the presence of parametric uncertainty. During each round\\nof operation, environment selects system parameters according to a fixed but\\nunknown probability distribution. These parameters govern the dynamics of a\\nplant. An agent chooses a control input to the plant and is then revealed the\\ncost of the choice. In this setting, we design an agent that personalizes the\\ncontrol input to this plant taking into account the stochasticity involved. We\\ndemonstrate the effectiveness of our approach on a simulated system.\\n</summary>\\n    <author>\\n      <name>Theja Tulabandhula</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This work was presented at the NIPS 2015 Workshop: Machine Learning\\n  From and For Adaptive User Technologies: From Active Learning &amp;\\n  Experimentation to Optimization &amp; Personalization (ref.\\n  https://sites.google.com/site/mlaihci)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1609.05536v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.05536v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.07608v4</id>\\n    <updated>2019-03-04T23:48:32Z</updated>\\n    <published>2017-03-22T11:53:53Z</published>\\n    <title>Deep Exploration via Randomized Value Functions</title>\\n    <summary>  We study the use of randomized value functions to guide deep exploration in\\nreinforcement learning. This offers an elegant means for synthesizing\\nstatistically and computationally efficient exploration with common practical\\napproaches to value function learning. We present several reinforcement\\nlearning algorithms that leverage randomized value functions and demonstrate\\ntheir efficacy through computational studies. We also prove a regret bound that\\nestablishes statistical efficiency with a tabular representation.\\n</summary>\\n    <author>\\n      <name>Ian Osband</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Van Roy</name>\\n    </author>\\n    <author>\\n      <name>Daniel Russo</name>\\n    </author>\\n    <author>\\n      <name>Zheng Wen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for publication in Journal of Machine Learning Research 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.07608v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.07608v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.05098v1</id>\\n    <updated>2017-06-15T21:38:12Z</updated>\\n    <published>2017-06-15T21:38:12Z</published>\\n    <title>An Overview of Multi-Task Learning in Deep Neural Networks</title>\\n    <summary>  Multi-task learning (MTL) has led to successes in many applications of\\nmachine learning, from natural language processing and speech recognition to\\ncomputer vision and drug discovery. This article aims to give a general\\noverview of MTL, particularly in deep neural networks. It introduces the two\\nmost common methods for MTL in Deep Learning, gives an overview of the\\nliterature, and discusses recent advances. In particular, it seeks to help ML\\npractitioners apply MTL by shedding light on how MTL works and providing\\nguidelines for choosing appropriate auxiliary tasks.\\n</summary>\\n    <author>\\n      <name>Sebastian Ruder</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.05098v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.05098v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.06046v2</id>\\n    <updated>2018-01-10T00:46:13Z</updated>\\n    <published>2017-08-21T01:28:37Z</published>\\n    <title>nuts-flow/ml: data pre-processing for deep learning</title>\\n    <summary>  Data preprocessing is a fundamental part of any machine learning application\\nand frequently the most time-consuming aspect when developing a machine\\nlearning solution. Preprocessing for deep learning is characterized by\\npipelines that lazily load data and perform data transformation, augmentation,\\nbatching and logging. Many of these functions are common across applications\\nbut require different arrangements for training, testing or inference. Here we\\nintroduce a novel software framework named nuts-flow/ml that encapsulates\\ncommon preprocessing operations as components, which can be flexibly arranged\\nto rapidly construct efficient preprocessing pipelines for deep learning.\\n</summary>\\n    <author>\\n      <name>S. Maetschke</name>\\n    </author>\\n    <author>\\n      <name>R. Tennakoon</name>\\n    </author>\\n    <author>\\n      <name>C. Vecchiola</name>\\n    </author>\\n    <author>\\n      <name>R. Garnavi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1708.06046v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.06046v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.06788v2</id>\\n    <updated>2018-06-20T02:19:13Z</updated>\\n    <published>2017-11-18T01:42:04Z</published>\\n    <title>MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural\\n  Networks</title>\\n    <summary>  We introduce MinimalRNN, a new recurrent neural network architecture that\\nachieves comparable performance as the popular gated RNNs with a simplified\\nstructure. It employs minimal updates within RNN, which not only leads to\\nefficient learning and testing but more importantly better interpretability and\\ntrainability. We demonstrate that by endorsing the more restrictive update\\nrule, MinimalRNN learns disentangled RNN states. We further examine the\\nlearning dynamics of different RNN structures using input-output Jacobians, and\\nshow that MinimalRNN is able to capture longer range dependencies than existing\\nRNN architectures.\\n</summary>\\n    <author>\\n      <name>Minmin Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2017 Symposium on Interpretable Machine Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.06788v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.06788v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.00745v2</id>\\n    <updated>2019-01-09T03:56:36Z</updated>\\n    <published>2018-03-02T07:52:48Z</published>\\n    <title>Quantum Circuit Learning</title>\\n    <summary>  We propose a classical-quantum hybrid algorithm for machine learning on\\nnear-term quantum processors, which we call quantum circuit learning. A quantum\\ncircuit driven by our framework learns a given task by tuning parameters\\nimplemented on it. The iterative optimization of the parameters allows us to\\ncircumvent the high-depth circuit. Theoretical investigation shows that a\\nquantum circuit can approximate nonlinear functions, which is further confirmed\\nby numerical simulations. Hybridizing a low-depth quantum circuit and a\\nclassical computer for machine learning, the proposed framework paves the way\\ntoward applications of near-term quantum devices for quantum machine learning.\\n</summary>\\n    <author>\\n      <name>Kosuke Mitarai</name>\\n    </author>\\n    <author>\\n      <name>Makoto Negoro</name>\\n    </author>\\n    <author>\\n      <name>Masahiro Kitagawa</name>\\n    </author>\\n    <author>\\n      <name>Keisuke Fujii</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevA.98.032309</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevA.98.032309\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Phys. Rev. A 98, 032309 (2018)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1803.00745v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.00745v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.07193v3</id>\\n    <updated>2018-07-27T12:40:44Z</updated>\\n    <published>2018-04-19T14:29:41Z</published>\\n    <title>Lipschitz Continuity in Model-based Reinforcement Learning</title>\\n    <summary>  We examine the impact of learning Lipschitz continuous models in the context\\nof model-based reinforcement learning. We provide a novel bound on multi-step\\nprediction error of Lipschitz models where we quantify the error using the\\nWasserstein metric. We go on to prove an error bound for the value-function\\nestimate arising from Lipschitz models and show that the estimated value\\nfunction is itself Lipschitz. We conclude with empirical results that show the\\nbenefits of controlling the Lipschitz constant of neural-network models.\\n</summary>\\n    <author>\\n      <name>Kavosh Asadi</name>\\n    </author>\\n    <author>\\n      <name>Dipendra Misra</name>\\n    </author>\\n    <author>\\n      <name>Michael L. Littman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for the 35th International Conference on Machine Learning\\n  (ICML 2018)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1804.07193v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.07193v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.02322v1</id>\\n    <updated>2018-06-06T17:39:00Z</updated>\\n    <published>2018-06-06T17:39:00Z</published>\\n    <title>Learning Kolmogorov Models for Binary Random Variables</title>\\n    <summary>  We summarize our recent findings, where we proposed a framework for learning\\na Kolmogorov model, for a collection of binary random variables. More\\nspecifically, we derive conditions that link outcomes of specific random\\nvariables, and extract valuable relations from the data. We also propose an\\nalgorithm for computing the model and show its first-order optimality, despite\\nthe combinatorial nature of the learning problem. We apply the proposed\\nalgorithm to recommendation systems, although it is applicable to other\\nscenarios. We believe that the work is a significant step toward interpretable\\nmachine learning.\\n</summary>\\n    <author>\\n      <name>Hadi Ghauch</name>\\n    </author>\\n    <author>\\n      <name>Mikael Skoglund</name>\\n    </author>\\n    <author>\\n      <name>Hossein Shokri-Ghadikolaei</name>\\n    </author>\\n    <author>\\n      <name>Carlo Fischione</name>\\n    </author>\\n    <author>\\n      <name>Ali H. Sayed</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, accecpted to ICML 2018: Workshop on Nonconvex Optimization</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.02322v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.02322v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.04242v2</id>\\n    <updated>2018-07-02T18:49:35Z</updated>\\n    <published>2018-06-11T21:02:50Z</published>\\n    <title>The Potential of the Return Distribution for Exploration in RL</title>\\n    <summary>  This paper studies the potential of the return distribution for exploration\\nin deterministic reinforcement learning (RL) environments. We study network\\nlosses and propagation mechanisms for Gaussian, Categorical and Gaussian\\nmixture distributions. Combined with exploration policies that leverage this\\nreturn distribution, we solve, for example, a randomized Chain task of length\\n100, which has not been reported before when learning with neural networks.\\n</summary>\\n    <author>\\n      <name>Thomas M. Moerland</name>\\n    </author>\\n    <author>\\n      <name>Joost Broekens</name>\\n    </author>\\n    <author>\\n      <name>Catholijn M. Jonker</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published at the Exploration in Reinforcement Learning Workshop at\\n  the 35th International Conference on Machine Learning, Stockholm, Sweden</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.04242v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.04242v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.06415v1</id>\\n    <updated>2018-06-17T17:14:17Z</updated>\\n    <published>2018-06-17T17:14:17Z</published>\\n    <title>Feature Learning and Classification in Neuroimaging: Predicting\\n  Cognitive Impairment from Magnetic Resonance Imaging</title>\\n    <summary>  Due to the rapid innovation of technology and the desire to find and employ\\nbiomarkers for neurodegenerative disease, high-dimensional data classification\\nproblems are routinely encountered in neuroimaging studies. To avoid\\nover-fitting and to explore relationships between disease and potential\\nbiomarkers, feature learning and selection plays an important role in\\nclassifier construction and is an important area in machine learning. In this\\narticle, we review several important feature learning and selection techniques\\nincluding lasso-based methods, PCA, the two-sample t-test, and stacked\\nauto-encoders. We compare these approaches using a numerical study involving\\nthe prediction of Alzheimer\\'s disease from Magnetic Resonance Imaging.\\n</summary>\\n    <author>\\n      <name>Shan Shi</name>\\n    </author>\\n    <author>\\n      <name>Farouk Nathoo</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.06415v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.06415v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.09777v1</id>\\n    <updated>2018-06-26T03:08:21Z</updated>\\n    <published>2018-06-26T03:08:21Z</published>\\n    <title>On the Implicit Bias of Dropout</title>\\n    <summary>  Algorithmic approaches endow deep learning systems with implicit bias that\\nhelps them generalize even in over-parametrized settings. In this paper, we\\nfocus on understanding such a bias induced in learning through dropout, a\\npopular technique to avoid overfitting in deep learning. For single\\nhidden-layer linear neural networks, we show that dropout tends to make the\\nnorm of incoming/outgoing weight vectors of all the hidden nodes equal. In\\naddition, we provide a complete characterization of the optimization landscape\\ninduced by dropout.\\n</summary>\\n    <author>\\n      <name>Poorya Mianjy</name>\\n    </author>\\n    <author>\\n      <name>Raman Arora</name>\\n    </author>\\n    <author>\\n      <name>Rene Vidal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 3 figures, In Proceedings of the Thirty-fifth International\\n  Conference on Machine Learning (ICML), 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.09777v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.09777v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.04883v3</id>\\n    <updated>2018-12-01T17:29:52Z</updated>\\n    <published>2018-08-13T11:45:16Z</published>\\n    <title>COLA: Decentralized Linear Learning</title>\\n    <summary>  Decentralized machine learning is a promising emerging paradigm in view of\\nglobal challenges of data ownership and privacy. We consider learning of linear\\nclassification and regression models, in the setting where the training data is\\ndecentralized over many user devices, and the learning algorithm must run\\non-device, on an arbitrary communication network, without a central\\ncoordinator. We propose COLA, a new decentralized training algorithm with\\nstrong theoretical guarantees and superior practical performance. Our framework\\novercomes many limitations of existing methods, and achieves communication\\nefficiency, scalability, elasticity as well as resilience to changes in data\\nand participating devices.\\n</summary>\\n    <author>\\n      <name>Lie He</name>\\n    </author>\\n    <author>\\n      <name>An Bian</name>\\n    </author>\\n    <author>\\n      <name>Martin Jaggi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.04883v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.04883v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.03222v2</id>\\n    <updated>2019-03-26T14:34:35Z</updated>\\n    <published>2018-12-07T21:42:20Z</published>\\n    <title>Phenotype Inference with Semi-Supervised Mixed Membership Models</title>\\n    <summary>  Disease phenotyping algorithms process observational clinical data to\\nidentify patients with specific diseases. Supervised phenotyping methods\\nrequire significant quantities of expert-labeled data, while unsupervised\\nmethods may learn non-disease phenotypes. To address these limitations, we\\npropose the Semi-Supervised Mixed Membership Model (SS3M) -- a probabilistic\\ngraphical model for learning disease phenotypes from clinical data with\\nrelatively few labels. We show SS3M can learn interpretable, disease-specific\\nphenotypes which capture the clinical characteristics of the diseases specified\\nby the labels provided.\\n</summary>\\n    <author>\\n      <name>Victor Rodriguez</name>\\n    </author>\\n    <author>\\n      <name>Adler Perotte</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  https://urldefense.proofpoint.com/v2/url?u=https-3A__arxiv.org_abs_1811.07216&amp;d=DwIEaQ&amp;c=G2MiLlal7SXE3PeSnG8W6_JBU6FcdVjSsBSbw6gcR0U&amp;r=V4N8fh0BvUFXSfHS_5FyHekWwaQfwQATAFihsExKikM&amp;m=3ZpgK5EnFWR2OpdKWz1sCspjnLWOElIt_VHn1RMHJ5U&amp;s=iE0HP7cbAigUopbIm2O8hByUTkVYvOw7R2y8QoF6GRg&amp;e=</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.03222v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.03222v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.10738v1</id>\\n    <updated>2019-01-30T10:07:45Z</updated>\\n    <published>2019-01-30T10:07:45Z</published>\\n    <title>Unsupervised Scalable Representation Learning for Multivariate Time\\n  Series</title>\\n    <summary>  Time series constitute a challenging data type for machine learning\\nalgorithms, due to their highly variable lengths and sparse labeling in\\npractice. In this paper, we tackle this challenge by proposing an unsupervised\\nmethod to learn universal embeddings of time series. Unlike previous works, it\\nis scalable with respect to their length and we demonstrate the quality,\\ntransferability and practicability of the learned representations with thorough\\nexperiments and comparisons. To this end, we combine an encoder based on causal\\ndilated convolutions with a triplet loss employing time-based negative\\nsampling, obtaining general-purpose representations for variable length and\\nmultivariate time series.\\n</summary>\\n    <author>\\n      <name>Jean-Yves Franceschi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">MLIA</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Aymeric Dieuleveut</name>\\n    </author>\\n    <author>\\n      <name>Martin Jaggi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.10738v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.10738v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.06497v1</id>\\n    <updated>2019-02-18T10:31:28Z</updated>\\n    <published>2019-02-18T10:31:28Z</published>\\n    <title>Differentially Private Continual Learning</title>\\n    <summary>  Catastrophic forgetting can be a significant problem for institutions that\\nmust delete historic data for privacy reasons. For example, hospitals might not\\nbe able to retain patient data permanently. But neural networks trained on\\nrecent data alone will tend to forget lessons learned on old data. We present a\\ndifferentially private continual learning framework based on variational\\ninference. We estimate the likelihood of past data given the current model\\nusing differentially private generative models of old datasets.\\n</summary>\\n    <author>\\n      <name>Sebastian Farquhar</name>\\n    </author>\\n    <author>\\n      <name>Yarin Gal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the Privacy in Machine Learning and AI workshop at ICML\\n  2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.06497v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.06497v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.10978v1</id>\\n    <updated>2019-03-26T16:07:18Z</updated>\\n    <published>2019-03-26T16:07:18Z</published>\\n    <title>Sparse Learning for Variable Selection with Structures and\\n  Nonlinearities</title>\\n    <summary>  In this thesis we discuss machine learning methods performing automated\\nvariable selection for learning sparse predictive models. There are multiple\\nreasons for promoting sparsity in the predictive models. By relying on a\\nlimited set of input variables the models naturally counteract the overfitting\\nproblem ubiquitous in learning from finite sets of training points. Sparse\\nmodels are cheaper to use for predictions, they usually require lower\\ncomputational resources and by relying on smaller sets of inputs can possibly\\nreduce costs for data collection and storage. Sparse models can also contribute\\nto better understanding of the investigated phenomenons as they are easier to\\ninterpret than full models.\\n</summary>\\n    <author>\\n      <name>Magda Gregorova</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PhD thesis</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.10978v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.10978v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.12266v1</id>\\n    <updated>2019-03-16T18:10:35Z</updated>\\n    <published>2019-03-16T18:10:35Z</published>\\n    <title>Generative Adversarial Networks: recent developments</title>\\n    <summary>  In traditional generative modeling, good data representation is very often a\\nbase for a good machine learning model. It can be linked to good\\nrepresentations encoding more explanatory factors that are hidden in the\\noriginal data. With the invention of Generative Adversarial Networks (GANs), a\\nsubclass of generative models that are able to learn representations in an\\nunsupervised and semi-supervised fashion, we are now able to adversarially\\nlearn good mappings from a simple prior distribution to a target data\\ndistribution. This paper presents an overview of recent developments in GANs\\nwith a focus on learning latent space representations.\\n</summary>\\n    <author>\\n      <name>Maciej Zamorski</name>\\n    </author>\\n    <author>\\n      <name>Adrian Zdobylak</name>\\n    </author>\\n    <author>\\n      <name>Maciej Zi\\xc4\\x99ba</name>\\n    </author>\\n    <author>\\n      <name>Jerzy \\xc5\\x9awi\\xc4\\x85tek</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.12266v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.12266v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cond-mat/9802179v1</id>\\n    <updated>1998-02-17T15:38:01Z</updated>\\n    <published>1998-02-17T15:38:01Z</published>\\n    <title>Learning properties of Support Vector Machines</title>\\n    <summary>  We study the typical learning properties of the recently proposed Support\\nVectors Machines. The generalization error on linearly separable tasks, the\\ncapacity, the typical number of Support Vectors, the margin, and the robustness\\nor noise tolerance of a class of Support Vector Machines are determined in the\\nframework of Statistical Mechanics. The robustness is shown to be closely\\nrelated to the generalization properties of these machines.\\n</summary>\\n    <author>\\n      <name>A. Buhot</name>\\n    </author>\\n    <author>\\n      <name>Mirta B. Gordon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages Latex</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cond-mat/9802179v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cond-mat/9802179v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.6944v1</id>\\n    <updated>2013-01-29T15:09:56Z</updated>\\n    <published>2013-01-29T15:09:56Z</published>\\n    <title>On the Consistency of the Bootstrap Approach for Support Vector Machines\\n  and Related Kernel Based Methods</title>\\n    <summary>  It is shown that bootstrap approximations of support vector machines (SVMs)\\nbased on a general convex and smooth loss function and on a general kernel are\\nconsistent. This result is useful to approximate the unknown finite sample\\ndistribution of SVMs by the bootstrap approach.\\n</summary>\\n    <author>\\n      <name>Andreas Christmann</name>\\n    </author>\\n    <author>\\n      <name>Robert Hable</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.6944v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.6944v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62G08, 62G09, 62G20, 62G86\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.05532v1</id>\\n    <updated>2017-07-18T09:16:50Z</updated>\\n    <published>2017-07-18T09:16:50Z</published>\\n    <title>Bayesian Nonlinear Support Vector Machines for Big Data</title>\\n    <summary>  We propose a fast inference method for Bayesian nonlinear support vector\\nmachines that leverages stochastic variational inference and inducing points.\\nOur experiments show that the proposed method is faster than competing Bayesian\\napproaches and scales easily to millions of data points. It provides additional\\nfeatures over frequentist competitors such as accurate predictive uncertainty\\nestimates and automatic hyperparameter search.\\n</summary>\\n    <author>\\n      <name>Florian Wenzel</name>\\n    </author>\\n    <author>\\n      <name>Theo Galy-Fajou</name>\\n    </author>\\n    <author>\\n      <name>Matthaeus Deutsch</name>\\n    </author>\\n    <author>\\n      <name>Marius Kloft</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">accepted as conference paper at ECML-PKDD 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.05532v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.05532v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.02999v2</id>\\n    <updated>2018-12-12T06:48:21Z</updated>\\n    <published>2018-07-09T09:19:46Z</published>\\n    <title>Decreasing the size of the Restricted Boltzmann machine</title>\\n    <summary>  We propose a method to decrease the number of hidden units of the restricted\\nBoltzmann machine while avoiding decrease of the performance measured by the\\nKullback-Leibler divergence. Then, we demonstrate our algorithm by using\\nnumerical simulations.\\n</summary>\\n    <author>\\n      <name>Yohei Saito</name>\\n    </author>\\n    <author>\\n      <name>Takuya Kato</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.02999v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.02999v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.02156v1</id>\\n    <updated>2017-03-07T00:09:31Z</updated>\\n    <published>2017-03-07T00:09:31Z</published>\\n    <title>On the Limits of Learning Representations with Label-Based Supervision</title>\\n    <summary>  Advances in neural network based classifiers have transformed automatic\\nfeature learning from a pipe dream of stronger AI to a routine and expected\\nproperty of practical systems. Since the emergence of AlexNet every winning\\nsubmission of the ImageNet challenge has employed end-to-end representation\\nlearning, and due to the utility of good representations for transfer learning,\\nrepresentation learning has become as an important and distinct task from\\nsupervised learning. At present, this distinction is inconsequential, as\\nsupervised methods are state-of-the-art in learning transferable\\nrepresentations. But recent work has shown that generative models can also be\\npowerful agents of representation learning. Will the representations learned\\nfrom these generative methods ever rival the quality of those from their\\nsupervised competitors? In this work, we argue in the affirmative, that from an\\ninformation theoretic perspective, generative models have greater potential for\\nrepresentation learning. Based on several experimentally validated assumptions,\\nwe show that supervised learning is upper bounded in its capacity for\\nrepresentation learning in ways that certain generative models, such as\\nGenerative Adversarial Networks (GANs) are not. We hope that our analysis will\\nprovide a rigorous motivation for further exploration of generative\\nrepresentation learning.\\n</summary>\\n    <author>\\n      <name>Jiaming Song</name>\\n    </author>\\n    <author>\\n      <name>Russell Stewart</name>\\n    </author>\\n    <author>\\n      <name>Shengjia Zhao</name>\\n    </author>\\n    <author>\\n      <name>Stefano Ermon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to ICLR 2017 Workshop Track</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.02156v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.02156v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.03511v1</id>\\n    <updated>2019-02-16T01:14:19Z</updated>\\n    <published>2019-02-16T01:14:19Z</published>\\n    <title>Realizing Continual Learning through Modeling a Learning System as a\\n  Fiber Bundle</title>\\n    <summary>  A human brain is capable of continual learning by nature; however the current\\nmainstream deep neural networks suffer from a phenomenon named catastrophic\\nforgetting (i.e., learning a new set of patterns suddenly and completely would\\nresult in fully forgetting what has already been learned). In this paper we\\npropose a generic learning model, which regards a learning system as a fiber\\nbundle. By comparing the learning performance of our model with conventional\\nones whose neural networks are multilayer perceptrons through a variety of\\nmachine-learning experiments, we found our proposed model not only enjoys a\\ndistinguished capability of continual learning but also bears a high\\ninformation capacity. In addition, we found in some learning scenarios the\\nlearning performance can be further enhanced by making the learning time-aware\\nto mimic the episodic memory in human brain. Last but not least, we found that\\nthe properties of forgetting in our model correspond well to those of human\\nmemory. This work may shed light on how a human brain learns.\\n</summary>\\n    <author>\\n      <name>Zhenfeng Cao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.03511v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.03511v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.bio-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0912.0874v2</id>\\n    <updated>2011-11-03T14:37:26Z</updated>\\n    <published>2009-12-04T15:07:30Z</published>\\n    <title>Qualitative Robustness of Support Vector Machines</title>\\n    <summary>  Support vector machines have attracted much attention in theoretical and in\\napplied statistics. Main topics of recent interest are consistency, learning\\nrates and robustness. In this article, it is shown that support vector machines\\nare qualitatively robust. Since support vector machines can be represented by a\\nfunctional on the set of all probability measures, qualitative robustness is\\nproven by showing that this functional is continuous with respect to the\\ntopology generated by weak convergence of probability measures. Combined with\\nthe existence and uniqueness of support vector machines, our results show that\\nsupport vector machines are the solutions of a well-posed mathematical problem\\nin Hadamard\\'s sense.\\n</summary>\\n    <author>\\n      <name>Robert Hable</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Bayreuth</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Andreas Christmann</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Bayreuth</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0912.0874v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0912.0874v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62G08, 62G35\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1001.4019v1</id>\\n    <updated>2010-01-22T15:20:11Z</updated>\\n    <published>2010-01-22T15:20:11Z</published>\\n    <title>Classifying Network Data with Deep Kernel Machines</title>\\n    <summary>  Inspired by a growing interest in analyzing network data, we study the\\nproblem of node classification on graphs, focusing on approaches based on\\nkernel machines. Conventionally, kernel machines are linear classifiers in the\\nimplicit feature space. We argue that linear classification in the feature\\nspace of kernels commonly used for graphs is often not enough to produce good\\nresults. When this is the case, one naturally considers nonlinear classifiers\\nin the feature space. We show that repeating this process produces something we\\ncall \"deep kernel machines.\" We provide some examples where deep kernel\\nmachines can make a big difference in classification performance, and point out\\nsome connections to various recent literature on deep architectures in\\nartificial intelligence and machine learning.\\n</summary>\\n    <author>\\n      <name>Xiao Tang</name>\\n    </author>\\n    <author>\\n      <name>Mu Zhu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1001.4019v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1001.4019v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.06599v1</id>\\n    <updated>2017-09-19T18:37:42Z</updated>\\n    <published>2017-09-19T18:37:42Z</published>\\n    <title>Unsupervised Machine Learning for Networking: Techniques, Applications\\n  and Research Challenges</title>\\n    <summary>  While machine learning and artificial intelligence have long been applied in\\nnetworking research, the bulk of such works has focused on supervised learning.\\nRecently there has been a rising trend of employing unsupervised machine\\nlearning using unstructured raw network data to improve network performance and\\nprovide services such as traffic engineering, anomaly detection, Internet\\ntraffic classification, and quality of service optimization. The interest in\\napplying unsupervised learning techniques in networking emerges from their\\ngreat success in other fields such as computer vision, natural language\\nprocessing, speech recognition, and optimal control (e.g., for developing\\nautonomous self-driving cars). Unsupervised learning is interesting since it\\ncan unconstrain us from the need of labeled data and manual handcrafted feature\\nengineering thereby facilitating flexible, general, and automated methods of\\nmachine learning. The focus of this survey paper is to provide an overview of\\nthe applications of unsupervised learning in the domain of networking. We\\nprovide a comprehensive survey highlighting the recent advancements in\\nunsupervised learning techniques and describe their applications for various\\nlearning tasks in the context of networking. We also provide a discussion on\\nfuture directions and open research issues, while also identifying potential\\npitfalls. While a few survey papers focusing on the applications of machine\\nlearning in networking have previously been published, a survey of similar\\nscope and breadth is missing in literature. Through this paper, we advance the\\nstate of knowledge by carefully synthesizing the insights from these survey\\npapers while also providing contemporary coverage of recent advances.\\n</summary>\\n    <author>\\n      <name>Muhammad Usama</name>\\n    </author>\\n    <author>\\n      <name>Junaid Qadir</name>\\n    </author>\\n    <author>\\n      <name>Aunn Raza</name>\\n    </author>\\n    <author>\\n      <name>Hunain Arif</name>\\n    </author>\\n    <author>\\n      <name>Kok-Lim Alvin Yau</name>\\n    </author>\\n    <author>\\n      <name>Yehia Elkhatib</name>\\n    </author>\\n    <author>\\n      <name>Amir Hussain</name>\\n    </author>\\n    <author>\\n      <name>Ala Al-Fuqaha</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1709.06599v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.06599v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.07274v6</id>\\n    <updated>2018-11-26T04:56:31Z</updated>\\n    <published>2017-01-25T11:52:11Z</published>\\n    <title>Deep Reinforcement Learning: An Overview</title>\\n    <summary>  We give an overview of recent exciting achievements of deep reinforcement\\nlearning (RL). We discuss six core elements, six important mechanisms, and\\ntwelve applications. We start with background of machine learning, deep\\nlearning and reinforcement learning. Next we discuss core RL elements,\\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\\nmodel, planning, and exploration. After that, we discuss important mechanisms\\nfor RL, including attention and memory, unsupervised learning, transfer\\nlearning, multi-agent RL, hierarchical RL, and learning to learn. Then we\\ndiscuss various applications of RL, including games, in particular, AlphaGo,\\nrobotics, natural language processing, including dialogue systems, machine\\ntranslation, and text generation, computer vision, neural architecture design,\\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\\ntransportation systems, and computer systems. We mention topics not reviewed\\nyet, and list a collection of RL resources. After presenting a brief summary,\\nwe close with discussions.\\n  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant\\nupdate.\\n</summary>\\n    <author>\\n      <name>Yuxi Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Please see Deep Reinforcement Learning, arXiv:1810.06339, for a\\n  significant update</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1701.07274v6\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.07274v6\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.01630v4</id>\\n    <updated>2019-03-27T01:55:56Z</updated>\\n    <published>2018-08-05T14:51:07Z</published>\\n    <title>A Review of Learning with Deep Generative Models from Perspective of\\n  Graphical Modeling</title>\\n    <summary>  This document aims to provide a review on learning with deep generative\\nmodels (DGMs), which is an highly-active area in machine learning and more\\ngenerally, artificial intelligence. This review is not meant to be a tutorial,\\nbut when necessary, we provide self-contained derivations for completeness.\\nThis review has two features. First, though there are different perspectives to\\nclassify DGMs, we choose to organize this review from the perspective of\\ngraphical modeling, because the learning methods for directed DGMs and\\nundirected DGMs are fundamentally different. Second, we differentiate model\\ndefinitions from model learning algorithms, since different learning algorithms\\ncan be applied to solve the learning problem on the same model, and an\\nalgorithm can be applied to learn different models. We thus separate model\\ndefinition and model learning, with more emphasis on reviewing, differentiating\\nand connecting different learning algorithms. We also discuss promising future\\nresearch directions.\\n</summary>\\n    <author>\\n      <name>Zhijian Ou</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">add SN-GANs, SA-GANs, conditional generation (cGANs, AC-GANs). arXiv\\n  admin note: text overlap with arXiv:1606.00709, arXiv:1801.03558 by other\\n  authors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.01630v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.01630v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.02229v2</id>\\n    <updated>2018-08-13T02:19:08Z</updated>\\n    <published>2018-08-07T06:54:06Z</published>\\n    <title>Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep\\n  Learning</title>\\n    <summary>  Modern machine learning algorithms have been adopted in a range of\\nsignal-processing applications spanning computer vision, natural language\\nprocessing, and artificial intelligence. Many relevant problems involve\\nsubspace-structured features, orthogonality constrained or low-rank constrained\\nobjective functions, or subspace distances. These mathematical characteristics\\nare expressed naturally using the Grassmann manifold. Unfortunately, this fact\\nis not yet explored in many traditional learning algorithms. In the last few\\nyears, there have been growing interests in studying Grassmann manifold to\\ntackle new learning problems. Such attempts have been reassured by substantial\\nperformance improvements in both classic learning and learning using deep\\nneural networks. We term the former as shallow and the latter deep Grassmannian\\nlearning. The aim of this paper is to introduce the emerging area of\\nGrassmannian learning by surveying common mathematical problems and primary\\nsolution approaches, and overviewing various applications. We hope to inspire\\npractitioners in different fields to adopt the powerful tool of Grassmannian\\nlearning in their research.\\n</summary>\\n    <author>\\n      <name>Jiayao Zhang</name>\\n    </author>\\n    <author>\\n      <name>Guangxu Zhu</name>\\n    </author>\\n    <author>\\n      <name>Robert W. Heath Jr.</name>\\n    </author>\\n    <author>\\n      <name>Kaibin Huang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to IEEE Signal Processing Magazine</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.02229v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.02229v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.02591v1</id>\\n    <updated>2018-09-07T17:32:19Z</updated>\\n    <published>2018-09-07T17:32:19Z</published>\\n    <title>Learning Invariances for Policy Generalization</title>\\n    <summary>  While recent progress has spawned very powerful machine learning systems,\\nthose agents remain extremely specialized and fail to transfer the knowledge\\nthey gain to similar yet unseen tasks. In this paper, we study a simple\\nreinforcement learning problem and focus on learning policies that encode the\\nproper invariances for generalization to different settings. We evaluate three\\npotential methods for policy generalization: data augmentation, meta-learning\\nand adversarial training. We find our data augmentation method to be effective,\\nand study the potential of meta-learning and adversarial learning as\\nalternative task-agnostic approaches.\\n  Keywords: reinforcement learning, generalization, data augmentation,\\nmeta-learning, adversarial learning.\\n</summary>\\n    <author>\\n      <name>Remi Tachet des Combes</name>\\n    </author>\\n    <author>\\n      <name>Philip Bachman</name>\\n    </author>\\n    <author>\\n      <name>Harm van Seijen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.02591v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.02591v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.03252v1</id>\\n    <updated>2019-03-08T02:29:22Z</updated>\\n    <published>2019-03-08T02:29:22Z</published>\\n    <title>Learning Feature Relevance Through Step Size Adaptation in\\n  Temporal-Difference Learning</title>\\n    <summary>  There is a long history of using meta learning as representation learning,\\nspecifically for determining the relevance of inputs. In this paper, we examine\\nan instance of meta-learning in which feature relevance is learned by adapting\\nstep size parameters of stochastic gradient descent---building on a variety of\\nprior work in stochastic approximation, machine learning, and artificial neural\\nnetworks. In particular, we focus on stochastic meta-descent introduced in the\\nIncremental Delta-Bar-Delta (IDBD) algorithm for setting individual step sizes\\nfor each feature of a linear function approximator. Using IDBD, a feature with\\nlarge or small step sizes will have a large or small impact on generalization\\nfrom training examples. As a main contribution of this work, we extend IDBD to\\ntemporal-difference (TD) learning---a form of learning which is effective in\\nsequential, non i.i.d. problems. We derive a variety of IDBD generalizations\\nfor TD learning, demonstrating that they are able to distinguish which features\\nare relevant and which are not. We demonstrate that TD IDBD is effective at\\nlearning feature relevance in both an idealized gridworld and a real-world\\nrobotic prediction task.\\n</summary>\\n    <author>\\n      <name>Alex Kearney</name>\\n    </author>\\n    <author>\\n      <name>Vivek Veeriah</name>\\n    </author>\\n    <author>\\n      <name>Jaden Travnik</name>\\n    </author>\\n    <author>\\n      <name>Patrick M. Pilarski</name>\\n    </author>\\n    <author>\\n      <name>Richard S. Sutton</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.03252v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.03252v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.1054v1</id>\\n    <updated>2014-10-04T15:55:56Z</updated>\\n    <published>2014-10-04T15:55:56Z</published>\\n    <title>Experimental Realization of Quantum Artificial Intelligence</title>\\n    <summary>  Machines are possible to have some artificial intelligence like human beings\\nowing to particular algorithms or software. Such machines could learn knowledge\\nfrom what people taught them and do works according to the knowledge. In\\npractical learning cases, the data is often extremely complicated and large,\\nthus classical learning machines often need huge computational resources.\\nQuantum machine learning algorithm, on the other hand, could be exponentially\\nfaster than classical machines using quantum parallelism. Here, we demonstrate\\na quantum machine learning algorithm on a four-qubit NMR test bench to solve an\\noptical character recognition problem, also known as the handwriting\\nrecognition. The quantum machine learns standard character fonts and then\\nrecognize handwritten characters from a set with two candidates. To our best\\nknowledge, this is the first artificial intelligence realized on a quantum\\nprocessor. Due to the widespreading importance of artificial intelligence and\\nits tremendous consuming of computational resources, quantum speedup would be\\nextremely attractive against the challenges from the Big Data.\\n</summary>\\n    <author>\\n      <name>Li Zhaokai</name>\\n    </author>\\n    <author>\\n      <name>Liu Xiaomei</name>\\n    </author>\\n    <author>\\n      <name>Xu Nanyang</name>\\n    </author>\\n    <author>\\n      <name>Du jiangfeng</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevLett.114.140504</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevLett.114.140504\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 4 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Phys. Rev. Lett. 114, 140504 (2015)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1410.1054v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.1054v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.07804v1</id>\\n    <updated>2016-07-03T16:34:24Z</updated>\\n    <published>2016-07-03T16:34:24Z</published>\\n    <title>Error-Resilient Machine Learning in Near Threshold Voltage via\\n  Classifier Ensemble</title>\\n    <summary>  In this paper, we present the design of error-resilient machine learning\\narchitectures by employing a distributed machine learning framework referred to\\nas classifier ensemble (CE). CE combines several simple classifiers to obtain a\\nstrong one. In contrast, centralized machine learning employs a single complex\\nblock. We compare the random forest (RF) and the support vector machine (SVM),\\nwhich are representative techniques from the CE and centralized frameworks,\\nrespectively. Employing the dataset from UCI machine learning repository and\\narchitectural-level error models in a commercial 45 nm CMOS process, it is\\ndemonstrated that RF-based architectures are significantly more robust than SVM\\narchitectures in presence of timing errors due to process variations in\\nnear-threshold voltage (NTV) regions (0.3 V - 0.7 V). In particular, the RF\\narchitecture exhibits a detection accuracy (P_{det}) that varies by 3.2% while\\nmaintaining a median P_{det} &gt; 0.9 at a gate level delay variation of 28.9% .\\nIn comparison, SVM exhibits a P_{det} that varies by 16.8%. Additionally, we\\npropose an error weighted voting technique that incorporates the timing error\\nstatistics of the NTV circuit fabric to further enhance robustness. Simulation\\nresults confirm that the error weighted voting achieves a P_{det} that varies\\nby only 1.4%, which is 12X lower compared to SVM.\\n</summary>\\n    <author>\\n      <name>Sai Zhang</name>\\n    </author>\\n    <author>\\n      <name>Naresh Shanbhag</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1607.07804v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.07804v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.00909v2</id>\\n    <updated>2018-05-04T16:58:31Z</updated>\\n    <published>2017-08-02T19:53:22Z</published>\\n    <title>Machine learning for neural decoding</title>\\n    <summary>  Despite rapid advances in machine learning tools, the majority of neural\\ndecoding approaches still use traditional methods. Improving the performance of\\nneural decoding algorithms allows us to better understand the information\\ncontained in a neural population, and can help advance engineering applications\\nsuch as brain machine interfaces. Here, we apply modern machine learning\\ntechniques, including neural networks and gradient boosting, to decode from\\nspiking activity in 1) motor cortex, 2) somatosensory cortex, and 3)\\nhippocampus. We compare the predictive ability of these modern methods with\\ntraditional decoding methods such as Wiener and Kalman filters. Modern methods,\\nin particular neural networks and ensembles, significantly outperformed the\\ntraditional approaches. For instance, for all of the three brain areas, an LSTM\\ndecoder explained over 40% of the unexplained variance from a Wiener filter.\\nThese results suggest that modern machine learning techniques should become the\\nstandard methodology for neural decoding. We provide a tutorial and code to\\nfacilitate wider implementation of these methods.\\n</summary>\\n    <author>\\n      <name>Joshua I. Glaser</name>\\n    </author>\\n    <author>\\n      <name>Raeed H. Chowdhury</name>\\n    </author>\\n    <author>\\n      <name>Matthew G. Perich</name>\\n    </author>\\n    <author>\\n      <name>Lee E. Miller</name>\\n    </author>\\n    <author>\\n      <name>Konrad P. Kording</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.00909v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.00909v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.05448v1</id>\\n    <updated>2017-08-17T21:53:47Z</updated>\\n    <published>2017-08-17T21:53:47Z</published>\\n    <title>On Ensuring that Intelligent Machines Are Well-Behaved</title>\\n    <summary>  Machine learning algorithms are everywhere, ranging from simple data analysis\\nand pattern recognition tools used across the sciences to complex systems that\\nachieve super-human performance on various tasks. Ensuring that they are\\nwell-behaved---that they do not, for example, cause harm to humans or act in a\\nracist or sexist way---is therefore not a hypothetical problem to be dealt with\\nin the future, but a pressing one that we address here. We propose a new\\nframework for designing machine learning algorithms that simplifies the problem\\nof specifying and regulating undesirable behaviors. To show the viability of\\nthis new framework, we use it to create new machine learning algorithms that\\npreclude the sexist and harmful behaviors exhibited by standard machine\\nlearning algorithms in our experiments. Our framework for designing machine\\nlearning algorithms simplifies the safe and responsible application of machine\\nlearning.\\n</summary>\\n    <author>\\n      <name>Philip S. Thomas</name>\\n    </author>\\n    <author>\\n      <name>Bruno Castro da Silva</name>\\n    </author>\\n    <author>\\n      <name>Andrew G. Barto</name>\\n    </author>\\n    <author>\\n      <name>Emma Brunskill</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.05448v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.05448v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.01825v2</id>\\n    <updated>2018-05-14T04:00:10Z</updated>\\n    <published>2018-04-04T02:40:43Z</published>\\n    <title>Evaluating Hospital Case Cost Prediction Models Using Azure Machine\\n  Learning Studio</title>\\n    <summary>  Ability for accurate hospital case cost modelling and prediction is critical\\nfor efficient health care financial management and budgetary planning. A\\nvariety of regression machine learning algorithms are known to be effective for\\nhealth care cost predictions. The purpose of this experiment was to build an\\nAzure Machine Learning Studio tool for rapid assessment of multiple types of\\nregression models. The tool offers environment for comparing 14 types of\\nregression models in a unified experiment: linear regression, Bayesian linear\\nregression, decision forest regression, boosted decision tree regression,\\nneural network regression, Poisson regression, Gaussian processes for\\nregression, gradient boosted machine, nonlinear least squares regression,\\nprojection pursuit regression, random forest regression, robust regression,\\nrobust regression with mm-type estimators, support vector regression. The tool\\npresents assessment results arranged by model accuracy in a single table using\\nfive performance metrics. Evaluation of regression machine learning models for\\nperforming hospital case cost prediction demonstrated advantage of robust\\nregression model, boosted decision tree regression and decision forest\\nregression. The operational tool has been published to the web and openly\\navailable for experiments and extensions.\\n</summary>\\n    <author>\\n      <name>Alexei Botchkarev</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.01825v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.01825v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.EC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.02356v1</id>\\n    <updated>2018-05-07T06:12:32Z</updated>\\n    <published>2018-05-07T06:12:32Z</published>\\n    <title>Multimodal Machine Translation with Reinforcement Learning</title>\\n    <summary>  Multimodal machine translation is one of the applications that integrates\\ncomputer vision and language processing. It is a unique task given that in the\\nfield of machine translation, many state-of-the-arts algorithms still only\\nemploy textual information. In this work, we explore the effectiveness of\\nreinforcement learning in multimodal machine translation. We present a novel\\nalgorithm based on the Advantage Actor-Critic (A2C) algorithm that specifically\\ncater to the multimodal machine translation task of the EMNLP 2018 Third\\nConference on Machine Translation (WMT18). We experiment our proposed algorithm\\non the Multi30K multilingual English-German image description dataset and the\\nFlickr30K image entity dataset. Our model takes two channels of inputs, image\\nand text, uses translation evaluation metrics as training rewards, and achieves\\nbetter results than supervised learning MLE baseline models. Furthermore, we\\ndiscuss the prospects and limitations of using reinforcement learning for\\nmachine translation. Our experiment results suggest a promising reinforcement\\nlearning solution to the general task of multimodal sequence to sequence\\nlearning.\\n</summary>\\n    <author>\\n      <name>Xin Qian</name>\\n    </author>\\n    <author>\\n      <name>Ziyi Zhong</name>\\n    </author>\\n    <author>\\n      <name>Jieli Zhou</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.02356v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.02356v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.10320v1</id>\\n    <updated>2018-07-26T18:51:42Z</updated>\\n    <published>2018-07-26T18:51:42Z</published>\\n    <title>High Dimensional Model Representation as a Glass Box in Supervised\\n  Machine Learning</title>\\n    <summary>  Prediction and explanation are key objects in supervised machine learning,\\nwhere predictive models are known as black boxes and explanatory models are\\nknown as glass boxes. Explanation provides the necessary and sufficient\\ninformation to interpret the model output in terms of the model input. It\\nincludes assessments of model output dependence on important input variables\\nand measures of input variable importance to model output. High dimensional\\nmodel representation (HDMR), also known as the generalized functional ANOVA\\nexpansion, provides useful insight into the input-output behavior of supervised\\nmachine learning models. This article gives applications of HDMR in supervised\\nmachine learning. The first application is characterizing information leakage\\nin ``big-data\\'\\' settings. The second application is reduced-order\\nrepresentation of elementary symmetric polynomials. The third application is\\nanalysis of variance with correlated variables. The last application is\\nestimation of HDMR from kernel machine and decision tree black box\\nrepresentations. These results suggest HDMR to have broad utility within\\nmachine learning as a glass box representation.\\n</summary>\\n    <author>\\n      <name>Caleb Deen Bastian</name>\\n    </author>\\n    <author>\\n      <name>Herschel Rabitz</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">54 pages, 23 figures, 5 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.10320v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.10320v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.11959v1</id>\\n    <updated>2018-10-29T05:22:51Z</updated>\\n    <published>2018-10-29T05:22:51Z</published>\\n    <title>An Amalgamation of Classical and Quantum Machine Learning For the\\n  Classification of Adenocarcinoma and Squamous Cell Carcinoma Patients</title>\\n    <summary>  The ability to accurately classify disease subtypes is of vital importance,\\nespecially in oncology where this capability could have a life saving impact.\\nHere we report a classification between two subtypes of non-small cell lung\\ncancer, namely Adeno- carcinoma vs Squamous cell carcinoma. The data consists\\nof approximately 20,000 gene expression values for each of 104 patients. The\\ndata was curated from [1] [2]. We used an amalgamation of classical and and\\nquantum machine learning models to successfully classify these patients. We\\nutilized feature selection methods based on univariate statistics in addition\\nto XGBoost [3]. A novel and proprietary data representation method developed by\\none of the authors called QCrush was also used as it was designed to\\nincorporate a maximal amount of information under the size constraints of the\\nD-Wave quantum annealing computer. The machine learning was performed by a\\nQuantum Boltzmann Machine. This paper will report our results, the various\\nclassical methods, and the quantum machine learning approach we utilized.\\n</summary>\\n    <author>\\n      <name>Siddhant Jain</name>\\n    </author>\\n    <author>\\n      <name>Jalal Ziauddin</name>\\n    </author>\\n    <author>\\n      <name>Paul Leonchyk</name>\\n    </author>\\n    <author>\\n      <name>Joseph Geraci</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.11959v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.11959v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.04238v1</id>\\n    <updated>2018-12-11T07:04:44Z</updated>\\n    <published>2018-12-11T07:04:44Z</published>\\n    <title>Machine Translation : From Statistical to modern Deep-learning practices</title>\\n    <summary>  Machine translation (MT) is an area of study in Natural Language processing\\nwhich deals with the automatic translation of human language, from one language\\nto another by the computer. Having a rich research history spanning nearly\\nthree decades, Machine translation is one of the most sought after area of\\nresearch in the linguistics and computational community. In this paper, we\\ninvestigate the models based on deep learning that have achieved substantial\\nprogress in recent years and becoming the prominent method in MT. We shall\\ndiscuss the two main deep-learning based Machine Translation methods, one at\\ncomponent or domain level which leverages deep learning models to enhance the\\nefficacy of Statistical Machine Translation (SMT) and end-to-end deep learning\\nmodels in MT which uses neural networks to find correspondence between the\\nsource and target languages using the encoder-decoder architecture. We conclude\\nthis paper by providing a time line of the major research problems solved by\\nthe researchers and also provide a comprehensive overview of present areas of\\nresearch in Neural Machine Translation.\\n</summary>\\n    <author>\\n      <name>Siddhant Srivastava</name>\\n    </author>\\n    <author>\\n      <name>Anupam Shukla</name>\\n    </author>\\n    <author>\\n      <name>Ritu Tiwari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.04238v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.04238v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.09138v1</id>\\n    <updated>2018-12-21T14:18:56Z</updated>\\n    <published>2018-12-21T14:18:56Z</published>\\n    <title>Ecological Data Analysis Based on Machine Learning Algorithms</title>\\n    <summary>  Classification is an important supervised machine learning method, which is\\nnecessary and challenging issue for ecological research. It offers a way to\\nclassify a dataset into subsets that share common patterns. Notably, there are\\nmany classification algorithms to choose from, each making certain assumptions\\nabout the data and about how classification should be formed. In this paper, we\\napplied eight machine learning classification algorithms such as Decision\\nTrees, Random Forest, Artificial Neural Network, Support Vector Machine, Linear\\nDiscriminant Analysis, k-nearest neighbors, Logistic Regression and Naive Bayes\\non ecological data. The goal of this study is to compare different machine\\nlearning classification algorithms in ecological dataset. In this analysis we\\nhave checked the accuracy test among the algorithms. In our study we conclude\\nthat Linear Discriminant Analysis and k-nearest neighbors are the best methods\\namong all other methods\\n</summary>\\n    <author>\\n      <name>Md. Siraj-Ud-Doula</name>\\n    </author>\\n    <author>\\n      <name>Md. Ashad Alam</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, 20 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.09138v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.09138v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.02256v2</id>\\n    <updated>2019-02-03T17:26:14Z</updated>\\n    <published>2019-01-08T11:12:37Z</published>\\n    <title>Artificial Intelligence and Machine Learning to Predict and Improve\\n  Efficiency in Manufacturing Industry</title>\\n    <summary>  The overall equipment effectiveness (OEE) is a performance measurement metric\\nwidely used. Its calculation provides to the managers the possibility to\\nidentify the main losses that reduce the machine effectiveness and then take\\nthe necessary decisions in order to improve the situation. However, this\\ncalculation is done a-posterior which is often too late. In the present\\nresearch, we implemented different Machine Learning algorithms namely; Support\\nvector machine, Optimized Support vector Machine (using Genetic Algorithm),\\nRandom Forest, XGBoost and Deep Learning to predict the estimate OEE value. The\\ndata used to train our models was provided by an automotive cable production\\nindustry. The results show that the Deep Learning and Random Forest are more\\naccurate and present better performance for the prediction of the overall\\nequipment effectiveness in our case study.\\n</summary>\\n    <author>\\n      <name>Ibtissam El Hassani</name>\\n    </author>\\n    <author>\\n      <name>Choumicha El Mazgualdi</name>\\n    </author>\\n    <author>\\n      <name>Tawfik Masrour</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.02256v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.02256v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.00562v1</id>\\n    <updated>2019-02-01T20:58:34Z</updated>\\n    <published>2019-02-01T20:58:34Z</published>\\n    <title>The Spatially-Conscious Machine Learning Model</title>\\n    <summary>  Successfully predicting gentrification could have many social and commercial\\napplications; however, real estate sales are difficult to predict because they\\nbelong to a chaotic system comprised of intrinsic and extrinsic\\ncharacteristics, perceived value, and market speculation. Using New York City\\nreal estate as our subject, we combine modern techniques of data science and\\nmachine learning with traditional spatial analysis to create robust real estate\\nprediction models for both classification and regression tasks. We compare\\nseveral cutting edge machine learning algorithms across spatial, semi-spatial\\nand non-spatial feature engineering techniques, and we empirically show that\\nspatially-conscious machine learning models outperform non-spatial models when\\nmarried with advanced prediction techniques such as feed-forward artificial\\nneural networks and gradient boosting machine models.\\n</summary>\\n    <author>\\n      <name>Timothy J. Kiely</name>\\n    </author>\\n    <author>\\n      <name>Nathaniel D. Bastian</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.00562v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.00562v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.5744v7</id>\\n    <updated>2017-04-19T21:17:10Z</updated>\\n    <published>2014-12-18T07:51:24Z</published>\\n    <title>Stochastic Descent Analysis of Representation Learning Algorithms</title>\\n    <summary>  Although stochastic approximation learning methods have been widely used in\\nthe machine learning literature for over 50 years, formal theoretical analyses\\nof specific machine learning algorithms are less common because stochastic\\napproximation theorems typically possess assumptions which are difficult to\\ncommunicate and verify. This paper presents a new stochastic approximation\\ntheorem for state-dependent noise with easily verifiable assumptions applicable\\nto the analysis and design of important deep learning algorithms including:\\nadaptive learning, contrastive divergence learning, stochastic descent\\nexpectation maximization, and active learning.\\n</summary>\\n    <author>\\n      <name>Richard M. Golden</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Version: April 27, 2015. This paper has been withdrawn by the author\\n  because of a minor problem with the proof which has since been corrected. The\\n  revised manuscript will eventually be published</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1412.5744v7\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.5744v7\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.1890v1</id>\\n    <updated>2014-07-07T21:23:42Z</updated>\\n    <published>2014-07-07T21:23:42Z</published>\\n    <title>Recommending Learning Algorithms and Their Associated Hyperparameters</title>\\n    <summary>  The success of machine learning on a given task dependson, among other\\nthings, which learning algorithm is selected and its associated\\nhyperparameters. Selecting an appropriate learning algorithm and setting its\\nhyperparameters for a given data set can be a challenging task, especially for\\nusers who are not experts in machine learning. Previous work has examined using\\nmeta-features to predict which learning algorithm and hyperparameters should be\\nused. However, choosing a set of meta-features that are predictive of algorithm\\nperformance is difficult. Here, we propose to apply collaborative filtering\\ntechniques to learning algorithm and hyperparameter selection, and find that\\ndoing so avoids determining which meta-features to use and outperforms\\ntraditional meta-learning approaches in many cases.\\n</summary>\\n    <author>\\n      <name>Michael R. Smith</name>\\n    </author>\\n    <author>\\n      <name>Logan Mitchell</name>\\n    </author>\\n    <author>\\n      <name>Christophe Giraud-Carrier</name>\\n    </author>\\n    <author>\\n      <name>Tony Martinez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Short paper--2 pages, 2 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.1890v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.1890v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.01575v1</id>\\n    <updated>2018-03-05T09:49:55Z</updated>\\n    <published>2018-03-05T09:49:55Z</published>\\n    <title>A Comparative Study of Pairwise Learning Methods based on Kernel Ridge\\n  Regression</title>\\n    <summary>  Many machine learning problems can be formulated as predicting labels for a\\npair of objects. Problems of that kind are often referred to as pairwise\\nlearning, dyadic prediction or network inference problems. During the last\\ndecade kernel methods have played a dominant role in pairwise learning. They\\nstill obtain a state-of-the-art predictive performance, but a theoretical\\nanalysis of their behavior has been underexplored in the machine learning\\nliterature.\\n  In this work we review and unify existing kernel-based algorithms that are\\ncommonly used in different pairwise learning settings, ranging from matrix\\nfiltering to zero-shot learning. To this end, we focus on closed-form efficient\\ninstantiations of Kronecker kernel ridge regression. We show that independent\\ntask kernel ridge regression, two-step kernel ridge regression and a linear\\nmatrix filter arise naturally as a special case of Kronecker kernel ridge\\nregression, implying that all these methods implicitly minimize a squared loss.\\nIn addition, we analyze universality, consistency and spectral filtering\\nproperties. Our theoretical results provide valuable insights in assessing the\\nadvantages and limitations of existing pairwise learning methods.\\n</summary>\\n    <author>\\n      <name>Michiel Stock</name>\\n    </author>\\n    <author>\\n      <name>Tapio Pahikkala</name>\\n    </author>\\n    <author>\\n      <name>Antti Airola</name>\\n    </author>\\n    <author>\\n      <name>Bernard De Baets</name>\\n    </author>\\n    <author>\\n      <name>Willem Waegeman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1606.04275</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.01575v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.01575v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.01258v1</id>\\n    <updated>2018-06-04T17:55:12Z</updated>\\n    <published>2018-06-04T17:55:12Z</published>\\n    <title>Agreement-based Learning</title>\\n    <summary>  Model selection is a problem that has occupied machine learning researchers\\nfor a long time. Recently, its importance has become evident through\\napplications in deep learning. We propose an agreement-based learning framework\\nthat prevents many of the pitfalls associated with model selection. It relies\\non coupling the training of multiple models by encouraging them to agree on\\ntheir predictions while training. In contrast with other model selection and\\ncombination approaches used in machine learning, the proposed framework is\\ninspired by human learning. We also propose a learning algorithm defined within\\nthis framework which manages to significantly outperform alternatives in\\npractice, and whose performance improves further with the availability of\\nunlabeled data. Finally, we describe a number of potential directions for\\ndeveloping more flexible agreement-based learning algorithms.\\n</summary>\\n    <author>\\n      <name>Emmanouil Antonios Platanios</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.01258v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.01258v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.07066v1</id>\\n    <updated>2018-06-19T06:53:15Z</updated>\\n    <published>2018-06-19T06:53:15Z</published>\\n    <title>Restricted Boltzmann Machines: Introduction and Review</title>\\n    <summary>  The restricted Boltzmann machine is a network of stochastic units with\\nundirected interactions between pairs of visible and hidden units. This model\\nwas popularized as a building block of deep learning architectures and has\\ncontinued to play an important role in applied and theoretical machine\\nlearning. Restricted Boltzmann machines carry a rich structure, with\\nconnections to geometry, applied algebra, probability, statistics, machine\\nlearning, and other areas. The analysis of these models is attractive in its\\nown right and also as a platform to combine and generalize mathematical tools\\nfor graphical models with hidden variables. This article gives an introduction\\nto the mathematical analysis of restricted Boltzmann machines, reviews recent\\nresults on the geometry of the sets of probability distributions representable\\nby these models, and suggests a few directions for further investigation.\\n</summary>\\n    <author>\\n      <name>Guido Montufar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">40 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.07066v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.07066v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.7770v3</id>\\n    <updated>2015-02-08T10:43:11Z</updated>\\n    <published>2014-09-27T06:31:08Z</published>\\n    <title>Entanglement-Based Machine Learning on a Quantum Computer</title>\\n    <summary>  Machine learning, a branch of artificial intelligence, learns from previous\\nexperience to optimize performance, which is ubiquitous in various fields such\\nas computer sciences, financial analysis, robotics, and bioinformatics. A\\nchallenge is that machine learning with the rapidly growing \"big data\" could\\nbecome intractable for classical computers. Recently, quantum machine learning\\nalgorithms [Lloyd, Mohseni, and Rebentrost, arXiv.1307.0411] were proposed\\nwhich could offer an exponential speedup over classical algorithms. Here, we\\nreport the first experimental entanglement-based classification of 2-, 4-, and\\n8-dimensional vectors to different clusters using a small-scale photonic\\nquantum computer, which are then used to implement supervised and unsupervised\\nmachine learning. The results demonstrate the working principle of using\\nquantum computers to manipulate and classify high-dimensional vectors, the core\\nmathematical routine in machine learning. The method can in principle be scaled\\nto larger number of qubits, and may provide a new route to accelerate machine\\nlearning.\\n</summary>\\n    <author>\\n      <name>X. -D. Cai</name>\\n    </author>\\n    <author>\\n      <name>D. Wu</name>\\n    </author>\\n    <author>\\n      <name>Z. -E. Su</name>\\n    </author>\\n    <author>\\n      <name>M. -C. Chen</name>\\n    </author>\\n    <author>\\n      <name>X. -L. Wang</name>\\n    </author>\\n    <author>\\n      <name>L. Li</name>\\n    </author>\\n    <author>\\n      <name>N. -L. Liu</name>\\n    </author>\\n    <author>\\n      <name>C. -Y. Lu</name>\\n    </author>\\n    <author>\\n      <name>J. -W. Pan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevLett.114.110504</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevLett.114.110504\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 4 figures, 2 table, revised version with additional data on\\n  unsupervised machine learning</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Phys. Rev. Lett. 114, 110504 (2015)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1409.7770v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.7770v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.other\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.04435v1</id>\\n    <updated>2016-05-14T16:37:54Z</updated>\\n    <published>2016-05-14T16:37:54Z</published>\\n    <title>Proceedings of the 5th Workshop on Machine Learning and Interpretation\\n  in Neuroimaging (MLINI) at NIPS 2015</title>\\n    <summary>  This volume is a collection of contributions from the 5th Workshop on Machine\\nLearning and Interpretation in Neuroimaging (MLINI) at the Neural Information\\nProcessing Systems (NIPS 2015) conference. Modern multivariate statistical\\nmethods developed in the rapidly growing field of machine learning are being\\nincreasingly applied to various problems in neuroimaging, from cognitive state\\ndetection to clinical diagnosis and prognosis. Multivariate pattern analysis\\nmethods are designed to examine complex relationships between high-dimensional\\nsignals, such as brain images, and outcomes of interest, such as the category\\nof a stimulus, a type of a mental state of a subject, or a specific mental\\ndisorder. Such techniques are in contrast with the traditional mass-univariate\\napproaches that dominated neuroimaging in the past and treated each individual\\nimaging measurement in isolation.\\n  We believe that machine learning has a prominent role in shaping how\\nquestions in neuroscience are framed, and that the machine-learning mind set is\\nnow entering modern psychology and behavioral studies. It is also equally\\nimportant that practical applications in these fields motivate a rapidly\\nevolving line or research in the machine learning community. In parallel, there\\nis an intense interest in learning more about brain function in the context of\\nrich naturalistic environments and scenes. Efforts to go beyond highly specific\\nparadigms that pinpoint a single function, towards schemes for measuring the\\ninteraction with natural and more varied scene are made. The goal of the\\nworkshop is to pinpoint the most pressing issues and common challenges across\\nthe neuroscience, neuroimaging, psychology and machine learning fields, and to\\nsketch future directions and open questions in the light of novel methodology.\\n</summary>\\n    <author>\\n      <name>I. Rish</name>\\n    </author>\\n    <author>\\n      <name>L. Wehbe</name>\\n    </author>\\n    <author>\\n      <name>G. Langs</name>\\n    </author>\\n    <author>\\n      <name>M. Grosse-Wentrup</name>\\n    </author>\\n    <author>\\n      <name>B. Murphy</name>\\n    </author>\\n    <author>\\n      <name>G. Cecchi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1605.04435v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.04435v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.05463v2</id>\\n    <updated>2017-09-19T21:49:59Z</updated>\\n    <published>2017-03-16T02:56:54Z</published>\\n    <title>Using Human Brain Activity to Guide Machine Learning</title>\\n    <summary>  Machine learning is a field of computer science that builds algorithms that\\nlearn. In many cases, machine learning algorithms are used to recreate a human\\nability like adding a caption to a photo, driving a car, or playing a game.\\nWhile the human brain has long served as a source of inspiration for machine\\nlearning, little effort has been made to directly use data collected from\\nworking brains as a guide for machine learning algorithms. Here we demonstrate\\na new paradigm of \"neurally-weighted\" machine learning, which takes fMRI\\nmeasurements of human brain activity from subjects viewing images, and infuses\\nthese data into the training process of an object recognition learning\\nalgorithm to make it more consistent with the human brain. After training,\\nthese neurally-weighted classifiers are able to classify images without\\nrequiring any additional neural data. We show that our neural-weighting\\napproach can lead to large performance gains when used with traditional machine\\nvision features, as well as to significant improvements with already\\nhigh-performing convolutional neural network features. The effectiveness of\\nthis approach points to a path forward for a new class of hybrid machine\\nlearning algorithms which take both inspiration and direct constraints from\\nneuronal data.\\n</summary>\\n    <author>\\n      <name>Ruth Fong</name>\\n    </author>\\n    <author>\\n      <name>Walter Scheirer</name>\\n    </author>\\n    <author>\\n      <name>David Cox</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Supplemental material can be downloaded here:\\n  http://www.wjscheirer.com/misc/activity_weights/fong-et-al-supplementary.pdf</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.05463v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.05463v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.03965v1</id>\\n    <updated>2018-03-11T14:15:50Z</updated>\\n    <published>2018-03-11T14:15:50Z</published>\\n    <title>BEBP: An Poisoning Method Against Machine Learning Based IDSs</title>\\n    <summary>  In big data era, machine learning is one of fundamental techniques in\\nintrusion detection systems (IDSs). However, practical IDSs generally update\\ntheir decision module by feeding new data then retraining learning models in a\\nperiodical way. Hence, some attacks that comprise the data for training or\\ntesting classifiers significantly challenge the detecting capability of machine\\nlearning-based IDSs. Poisoning attack, which is one of the most recognized\\nsecurity threats towards machine learning-based IDSs, injects some adversarial\\nsamples into the training phase, inducing data drifting of training data and a\\nsignificant performance decrease of target IDSs over testing data. In this\\npaper, we adopt the Edge Pattern Detection (EPD) algorithm to design a novel\\npoisoning method that attack against several machine learning algorithms used\\nin IDSs. Specifically, we propose a boundary pattern detection algorithm to\\nefficiently generate the points that are near to abnormal data but considered\\nto be normal ones by current classifiers. Then, we introduce a Batch-EPD\\nBoundary Pattern (BEBP) detection algorithm to overcome the limitation of the\\nnumber of edge pattern points generated by EPD and to obtain more useful\\nadversarial samples. Based on BEBP, we further present a moderate but effective\\npoisoning method called chronic poisoning attack. Extensive experiments on\\nsynthetic and three real network data sets demonstrate the performance of the\\nproposed poisoning method against several well-known machine learning\\nalgorithms and a practical intrusion detection method named FMIFS-LSSVM-IDS.\\n</summary>\\n    <author>\\n      <name>Pan Li</name>\\n    </author>\\n    <author>\\n      <name>Qiang Liu</name>\\n    </author>\\n    <author>\\n      <name>Wentao Zhao</name>\\n    </author>\\n    <author>\\n      <name>Dongxu Wang</name>\\n    </author>\\n    <author>\\n      <name>Siqi Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages,5figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.03965v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.03965v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.09111v3</id>\\n    <updated>2018-06-26T01:29:25Z</updated>\\n    <published>2018-03-24T13:48:33Z</published>\\n    <title>Entanglement-guided architectures of machine learning by quantum tensor\\n  network</title>\\n    <summary>  It is a fundamental, but still elusive question whether the schemes based on\\nquantum mechanics, in particular on quantum entanglement, can be used for\\nclassical information processing and machine learning. Even partial answer to\\nthis question would bring important insights to both fields of machine learning\\nand quantum mechanics. In this work, we implement simple numerical experiments,\\nrelated to pattern/images classification, in which we represent the classifiers\\nby many-qubit quantum states written in the matrix product states (MPS).\\nClassical machine learning algorithm is applied to these quantum states to\\nlearn the classical data. We explicitly show how quantum entanglement (i.e.,\\nsingle-site and bipartite entanglement) can emerge in such represented images.\\nEntanglement characterizes here the importance of data, and such information\\nare practically used to guide the architecture of MPS, and improve the\\nefficiency. The number of needed qubits can be reduced to less than 1/10 of the\\noriginal number, which is within the access of the state-of-the-art quantum\\ncomputers. We expect such numerical experiments could open new paths in\\ncharactering classical machine learning algorithms, and at the same time shed\\nlights on the generic quantum simulations/computations of machine learning\\ntasks.\\n</summary>\\n    <author>\\n      <name>Yuhan Liu</name>\\n    </author>\\n    <author>\\n      <name>Xiao Zhang</name>\\n    </author>\\n    <author>\\n      <name>Maciej Lewenstein</name>\\n    </author>\\n    <author>\\n      <name>Shi-Ju Ran</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.09111v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.09111v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.str-el\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.09878v1</id>\\n    <updated>2018-10-23T14:23:43Z</updated>\\n    <published>2018-10-23T14:23:43Z</published>\\n    <title>Feasibility of Supervised Machine Learning for Cloud Security</title>\\n    <summary>  Cloud computing is gaining significant attention, however, security is the\\nbiggest hurdle in its wide acceptance. Users of cloud services are under\\nconstant fear of data loss, security threats and availability issues. Recently,\\nlearning-based methods for security applications are gaining popularity in the\\nliterature with the advents in machine learning techniques. However, the major\\nchallenge in these methods is obtaining real-time and unbiased datasets. Many\\ndatasets are internal and cannot be shared due to privacy issues or may lack\\ncertain statistical characteristics. As a result of this, researchers prefer to\\ngenerate datasets for training and testing purpose in the simulated or closed\\nexperimental environments which may lack comprehensiveness. Machine learning\\nmodels trained with such a single dataset generally result in a semantic gap\\nbetween results and their application. There is a dearth of research work which\\ndemonstrates the effectiveness of these models across multiple datasets\\nobtained in different environments. We argue that it is necessary to test the\\nrobustness of the machine learning models, especially in diversified operating\\nconditions, which are prevalent in cloud scenarios. In this work, we use the\\nUNSW dataset to train the supervised machine learning models. We then test\\nthese models with ISOT dataset. We present our results and argue that more\\nresearch in the field of machine learning is still required for its\\napplicability to the cloud security.\\n</summary>\\n    <author>\\n      <name>Deval Bhamare</name>\\n    </author>\\n    <author>\\n      <name>Tara Salman</name>\\n    </author>\\n    <author>\\n      <name>Mohammed Samaka</name>\\n    </author>\\n    <author>\\n      <name>Aiman Erbad</name>\\n    </author>\\n    <author>\\n      <name>Raj Jain</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ICISSEC.2016.7885853</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ICISSEC.2016.7885853\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2016 International Conference on Information Science and Security\\n  (ICISS)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1810.09878v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.09878v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.12125v2</id>\\n    <updated>2018-12-12T11:58:22Z</updated>\\n    <published>2018-10-29T13:47:52Z</published>\\n    <title>Gradual Machine Learning for Entity Resolution</title>\\n    <summary>  Usually considered as a classification problem, entity resolution can be very\\nchallenging on real data due to the prevalence of dirty values. The\\nstate-of-the-art solutions for ER were built on a variety of learning models\\n(most notably deep neural networks), which require lots of accurately labeled\\ntraining data. Unfortunately, high-quality labeled data usually require\\nexpensive manual work, and are therefore not readily available in many real\\nscenarios. In this paper, we propose a novel learning paradigm for ER, called\\ngradual machine learning, which aims to enable effective machine learning\\nwithout the requirement for manual labeling effort. It begins with some easy\\ninstances in a task, which can be automatically labeled by the machine with\\nhigh accuracy, and then gradually labels more challenging instances based on\\niterative factor graph inference. In gradual machine learning, the hard\\ninstances in a task are gradually labeled in small stages based on the\\nestimated evidential certainty provided by the labeled easier instances. Our\\nextensive experiments on real data have shown that the proposed approach\\nperforms considerably better than its unsupervised alternatives, and it is\\nhighly competitive with the state-of-the-art supervised techniques. Using ER as\\na test case, we demonstrate that gradual machine learning is a promising\\nparadigm potentially applicable to other challenging classification tasks\\nrequiring extensive labeling effort.\\n</summary>\\n    <author>\\n      <name>Boyi Hou</name>\\n    </author>\\n    <author>\\n      <name>Qun Chen</name>\\n    </author>\\n    <author>\\n      <name>Yanyan Wang</name>\\n    </author>\\n    <author>\\n      <name>Ping Zhong</name>\\n    </author>\\n    <author>\\n      <name>Ahmed Murtadha</name>\\n    </author>\\n    <author>\\n      <name>Zhaoqiang Chen</name>\\n    </author>\\n    <author>\\n      <name>Zhanhuai Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.12125v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.12125v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.3298v1</id>\\n    <updated>2011-08-16T18:06:29Z</updated>\\n    <published>2011-08-16T18:06:29Z</published>\\n    <title>A Machine Learning Perspective on Predictive Coding with PAQ</title>\\n    <summary>  PAQ8 is an open source lossless data compression algorithm that currently\\nachieves the best compression rates on many benchmarks. This report presents a\\ndetailed description of PAQ8 from a statistical machine learning perspective.\\nIt shows that it is possible to understand some of the modules of PAQ8 and use\\nthis understanding to improve the method. However, intuitive statistical\\nexplanations of the behavior of other modules remain elusive. We hope the\\ndescription in this report will be a starting point for discussions that will\\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\\nrecurrent neural networks and stochastic memoizers. Finally, the report\\npresents a broad range of new applications of PAQ to machine learning tasks\\nincluding language modeling and adaptive text prediction, adaptive game\\nplaying, classification, and compression using features from the field of deep\\nlearning.\\n</summary>\\n    <author>\\n      <name>Byron Knoll</name>\\n    </author>\\n    <author>\\n      <name>Nando de Freitas</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1108.3298v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.3298v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06807v1</id>\\n    <updated>2015-05-26T05:12:23Z</updated>\\n    <published>2015-05-26T05:12:23Z</published>\\n    <title>MLlib: Machine Learning in Apache Spark</title>\\n    <summary>  Apache Spark is a popular open-source platform for large-scale data\\nprocessing that is well-suited for iterative machine learning tasks. In this\\npaper we present MLlib, Spark\\'s open-source distributed machine learning\\nlibrary. MLlib provides efficient functionality for a wide range of learning\\nsettings and includes several underlying statistical, optimization, and linear\\nalgebra primitives. Shipped with Spark, MLlib supports several languages and\\nprovides a high-level API that leverages Spark\\'s rich ecosystem to simplify the\\ndevelopment of end-to-end machine learning pipelines. MLlib has experienced a\\nrapid growth due to its vibrant open-source community of over 140 contributors,\\nand includes extensive documentation to support further growth and to let users\\nquickly get up to speed.\\n</summary>\\n    <author>\\n      <name>Xiangrui Meng</name>\\n    </author>\\n    <author>\\n      <name>Joseph Bradley</name>\\n    </author>\\n    <author>\\n      <name>Burak Yavuz</name>\\n    </author>\\n    <author>\\n      <name>Evan Sparks</name>\\n    </author>\\n    <author>\\n      <name>Shivaram Venkataraman</name>\\n    </author>\\n    <author>\\n      <name>Davies Liu</name>\\n    </author>\\n    <author>\\n      <name>Jeremy Freeman</name>\\n    </author>\\n    <author>\\n      <name>DB Tsai</name>\\n    </author>\\n    <author>\\n      <name>Manish Amde</name>\\n    </author>\\n    <author>\\n      <name>Sean Owen</name>\\n    </author>\\n    <author>\\n      <name>Doris Xin</name>\\n    </author>\\n    <author>\\n      <name>Reynold Xin</name>\\n    </author>\\n    <author>\\n      <name>Michael J. Franklin</name>\\n    </author>\\n    <author>\\n      <name>Reza Zadeh</name>\\n    </author>\\n    <author>\\n      <name>Matei Zaharia</name>\\n    </author>\\n    <author>\\n      <name>Ameet Talwalkar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1505.06807v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06807v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.03508v1</id>\\n    <updated>2016-06-11T00:18:35Z</updated>\\n    <published>2016-06-11T00:18:35Z</published>\\n    <title>Distributed Machine Learning in Materials that Couple Sensing,\\n  Actuation, Computation and Communication</title>\\n    <summary>  This paper reviews machine learning applications and approaches to detection,\\nclassification and control of intelligent materials and structures with\\nembedded distributed computation elements. The purpose of this survey is to\\nidentify desired tasks to be performed in each type of material or structure\\n(e.g., damage detection in composites), identify and compare common approaches\\nto learning such tasks, and investigate models and training paradigms used.\\nMachine learning approaches and common temporal features used in the domains of\\nstructural health monitoring, morphable aircraft, wearable computing and\\nrobotic skins are explored. As the ultimate goal of this research is to\\nincorporate the approaches described in this survey into a robotic material\\nparadigm, the potential for adapting the computational models used in these\\napplications, and corresponding training algorithms, to an amorphous network of\\ncomputing nodes is considered. Distributed versions of support vector machines,\\ngraphical models and mixture models developed in the field of wireless sensor\\nnetworks are reviewed. Potential areas of investigation, including possible\\narchitectures for incorporating machine learning into robotic nodes, training\\napproaches, and the possibility of using deep learning approaches for automatic\\nfeature extraction, are discussed.\\n</summary>\\n    <author>\\n      <name>Dana Hughes</name>\\n    </author>\\n    <author>\\n      <name>Nikolaus Correll</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1606.03508v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.03508v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1604.04505v1</id>\\n    <updated>2016-04-15T13:51:57Z</updated>\\n    <published>2016-04-15T13:51:57Z</published>\\n    <title>A short note on extension theorems and their connection to universal\\n  consistency in machine learning</title>\\n    <summary>  Statistical machine learning plays an important role in modern statistics and\\ncomputer science. One main goal of statistical machine learning is to provide\\nuniversally consistent algorithms, i.e., the estimator converges in probability\\nor in some stronger sense to the Bayes risk or to the Bayes decision function.\\nKernel methods based on minimizing the regularized risk over a reproducing\\nkernel Hilbert space (RKHS) belong to these statistical machine learning\\nmethods. It is in general unknown which kernel yields optimal results for a\\nparticular data set or for the unknown probability measure. Hence various\\nkernel learning methods were proposed to choose the kernel and therefore also\\nits RKHS in a data adaptive manner. Nevertheless, many practitioners often use\\nthe classical Gaussian RBF kernel or certain Sobolev kernels with good success.\\nThe goal of this short note is to offer one possible theoretical explanation\\nfor this empirical fact.\\n</summary>\\n    <author>\\n      <name>Andreas Christmann</name>\\n    </author>\\n    <author>\\n      <name>Florian Dumpert</name>\\n    </author>\\n    <author>\\n      <name>Dao-Hong Xiang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1604.04505v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1604.04505v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.00193v2</id>\\n    <updated>2017-04-25T10:03:41Z</updated>\\n    <published>2016-12-01T10:23:59Z</published>\\n    <title>Learning molecular energies using localized graph kernels</title>\\n    <summary>  Recent machine learning methods make it possible to model potential energy of\\natomic configurations with chemical-level accuracy (as calculated from\\nab-initio calculations) and at speeds suitable for molecular dynam- ics\\nsimulation. Best performance is achieved when the known physical constraints\\nare encoded in the machine learning models. For example, the atomic energy is\\ninvariant under global translations and rotations, it is also invariant to\\npermutations of same-species atoms. Although simple to state, these symmetries\\nare complicated to encode into machine learning algorithms. In this paper, we\\npresent a machine learning approach based on graph theory that naturally\\nincorporates translation, rotation, and permutation symmetries. Specifically,\\nwe use a random walk graph kernel to measure the similarity of two adjacency\\nmatrices, each of which represents a local atomic environment. This Graph\\nApproximated Energy (GRAPE) approach is flexible and admits many possible\\nextensions. We benchmark a simple version of GRAPE by predicting atomization\\nenergies on a standard dataset of organic molecules.\\n</summary>\\n    <author>\\n      <name>G. Ferr\\xc3\\xa9</name>\\n    </author>\\n    <author>\\n      <name>T. Haut</name>\\n    </author>\\n    <author>\\n      <name>K. Barros</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.4978623</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.4978623\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The Journal of Chemical Physics, 146(11), 114107 (2017)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1612.00193v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.00193v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.03079v2</id>\\n    <updated>2017-02-28T17:21:33Z</updated>\\n    <published>2016-12-09T16:29:16Z</published>\\n    <title>Clipper: A Low-Latency Online Prediction Serving System</title>\\n    <summary>  Machine learning is being deployed in a growing number of applications which\\ndemand real-time, accurate, and robust predictions under heavy query load.\\nHowever, most machine learning frameworks and systems only address model\\ntraining and not deployment.\\n  In this paper, we introduce Clipper, a general-purpose low-latency prediction\\nserving system. Interposing between end-user applications and a wide range of\\nmachine learning frameworks, Clipper introduces a modular architecture to\\nsimplify model deployment across frameworks and applications. Furthermore, by\\nintroducing caching, batching, and adaptive model selection techniques, Clipper\\nreduces prediction latency and improves prediction throughput, accuracy, and\\nrobustness without modifying the underlying machine learning frameworks. We\\nevaluate Clipper on four common machine learning benchmark datasets and\\ndemonstrate its ability to meet the latency, accuracy, and throughput demands\\nof online serving applications. Finally, we compare Clipper to the TensorFlow\\nServing system and demonstrate that we are able to achieve comparable\\nthroughput and latency while enabling model composition and online learning to\\nimprove accuracy and render more robust predictions.\\n</summary>\\n    <author>\\n      <name>Daniel Crankshaw</name>\\n    </author>\\n    <author>\\n      <name>Xin Wang</name>\\n    </author>\\n    <author>\\n      <name>Giulio Zhou</name>\\n    </author>\\n    <author>\\n      <name>Michael J. Franklin</name>\\n    </author>\\n    <author>\\n      <name>Joseph E. Gonzalez</name>\\n    </author>\\n    <author>\\n      <name>Ion Stoica</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.03079v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.03079v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.02757v1</id>\\n    <updated>2017-03-08T09:26:36Z</updated>\\n    <published>2017-03-08T09:26:36Z</published>\\n    <title>Byzantine-Tolerant Machine Learning</title>\\n    <summary>  The growth of data, the need for scalability and the complexity of models\\nused in modern machine learning calls for distributed implementations. Yet, as\\nof today, distributed machine learning frameworks have largely ignored the\\npossibility of arbitrary (i.e., Byzantine) failures. In this paper, we study\\nthe robustness to Byzantine failures at the fundamental level of stochastic\\ngradient descent (SGD), the heart of most machine learning algorithms. Assuming\\na set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can\\nSGD be, without limiting the dimension, nor the size of the parameter space.\\n  We first show that no gradient descent update rule based on a linear\\ncombination of the vectors proposed by the workers (i.e, current approaches)\\ntolerates a single Byzantine failure. We then formulate a resilience property\\nof the update rule capturing the basic requirements to guarantee convergence\\ndespite $f$ Byzantine workers. We finally propose Krum, an update rule that\\nsatisfies the resilience property aforementioned. For a $d$-dimensional\\nlearning problem, the time complexity of Krum is $O(n^2 \\\\cdot (d + \\\\log n))$.\\n</summary>\\n    <author>\\n      <name>Peva Blanchard</name>\\n    </author>\\n    <author>\\n      <name>El Mahdi El Mhamdi</name>\\n    </author>\\n    <author>\\n      <name>Rachid Guerraoui</name>\\n    </author>\\n    <author>\\n      <name>Julien Stainer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.02757v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.02757v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.00241v1</id>\\n    <updated>2017-06-01T10:17:12Z</updated>\\n    <published>2017-06-01T10:17:12Z</published>\\n    <title>Krylov Subspace Recycling for Fast Iterative Least-Squares in Machine\\n  Learning</title>\\n    <summary>  Solving symmetric positive definite linear problems is a fundamental\\ncomputational task in machine learning. The exact solution, famously, is\\ncubicly expensive in the size of the matrix. To alleviate this problem, several\\nlinear-time approximations, such as spectral and inducing-point methods, have\\nbeen suggested and are now in wide use. These are low-rank approximations that\\nchoose the low-rank space a priori and do not refine it over time. While this\\nallows linear cost in the data-set size, it also causes a finite, uncorrected\\napproximation error. Authors from numerical linear algebra have explored ways\\nto iteratively refine such low-rank approximations, at a cost of a small number\\nof matrix-vector multiplications. This idea is particularly interesting in the\\nmany situations in machine learning where one has to solve a sequence of\\nrelated symmetric positive definite linear problems. From the machine learning\\nperspective, such deflation methods can be interpreted as transfer learning of\\na low-rank approximation across a time-series of numerical tasks. We study the\\nuse of such methods for our field. Our empirical results show that, on\\nregression and classification problems of intermediate size, this approach can\\ninterpolate between low computational cost and numerical precision.\\n</summary>\\n    <author>\\n      <name>Filip de Roos</name>\\n    </author>\\n    <author>\\n      <name>Philipp Hennig</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.00241v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.00241v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.03413v1</id>\\n    <updated>2017-09-08T17:39:26Z</updated>\\n    <published>2017-09-08T17:39:26Z</published>\\n    <title>Gigamachine: incremental machine learning on desktop computers</title>\\n    <summary>  We present a concrete design for Solomonoff\\'s incremental machine learning\\nsystem suitable for desktop computers. We use R5RS Scheme and its standard\\nlibrary with a few omissions as the reference machine. We introduce a Levin\\nSearch variant based on a stochastic Context Free Grammar together with new\\nupdate algorithms that use the same grammar as a guiding probability\\ndistribution for incremental machine learning. The updates include adjusting\\nproduction probabilities, re-using previous solutions, learning programming\\nidioms and discovery of frequent subprograms. The issues of extending the a\\npriori probability distribution and bootstrapping are discussed. We have\\nimplemented a good portion of the proposed algorithms. Experiments with toy\\nproblems show that the update algorithms work as expected.\\n</summary>\\n    <author>\\n      <name>Eray \\xc3\\x96zkural</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This is the original submission for my AGI-2010 paper titled\\n  Stochastic Grammar Based Incremental Machine Learning Using Scheme which may\\n  be found on http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_24.pdf\\n  and presented a partial but general solution to the transfer learning problem\\n  in AI. arXiv admin note: substantial text overlap with arXiv:1103.1003</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Artificial General Intelligence 2010, p. 190</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1709.03413v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.03413v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.10781v1</id>\\n    <updated>2017-11-29T11:29:09Z</updated>\\n    <published>2017-11-29T11:29:09Z</published>\\n    <title>Introduction to Tensor Decompositions and their Applications in Machine\\n  Learning</title>\\n    <summary>  Tensors are multidimensional arrays of numerical values and therefore\\ngeneralize matrices to multiple dimensions. While tensors first emerged in the\\npsychometrics community in the $20^{\\\\text{th}}$ century, they have since then\\nspread to numerous other disciplines, including machine learning. Tensors and\\ntheir decompositions are especially beneficial in unsupervised learning\\nsettings, but are gaining popularity in other sub-disciplines like temporal and\\nmulti-relational data analysis, too.\\n  The scope of this paper is to give a broad overview of tensors, their\\ndecompositions, and how they are used in machine learning. As part of this, we\\nare going to introduce basic tensor concepts, discuss why tensors can be\\nconsidered more rigid than matrices with respect to the uniqueness of their\\ndecomposition, explain the most important factorization algorithms and their\\nproperties, provide concrete examples of tensor decomposition applications in\\nmachine learning, conduct a case study on tensor-based estimation of mixture\\nmodels, talk about the current state of research, and provide references to\\navailable software libraries.\\n</summary>\\n    <author>\\n      <name>Stephan Rabanser</name>\\n    </author>\\n    <author>\\n      <name>Oleksandr Shchur</name>\\n    </author>\\n    <author>\\n      <name>Stephan G\\xc3\\xbcnnemann</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 12 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.10781v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.10781v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.03041v1</id>\\n    <updated>2018-02-08T20:34:19Z</updated>\\n    <published>2018-02-08T20:34:19Z</published>\\n    <title>Detection of Adversarial Training Examples in Poisoning Attacks through\\n  Anomaly Detection</title>\\n    <summary>  Machine learning has become an important component for many systems and\\napplications including computer vision, spam filtering, malware and network\\nintrusion detection, among others. Despite the capabilities of machine learning\\nalgorithms to extract valuable information from data and produce accurate\\npredictions, it has been shown that these algorithms are vulnerable to attacks.\\nData poisoning is one of the most relevant security threats against machine\\nlearning systems, where attackers can subvert the learning process by injecting\\nmalicious samples in the training data. Recent work in adversarial machine\\nlearning has shown that the so-called optimal attack strategies can\\nsuccessfully poison linear classifiers, degrading the performance of the system\\ndramatically after compromising a small fraction of the training dataset. In\\nthis paper we propose a defence mechanism to mitigate the effect of these\\noptimal poisoning attacks based on outlier detection. We show empirically that\\nthe adversarial examples generated by these attack strategies are quite\\ndifferent from genuine points, as no detectability constrains are considered to\\ncraft the attack. Hence, they can be detected with an appropriate pre-filtering\\nof the training dataset.\\n</summary>\\n    <author>\\n      <name>Andrea Paudice</name>\\n    </author>\\n    <author>\\n      <name>Luis Mu\\xc3\\xb1oz-Gonz\\xc3\\xa1lez</name>\\n    </author>\\n    <author>\\n      <name>Andras Gyorgy</name>\\n    </author>\\n    <author>\\n      <name>Emil C. Lupu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.03041v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.03041v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.04253v2</id>\\n    <updated>2018-05-23T04:05:40Z</updated>\\n    <published>2018-02-11T00:24:32Z</published>\\n    <title>Global Model Interpretation via Recursive Partitioning</title>\\n    <summary>  In this work, we propose a simple but effective method to interpret black-box\\nmachine learning models globally. That is, we use a compact binary tree, the\\ninterpretation tree, to explicitly represent the most important decision rules\\nthat are implicitly contained in the black-box machine learning models. This\\ntree is learned from the contribution matrix which consists of the\\ncontributions of input variables to predicted scores for each single\\nprediction. To generate the interpretation tree, a unified process recursively\\npartitions the input variable space by maximizing the difference in the average\\ncontribution of the split variable between the divided spaces. We demonstrate\\nthe effectiveness of our method in diagnosing machine learning models on\\nmultiple tasks. Also, it is useful for new knowledge discovery as such insights\\nare not easily identifiable when only looking at single predictions. In\\ngeneral, our work makes it easier and more efficient for human beings to\\nunderstand machine learning models.\\n</summary>\\n    <author>\\n      <name>Chengliang Yang</name>\\n    </author>\\n    <author>\\n      <name>Anand Rangarajan</name>\\n    </author>\\n    <author>\\n      <name>Sanjay Ranka</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by The 4th IEEE International Conference on Data Science and\\n  Systems (DSS-2018)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.04253v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.04253v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.00897v1</id>\\n    <updated>2018-03-02T15:35:18Z</updated>\\n    <published>2018-03-02T15:35:18Z</published>\\n    <title>Impact of Biases in Big Data</title>\\n    <summary>  The underlying paradigm of big data-driven machine learning reflects the\\ndesire of deriving better conclusions from simply analyzing more data, without\\nthe necessity of looking at theory and models. Is having simply more data\\nalways helpful? In 1936, The Literary Digest collected 2.3M filled in\\nquestionnaires to predict the outcome of that year\\'s US presidential election.\\nThe outcome of this big data prediction proved to be entirely wrong, whereas\\nGeorge Gallup only needed 3K handpicked people to make an accurate prediction.\\nGenerally, biases occur in machine learning whenever the distributions of\\ntraining set and test set are different. In this work, we provide a review of\\ndifferent sorts of biases in (big) data sets in machine learning. We provide\\ndefinitions and discussions of the most commonly appearing biases in machine\\nlearning: class imbalance and covariate shift. We also show how these biases\\ncan be quantified and corrected. This work is an introductory text for both\\nresearchers and practitioners to become more aware of this topic and thus to\\nderive more reliable models for their learning problems.\\n</summary>\\n    <author>\\n      <name>Patrick Glauner</name>\\n    </author>\\n    <author>\\n      <name>Petko Valtchev</name>\\n    </author>\\n    <author>\\n      <name>Radu State</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 26th European Symposium on Artificial Neural\\n  Networks, Computational Intelligence and Machine Learning (ESANN 2018)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1803.00897v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.00897v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00601v1</id>\\n    <updated>2018-08-01T23:56:28Z</updated>\\n    <published>2018-08-01T23:56:28Z</published>\\n    <title>Classification of Building Information Model (BIM) Structures with Deep\\n  Learning</title>\\n    <summary>  In this work we study an application of machine learning to the construction\\nindustry and we use classical and modern machine learning methods to categorize\\nimages of building designs into three classes: Apartment building, Industrial\\nbuilding or Other. No real images are used, but only images extracted from\\nBuilding Information Model (BIM) software, as these are used by the\\nconstruction industry to store building designs. For this task, we compared\\nfour different methods: the first is based on classical machine learning, where\\nHistogram of Oriented Gradients (HOG) was used for feature extraction and a\\nSupport Vector Machine (SVM) for classification; the other three methods are\\nbased on deep learning, covering common pre-trained networks as well as ones\\ndesigned from scratch. To validate the accuracy of the models, a database of\\n240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and\\nabove 89% for the neural networks.\\n</summary>\\n    <author>\\n      <name>Francesco Lomio</name>\\n    </author>\\n    <author>\\n      <name>Ricardo Farinha</name>\\n    </author>\\n    <author>\\n      <name>Mauri Laasonen</name>\\n    </author>\\n    <author>\\n      <name>Heikki Huttunen</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/EUVIP.2018.8611701</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/EUVIP.2018.8611701\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This work has been submitted to the IEEE for possible publication.\\n  Copyright may be transferred without notice, after which this version may no\\n  longer be accessible</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.00601v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00601v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.06443v1</id>\\n    <updated>2018-10-15T15:05:21Z</updated>\\n    <published>2018-10-15T15:05:21Z</published>\\n    <title>Hedging Algorithms and Repeated Matrix Games</title>\\n    <summary>  Playing repeated matrix games (RMG) while maximizing the cumulative returns\\nis a basic method to evaluate multi-agent learning (MAL) algorithms. Previous\\nwork has shown that $UCB$, $M3$, $S$ or $Exp3$ algorithms have good behaviours\\non average in RMG. Besides, hedging algorithms have been shown to be effective\\non prediction problems. An hedging algorithm is made up with a top-level\\nalgorithm and a set of basic algorithms. To make its decision, an hedging\\nalgorithm uses its top-level algorithm to choose a basic algorithm, and the\\nchosen algorithm makes the decision. This paper experimentally shows that\\nwell-selected hedging algorithms are better on average than all previous MAL\\nalgorithms on the task of playing RMG against various players. $S$ is a very\\ngood top-level algorithm, and $UCB$ and $M3$ are very good basic algorithms.\\nFurthermore, two-level hedging algorithms are more effective than one-level\\nhedging algorithms, and three levels are not better than two levels.\\n</summary>\\n    <author>\\n      <name>Bruno Bouzy</name>\\n    </author>\\n    <author>\\n      <name>Marc M\\xc3\\xa9tivier</name>\\n    </author>\\n    <author>\\n      <name>Damien Pellier</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, Workshop of the European Conference on Machine Learning on\\n  Machine Learning and Data Mining in and around Games, 2011</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Workshop of the European Conference on Machine Learning on Machine\\n  Learning and Data Mining in and around Games, 2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1810.06443v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.06443v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.09957v1</id>\\n    <updated>2018-10-08T04:30:44Z</updated>\\n    <published>2018-10-08T04:30:44Z</published>\\n    <title>NSML: Meet the MLaaS platform with a real-world case study</title>\\n    <summary>  The boom of deep learning induced many industries and academies to introduce\\nmachine learning based approaches into their concern, competitively. However,\\nexisting machine learning frameworks are limited to sufficiently fulfill the\\ncollaboration and management for both data and models. We proposed NSML, a\\nmachine learning as a service (MLaaS) platform, to meet these demands. NSML\\nhelps machine learning work be easily launched on a NSML cluster and provides a\\ncollaborative environment which can afford development at enterprise scale.\\nFinally, NSML users can deploy their own commercial services with NSML cluster.\\nIn addition, NSML furnishes convenient visualization tools which assist the\\nusers in analyzing their work. To verify the usefulness and accessibility of\\nNSML, we performed some experiments with common examples. Furthermore, we\\nexamined the collaborative advantages of NSML through three competitions with\\nreal-world use cases.\\n</summary>\\n    <author>\\n      <name>Hanjoo Kim</name>\\n    </author>\\n    <author>\\n      <name>Minkyu Kim</name>\\n    </author>\\n    <author>\\n      <name>Dongjoo Seo</name>\\n    </author>\\n    <author>\\n      <name>Jinwoong Kim</name>\\n    </author>\\n    <author>\\n      <name>Heungseok Park</name>\\n    </author>\\n    <author>\\n      <name>Soeun Park</name>\\n    </author>\\n    <author>\\n      <name>Hyunwoo Jo</name>\\n    </author>\\n    <author>\\n      <name>KyungHyun Kim</name>\\n    </author>\\n    <author>\\n      <name>Youngil Yang</name>\\n    </author>\\n    <author>\\n      <name>Youngkwan Kim</name>\\n    </author>\\n    <author>\\n      <name>Nako Sung</name>\\n    </author>\\n    <author>\\n      <name>Jung-Woo Ha</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.09957v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.09957v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.01074v1</id>\\n    <updated>2018-12-03T20:37:21Z</updated>\\n    <published>2018-12-03T20:37:21Z</published>\\n    <title>Distilling Information from a Flood: A Possibility for the Use of\\n  Meta-Analysis and Systematic Review in Machine Learning Research</title>\\n    <summary>  The current flood of information in all areas of machine learning research,\\nfrom computer vision to reinforcement learning, has made it difficult to make\\naggregate scientific inferences. It can be challenging to distill a myriad of\\nsimilar papers into a set of useful principles, to determine which new\\nmethodologies to use for a particular application, and to be confident that one\\nhas compared against all relevant related work when developing new ideas.\\nHowever, such a rapidly growing body of research literature is a problem that\\nother fields have already faced - in particular, medicine and epidemiology. In\\nthose fields, systematic reviews and meta-analyses have been used exactly for\\ndealing with these issues and it is not uncommon for entire journals to be\\ndedicated to such analyses. Here, we suggest the field of machine learning\\nmight similarly benefit from meta-analysis and systematic review, and we\\nencourage further discussion and development along this direction.\\n</summary>\\n    <author>\\n      <name>Peter Henderson</name>\\n    </author>\\n    <author>\\n      <name>Emma Brunskill</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to the Critiquing and Correcting Trends in Machine Learning\\n  Workshop (CRACT) at NeurIPS 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.01074v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.01074v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.02320v2</id>\\n    <updated>2019-01-28T22:33:59Z</updated>\\n    <published>2018-11-21T14:50:37Z</published>\\n    <title>Steerable Wavelet Scattering for 3D Atomic Systems with Application to\\n  Li-Si Energy Prediction</title>\\n    <summary>  A general machine learning architecture is introduced that uses wavelet\\nscattering coefficients of an inputted three dimensional signal as features.\\nSolid harmonic wavelet scattering transforms of three dimensional signals were\\npreviously introduced in a machine learning framework for the regression of\\nproperties of small organic molecules. Here this approach is extended for\\ngeneral steerable wavelets which are equivariant to translations and rotations,\\nresulting in a sparse model of the target function. The scattering coefficients\\ninherit from the wavelets invariance to translations and rotations. As an\\nillustration of this approach a linear regression model is learned for the\\nformation energy of amorphous lithium-silicon material states trained over a\\ndatabase generated using plane-wave Density Functional Theory methods.\\nState-of-the-art results are produced as compared to other machine learning\\napproaches over similarly generated databases.\\n</summary>\\n    <author>\\n      <name>Xavier Brumwell</name>\\n    </author>\\n    <author>\\n      <name>Paul Sinz</name>\\n    </author>\\n    <author>\\n      <name>Kwang Jin Kim</name>\\n    </author>\\n    <author>\\n      <name>Yue Qi</name>\\n    </author>\\n    <author>\\n      <name>Matthew Hirn</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2018 Workshop on Machine Learning for Molecules and Materials,\\n  Montreal, Canada</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.02320v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.02320v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.05443v1</id>\\n    <updated>2018-10-23T14:39:24Z</updated>\\n    <published>2018-10-23T14:39:24Z</published>\\n    <title>Machine Learning for Anomaly Detection and Categorization in Multi-cloud\\n  Environments</title>\\n    <summary>  Recently, advances in machine learning techniques have attracted the\\nattention of the research community to build intrusion detection systems (IDS)\\nthat can detect anomalies in the network traffic. Most of the research works,\\nhowever, do not differentiate among different types of attacks. This is, in\\nfact, necessary for appropriate countermeasures and defense against attacks. In\\nthis paper, we investigate both detecting and categorizing anomalies rather\\nthan just detecting, which is a common trend in the contemporary research\\nworks. We have used a popular publicly available dataset to build and test\\nlearning models for both detection and categorization of different attacks. To\\nbe precise, we have used two supervised machine learning techniques, namely\\nlinear regression (LR) and random forest (RF). We show that even if detection\\nis perfect, categorization can be less accurate due to similarities between\\nattacks. Our results demonstrate more than 99% detection accuracy and\\ncategorization accuracy of 93.6%, with the inability to categorize some\\nattacks. Further, we argue that such categorization can be applied to\\nmulti-cloud environments using the same machine learning techniques.\\n</summary>\\n    <author>\\n      <name>Tara Salman</name>\\n    </author>\\n    <author>\\n      <name>Deval Bhamare</name>\\n    </author>\\n    <author>\\n      <name>Aiman Erbad</name>\\n    </author>\\n    <author>\\n      <name>Raj Jain</name>\\n    </author>\\n    <author>\\n      <name>Mohammed Samaka</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CSCloud.2017.15</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CSCloud.2017.15\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CSCLoud17</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CSCLOUD 2017</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1812.05443v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.05443v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.06415v1</id>\\n    <updated>2018-12-16T08:00:36Z</updated>\\n    <published>2018-12-16T08:00:36Z</published>\\n    <title>Stochastic Distributed Optimization for Machine Learning from\\n  Decentralized Features</title>\\n    <summary>  Distributed machine learning has been widely studied in the literature to\\nscale up machine learning model training in the presence of an ever-increasing\\namount of data. We study distributed machine learning from another perspective,\\nwhere the information about the training same samples are inherently\\ndecentralized and located on different parities. We propose an asynchronous\\nstochastic gradient descent (SGD) algorithm for such a feature distributed\\nmachine learning (FDML) problem, to jointly learn from decentralized features,\\nwith theoretical convergence guarantees under bounded asynchrony. Our algorithm\\ndoes not require sharing the original feature data or even local model\\nparameters between parties, thus preserving a high level of data\\nconfidentiality. We implement our algorithm for FDML in a parameter server\\narchitecture. We compare our system with fully centralized training (which\\nviolates data locality requirements) and training only based on local features,\\nthrough extensive experiments performed on a large amount of data from a\\nreal-world application, involving 5 million samples and $8700$ features in\\ntotal. Experimental results have demonstrated the effectiveness and efficiency\\nof the proposed FDML system.\\n</summary>\\n    <author>\\n      <name>Yaochen Hu</name>\\n    </author>\\n    <author>\\n      <name>Di Niu</name>\\n    </author>\\n    <author>\\n      <name>Jianming Yang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.06415v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.06415v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.06469v5</id>\\n    <updated>2019-03-24T20:30:26Z</updated>\\n    <published>2018-12-16T14:19:44Z</published>\\n    <title>The Adverse Effects of Code Duplication in Machine Learning Models of\\n  Code</title>\\n    <summary>  The field of big code relies on mining large corpora of code to perform some\\nlearning task. A significant threat to this approach has been recently\\nidentified by Lopes et al. (2017) who found a large amount of near-duplicate\\ncode on GitHub. However, the impact of code duplication has not been noticed by\\nresearchers devising machine learning models for source code. In this article,\\nwe examine the effect of code duplication on machine learning models showing\\nthat reported metrics are sometimes inflated by up to 100% when testing on\\nduplicated code corpora compared to the performance on de-duplicated corpora\\nwhich more accurately represent how machine learning models of code are used by\\nsoftware engineers. We present an \"errata\" for widely used datasets, list best\\npractices for collecting code corpora and evaluating machine learning models on\\nthem, and release tools to help the community avoid this problem in future\\nresearch.\\n</summary>\\n    <author>\\n      <name>Miltiadis Allamanis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.06469v5\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.06469v5\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.04797v1</id>\\n    <updated>2019-03-12T09:28:05Z</updated>\\n    <published>2019-03-12T09:28:05Z</published>\\n    <title>Elements of Sequential Monte Carlo</title>\\n    <summary>  A core problem in statistics and probabilistic machine learning is to compute\\nprobability distributions and expectations. This is the fundamental problem of\\nBayesian statistics and machine learning, which frames all inference as\\nexpectations with respect to the posterior distribution. The key challenge is\\nto approximate these intractable expectations. In this tutorial, we review\\nsequential Monte Carlo (SMC), a random-sampling-based class of methods for\\napproximate inference. First, we explain the basics of SMC, discuss practical\\nissues, and review theoretical results. We then examine two of the main user\\ndesign choices: the proposal distributions and the so called intermediate\\ntarget distributions. We review recent results on how variational inference and\\namortization can be used to learn efficient proposals and target distributions.\\nNext, we discuss the SMC estimate of the normalizing constant, how this can be\\nused for pseudo-marginal inference and inference evaluation. Throughout the\\ntutorial we illustrate the use of SMC on various models commonly used in\\nmachine learning, such as stochastic recurrent neural networks, probabilistic\\ngraphical models, and probabilistic programs.\\n</summary>\\n    <author>\\n      <name>Christian A. Naesseth</name>\\n    </author>\\n    <author>\\n      <name>Fredrik Lindsten</name>\\n    </author>\\n    <author>\\n      <name>Thomas B. Sch\\xc3\\xb6n</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Under review at Foundations and Trends in Machine Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.04797v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.04797v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.05510v1</id>\\n    <updated>2018-05-15T01:10:18Z</updated>\\n    <published>2018-05-15T01:10:18Z</published>\\n    <title>Online Deep Metric Learning</title>\\n    <summary>  Metric learning learns a metric function from training data to calculate the\\nsimilarity or distance between samples. From the perspective of feature\\nlearning, metric learning essentially learns a new feature space by feature\\ntransformation (e.g., Mahalanobis distance metric). However, traditional metric\\nlearning algorithms are shallow, which just learn one metric space (feature\\ntransformation). Can we further learn a better metric space from the learnt\\nmetric space? In other words, can we learn metric progressively and nonlinearly\\nlike deep learning by just using the existing metric learning algorithms? To\\nthis end, we present a hierarchical metric learning scheme and implement an\\nonline deep metric learning framework, namely ODML. Specifically, we take one\\nonline metric learning algorithm as a metric layer, followed by a nonlinear\\nlayer (i.e., ReLU), and then stack these layers modelled after the deep\\nlearning. The proposed ODML enjoys some nice properties, indeed can learn\\nmetric progressively and performs superiorly on some datasets. Various\\nexperiments with different settings have been conducted to verify these\\nproperties of the proposed ODML.\\n</summary>\\n    <author>\\n      <name>Wenbin Li</name>\\n    </author>\\n    <author>\\n      <name>Jing Huo</name>\\n    </author>\\n    <author>\\n      <name>Yinghuan Shi</name>\\n    </author>\\n    <author>\\n      <name>Yang Gao</name>\\n    </author>\\n    <author>\\n      <name>Lei Wang</name>\\n    </author>\\n    <author>\\n      <name>Jiebo Luo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.05510v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.05510v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.2474v1</id>\\n    <updated>2012-10-19T15:06:27Z</updated>\\n    <published>2012-10-19T15:06:27Z</published>\\n    <title>Learning Riemannian Metrics</title>\\n    <summary>  We propose a solution to the problem of estimating a Riemannian metric\\nassociated with a given differentiable manifold. The metric learning problem is\\nbased on minimizing the relative volume of a given set of points. We derive the\\ndetails for a family of metrics on the multinomial simplex. The resulting\\nmetric has applications in text classification and bears some similarity to\\nTFIDF representation of text documents.\\n</summary>\\n    <author>\\n      <name>Guy Lebanon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Nineteenth Conference on Uncertainty in\\n  Artificial Intelligence (UAI2003)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1212.2474v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.2474v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.2065v1</id>\\n    <updated>2014-08-09T06:05:51Z</updated>\\n    <published>2014-08-09T06:05:51Z</published>\\n    <title>Normalized Online Learning</title>\\n    <summary>  We introduce online learning algorithms which are independent of feature\\nscales, proving regret bounds dependent on the ratio of scales existent in the\\ndata rather than the absolute scale. This has several useful effects: there is\\nno need to pre-normalize data, the test-time and test-space complexity are\\nreduced, and the algorithms are more robust.\\n</summary>\\n    <author>\\n      <name>Stephane Ross</name>\\n    </author>\\n    <author>\\n      <name>Paul Mineiro</name>\\n    </author>\\n    <author>\\n      <name>John Langford</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\\n  in Artificial Intelligence (UAI2013)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.2065v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.2065v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.5449v1</id>\\n    <updated>2014-08-23T01:23:23Z</updated>\\n    <published>2014-08-23T01:23:23Z</published>\\n    <title>Stretchy Polynomial Regression</title>\\n    <summary>  This article proposes a novel solution for stretchy polynomial regression\\nlearning. The solution comes in primal and dual closed-forms similar to that of\\nridge regression. Essentially, the proposed solution stretches the covariance\\ncomputation via a power term thereby compresses or amplifies the estimation.\\nOur experiments on both synthetic data and real-world data show effectiveness\\nof the proposed method for compressive learning.\\n</summary>\\n    <author>\\n      <name>Kar-Ann Toh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Article created in April and revised in August 2014. Submitted to\\n  ICARCV 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.5449v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.5449v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.5143v2</id>\\n    <updated>2014-06-21T08:27:42Z</updated>\\n    <published>2014-06-19T18:42:12Z</published>\\n    <title>The Sample Complexity of Learning Linear Predictors with the Squared\\n  Loss</title>\\n    <summary>  In this short note, we provide tight sample complexity bounds for learning\\nlinear predictors with respect to the squared loss. Our focus is on an agnostic\\nsetting, where no assumptions are made on the data distribution. This contrasts\\nwith standard results in the literature, which either make distributional\\nassumptions, refer to specific parameter settings, or use other performance\\nmeasures.\\n</summary>\\n    <author>\\n      <name>Ohad Shamir</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.5143v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.5143v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6361v1</id>\\n    <updated>2012-06-27T18:37:50Z</updated>\\n    <published>2012-06-27T18:37:50Z</published>\\n    <title>Learning Markov Network Structure using Brownian Distance Covariance</title>\\n    <summary>  In this paper, we present a simple non-parametric method for learning the\\nstructure of undirected graphs from data that drawn from an underlying unknown\\ndistribution. We propose to use Brownian distance covariance to estimate the\\nconditional independences between the random variables and encodes pairwise\\nMarkov graph. This framework can be applied in high-dimensional setting, where\\nthe number of parameters much be larger than the sample size.\\n</summary>\\n    <author>\\n      <name>Ehsan Khoshgnauz</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6361v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6361v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.03086v1</id>\\n    <updated>2015-11-10T12:30:42Z</updated>\\n    <published>2015-11-10T12:30:42Z</published>\\n    <title>The CTU Prague Relational Learning Repository</title>\\n    <summary>  The aim of the CTU Prague Relational Learning Repository is to support\\nmachine learning research with multi-relational data. The repository currently\\ncontains 50 SQL databases hosted on a public MySQL server located at\\nrelational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., the\\nnumber of tables in the database, the number of rows and columns in the tables,\\nthe number of foreign key constraints between tables).\\n</summary>\\n    <author>\\n      <name>Jan Motl</name>\\n    </author>\\n    <author>\\n      <name>Oliver Schulte</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1511.03086v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.03086v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6; H.2.8\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.6646v1</id>\\n    <updated>2013-05-28T22:12:59Z</updated>\\n    <published>2013-05-28T22:12:59Z</published>\\n    <title>Normalized Online Learning</title>\\n    <summary>  We introduce online learning algorithms which are independent of feature\\nscales, proving regret bounds dependent on the ratio of scales existent in the\\ndata rather than the absolute scale. This has several useful effects: there is\\nno need to pre-normalize data, the test-time and test-space complexity are\\nreduced, and the algorithms are more robust.\\n</summary>\\n    <author>\\n      <name>Stephane Ross</name>\\n    </author>\\n    <author>\\n      <name>Paul Mineiro</name>\\n    </author>\\n    <author>\\n      <name>John Langford</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\\n  in Artificial Intelligence (UAI2013)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1305.6646v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.6646v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.3436v2</id>\\n    <updated>2017-04-08T06:06:38Z</updated>\\n    <published>2014-11-13T03:34:32Z</published>\\n    <title>SelfieBoost: A Boosting Algorithm for Deep Learning</title>\\n    <summary>  We describe and analyze a new boosting algorithm for deep learning called\\nSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct\\nensembles of classifiers, SelfieBoost boosts the accuracy of a single network.\\nWe prove a $\\\\log(1/\\\\epsilon)$ convergence rate for SelfieBoost under some \"SGD\\nsuccess\" assumption which seems to hold in practice.\\n</summary>\\n    <author>\\n      <name>Shai Shalev-Shwartz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.3436v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.3436v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.08833v1</id>\\n    <updated>2016-05-28T02:39:24Z</updated>\\n    <published>2016-05-28T02:39:24Z</published>\\n    <title>Muffled Semi-Supervised Learning</title>\\n    <summary>  We explore a novel approach to semi-supervised learning. This approach is\\ncontrary to the common approach in that the unlabeled examples serve to\\n\"muffle,\" rather than enhance, the guidance provided by the labeled examples.\\nWe provide several variants of the basic algorithm and show experimentally that\\nthey can achieve significantly higher AUC than boosted trees, random forests\\nand logistic regression when unlabeled examples are available.\\n</summary>\\n    <author>\\n      <name>Akshay Balsubramani</name>\\n    </author>\\n    <author>\\n      <name>Yoav Freund</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1605.08833v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.08833v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.00564v1</id>\\n    <updated>2016-10-03T14:22:19Z</updated>\\n    <published>2016-10-03T14:22:19Z</published>\\n    <title>End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural\\n  Networks</title>\\n    <summary>  We investigate sequence machine learning techniques on raw radio signal\\ntime-series data. By applying deep recurrent neural networks we learn to\\ndiscriminate between several application layer traffic types on top of a\\nconstant envelope modulation without using an expert demodulation algorithm. We\\nshow that complex protocol sequences can be learned and used for both\\nclassification and generation tasks using this approach.\\n</summary>\\n    <author>\\n      <name>Timothy J. O\\'Shea</name>\\n    </author>\\n    <author>\\n      <name>Seth Hitefield</name>\\n    </author>\\n    <author>\\n      <name>Johnathan Corgan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1610.00564v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.00564v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.02850v1</id>\\n    <updated>2017-05-08T12:56:57Z</updated>\\n    <published>2017-05-08T12:56:57Z</published>\\n    <title>Learning Product Automata</title>\\n    <summary>  In this paper we give an optimization for active learning algorithms,\\napplicable to learning Moore machines where the output comprises several\\nobservables. These machines can be decomposed themselves by projecting on each\\nobservable, resulting in smaller components. These components can then be\\nlearnt with fewer queries. This is in particular interesting for learning\\nsoftware, where compositional methods are important for guaranteeing\\nscalability.\\n</summary>\\n    <author>\\n      <name>Joshua Moerman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to LearnAut 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.02850v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.02850v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.FL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.06503v1</id>\\n    <updated>2018-01-19T17:40:09Z</updated>\\n    <published>2018-01-19T17:40:09Z</published>\\n    <title>Global overview of Imitation Learning</title>\\n    <summary>  Imitation Learning is a sequential task where the learner tries to mimic an\\nexpert\\'s action in order to achieve the best performance. Several algorithms\\nhave been proposed recently for this task. In this project, we aim at proposing\\na wide review of these algorithms, presenting their main features and comparing\\nthem on their performance and their regret bounds.\\n</summary>\\n    <author>\\n      <name>Alexandre Attia</name>\\n    </author>\\n    <author>\\n      <name>Sharone Dayan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 5 figures, 5 appendix pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.06503v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.06503v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.03360v3</id>\\n    <updated>2018-03-21T14:45:32Z</updated>\\n    <published>2018-02-09T17:25:43Z</published>\\n    <title>Information Planning for Text Data</title>\\n    <summary>  Information planning enables faster learning with fewer training examples. It\\nis particularly applicable when training examples are costly to obtain. This\\nwork examines the advantages of information planning for text data by focusing\\non three supervised models: Naive Bayes, supervised LDA and deep neural\\nnetworks. We show that planning based on entropy and mutual information\\noutperforms random selection baseline and therefore accelerates learning.\\n</summary>\\n    <author>\\n      <name>Vadim Smolyakov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.03360v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.03360v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.05834v1</id>\\n    <updated>2018-04-14T23:17:07Z</updated>\\n    <published>2018-04-14T23:17:07Z</published>\\n    <title>CytonRL: an Efficient Reinforcement Learning Open-source Toolkit\\n  Implemented in C++</title>\\n    <summary>  This paper presents an open-source enforcement learning toolkit named CytonRL\\n(https://github.com/arthurxlw/cytonRL). The toolkit implements four recent\\nadvanced deep Q-learning algorithms from scratch using C++ and NVIDIA\\'s\\nGPU-accelerated libraries. The code is simple and elegant, owing to an\\nopen-source general-purpose neural network library named CytonLib. Benchmark\\nshows that the toolkit achieves competitive performances on the popular Atari\\ngame of Breakout.\\n</summary>\\n    <author>\\n      <name>Xiaolin Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.05834v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.05834v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.03785v1</id>\\n    <updated>2018-05-10T02:08:15Z</updated>\\n    <published>2018-05-10T02:08:15Z</published>\\n    <title>Deep Learning of Geometric Constellation Shaping including Fiber\\n  Nonlinearities</title>\\n    <summary>  A new geometric shaping method is proposed, leveraging unsupervised machine\\nlearning to optimize the constellation design. The learned constellation\\nmitigates nonlinear effects with gains up to 0.13 bit/4D when trained with a\\nsimplified fiber channel model.\\n</summary>\\n    <author>\\n      <name>Rasmus T. Jones</name>\\n    </author>\\n    <author>\\n      <name>Tobias A. Eriksson</name>\\n    </author>\\n    <author>\\n      <name>Metodi P. Yankov</name>\\n    </author>\\n    <author>\\n      <name>Darko Zibar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 6 figures, submitted to ECOC 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.03785v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.03785v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08728v1</id>\\n    <updated>2018-05-22T16:38:41Z</updated>\\n    <published>2018-05-22T16:38:41Z</published>\\n    <title>Efficient Stochastic Gradient Descent for Distributionally Robust\\n  Learning</title>\\n    <summary>  We consider a new stochastic gradient descent algorithm for efficiently\\nsolving general min-max optimization problems that arise naturally in\\ndistributionally robust learning. By focusing on the entire dataset, current\\napproaches do not scale well. We address this issue by initially focusing on a\\nsubset of the data and progressively increasing this support to statistically\\ncover the entire dataset.\\n</summary>\\n    <author>\\n      <name>Soumyadip Ghosh</name>\\n    </author>\\n    <author>\\n      <name>Mark Squillante</name>\\n    </author>\\n    <author>\\n      <name>Ebisa Wollega</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 Pages, NIPS 2018 submission</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.08728v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08728v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.04650v1</id>\\n    <updated>2018-12-11T19:05:23Z</updated>\\n    <published>2018-12-11T19:05:23Z</published>\\n    <title>Reproduction Report on \"Learn to Pay Attention\"</title>\\n    <summary>  We have successfully implemented the \"Learn to Pay Attention\" model of\\nattention mechanism in convolutional neural networks, and have replicated the\\nresults of the original paper in the categories of image classification and\\nfine-grained recognition.\\n</summary>\\n    <author>\\n      <name>Levan Shugliashvili</name>\\n    </author>\\n    <author>\\n      <name>Davit Soselia</name>\\n    </author>\\n    <author>\\n      <name>Shota Amashukeli</name>\\n    </author>\\n    <author>\\n      <name>Irakli Koberidze</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 2 tables, originally made for the ICLR 2018 Reproducibility\\n  Challenge</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.04650v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.04650v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.01427v1</id>\\n    <updated>2019-01-05T15:07:35Z</updated>\\n    <published>2019-01-05T15:07:35Z</published>\\n    <title>Poincar\\xc3\\xa9 Wasserstein Autoencoder</title>\\n    <summary>  This work presents a reformulation of the recently proposed Wasserstein\\nautoencoder framework on a non-Euclidean manifold, the Poincar\\\\\\'e ball model of\\nthe hyperbolic space. By assuming the latent space to be hyperbolic, we can use\\nits intrinsic hierarchy to impose structure on the learned latent space\\nrepresentations. We demonstrate the model in the visual domain to analyze some\\nof its properties and show competitive results on a graph link prediction task.\\n</summary>\\n    <author>\\n      <name>Ivan Ovinnikov</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Bayesian Deep Learning Workshop (NeurIPS 2018)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1901.01427v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.01427v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.05590v1</id>\\n    <updated>2019-01-17T02:21:24Z</updated>\\n    <published>2019-01-17T02:21:24Z</published>\\n    <title>Disentangling Video with Independent Prediction</title>\\n    <summary>  We propose an unsupervised variational model for disentangling video into\\nindependent factors, i.e. each factor\\'s future can be predicted from its past\\nwithout considering the others. We show that our approach often learns factors\\nwhich are interpretable as objects in a scene.\\n</summary>\\n    <author>\\n      <name>William F. Whitney</name>\\n    </author>\\n    <author>\\n      <name>Rob Fergus</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the Learning Disentangled Representations: from\\n  Perception to Control workshop at NIPS 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.05590v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.05590v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.06223v2</id>\\n    <updated>2019-02-23T09:53:27Z</updated>\\n    <published>2019-02-17T08:54:05Z</published>\\n    <title>Learning Linear-Quadratic Regulators Efficiently with only $\\\\sqrt{T}$\\n  Regret</title>\\n    <summary>  We present the first computationally-efficient algorithm with $\\\\widetilde\\nO(\\\\sqrt{T})$ regret for learning in Linear Quadratic Control systems with\\nunknown dynamics. By that, we resolve an open question of Abbasi-Yadkori and\\nSzepesv\\\\\\'ari (2011) and Dean, Mania, Matni, Recht, and Tu (2018).\\n</summary>\\n    <author>\\n      <name>Alon Cohen</name>\\n    </author>\\n    <author>\\n      <name>Tomer Koren</name>\\n    </author>\\n    <author>\\n      <name>Yishay Mansour</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.06223v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.06223v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.09009v1</id>\\n    <updated>2019-02-24T20:14:58Z</updated>\\n    <published>2019-02-24T20:14:58Z</published>\\n    <title>Efficient Private Algorithms for Learning Halfspaces</title>\\n    <summary>  We present new differentially private algorithms for learning a large-margin\\nhalfspace. In contrast to previous algorithms, which are based on either\\ndifferentially private simulations of the statistical query model or on private\\nconvex optimization, the sample complexity of our algorithms depends only on\\nthe margin of the data, and not on the dimension.\\n</summary>\\n    <author>\\n      <name>Huy L. Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Ullman</name>\\n    </author>\\n    <author>\\n      <name>Lydia Zakynthinou</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.09009v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.09009v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.09418v1</id>\\n    <updated>2017-12-26T21:14:09Z</updated>\\n    <published>2017-12-26T21:14:09Z</published>\\n    <title>Horn-ICE Learning for Synthesizing Invariants and Contracts</title>\\n    <summary>  We design learning algorithms for synthesizing invariants using Horn\\nimplication counterexamples (Horn-ICE), extending the ICE-learning model. In\\nparticular, we describe a decision-tree learning algorithm that learns from\\nHorn-ICE samples, works in polynomial time, and uses statistical heuristics to\\nlearn small trees that satisfy the samples. Since most verification proofs can\\nbe modeled using Horn clauses, Horn-ICE learning is a more robust technique to\\nlearn inductive annotations that prove programs correct. Our experiments show\\nthat an implementation of our algorithm is able to learn adequate inductive\\ninvariants and contracts efficiently for a variety of sequential and concurrent\\nprograms.\\n</summary>\\n    <author>\\n      <name>Deepak D\\'Souza</name>\\n    </author>\\n    <author>\\n      <name>P. Ezudheen</name>\\n    </author>\\n    <author>\\n      <name>Pranav Garg</name>\\n    </author>\\n    <author>\\n      <name>P. Madhusudan</name>\\n    </author>\\n    <author>\\n      <name>Daniel Neider</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3276501</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3276501\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1712.09418v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.09418v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Q60, 68Q32\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0910.3713v1</id>\\n    <updated>2009-10-19T21:55:11Z</updated>\\n    <published>2009-10-19T21:55:11Z</published>\\n    <title>On Learning Finite-State Quantum Sources</title>\\n    <summary>  We examine the complexity of learning the distributions produced by\\nfinite-state quantum sources. We show how prior techniques for learning hidden\\nMarkov models can be adapted to the quantum generator model to find that the\\nanalogous state of affairs holds: information-theoretically, a polynomial\\nnumber of samples suffice to approximately identify the distribution, but\\ncomputationally, the problem is as hard as learning parities with noise, a\\nnotorious open question in computational learning theory.\\n</summary>\\n    <author>\\n      <name>Brendan Juba</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0910.3713v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0910.3713v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.09718v1</id>\\n    <updated>2017-10-26T14:06:52Z</updated>\\n    <published>2017-10-26T14:06:52Z</published>\\n    <title>Learning Approximate Stochastic Transition Models</title>\\n    <summary>  We examine the problem of learning mappings from state to state, suitable for\\nuse in a model-based reinforcement-learning setting, that simultaneously\\ngeneralize to novel states and can capture stochastic transitions. We show that\\ncurrently popular generative adversarial networks struggle to learn these\\nstochastic transition models but a modification to their loss functions results\\nin a powerful learning algorithm for this class of problems.\\n</summary>\\n    <author>\\n      <name>Yuhang Song</name>\\n    </author>\\n    <author>\\n      <name>Christopher Grimm</name>\\n    </author>\\n    <author>\\n      <name>Xianming Wang</name>\\n    </author>\\n    <author>\\n      <name>Michael L. Littman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1710.09718v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.09718v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.07895v1</id>\\n    <updated>2016-08-29T02:37:21Z</updated>\\n    <published>2016-08-29T02:37:21Z</published>\\n    <title>Human-Algorithm Interaction Biases in the Big Data Cycle: A Markov Chain\\n  Iterated Learning Framework</title>\\n    <summary>  Early supervised machine learning algorithms have relied on reliable expert\\nlabels to build predictive models. However, the gates of data generation have\\nrecently been opened to a wider base of users who started participating\\nincreasingly with casual labeling, rating, annotating, etc. The increased\\nonline presence and participation of humans has led not only to a\\ndemocratization of unchecked inputs to algorithms, but also to a wide\\ndemocratization of the \"consumption\" of machine learning algorithms\\' outputs by\\ngeneral users. Hence, these algorithms, many of which are becoming essential\\nbuilding blocks of recommender systems and other information filters, started\\ninteracting with users at unprecedented rates. The result is machine learning\\nalgorithms that consume more and more data that is unchecked, or at the very\\nleast, not fitting conventional assumptions made by various machine learning\\nalgorithms. These include biased samples, biased labels, diverging training and\\ntesting sets, and cyclical interaction between algorithms, humans, information\\nconsumed by humans, and data consumed by algorithms. Yet, the continuous\\ninteraction between humans and algorithms is rarely taken into account in\\nmachine learning algorithm design and analysis. In this paper, we present a\\npreliminary theoretical model and analysis of the mutual interaction between\\nhumans and algorithms, based on an iterated learning framework that is inspired\\nfrom the study of human language evolution. We also define the concepts of\\nhuman and algorithm blind spots and outline machine learning approaches to mend\\niterated bias through two novel notions: antidotes and reactive learning.\\n</summary>\\n    <author>\\n      <name>Olfa Nasraoui</name>\\n    </author>\\n    <author>\\n      <name>Patrick Shafto</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This research was supported by National Science Foundation grant\\n  NSF-1549981</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1608.07895v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.07895v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6; K.4.m; H.2.8; H.3.3; H.1.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.02871v2</id>\\n    <updated>2018-10-22T06:22:23Z</updated>\\n    <published>2018-02-08T14:18:23Z</published>\\n    <title>Online Learning: A Comprehensive Survey</title>\\n    <summary>  Online learning represents an important family of machine learning\\nalgorithms, in which a learner attempts to resolve an online prediction (or any\\ntype of decision-making) task by learning a model/hypothesis from a sequence of\\ndata instances one at a time. The goal of online learning is to ensure that the\\nonline learner would make a sequence of accurate predictions (or correct\\ndecisions) given the knowledge of correct answers to previous prediction or\\nlearning tasks and possibly additional information. This is in contrast to many\\ntraditional batch learning or offline machine learning algorithms that are\\noften designed to train a model in batch from a given collection of training\\ndata instances. This survey aims to provide a comprehensive survey of the\\nonline machine learning literatures through a systematic review of basic ideas\\nand key principles and a proper categorization of different algorithms and\\ntechniques. Generally speaking, according to the learning type and the forms of\\nfeedback information, the existing online learning works can be classified into\\nthree major categories: (i) supervised online learning where full feedback\\ninformation is always available, (ii) online learning with limited feedback,\\nand (iii) unsupervised online learning where there is no feedback available.\\nDue to space limitation, the survey will be mainly focused on the first\\ncategory, but also briefly cover some basics of the other two categories.\\nFinally, we also discuss some open issues and attempt to shed light on\\npotential future research directions in this field.\\n</summary>\\n    <author>\\n      <name>Steven C. H. Hoi</name>\\n    </author>\\n    <author>\\n      <name>Doyen Sahoo</name>\\n    </author>\\n    <author>\\n      <name>Jing Lu</name>\\n    </author>\\n    <author>\\n      <name>Peilin Zhao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">100 pages, ~400 references</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.02871v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.02871v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.05009v3</id>\\n    <updated>2018-07-16T09:34:53Z</updated>\\n    <published>2018-06-13T13:08:16Z</published>\\n    <title>Tree Edit Distance Learning via Adaptive Symbol Embeddings</title>\\n    <summary>  Metric learning has the aim to improve classification accuracy by learning a\\ndistance measure which brings data points from the same class closer together\\nand pushes data points from different classes further apart. Recent research\\nhas demonstrated that metric learning approaches can also be applied to trees,\\nsuch as molecular structures, abstract syntax trees of computer programs, or\\nsyntax trees of natural language, by learning the cost function of an edit\\ndistance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.\\nHowever, learning such costs directly may yield an edit distance which violates\\nmetric axioms, is challenging to interpret, and may not generalize well. In\\nthis contribution, we propose a novel metric learning approach for trees which\\nwe call embedding edit distance learning (BEDL) and which learns an edit\\ndistance indirectly by embedding the tree nodes as vectors, such that the\\nEuclidean distance between those vectors supports class discrimination. We\\nlearn such embeddings by reducing the distance to prototypical trees from the\\nsame class and increasing the distance to prototypical trees from different\\nclasses. In our experiments, we show that BEDL improves upon the\\nstate-of-the-art in metric learning for trees on six benchmark data sets,\\nranging from computer science over biomedical data to a natural-language\\nprocessing data set containing over 300,000 nodes.\\n</summary>\\n    <author>\\n      <name>Benjamin Paa\\xc3\\x9fen</name>\\n    </author>\\n    <author>\\n      <name>Claudio Gallicchio</name>\\n    </author>\\n    <author>\\n      <name>Alessio Micheli</name>\\n    </author>\\n    <author>\\n      <name>Barbara Hammer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Paper at the International Conference of Machine Learning (2018),\\n  2018-07-10 to 2018-07-15 in Stockholm, Sweden</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of Machine Learning Research 80 (2018) 3973-3982</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1806.05009v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.05009v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.06740v1</id>\\n    <updated>2019-02-16T19:54:52Z</updated>\\n    <published>2019-02-16T19:54:52Z</published>\\n    <title>Communication Topologies Between Learning Agents in Deep Reinforcement\\n  Learning</title>\\n    <summary>  A common technique to improve speed and robustness of learning in deep\\nreinforcement learning (DRL) and many other machine learning algorithms is to\\nrun multiple learning agents in parallel. A neglected component in the\\ndevelopment of these algorithms has been how best to arrange the learning\\nagents involved to better facilitate distributed search. Here we draw upon\\nresults from the networked optimization and collective intelligence literatures\\nsuggesting that arranging learning agents in less than fully connected\\ntopologies (the implicit way agents are commonly arranged in) can improve\\nlearning. We explore the relative performance of four popular families of\\ngraphs and observe that one such family (Erdos-Renyi random graphs) empirically\\noutperforms the standard fully-connected communication topology across several\\nDRL benchmark tasks. We observe that 1000 learning agents arranged in an\\nErdos-Renyi graph can perform as well as 3000 agents arranged in the standard\\nfully-connected topology, showing the large learning improvement possible when\\ncarefully designing the topology over which agents communicate. We complement\\nthese empirical results with a preliminary theoretical investigation of why\\nless than fully connected topologies can perform better. Overall, our work\\nsuggests that distributed machine learning algorithms could be made more\\nefficient if the communication topology between learning agents was optimized.\\n</summary>\\n    <author>\\n      <name>Dhaval Adjodah</name>\\n    </author>\\n    <author>\\n      <name>Dan Calacci</name>\\n    </author>\\n    <author>\\n      <name>Abhimanyu Dubey</name>\\n    </author>\\n    <author>\\n      <name>Anirudh Goyal</name>\\n    </author>\\n    <author>\\n      <name>Peter Krafft</name>\\n    </author>\\n    <author>\\n      <name>Esteban Moro</name>\\n    </author>\\n    <author>\\n      <name>Alex Pentland</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1811.12556</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.06740v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.06740v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1510.02533v2</id>\\n    <updated>2016-03-19T02:00:45Z</updated>\\n    <published>2015-10-09T00:34:12Z</published>\\n    <title>New Optimisation Methods for Machine Learning</title>\\n    <summary>  A thesis submitted for the degree of Doctor of Philosophy of The Australian\\nNational University.\\n  In this work we introduce several new optimisation methods for problems in\\nmachine learning. Our algorithms broadly fall into two categories: optimisation\\nof finite sums and of graph structured objectives. The finite sum problem is\\nsimply the minimisation of objective functions that are naturally expressed as\\na summation over a large number of terms, where each term has a similar or\\nidentical weight. Such objectives most often appear in machine learning in the\\nempirical risk minimisation framework in the non-online learning setting. The\\nsecond category, that of graph structured objectives, consists of objectives\\nthat result from applying maximum likelihood to Markov random field models.\\nUnlike the finite sum case, all the non-linearity is contained within a\\npartition function term, which does not readily decompose into a summation.\\n  For the finite sum problem, we introduce the Finito and SAGA algorithms, as\\nwell as variants of each.\\n  For graph-structured problems, we take three complementary approaches. We\\nlook at learning the parameters for a fixed structure, learning the structure\\nindependently, and learning both simultaneously. Specifically, for the combined\\napproach, we introduce a new method for encouraging graph structures with the\\n\"scale-free\" property. For the structure learning problem, we establish\\nSHORTCUT, a O(n^{2.5}) expected time approximate structure learning method for\\nGaussian graphical models. For problems where the structure is known but the\\nparameters unknown, we introduce an approximate maximum likelihood learning\\nalgorithm that is capable of learning a useful subclass of Gaussian graphical\\nmodels.\\n</summary>\\n    <author>\\n      <name>Aaron Defazio</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PhD thesis, 205 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1510.02533v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1510.02533v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0910.5932v1</id>\\n    <updated>2009-10-30T18:19:03Z</updated>\\n    <published>2009-10-30T18:19:03Z</published>\\n    <title>Metric and Kernel Learning using a Linear Transformation</title>\\n    <summary>  Metric and kernel learning are important in several machine learning\\napplications. However, most existing metric learning algorithms are limited to\\nlearning metrics over low-dimensional data, while existing kernel learning\\nalgorithms are often limited to the transductive setting and do not generalize\\nto new data points. In this paper, we study metric learning as a problem of\\nlearning a linear transformation of the input data. We show that for\\nhigh-dimensional data, a particular framework for learning a linear\\ntransformation of the data based on the LogDet divergence can be efficiently\\nkernelized to learn a metric (or equivalently, a kernel function) over an\\narbitrarily high dimensional space. We further demonstrate that a wide class of\\nconvex loss functions for learning linear transformations can similarly be\\nkernelized, thereby considerably expanding the potential applications of metric\\nlearning. We demonstrate our learning approach by applying it to large-scale\\nreal world problems in computer vision and text mining.\\n</summary>\\n    <author>\\n      <name>Prateek Jain</name>\\n    </author>\\n    <author>\\n      <name>Brian Kulis</name>\\n    </author>\\n    <author>\\n      <name>Jason V. Davis</name>\\n    </author>\\n    <author>\\n      <name>Inderjit S. Dhillon</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/0910.5932v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0910.5932v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.3536v1</id>\\n    <updated>2012-03-15T11:17:56Z</updated>\\n    <published>2012-03-15T11:17:56Z</published>\\n    <title>A Convex Formulation for Learning Task Relationships in Multi-Task\\n  Learning</title>\\n    <summary>  Multi-task learning is a learning paradigm which seeks to improve the\\ngeneralization performance of a learning task with the help of some other\\nrelated tasks. In this paper, we propose a regularization formulation for\\nlearning the relationships between tasks in multi-task learning. This\\nformulation can be viewed as a novel generalization of the regularization\\nframework for single-task learning. Besides modeling positive task correlation,\\nour method, called multi-task relationship learning (MTRL), can also describe\\nnegative task correlation and identify outlier tasks based on the same\\nunderlying principle. Under this regularization framework, the objective\\nfunction of MTRL is convex. For efficiency, we use an alternating method to\\nlearn the optimal model parameters for each task as well as the relationships\\nbetween tasks. We study MTRL in the symmetric multi-task learning setting and\\nthen generalize it to the asymmetric setting as well. We also study the\\nrelationships between MTRL and some existing multi-task learning methods.\\nExperiments conducted on a toy problem as well as several benchmark data sets\\ndemonstrate the effectiveness of MTRL.\\n</summary>\\n    <author>\\n      <name>Yu Zhang</name>\\n    </author>\\n    <author>\\n      <name>Dit-Yan Yeung</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\\n  in Artificial Intelligence (UAI2010)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1203.3536v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.3536v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.08829v2</id>\\n    <updated>2017-10-16T10:31:05Z</updated>\\n    <published>2017-04-28T07:31:11Z</published>\\n    <title>Deep Feature Learning for Graphs</title>\\n    <summary>  This paper presents a general graph representation learning framework called\\nDeepGL for learning deep node and edge representations from large (attributed)\\ngraphs. In particular, DeepGL begins by deriving a set of base features (e.g.,\\ngraphlet features) and automatically learns a multi-layered hierarchical graph\\nrepresentation where each successive layer leverages the output from the\\nprevious layer to learn features of a higher-order. Contrary to previous work,\\nDeepGL learns relational functions (each representing a feature) that\\ngeneralize across-networks and therefore useful for graph-based transfer\\nlearning tasks. Moreover, DeepGL naturally supports attributed graphs, learns\\ninterpretable features, and is space-efficient (by learning sparse feature\\nvectors). In addition, DeepGL is expressive, flexible with many interchangeable\\ncomponents, efficient with a time complexity of $\\\\mathcal{O}(|E|)$, and\\nscalable for large networks via an efficient parallel implementation. Compared\\nwith the state-of-the-art method, DeepGL is (1) effective for across-network\\ntransfer learning tasks and attributed graph representation learning, (2)\\nspace-efficient requiring up to 6x less memory, (3) fast with up to 182x\\nspeedup in runtime performance, and (4) accurate with an average improvement of\\n20% or more on many learning tasks.\\n</summary>\\n    <author>\\n      <name>Ryan A. Rossi</name>\\n    </author>\\n    <author>\\n      <name>Rong Zhou</name>\\n    </author>\\n    <author>\\n      <name>Nesreen K. Ahmed</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1704.08829v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.08829v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.05796v1</id>\\n    <updated>2017-02-19T21:13:45Z</updated>\\n    <published>2017-02-19T21:13:45Z</published>\\n    <title>Collaborative Deep Reinforcement Learning</title>\\n    <summary>  Besides independent learning, human learning process is highly improved by\\nsummarizing what has been learned, communicating it with peers, and\\nsubsequently fusing knowledge from different sources to assist the current\\nlearning goal. This collaborative learning procedure ensures that the knowledge\\nis shared, continuously refined, and concluded from different perspectives to\\nconstruct a more profound understanding. The idea of knowledge transfer has led\\nto many advances in machine learning and data mining, but significant\\nchallenges remain, especially when it comes to reinforcement learning,\\nheterogeneous model structures, and different learning tasks. Motivated by\\nhuman collaborative learning, in this paper we propose a collaborative deep\\nreinforcement learning (CDRL) framework that performs adaptive knowledge\\ntransfer among heterogeneous learning agents. Specifically, the proposed CDRL\\nconducts a novel deep knowledge distillation method to address the\\nheterogeneity among different learning tasks with a deep alignment network.\\nFurthermore, we present an efficient collaborative Asynchronous Advantage\\nActor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into\\nthe online training of agents, and demonstrate the effectiveness of the CDRL\\nframework using extensive empirical evaluation on OpenAI gym.\\n</summary>\\n    <author>\\n      <name>Kaixiang Lin</name>\\n    </author>\\n    <author>\\n      <name>Shu Wang</name>\\n    </author>\\n    <author>\\n      <name>Jiayu Zhou</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1702.05796v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.05796v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.08114v2</id>\\n    <updated>2018-07-27T03:17:17Z</updated>\\n    <published>2017-07-25T04:43:47Z</published>\\n    <title>A Survey on Multi-Task Learning</title>\\n    <summary>  Multi-Task Learning (MTL) is a learning paradigm in machine learning and its\\naim is to leverage useful information contained in multiple related tasks to\\nhelp improve the generalization performance of all the tasks. In this paper, we\\ngive a survey for MTL. First, we classify different MTL algorithms into several\\ncategories, including feature learning approach, low-rank approach, task\\nclustering approach, task relation learning approach, and decomposition\\napproach, and then discuss the characteristics of each approach. In order to\\nimprove the performance of learning tasks further, MTL can be combined with\\nother learning paradigms including semi-supervised learning, active learning,\\nunsupervised learning, reinforcement learning, multi-view learning and\\ngraphical models. When the number of tasks is large or the data dimensionality\\nis high, batch MTL models are difficult to handle this situation and online,\\nparallel and distributed MTL models as well as dimensionality reduction and\\nfeature hashing are reviewed to reveal their computational and storage\\nadvantages. Many real-world applications use MTL to boost their performance and\\nwe review representative works. Finally, we present theoretical analyses and\\ndiscuss several future directions for MTL.\\n</summary>\\n    <author>\\n      <name>Yu Zhang</name>\\n    </author>\\n    <author>\\n      <name>Qiang Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.08114v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.08114v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.04181v2</id>\\n    <updated>2018-06-05T13:52:23Z</updated>\\n    <published>2018-02-12T16:53:48Z</published>\\n    <title>State Representation Learning for Control: An Overview</title>\\n    <summary>  Representation learning algorithms are designed to learn abstract features\\nthat characterize data. State representation learning (SRL) focuses on a\\nparticular kind of representation learning where learned features are in low\\ndimension, evolve through time, and are influenced by actions of an agent. The\\nrepresentation is learned to capture the variation in the environment generated\\nby the agent\\'s actions; this kind of representation is particularly suitable\\nfor robotics and control scenarios. In particular, the low dimension\\ncharacteristic of the representation helps to overcome the curse of\\ndimensionality, provides easier interpretation and utilization by humans and\\ncan help improve performance and speed in policy learning algorithms such as\\nreinforcement learning.\\n  This survey aims at covering the state-of-the-art on state representation\\nlearning in the most recent years. It reviews different SRL methods that\\ninvolve interaction with the environment, their implementations and their\\napplications in robotics control tasks (simulated or real). In particular, it\\nhighlights how generic learning objectives are differently exploited in the\\nreviewed algorithms. Finally, it discusses evaluation methods to assess the\\nrepresentation learned and summarizes current and future lines of research.\\n</summary>\\n    <author>\\n      <name>Timoth\\xc3\\xa9e Lesort</name>\\n    </author>\\n    <author>\\n      <name>Natalia D\\xc3\\xadaz-Rodr\\xc3\\xadguez</name>\\n    </author>\\n    <author>\\n      <name>Jean-Fran\\xc3\\xa7ois Goudou</name>\\n    </author>\\n    <author>\\n      <name>David Filliat</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.neunet.2018.07.006</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.neunet.2018.07.006\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1802.04181v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.04181v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1211.3711v1</id>\\n    <updated>2012-11-14T19:25:21Z</updated>\\n    <published>2012-11-14T19:25:21Z</published>\\n    <title>Sequence Transduction with Recurrent Neural Networks</title>\\n    <summary>  Many machine learning tasks can be expressed as the transformation---or\\n\\\\emph{transduction}---of input sequences into output sequences: speech\\nrecognition, machine translation, protein secondary structure prediction and\\ntext-to-speech to name but a few. One of the key challenges in sequence\\ntransduction is learning to represent both the input and output sequences in a\\nway that is invariant to sequential distortions such as shrinking, stretching\\nand translating. Recurrent neural networks (RNNs) are a powerful sequence\\nlearning architecture that has proven capable of learning such representations.\\nHowever RNNs traditionally require a pre-defined alignment between the input\\nand output sequences to perform transduction. This is a severe limitation since\\n\\\\emph{finding} the alignment is the most difficult aspect of many sequence\\ntransduction problems. Indeed, even determining the length of the output\\nsequence is often challenging. This paper introduces an end-to-end,\\nprobabilistic sequence transduction system, based entirely on RNNs, that is in\\nprinciple able to transform any input sequence into any finite, discrete output\\nsequence. Experimental results for phoneme recognition are provided on the\\nTIMIT speech corpus.\\n</summary>\\n    <author>\\n      <name>Alex Graves</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">First published in the International Conference of Machine Learning\\n  (ICML) 2012 Workshop on Representation Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1211.3711v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.3711v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.04940v1</id>\\n    <updated>2017-07-16T19:57:28Z</updated>\\n    <published>2017-07-16T19:57:28Z</published>\\n    <title>Comparative Performance Analysis of Neural Networks Architectures on H2O\\n  Platform for Various Activation Functions</title>\\n    <summary>  Deep learning (deep structured learning, hierarchi- cal learning or deep\\nmachine learning) is a branch of machine learning based on a set of algorithms\\nthat attempt to model high- level abstractions in data by using multiple\\nprocessing layers with complex structures or otherwise composed of multiple\\nnon-linear transformations. In this paper, we present the results of testing\\nneural networks architectures on H2O platform for various activation functions,\\nstopping metrics, and other parameters of machine learning algorithm. It was\\ndemonstrated for the use case of MNIST database of handwritten digits in\\nsingle-threaded mode that blind selection of these parameters can hugely\\nincrease (by 2-3 orders) the runtime without the significant increase of\\nprecision. This result can have crucial influence for opitmization of available\\nand new machine learning methods, especially for image recognition problems.\\n</summary>\\n    <author>\\n      <name>Yuriy Kochura</name>\\n    </author>\\n    <author>\\n      <name>Sergii Stirenko</name>\\n    </author>\\n    <author>\\n      <name>Yuri Gordienko</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/YSF.2017.8126654</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/YSF.2017.8126654\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 6 figures, 6 tables; 2017 IEEE International Young\\n  Scientists Forum on Applied Physics and Engineering (YSF-2017) (Lviv,\\n  Ukraine)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.04940v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.04940v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.PF\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.04647v1</id>\\n    <updated>2018-02-08T23:54:37Z</updated>\\n    <published>2018-02-08T23:54:37Z</published>\\n    <title>Deep Learning with Apache SystemML</title>\\n    <summary>  Enterprises operate large data lakes using Hadoop and Spark frameworks that\\n(1) run a plethora of tools to automate powerful data\\npreparation/transformation pipelines, (2) run on shared, large clusters to (3)\\nperform many different analytics tasks ranging from model preparation,\\nbuilding, evaluation, and tuning for both machine learning and deep learning.\\nDeveloping machine/deep learning models on data in such shared environments is\\nchallenging. Apache SystemML provides a unified framework for implementing\\nmachine learning and deep learning algorithms in a variety of shared deployment\\nscenarios. SystemML\\'s novel compilation approach automatically generates\\nruntime execution plans for machine/deep learning algorithms that are composed\\nof single-node and distributed runtime operations depending on data and cluster\\ncharacteristics such as data size, data sparsity, cluster size, and memory\\nconfigurations, while still exploiting the capabilities of the underlying big\\ndata frameworks.\\n</summary>\\n    <author>\\n      <name>Niketan Pansare</name>\\n    </author>\\n    <author>\\n      <name>Michael Dusenberry</name>\\n    </author>\\n    <author>\\n      <name>Nakul Jindal</name>\\n    </author>\\n    <author>\\n      <name>Matthias Boehm</name>\\n    </author>\\n    <author>\\n      <name>Berthold Reinwald</name>\\n    </author>\\n    <author>\\n      <name>Prithviraj Sen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted at SysML 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.04647v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.04647v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.02329v1</id>\\n    <updated>2018-03-06T18:41:04Z</updated>\\n    <published>2018-03-06T18:41:04Z</published>\\n    <title>Learning Memory Access Patterns</title>\\n    <summary>  The explosion in workload complexity and the recent slow-down in Moore\\'s law\\nscaling call for new approaches towards efficient computing. Researchers are\\nnow beginning to use recent advances in machine learning in software\\noptimizations, augmenting or replacing traditional heuristics and data\\nstructures. However, the space of machine learning for computer hardware\\narchitecture is only lightly explored. In this paper, we demonstrate the\\npotential of deep learning to address the von Neumann bottleneck of memory\\nperformance. We focus on the critical problem of learning memory access\\npatterns, with the goal of constructing accurate and efficient memory\\nprefetchers. We relate contemporary prefetching strategies to n-gram models in\\nnatural language processing, and show how recurrent neural networks can serve\\nas a drop-in replacement. On a suite of challenging benchmark datasets, we find\\nthat neural networks consistently demonstrate superior performance in terms of\\nprecision and recall. This work represents the first step towards practical\\nneural-network based prefetching, and opens a wide range of exciting directions\\nfor machine learning in computer architecture research.\\n</summary>\\n    <author>\\n      <name>Milad Hashemi</name>\\n    </author>\\n    <author>\\n      <name>Kevin Swersky</name>\\n    </author>\\n    <author>\\n      <name>Jamie A. Smith</name>\\n    </author>\\n    <author>\\n      <name>Grant Ayers</name>\\n    </author>\\n    <author>\\n      <name>Heiner Litz</name>\\n    </author>\\n    <author>\\n      <name>Jichuan Chang</name>\\n    </author>\\n    <author>\\n      <name>Christos Kozyrakis</name>\\n    </author>\\n    <author>\\n      <name>Parthasarathy Ranganathan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.02329v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.02329v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00222v1</id>\\n    <updated>2018-08-01T08:27:27Z</updated>\\n    <published>2018-08-01T08:27:27Z</published>\\n    <title>Experience, Imitation and Reflection; Confucius\\' Conjecture and Machine\\n  Learning</title>\\n    <summary>  Artificial intelligence recently had a great advancements caused by the\\nemergence of new processing power and machine learning methods. Having said\\nthat, the learning capability of artificial intelligence is still at its\\ninfancy comparing to the learning capability of human and many animals. Many of\\nthe current artificial intelligence applications can only operate in a very\\norchestrated, specific environments with an extensive training set that exactly\\ndescribes the conditions that will occur during execution time. Having that in\\nmind, and considering the several existing machine learning methods this\\nquestion rises that \\'What are some of the best ways for a machine to learn?\\'\\nRegarding the learning methods of human, Confucius\\' point of view is that they\\nare by experience, imitation and reflection. This paper tries to explore and\\ndiscuss regarding these three ways of learning and their implementations in\\nmachines by having a look at how they happen in minds.\\n</summary>\\n    <author>\\n      <name>Amir Ramezani Dooraki</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.00222v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00222v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.03233v1</id>\\n    <updated>2018-08-09T16:56:04Z</updated>\\n    <published>2018-08-09T16:56:04Z</published>\\n    <title>OBOE: Collaborative Filtering for AutoML Initialization</title>\\n    <summary>  Algorithm selection and hyperparameter tuning remain two of the most\\nchallenging tasks in machine learning. The number of machine learning\\napplications is growing much faster than the number of machine learning\\nexperts, hence we see an increasing demand for efficient automation of learning\\nprocesses. Here, we introduce OBOE, an algorithm for time-constrained model\\nselection and hyperparameter tuning. Taking advantage of similarity between\\ndatasets, OBOE finds promising algorithm and hyperparameter configurations\\nthrough collaborative filtering. Our system explores these models under time\\nconstraints, so that rapid initializations can be provided to warm-start more\\nfine-grained optimization methods. One novel aspect of our approach is a new\\nheuristic for active learning in time-constrained matrix completion based on\\noptimal experiment design. Our experiments demonstrate that OBOE delivers\\nstate-of-the-art performance faster than competing approaches on a test bed of\\nsupervised learning problems.\\n</summary>\\n    <author>\\n      <name>Chengrun Yang</name>\\n    </author>\\n    <author>\\n      <name>Yuji Akimoto</name>\\n    </author>\\n    <author>\\n      <name>Dae Won Kim</name>\\n    </author>\\n    <author>\\n      <name>Madeleine Udell</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.03233v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.03233v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.02564v1</id>\\n    <updated>2018-11-06T00:05:00Z</updated>\\n    <published>2018-11-06T00:05:00Z</published>\\n    <title>On exponential convergence of SGD in non-convex over-parametrized\\n  learning</title>\\n    <summary>  Large over-parametrized models learned via stochastic gradient descent (SGD)\\nmethods have become a key element in modern machine learning. Although SGD\\nmethods are very effective in practice, most theoretical analyses of SGD\\nsuggest slower convergence than what is empirically observed. In our recent\\nwork [8] we analyzed how interpolation, common in modern over-parametrized\\nlearning, results in exponential convergence of SGD with constant step size for\\nconvex loss functions. In this note, we extend those results to a much broader\\nnon-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A\\nnumber of important non-convex problems in machine learning, including some\\nclasses of neural networks, have been recently shown to satisfy the PL\\ncondition. We argue that the PL condition provides a relevant and attractive\\nsetting for many machine learning problems, particularly in the\\nover-parametrized regime.\\n</summary>\\n    <author>\\n      <name>Raef Bassily</name>\\n    </author>\\n    <author>\\n      <name>Mikhail Belkin</name>\\n    </author>\\n    <author>\\n      <name>Siyuan Ma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.02564v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.02564v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.06885v2</id>\\n    <updated>2019-03-25T07:05:04Z</updated>\\n    <published>2018-11-16T16:07:23Z</published>\\n    <title>A Generalized Meta-loss function for regression and classification using\\n  privileged information</title>\\n    <summary>  Learning using privileged information (LUPI) is a powerful heterogenous\\nfeature space machine learning framework that allows a machine learning model\\nto learn from highly informative or privileged features which are available\\nduring training only to generate test predictions using input space features\\nwhich are available both during training and testing. LUPI can significantly\\nimprove prediction performance in a variety of machine learning problems.\\nHowever, existing large margin and neural network implementations of learning\\nusing privileged information are mostly designed for classification tasks. In\\nthis work, we have proposed a simple yet effective formulation that allows us\\nto perform regression using privileged information through a custom loss\\nfunction. Apart from regression, our formulation allows general application of\\nLUPI to classification and other related problems as well. We have verified the\\ncorrectness, applicability and effectiveness of our method on regression and\\nclassification problems over different synthetic and real-world problems. To\\ntest the usefulness of the proposed model in real-world problems, we have\\nevaluated our method on the problem of protein binding affinity prediction. The\\nproposed LUPI regression-based model has shown to outperform the current\\nstate-of-the-art predictor.\\n</summary>\\n    <author>\\n      <name>Amina Asif</name>\\n    </author>\\n    <author>\\n      <name>Muhammad Dawood</name>\\n    </author>\\n    <author>\\n      <name>Fayyaz ul Amir Afsar Minhas</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.06885v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.06885v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.04666v1</id>\\n    <updated>2019-03-12T00:03:44Z</updated>\\n    <published>2019-03-12T00:03:44Z</published>\\n    <title>Accelerated Learning in the Presence of Time Varying Features with\\n  Applications to Machine Learning and Adaptive Control</title>\\n    <summary>  Features in machine learning problems are often time varying and may be\\nrelated to outputs in an algebraic or dynamical manner. The dynamic nature of\\nthese machine learning problems renders current accelerated gradient descent\\nmethods unstable or weakens their convergence guarantees. This paper proposes\\nalgorithms for the case when time varying features are present, and\\ndemonstrates provable performance guarantees. We develop a variational\\nperspective within a continuous time algorithm. This variational perspective\\nincludes, among other things, higher-order learning concepts and normalization,\\nboth of which stem from adaptive control, and allows stability to be\\nestablished for dynamical machine learning problems. These higher-order\\nalgorithms are also examined for achieving accelerated learning in adaptive\\ncontrol. Simulations are provided to verify the theoretical results.\\n</summary>\\n    <author>\\n      <name>Joseph E. Gaudio</name>\\n    </author>\\n    <author>\\n      <name>Travis E. Gibson</name>\\n    </author>\\n    <author>\\n      <name>Anuradha M. Annaswamy</name>\\n    </author>\\n    <author>\\n      <name>Michael A. Bolender</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">23 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.04666v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.04666v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00971v2</id>\\n    <updated>2019-03-26T23:55:19Z</updated>\\n    <published>2018-12-03T18:46:02Z</published>\\n    <title>Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using\\n  Meta-Learning</title>\\n    <summary>  Learning is an inherently continuous phenomenon. When humans learn a new task\\nthere is no explicit distinction between training and inference. As we learn a\\ntask, we keep learning about it while performing the task. What we learn and\\nhow we learn it varies during different stages of learning. Learning how to\\nlearn and adapt is a key property that enables us to generalize effortlessly to\\nnew settings. This is in contrast with conventional settings in machine\\nlearning where a trained model is frozen during inference. In this paper we\\nstudy the problem of learning to learn at both training and test time in the\\ncontext of visual navigation. A fundamental challenge in navigation is\\ngeneralization to unseen scenes. In this paper we propose a self-adaptive\\nvisual navigation method (SAVN) which learns to adapt to new environments\\nwithout any explicit supervision. Our solution is a meta-reinforcement learning\\napproach where an agent learns a self-supervised interaction loss that\\nencourages effective navigation. Our experiments, performed in the AI2-THOR\\nframework, show major improvements in both success rate and SPL for visual\\nnavigation in novel scenes. Our code and data are available at:\\nhttps://github.com/allenai/savn .\\n</summary>\\n    <author>\\n      <name>Mitchell Wortsman</name>\\n    </author>\\n    <author>\\n      <name>Kiana Ehsani</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Rastegari</name>\\n    </author>\\n    <author>\\n      <name>Ali Farhadi</name>\\n    </author>\\n    <author>\\n      <name>Roozbeh Mottaghi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.00971v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00971v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.02323v1</id>\\n    <updated>2018-03-06T18:19:55Z</updated>\\n    <published>2018-03-06T18:19:55Z</published>\\n    <title>Deep Super Learner: A Deep Ensemble for Classification Problems</title>\\n    <summary>  Deep learning has become very popular for tasks such as predictive modeling\\nand pattern recognition in handling big data. Deep learning is a powerful\\nmachine learning method that extracts lower level features and feeds them\\nforward for the next layer to identify higher level features that improve\\nperformance. However, deep neural networks have drawbacks, which include many\\nhyper-parameters and infinite architectures, opaqueness into results, and\\nrelatively slower convergence on smaller datasets. While traditional machine\\nlearning algorithms can address these drawbacks, they are not typically capable\\nof the performance levels achieved by deep neural networks. To improve\\nperformance, ensemble methods are used to combine multiple base learners. Super\\nlearning is an ensemble that finds the optimal combination of diverse learning\\nalgorithms. This paper proposes deep super learning as an approach which\\nachieves log loss and accuracy results competitive to deep neural networks\\nwhile employing traditional machine learning algorithms in a hierarchical\\nstructure. The deep super learner is flexible, adaptable, and easy to train\\nwith good performance across different tasks using identical hyper-parameter\\nvalues. Using traditional machine learning requires fewer hyper-parameters,\\nallows transparency into results, and has relatively fast convergence on\\nsmaller datasets. Experimental results show that the deep super learner has\\nsuperior performance compared to the individual base learners, single-layer\\nensembles, and in some cases deep neural networks. Performance of the deep\\nsuper learner may further be improved with task-specific tuning.\\n</summary>\\n    <author>\\n      <name>Steven Young</name>\\n    </author>\\n    <author>\\n      <name>Tamer Abdou</name>\\n    </author>\\n    <author>\\n      <name>Ayse Bener</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 3 figures, accepted to the 31st Canadian Conference on\\n  Artificial Intelligence</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.02323v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.02323v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cmp-lg/9405014v1</id>\\n    <updated>1994-05-09T17:00:00Z</updated>\\n    <published>1994-05-09T17:00:00Z</published>\\n    <title>Classifying Cue Phrases in Text and Speech Using Machine Learning</title>\\n    <summary>  Cue phrases may be used in a discourse sense to explicitly signal discourse\\nstructure, but also in a sentential sense to convey semantic rather than\\nstructural information. This paper explores the use of machine learning for\\nclassifying cue phrases as discourse or sentential. Two machine learning\\nprograms (Cgrendel and C4.5) are used to induce classification rules from sets\\nof pre-classified cue phrases and their features. Machine learning is shown to\\nbe an effective technique for not only automating the generation of\\nclassification rules, but also for improving upon previous results.\\n</summary>\\n    <author>\\n      <name>Diane J. Litman</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">AT&amp;T Bell Laboratories, Murray Hill, NJ</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, PostScript File, to appear in the Proceedings of AAAI-94</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cmp-lg/9405014v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cmp-lg/9405014v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cmp-lg\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cmp-lg\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0408048v1</id>\\n    <updated>2004-08-21T16:57:34Z</updated>\\n    <published>2004-08-21T16:57:34Z</published>\\n    <title>Journal of New Democratic Methods: An Introduction</title>\\n    <summary>  This paper describes a new breed of academic journals that use statistical\\nmachine learning techniques to make them more democratic. In particular, not\\nonly can anyone submit an article, but anyone can also become a reviewer.\\nMachine learning is used to decide which reviewers accurately represent the\\nviews of the journal\\'s readers and thus deserve to have their opinions carry\\nmore weight. The paper concentrates on describing a specific experimental\\nprototype of a democratic journal called the Journal of New Democratic Methods\\n(JNDM). The paper also mentions the wider implications that machine learning\\nand the techniques used in the JNDM may have for representative democracy in\\ngeneral.\\n</summary>\\n    <author>\\n      <name>John David Funge</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/cs/0408048v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0408048v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6; J.1; K.4.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0907.5032v1</id>\\n    <updated>2009-07-29T01:21:36Z</updated>\\n    <published>2009-07-29T01:21:36Z</published>\\n    <title>Restart Strategy Selection using Machine Learning Techniques</title>\\n    <summary>  Restart strategies are an important factor in the performance of\\nconflict-driven Davis Putnam style SAT solvers. Selecting a good restart\\nstrategy for a problem instance can enhance the performance of a solver.\\nInspired by recent success applying machine learning techniques to predict the\\nruntime of SAT solvers, we present a method which uses machine learning to\\nboost solver performance through a smart selection of the restart strategy.\\nBased on easy to compute features, we train both a satisfiability classifier\\nand runtime models. We use these models to choose between restart strategies.\\nWe present experimental results comparing this technique with the most commonly\\nused restart strategies. Our results demonstrate that machine learning is\\neffective in improving solver performance.\\n</summary>\\n    <author>\\n      <name>Shai Haim</name>\\n    </author>\\n    <author>\\n      <name>Toby Walsh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0907.5032v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.5032v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.8\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1210.6293v1</id>\\n    <updated>2012-10-23T17:15:03Z</updated>\\n    <published>2012-10-23T17:15:03Z</published>\\n    <title>MLPACK: A Scalable C++ Machine Learning Library</title>\\n    <summary>  MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning\\nlibrary released in late 2011 offering both a simple, consistent API accessible\\nto novice users and high performance and flexibility to expert users by\\nleveraging modern features of C++. MLPACK provides cutting-edge algorithms\\nwhose benchmarks exhibit far better performance than other leading machine\\nlearning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available\\nat http://www.mlpack.org.\\n</summary>\\n    <author>\\n      <name>Ryan R. Curtin</name>\\n    </author>\\n    <author>\\n      <name>James R. Cline</name>\\n    </author>\\n    <author>\\n      <name>N. P. Slagle</name>\\n    </author>\\n    <author>\\n      <name>William B. March</name>\\n    </author>\\n    <author>\\n      <name>Parikshit Ram</name>\\n    </author>\\n    <author>\\n      <name>Nishant A. Mehta</name>\\n    </author>\\n    <author>\\n      <name>Alexander G. Gray</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to JMLR MLOSS (http://jmlr.csail.mit.edu/mloss/)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1210.6293v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.6293v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1212.4562v1</id>\\n    <updated>2012-12-19T03:06:13Z</updated>\\n    <published>2012-12-19T03:06:13Z</published>\\n    <title>A complexity analysis of statistical learning algorithms</title>\\n    <summary>  We apply information-based complexity analysis to support vector machine\\n(SVM) algorithms, with the goal of a comprehensive continuous algorithmic\\nanalysis of such algorithms. This involves complexity measures in which some\\nhigher order operations (e.g., certain optimizations) are considered primitive\\nfor the purposes of measuring complexity. We consider classes of information\\noperators and algorithms made up of scaled families, and investigate the\\nutility of scaling the complexities to minimize error. We look at the division\\nof statistical learning into information and algorithmic components, at the\\ncomplexities of each, and at applications to support vector machine (SVM) and\\nmore general machine learning algorithms. We give applications to SVM\\nalgorithms graded into linear and higher order components, and give an example\\nin biomedical informatics.\\n</summary>\\n    <author>\\n      <name>Mark A. Kon</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1212.4562v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.4562v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.3474v1</id>\\n    <updated>2013-06-14T18:24:19Z</updated>\\n    <published>2013-06-14T18:24:19Z</published>\\n    <title>Classifying Single-Trial EEG during Motor Imagery with a Small Training\\n  Set</title>\\n    <summary>  Before the operation of a motor imagery based brain-computer interface (BCI)\\nadopting machine learning techniques, a cumbersome training procedure is\\nunavoidable. The development of a practical BCI posed the challenge of\\nclassifying single-trial EEG with a small training set. In this letter, we\\naddressed this problem by employing a series of signal processing and machine\\nlearning approaches to alleviate overfitting and obtained test accuracy similar\\nto training accuracy on the datasets from BCI Competition III and our own\\nexperiments.\\n</summary>\\n    <author>\\n      <name>Yijun Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.3474v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.3474v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.0411v2</id>\\n    <updated>2013-11-04T22:51:55Z</updated>\\n    <published>2013-07-01T15:38:12Z</published>\\n    <title>Quantum algorithms for supervised and unsupervised machine learning</title>\\n    <summary>  Machine-learning tasks frequently involve problems of manipulating and\\nclassifying large numbers of vectors in high-dimensional spaces. Classical\\nalgorithms for solving such problems typically take time polynomial in the\\nnumber of vectors and the dimension of the space. Quantum computers are good at\\nmanipulating high-dimensional vectors in large tensor product spaces. This\\npaper provides supervised and unsupervised quantum machine learning algorithms\\nfor cluster assignment and cluster finding. Quantum machine learning can take\\ntime logarithmic in both the number of vectors and their dimension, an\\nexponential speed-up over classical algorithms.\\n</summary>\\n    <author>\\n      <name>Seth Lloyd</name>\\n    </author>\\n    <author>\\n      <name>Masoud Mohseni</name>\\n    </author>\\n    <author>\\n      <name>Patrick Rebentrost</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, Plain TeX</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1307.0411v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.0411v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.3169v1</id>\\n    <updated>2014-10-13T00:21:59Z</updated>\\n    <published>2014-10-13T00:21:59Z</published>\\n    <title>Multi-Scale Local Shape Analysis and Feature Selection in Machine\\n  Learning Applications</title>\\n    <summary>  We introduce a method called multi-scale local shape analysis, or MLSA, for\\nextracting features that describe the local structure of points within a\\ndataset. The method uses both geometric and topological features at multiple\\nlevels of granularity to capture diverse types of local information for\\nsubsequent machine learning algorithms operating on the dataset. Using\\nsynthetic and real dataset examples, we demonstrate significant performance\\nimprovement of classification algorithms constructed for these datasets with\\ncorrespondingly augmented features.\\n</summary>\\n    <author>\\n      <name>Paul Bendich</name>\\n    </author>\\n    <author>\\n      <name>Ellen Gasparovic</name>\\n    </author>\\n    <author>\\n      <name>John Harer</name>\\n    </author>\\n    <author>\\n      <name>Rauf Izmailov</name>\\n    </author>\\n    <author>\\n      <name>Linda Ness</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 6 figures, 8 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1410.3169v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.3169v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.4062v1</id>\\n    <updated>2014-10-15T13:50:34Z</updated>\\n    <published>2014-10-15T13:50:34Z</published>\\n    <title>Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms\\n  for Machine Learning</title>\\n    <summary>  Frank-Wolfe algorithms for convex minimization have recently gained\\nconsiderable attention from the Optimization and Machine Learning communities,\\nas their properties make them a suitable choice in a variety of applications.\\nHowever, as each iteration requires to optimize a linear model, a clever\\nimplementation is crucial to make such algorithms viable on large-scale\\ndatasets. For this purpose, approximation strategies based on a random sampling\\nhave been proposed by several researchers. In this work, we perform an\\nexperimental study on the effectiveness of these techniques, analyze possible\\nalternatives and provide some guidelines based on our results.\\n</summary>\\n    <author>\\n      <name>Emanuele Frandi</name>\\n    </author>\\n    <author>\\n      <name>Ricardo Nanculef</name>\\n    </author>\\n    <author>\\n      <name>Johan Suykens</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1410.4062v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.4062v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1512.06228v1</id>\\n    <updated>2015-12-19T11:45:13Z</updated>\\n    <published>2015-12-19T11:45:13Z</published>\\n    <title>Using machine learning for medium frequency derivative portfolio trading</title>\\n    <summary>  We use machine learning for designing a medium frequency trading strategy for\\na portfolio of 5 year and 10 year US Treasury note futures. We formulate this\\nas a classification problem where we predict the weekly direction of movement\\nof the portfolio using features extracted from a deep belief network trained on\\ntechnical indicators of the portfolio constituents. The experimentation shows\\nthat the resulting pipeline is effective in making a profitable trade.\\n</summary>\\n    <author>\\n      <name>Abhijit Sharang</name>\\n    </author>\\n    <author>\\n      <name>Chetan Rao</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1512.06228v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.06228v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-fin.TR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.TR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.05560v1</id>\\n    <updated>2016-06-16T15:50:25Z</updated>\\n    <published>2016-06-16T15:50:25Z</published>\\n    <title>Estimation of matrix trace using machine learning</title>\\n    <summary>  We present a new trace estimator of the matrix whose explicit form is not\\ngiven but its matrix multiplication to a vector is available. The form of the\\nestimator is similar to the Hutchison stochastic trace estimator, but instead\\nof the random noise vectors in Hutchison estimator, we use small number of\\nprobing vectors determined by machine learning. Evaluation of the quality of\\nestimates and bias correction are discussed. An unbiased estimator is proposed\\nfor the calculation of the expectation value of a function of traces. In the\\nnumerical experiments with random matrices, it is shown that the precision of\\ntrace estimates with $\\\\mathcal{O}(10)$ probing vectors determined by the\\nmachine learning is similar to that with $\\\\mathcal{O}(10000)$ random noise\\nvectors.\\n</summary>\\n    <author>\\n      <name>Boram Yoon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.05560v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.05560v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.04688v1</id>\\n    <updated>2017-04-15T20:49:09Z</updated>\\n    <published>2017-04-15T20:49:09Z</published>\\n    <title>Machine Learning and the Future of Realism</title>\\n    <summary>  The preceding three decades have seen the emergence, rise, and proliferation\\nof machine learning (ML). From half-recognised beginnings in perceptrons,\\nneural nets, and decision trees, algorithms that extract correlations (that is,\\npatterns) from a set of data points have broken free from their origin in\\ncomputational cognition to embrace all forms of problem solving, from voice\\nrecognition to medical diagnosis to automated scientific research and\\ndriverless cars, and it is now widely opined that the real industrial\\nrevolution lies less in mobile phone and similar than in the maturation and\\nuniversal application of ML. Among the consequences just might be the triumph\\nof anti-realism over realism.\\n</summary>\\n    <author>\\n      <name>Giles Hooker</name>\\n    </author>\\n    <author>\\n      <name>Cliff Hooker</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1704.04688v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.04688v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.3120v1</id>\\n    <updated>2013-05-14T11:49:34Z</updated>\\n    <published>2013-05-14T11:49:34Z</published>\\n    <title>Optimization with First-Order Surrogate Functions</title>\\n    <summary>  In this paper, we study optimization methods consisting of iteratively\\nminimizing surrogates of an objective function. By proposing several\\nalgorithmic variants and simple convergence analyses, we make two main\\ncontributions. First, we provide a unified viewpoint for several first-order\\noptimization techniques such as accelerated proximal gradient, block coordinate\\ndescent, or Frank-Wolfe algorithms. Second, we introduce a new incremental\\nscheme that experimentally matches or outperforms state-of-the-art solvers for\\nlarge-scale optimization problems typically arising in machine learning.\\n</summary>\\n    <author>\\n      <name>Julien Mairal</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Grenoble Rh\\xc3\\xb4ne-Alpes / LJK Laboratoire Jean Kuntzmann</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">to appear in the proceedings of ICML 2013; the arxiv paper contains\\n  the 9 pages main text followed by 26 pages of supplemental material.\\n  International Conference on Machine Learning (ICML 2013) (2013)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1305.3120v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.3120v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.6176v1</id>\\n    <updated>2013-09-23T13:51:28Z</updated>\\n    <published>2013-09-23T13:51:28Z</published>\\n    <title>Feature Learning with Gaussian Restricted Boltzmann Machine for Robust\\n  Speech Recognition</title>\\n    <summary>  In this paper, we first present a new variant of Gaussian restricted\\nBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann\\nmachine (MGRBM), with its definition and learning algorithm. Then we propose\\nusing a learned GRBM or MGRBM to extract better features for robust speech\\nrecognition. Our experiments on Aurora2 show that both GRBM-extracted and\\nMGRBM-extracted feature performs much better than Mel-frequency cepstral\\ncoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)\\nacoustic model, and MGRBM-extracted feature is slightly better.\\n</summary>\\n    <author>\\n      <name>Xin Zheng</name>\\n    </author>\\n    <author>\\n      <name>Zhiyong Wu</name>\\n    </author>\\n    <author>\\n      <name>Helen Meng</name>\\n    </author>\\n    <author>\\n      <name>Weifeng Li</name>\\n    </author>\\n    <author>\\n      <name>Lianhong Cai</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1309.6176v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.6176v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.03491v1</id>\\n    <updated>2015-02-11T23:44:02Z</updated>\\n    <published>2015-02-11T23:44:02Z</published>\\n    <title>How to show a probabilistic model is better</title>\\n    <summary>  We present a simple theoretical framework, and corresponding practical\\nprocedures, for comparing probabilistic models on real data in a traditional\\nmachine learning setting. This framework is based on the theory of proper\\nscoring rules, but requires only basic algebra and probability theory to\\nunderstand and verify. The theoretical concepts presented are well-studied,\\nprimarily in the statistics literature. The goal of this paper is to advocate\\ntheir wider adoption for performance evaluation in empirical machine learning.\\n</summary>\\n    <author>\\n      <name>Mithun Chakraborty</name>\\n    </author>\\n    <author>\\n      <name>Sanmay Das</name>\\n    </author>\\n    <author>\\n      <name>Allen Lavoie</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1502.03491v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.03491v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.01900v2</id>\\n    <updated>2015-10-28T19:02:22Z</updated>\\n    <published>2015-06-05T13:24:17Z</published>\\n    <title>Communication Complexity of Distributed Convex Learning and Optimization</title>\\n    <summary>  We study the fundamental limits to communication-efficient distributed\\nmethods for convex learning and optimization, under different assumptions on\\nthe information available to individual machines, and the types of functions\\nconsidered. We identify cases where existing algorithms are already worst-case\\noptimal, as well as cases where room for further improvement is still possible.\\nAmong other things, our results indicate that without similarity between the\\nlocal objective functions (due to statistical data similarity or otherwise)\\nmany communication rounds may be required, even if the machines have unbounded\\ncomputational power.\\n</summary>\\n    <author>\\n      <name>Yossi Arjevani</name>\\n    </author>\\n    <author>\\n      <name>Ohad Shamir</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1506.01900v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.01900v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.04176v1</id>\\n    <updated>2015-06-12T20:38:57Z</updated>\\n    <published>2015-06-12T20:38:57Z</published>\\n    <title>Using the Mean Absolute Percentage Error for Regression Models</title>\\n    <summary>  We study in this paper the consequences of using the Mean Absolute Percentage\\nError (MAPE) as a measure of quality for regression models. We show that\\nfinding the best model under the MAPE is equivalent to doing weighted Mean\\nAbsolute Error (MAE) regression. We show that universal consistency of\\nEmpirical Risk Minimization remains possible using the MAPE instead of the MAE.\\n</summary>\\n    <author>\\n      <name>Arnaud De Myttenaere</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Boris Golden</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Viadeo</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>B\\xc3\\xa9n\\xc3\\xa9dicte Le Grand</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CRI</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fabrice Rossi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">European Symposium on Artificial Neural Networks, Computational\\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium. 2015,\\n  Proceedings of the 23-th European Symposium on Artificial Neural Networks,\\n  Computational Intelligence and Machine Learning (ESANN 2015)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1506.04176v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.04176v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.04177v1</id>\\n    <updated>2015-06-12T20:39:31Z</updated>\\n    <published>2015-06-12T20:39:31Z</published>\\n    <title>Search Strategies for Binary Feature Selection for a Naive Bayes\\n  Classifier</title>\\n    <summary>  We compare in this paper several feature selection methods for the Naive\\nBayes Classifier (NBC) when the data under study are described by a large\\nnumber of redundant binary indicators. Wrapper approaches guided by the NBC\\nestimation of the classification error probability out-perform filter\\napproaches while retaining a reasonable computational cost.\\n</summary>\\n    <author>\\n      <name>Tsirizo Rabenoro</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>J\\xc3\\xa9r\\xc3\\xb4me Lacaille</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Marie Cottrell</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fabrice Rossi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">European Symposium on Artificial Neural Networks, Computational\\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\\n  pp.291-296, 2015, Proceedings of the 23-th European Symposium on Artificial\\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1506.04177v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.04177v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.00703v1</id>\\n    <updated>2015-08-04T08:42:41Z</updated>\\n    <published>2015-08-04T08:42:41Z</published>\\n    <title>Parameter Database : Data-centric Synchronization for Scalable Machine\\n  Learning</title>\\n    <summary>  We propose a new data-centric synchronization framework for carrying out of\\nmachine learning (ML) tasks in a distributed environment. Our framework\\nexploits the iterative nature of ML algorithms and relaxes the application\\nagnostic bulk synchronization parallel (BSP) paradigm that has previously been\\nused for distributed machine learning. Data-centric synchronization complements\\nfunction-centric synchronization based on using stale updates to increase the\\nthroughput of distributed ML computations. Experiments to validate our\\nframework suggest that we can attain substantial improvement over BSP while\\nguaranteeing sequential correctness of ML tasks.\\n</summary>\\n    <author>\\n      <name>Naman Goel</name>\\n    </author>\\n    <author>\\n      <name>Divyakant Agrawal</name>\\n    </author>\\n    <author>\\n      <name>Sanjay Chawla</name>\\n    </author>\\n    <author>\\n      <name>Ahmed Elmagarmid</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1508.00703v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.00703v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1510.02840v1</id>\\n    <updated>2015-10-09T22:22:01Z</updated>\\n    <published>2015-10-09T22:22:01Z</published>\\n    <title>Concurrent Constraint Machine Improvisation: Models and Implementation</title>\\n    <summary>  Machine improvisation creates music either by explicit coding of rules or by\\napplying machine learning methods. We deal with the latter case. An\\nimprovisation system capable of real-time must execute two process\\nconcurrently: one to apply machine learning methods to musical sequences in\\norder to capture prominent musical features, and one to produce musical\\nsequences stylistically consistent with the learned material. As an example,\\nthe Concurrent Constraint Factor Oracle Model for Music Improvisation (ccfomi),\\nbased upon Non-deterministic Timed Concurrent Constraint (ntcc) calculus, uses\\nthe Factor Oracle to store the learned sequences.\\n</summary>\\n    <author>\\n      <name>Mauricio Toro</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1510.02840v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1510.02840v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"D.1.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.00316v1</id>\\n    <updated>2016-05-01T22:37:24Z</updated>\\n    <published>2016-05-01T22:37:24Z</published>\\n    <title>Directional Statistics in Machine Learning: a Brief Review</title>\\n    <summary>  The modern data analyst must cope with data encoded in various forms,\\nvectors, matrices, strings, graphs, or more. Consequently, statistical and\\nmachine learning models tailored to different data encodings are important. We\\nfocus on data encoded as normalized vectors, so that their \"direction\" is more\\nimportant than their magnitude. Specifically, we consider high-dimensional\\nvectors that lie either on the surface of the unit hypersphere or on the real\\nprojective plane. For such data, we briefly review common mathematical models\\nprevalent in machine learning, while also outlining some technical aspects,\\nsoftware, applications, and open mathematical challenges.\\n</summary>\\n    <author>\\n      <name>Suvrit Sra</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, slightly modified version of submitted book chapter</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1605.00316v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.00316v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.07991v1</id>\\n    <updated>2016-05-25T18:15:43Z</updated>\\n    <published>2016-05-25T18:15:43Z</published>\\n    <title>Efficient Distributed Learning with Sparsity</title>\\n    <summary>  We propose a novel, efficient approach for distributed sparse learning in\\nhigh-dimensions, where observations are randomly partitioned across machines.\\nComputationally, at each round our method only requires the master machine to\\nsolve a shifted ell_1 regularized M-estimation problem, and other workers to\\ncompute the gradient. In respect of communication, the proposed approach\\nprovably matches the estimation error bound of centralized methods within\\nconstant rounds of communications (ignoring logarithmic factors). We conduct\\nextensive experiments on both simulated and real world datasets, and\\ndemonstrate encouraging performances on high-dimensional regression and\\nclassification tasks.\\n</summary>\\n    <author>\\n      <name>Jialei Wang</name>\\n    </author>\\n    <author>\\n      <name>Mladen Kolar</name>\\n    </author>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <author>\\n      <name>Tong Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1605.07991v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.07991v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.00904v1</id>\\n    <updated>2016-09-04T08:45:26Z</updated>\\n    <published>2016-09-04T08:45:26Z</published>\\n    <title>High Dimensional Human Guided Machine Learning</title>\\n    <summary>  Have you ever looked at a machine learning classification model and thought,\\nI could have made that? Well, that is what we test in this project, comparing\\nXGBoost trained on human engineered features to training directly on data. The\\nhuman engineered features do not outperform XGBoost trained di- rectly on the\\ndata, but they are comparable. This project con- tributes a novel method for\\nutilizing human created classifi- cation models on high dimensional datasets.\\n</summary>\\n    <author>\\n      <name>Eric Holloway</name>\\n    </author>\\n    <author>\\n      <name>Robert Marks II</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 1 figure, HCOMP 2016 submission, work in progress</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1609.00904v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.00904v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.07634v1</id>\\n    <updated>2016-11-23T04:21:51Z</updated>\\n    <published>2016-11-23T04:21:51Z</published>\\n    <title>Interpretation of Prediction Models Using the Input Gradient</title>\\n    <summary>  State of the art machine learning algorithms are highly optimized to provide\\nthe optimal prediction possible, naturally resulting in complex models. While\\nthese models often outperform simpler more interpretable models by order of\\nmagnitudes, in terms of understanding the way the model functions, we are often\\nfacing a \"black box\".\\n  In this paper we suggest a simple method to interpret the behavior of any\\npredictive model, both for regression and classification. Given a particular\\nmodel, the information required to interpret it can be obtained by studying the\\npartial derivatives of the model with respect to the input. We exemplify this\\ninsight by interpreting convolutional and multi-layer neural networks in the\\nfield of natural language processing.\\n</summary>\\n    <author>\\n      <name>Yotam Hechtlinger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\\n  Complex Systems</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.07634v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.07634v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.08903v1</id>\\n    <updated>2016-11-27T20:20:06Z</updated>\\n    <published>2016-11-27T20:20:06Z</published>\\n    <title>Should I use TensorFlow</title>\\n    <summary>  Google\\'s Machine Learning framework TensorFlow was open-sourced in November\\n2015 [1] and has since built a growing community around it. TensorFlow is\\nsupposed to be flexible for research purposes while also allowing its models to\\nbe deployed productively. This work is aimed towards people with experience in\\nMachine Learning considering whether they should use TensorFlow in their\\nenvironment. Several aspects of the framework important for such a decision are\\nexamined, such as the heterogenity, extensibility and its computation graph. A\\npure Python implementation of linear classification is compared with an\\nimplementation utilizing TensorFlow. I also contrast TensorFlow to other\\npopular frameworks with respect to modeling capability, deployment and\\nperformance and give a brief description of the current adaption of the\\nframework.\\n</summary>\\n    <author>\\n      <name>Martin Schrimpf</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Seminar Paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.08903v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.08903v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.02440v1</id>\\n    <updated>2017-01-10T05:14:22Z</updated>\\n    <published>2017-01-10T05:14:22Z</published>\\n    <title>Machine Learning of Linear Differential Equations using Gaussian\\n  Processes</title>\\n    <summary>  This work leverages recent advances in probabilistic machine learning to\\ndiscover conservation laws expressed by parametric linear equations. Such\\nequations involve, but are not limited to, ordinary and partial differential,\\nintegro-differential, and fractional order operators. Here, Gaussian process\\npriors are modified according to the particular form of such operators and are\\nemployed to infer parameters of the linear equations from scarce and possibly\\nnoisy observations. Such observations may come from experiments or \"black-box\"\\ncomputer simulations.\\n</summary>\\n    <author>\\n      <name>Maziar Raissi</name>\\n    </author>\\n    <author>\\n      <name>George Em. Karniadakis</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.jcp.2017.07.050</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.jcp.2017.07.050\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1701.02440v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.02440v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.01226v1</id>\\n    <updated>2017-02-04T02:23:55Z</updated>\\n    <published>2017-02-04T02:23:55Z</published>\\n    <title>Towards Better Analysis of Machine Learning Models: A Visual Analytics\\n  Perspective</title>\\n    <summary>  Interactive model analysis, the process of understanding, diagnosing, and\\nrefining a machine learning model with the help of interactive visualization,\\nis very important for users to efficiently solve real-world artificial\\nintelligence and data mining problems. Dramatic advances in big data analytics\\nhas led to a wide variety of interactive model analysis tasks. In this paper,\\nwe present a comprehensive analysis and interpretation of this rapidly\\ndeveloping area. Specifically, we classify the relevant work into three\\ncategories: understanding, diagnosis, and refinement. Each category is\\nexemplified by recent influential work. Possible future research opportunities\\nare also explored and discussed.\\n</summary>\\n    <author>\\n      <name>Shixia Liu</name>\\n    </author>\\n    <author>\\n      <name>Xiting Wang</name>\\n    </author>\\n    <author>\\n      <name>Mengchen Liu</name>\\n    </author>\\n    <author>\\n      <name>Jun Zhu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This article will be published in Visual Infomatics</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1702.01226v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.01226v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.05298v2</id>\\n    <updated>2017-03-16T08:32:19Z</updated>\\n    <published>2017-03-10T18:01:20Z</published>\\n    <title>Neural Networks for Beginners. A fast implementation in Matlab, Torch,\\n  TensorFlow</title>\\n    <summary>  This report provides an introduction to some Machine Learning tools within\\nthe most common development environments. It mainly focuses on practical\\nproblems, skipping any theoretical introduction. It is oriented to both\\nstudents trying to approach Machine Learning and experts looking for new\\nframeworks.\\n</summary>\\n    <author>\\n      <name>Francesco Giannini</name>\\n    </author>\\n    <author>\\n      <name>Vincenzo Laveglia</name>\\n    </author>\\n    <author>\\n      <name>Alessandro Rossi</name>\\n    </author>\\n    <author>\\n      <name>Dario Zanca</name>\\n    </author>\\n    <author>\\n      <name>Andrea Zugarini</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.05298v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.05298v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.06476v2</id>\\n    <updated>2017-06-04T22:40:16Z</updated>\\n    <published>2017-03-19T17:45:29Z</published>\\n    <title>Practical Coreset Constructions for Machine Learning</title>\\n    <summary>  We investigate coresets - succinct, small summaries of large data sets - so\\nthat solutions found on the summary are provably competitive with solution\\nfound on the full data set. We provide an overview over the state-of-the-art in\\ncoreset construction for machine learning. In Section 2, we present both the\\nintuition behind and a theoretically sound framework to construct coresets for\\ngeneral problems and apply it to $k$-means clustering. In Section 3 we\\nsummarize existing coreset construction algorithms for a variety of machine\\nlearning problems such as maximum likelihood estimation of mixture models,\\nBayesian non-parametric models, principal component analysis, regression and\\ngeneral empirical risk minimization.\\n</summary>\\n    <author>\\n      <name>Olivier Bachem</name>\\n    </author>\\n    <author>\\n      <name>Mario Lucic</name>\\n    </author>\\n    <author>\\n      <name>Andreas Krause</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.06476v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.06476v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.10082v2</id>\\n    <updated>2017-07-06T07:19:30Z</updated>\\n    <published>2017-06-30T09:33:50Z</published>\\n    <title>Persistence Diagrams with Linear Machine Learning Models</title>\\n    <summary>  Persistence diagrams have been widely recognized as a compact descriptor for\\ncharacterizing multiscale topological features in data. When many datasets are\\navailable, statistical features embedded in those persistence diagrams can be\\nextracted by applying machine learnings. In particular, the ability for\\nexplicitly analyzing the inverse in the original data space from those\\nstatistical features of persistence diagrams is significantly important for\\npractical applications. In this paper, we propose a unified method for the\\ninverse analysis by combining linear machine learning models with persistence\\nimages. The method is applied to point clouds and cubical sets, showing the\\nability of the statistical inverse analysis and its advantages.\\n</summary>\\n    <author>\\n      <name>Ippei Obayashi</name>\\n    </author>\\n    <author>\\n      <name>Yasuaki Hiraoka</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.10082v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.10082v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.04826v1</id>\\n    <updated>2017-07-16T05:58:40Z</updated>\\n    <published>2017-07-16T05:58:40Z</published>\\n    <title>Machine learning application in the life time of materials</title>\\n    <summary>  Materials design and development typically takes several decades from the\\ninitial discovery to commercialization with the traditional trial and error\\ndevelopment approach. With the accumulation of data from both experimental and\\ncomputational results, data based machine learning becomes an emerging field in\\nmaterials discovery, design and property prediction. This manuscript reviews\\nthe history of materials science as a disciplinary the most common machine\\nlearning method used in materials science, and specifically how they are used\\nin materials discovery, design, synthesis and even failure detection and\\nanalysis after materials are deployed in real application. Finally, the\\nlimitations of machine learning for application in materials science and\\nchallenges in this emerging field is discussed.\\n</summary>\\n    <author>\\n      <name>Xiaojiao Yu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.04826v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.04826v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.03406v1</id>\\n    <updated>2017-11-08T03:07:05Z</updated>\\n    <published>2017-11-08T03:07:05Z</published>\\n    <title>Machine Learning Based Fast Power Integrity Classifier</title>\\n    <summary>  In this paper, we proposed a new machine learning based fast power integrity\\nclassifier that quickly flags the EM/IR hotspots. We discussed the features to\\nextract to describe the power grid, cell power density, routing impact and\\ncontrolled collapse chip connection (C4) bumps, etc. The continuous and\\ndiscontinuous cases are identified and treated using different machine learning\\nmodels. Nearest neighbors, random forest and neural network models are compared\\nto select the best performance candidates. Experiments are run on open source\\nbenchmark, and result is showing promising prediction accuracy.\\n</summary>\\n    <author>\\n      <name>HuaChun Zhang</name>\\n    </author>\\n    <author>\\n      <name>Lynden Kagan</name>\\n    </author>\\n    <author>\\n      <name>Chen Zheng</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 4 figures, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.03406v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.03406v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.OH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.OH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.08042v2</id>\\n    <updated>2017-12-06T23:46:01Z</updated>\\n    <published>2017-11-20T05:25:06Z</published>\\n    <title>\"I know it when I see it\". Visualization and Intuitive Interpretability</title>\\n    <summary>  Most research on the interpretability of machine learning systems focuses on\\nthe development of a more rigorous notion of interpretability. I suggest that a\\nbetter understanding of the deficiencies of the intuitive notion of\\ninterpretability is needed as well. I show that visualization enables but also\\nimpedes intuitive interpretability, as it presupposes two levels of technical\\npre-interpretation: dimensionality reduction and regularization. Furthermore, I\\nargue that the use of positive concepts to emulate the distributed semantic\\nstructure of machine learning models introduces a significant human bias into\\nthe model. As a consequence, I suggest that, if intuitive interpretability is\\nneeded, singular representations of internal model states should be avoided.\\n</summary>\\n    <author>\\n      <name>Fabian Offert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2017 Symposium on Interpretable Machine Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.08042v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.08042v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.00336v1</id>\\n    <updated>2017-12-01T14:31:27Z</updated>\\n    <published>2017-12-01T14:31:27Z</published>\\n    <title>BioMM: Biologically-informed Multi-stage Machine learning for\\n  identification of epigenetic fingerprints</title>\\n    <summary>  The identification of reproducible biological patterns from high-dimensional\\ndata is a bottleneck for understanding the biology of complex illnesses such as\\nschizophrenia. To address this, we developed a biologically informed,\\nmulti-stage machine learning (BioMM) framework. BioMM incorporates biological\\npathway information to stratify and aggregate high-dimensional biological data.\\nWe demonstrate the utility of this method using genome-wide DNA methylation\\ndata and show that it substantially outperforms conventional machine learning\\napproaches. Therefore, the BioMM framework may be a fruitful machine learning\\nstrategy in high-dimensional data and be the basis for future, integrative\\nanalysis approaches.\\n</summary>\\n    <author>\\n      <name>Junfang Chen</name>\\n    </author>\\n    <author>\\n      <name>Emanuel Schwarz</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, NIPS 2017 ML4H</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.00336v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.00336v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.04852v2</id>\\n    <updated>2018-02-22T11:38:01Z</updated>\\n    <published>2018-02-13T20:49:01Z</published>\\n    <title>Persistence Codebooks for Topological Data Analysis</title>\\n    <summary>  Topological data analysis, such as persistent homology has shown beneficial\\nproperties for machine learning in many tasks. Topological representations,\\nsuch as the persistence diagram (PD), however, have a complex structure\\n(multiset of intervals) which makes it difficult to combine with typical\\nmachine learning workflows. We present novel compact fixed-size vectorial\\nrepresentations of PDs based on clustering and bag of words encodings that cope\\nwell with the inherent sparsity of PDs. Our novel representations outperform\\nstate-of-the-art approaches from topological data analysis and are\\ncomputationally more efficient.\\n</summary>\\n    <author>\\n      <name>Bartosz Zielinski</name>\\n    </author>\\n    <author>\\n      <name>Mateusz Juda</name>\\n    </author>\\n    <author>\\n      <name>Matthias Zeppelzauer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.04852v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.04852v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.01486v1</id>\\n    <updated>2018-03-05T03:53:07Z</updated>\\n    <published>2018-03-05T03:53:07Z</published>\\n    <title>Reconsider HHL algorithm and its related quantum machine learning\\n  algorithms</title>\\n    <summary>  HHL quantum algorithm to solve linear systems is one of the most important\\nsubroutines in many quantum machine learning algorithms. In this work, we\\npresent and analyze several other caveats in HHL algorithm, which have been\\nignored in the past. Their influences on the efficiency, accuracy and\\npracticability of HHL algorithm and several related quantum machine learning\\nalgorithms will be discussed. We also found that these caveats affect HHL\\nalgorithm much deeper than the already noticed caveats. In order to obtain more\\npractical quantum machine learning algorithms with less assumptions based on\\nHHL algorithm, we should pay more attention to these caveats.\\n</summary>\\n    <author>\\n      <name>Changpeng Shao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.01486v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.01486v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"46N50\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.04479v1</id>\\n    <updated>2018-03-12T19:32:21Z</updated>\\n    <published>2018-03-12T19:32:21Z</published>\\n    <title>Machine Learning Harnesses Molecular Dynamics to Discover New $\\xce\\xbc$\\n  Opioid Chemotypes</title>\\n    <summary>  Computational chemists typically assay drug candidates by virtually screening\\ncompounds against crystal structures of a protein despite the fact that some\\ntargets, like the $\\\\mu$ Opioid Receptor and other members of the GPCR family,\\ntraverse many non-crystallographic states. We discover new conformational\\nstates of $\\\\mu OR$ with molecular dynamics simulation and then machine learn\\nligand-structure relationships to predict opioid ligand function. These\\nartificial intelligence models identified a novel $\\\\mu$ opioid chemotype.\\n</summary>\\n    <author>\\n      <name>Evan N. Feinberg</name>\\n    </author>\\n    <author>\\n      <name>Amir Barati Farimani</name>\\n    </author>\\n    <author>\\n      <name>Rajendra Uprety</name>\\n    </author>\\n    <author>\\n      <name>Amanda Hunkele</name>\\n    </author>\\n    <author>\\n      <name>Gavril W. Pasternak</name>\\n    </author>\\n    <author>\\n      <name>Susruta Majumdar</name>\\n    </author>\\n    <author>\\n      <name>Vijay S. Pande</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">28 pages, machine learning, computational biology, GPCRs, molecular\\n  dynamics, molecular docking, molecular simulation</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.04479v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.04479v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07179v2</id>\\n    <updated>2018-05-23T19:36:44Z</updated>\\n    <published>2018-05-18T12:47:02Z</published>\\n    <title>Markov Chain Importance Sampling - a highly efficient estimator for MCMC</title>\\n    <summary>  Markov chain algorithms are ubiquitous in machine learning and statistics and\\nmany other disciplines. In this work we present a novel estimator applicable to\\nseveral classes of Markov chains, dubbed Markov chain importance sampling\\n(MCIS). For a broad class of Metropolis-Hastings algorithms, MCIS efficiently\\nmakes use of rejected proposals. For discretized Langevin diffusions, it\\nprovides a novel way of correcting the discretization error. Our estimator\\nsatisfies a central limit theorem and improves on error per CPU cycle, often to\\na large extent. As a by-product it enables estimating the normalizing constant,\\nan important quantity in Bayesian machine learning and statistics.\\n</summary>\\n    <author>\\n      <name>Ingmar Schuster</name>\\n    </author>\\n    <author>\\n      <name>Ilja Klebanov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.07179v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07179v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07431v3</id>\\n    <updated>2018-09-10T01:45:24Z</updated>\\n    <published>2018-05-18T20:32:03Z</published>\\n    <title>Can machine learning identify interesting mathematics? An exploration\\n  using empirically observed laws</title>\\n    <summary>  We explore the possibility of using machine learning to identify interesting\\nmathematical structures by using certain quantities that serve as fingerprints.\\nIn particular, we extract features from integer sequences using two empirical\\nlaws: Benford\\'s law and Taylor\\'s law and experiment with various classifiers to\\nidentify whether a sequence is, for example, nice, important, multiplicative,\\neasy to compute or related to primes or palindromes.\\n</summary>\\n    <author>\\n      <name>Chai Wah Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, minor edits and fixed typos</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.07431v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07431v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.04016v1</id>\\n    <updated>2018-06-11T14:22:48Z</updated>\\n    <published>2018-06-11T14:22:48Z</published>\\n    <title>Baselines and a datasheet for the Cerema AWP dataset</title>\\n    <summary>  This paper presents the recently published Cerema AWP (Adverse Weather\\nPedestrian) dataset for various machine learning tasks and its exports in\\nmachine learning friendly format. We explain why this dataset can be\\ninteresting (mainly because it is a greatly controlled and fully annotated\\nimage dataset) and present baseline results for various tasks. Moreover, we\\ndecided to follow the very recent suggestions of datasheets for dataset, trying\\nto standardize all the available information of the dataset, with a\\ntransparency objective.\\n</summary>\\n    <author>\\n      <name>Isma\\xc3\\xafla Seck</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LIMOS, LITIS</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Khouloud Dahmane</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Cerema</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Pierre Duthon</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Cerema</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Ga\\xc3\\xablle Loosli</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LIMOS</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Conf\\\\\\'erence d\\'Apprentissage CAp, Jun 2018, Rouen, France. 2018,\\n  Conf\\\\\\'erence d\\'Apprentissage Francophone 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.04016v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.04016v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.05138v1</id>\\n    <updated>2018-06-13T16:35:32Z</updated>\\n    <published>2018-06-13T16:35:32Z</published>\\n    <title>Generative Neural Machine Translation</title>\\n    <summary>  We introduce Generative Neural Machine Translation (GNMT), a latent variable\\narchitecture which is designed to model the semantics of the source and target\\nsentences. We modify an encoder-decoder translation model by adding a latent\\nvariable as a language agnostic representation which is encouraged to learn the\\nmeaning of the sentence. GNMT achieves competitive BLEU scores on pure\\ntranslation tasks, and is superior when there are missing words in the source\\nsentence. We augment the model to facilitate multilingual translation and\\nsemi-supervised learning without adding parameters. This framework\\nsignificantly reduces overfitting when there is limited paired data available,\\nand is effective for translating between pairs of languages not seen during\\ntraining.\\n</summary>\\n    <author>\\n      <name>Harshil Shah</name>\\n    </author>\\n    <author>\\n      <name>David Barber</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.05138v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.05138v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.09206v1</id>\\n    <updated>2018-06-24T20:28:49Z</updated>\\n    <published>2018-06-24T20:28:49Z</published>\\n    <title>N-Gram Graph, A Novel Molecule Representation</title>\\n    <summary>  Virtual high-throughput screening provides a strategy for prioritizing\\ncompounds for physical screens. Machine learning methods offer an ancillary\\nbenefit to make molecule predictions, yet the choice of representation has been\\nchallenging when selecting algorithms. We emphasize the effects of different\\nlevels of molecule representation. Then, we introduce N-gram graph, a novel\\nrepresentation for a molecular graph. We demonstrate that N-gram graph is able\\nto attain most accurate prediction with several non-deep machine learning\\nmethods on multiple tasks.\\n</summary>\\n    <author>\\n      <name>Shengchao Liu</name>\\n    </author>\\n    <author>\\n      <name>Thevaa Chandereng</name>\\n    </author>\\n    <author>\\n      <name>Yingyu Liang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.09206v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.09206v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.07924v1</id>\\n    <updated>2018-07-20T16:32:22Z</updated>\\n    <published>2018-07-20T16:32:22Z</published>\\n    <title>Optimal Bounds on the VC-dimension</title>\\n    <summary>  The VC-dimension of a set system is a way to capture its complexity and has\\nbeen a key parameter studied extensively in machine learning and geometry\\ncommunities. In this paper, we resolve two longstanding open problems on\\nbounding the VC-dimension of two fundamental set systems: $k$-fold\\nunions/intersections of half-spaces, and the simplices set system. Among other\\nimplications, it settles an open question in machine learning that was first\\nstudied in the 1989 foundational paper of Blumer, Ehrenfeucht, Haussler and\\nWarmuth as well as by Eisenstat and Angluin and Johnson.\\n</summary>\\n    <author>\\n      <name>Monika Csikos</name>\\n    </author>\\n    <author>\\n      <name>Andrey Kupavskii</name>\\n    </author>\\n    <author>\\n      <name>Nabil H. Mustafa</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.07924v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.07924v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00803v1</id>\\n    <updated>2018-08-02T13:31:23Z</updated>\\n    <published>2018-08-02T13:31:23Z</published>\\n    <title>Mobile big data analysis with machine learning</title>\\n    <summary>  This paper investigates to identify the requirement and the development of\\nmachine learning-based mobile big data analysis through discussing the insights\\nof challenges in the mobile big data (MBD). Furthermore, it reviews the\\nstate-of-the-art applications of data analysis in the area of MBD. Firstly, we\\nintroduce the development of MBD. Secondly, the frequently adopted methods of\\ndata analysis are reviewed. Three typical applications of MBD analysis, namely\\nwireless channel modeling, human online and offline behavior analysis, and\\nspeech recognition in the internet of vehicles, are introduced respectively.\\nFinally, we summarize the main challenges and future development directions of\\nmobile big data analysis.\\n</summary>\\n    <author>\\n      <name>Jiyang Xie</name>\\n    </author>\\n    <author>\\n      <name>Zeyu Song</name>\\n    </author>\\n    <author>\\n      <name>Yupeng Li</name>\\n    </author>\\n    <author>\\n      <name>Zhanyu Ma</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Version 0.1</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.00803v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00803v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.06492v1</id>\\n    <updated>2018-08-17T02:15:39Z</updated>\\n    <published>2018-08-17T02:15:39Z</published>\\n    <title>Benchmarking Automatic Machine Learning Frameworks</title>\\n    <summary>  AutoML serves as the bridge between varying levels of expertise when\\ndesigning machine learning systems and expedites the data science process. A\\nwide range of techniques is taken to address this, however there does not exist\\nan objective comparison of these techniques. We present a benchmark of current\\nopen source AutoML solutions using open source datasets. We test auto-sklearn,\\nTPOT, auto_ml, and H2O\\'s AutoML solution against a compiled set of regression\\nand classification datasets sourced from OpenML and find that auto-sklearn\\nperforms the best across classification datasets and TPOT performs the best\\nacross regression datasets.\\n</summary>\\n    <author>\\n      <name>Adithya Balaji</name>\\n    </author>\\n    <author>\\n      <name>Alexander Allen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 8 figures, 5 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.06492v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.06492v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.07069v1</id>\\n    <updated>2018-08-21T18:03:36Z</updated>\\n    <published>2018-08-21T18:03:36Z</published>\\n    <title>Machine learning non-local correlations</title>\\n    <summary>  The ability to witness non-local correlations lies at the core of\\nfoundational aspects of quantum mechanics and its application in the processing\\nof information. Commonly, this is achieved via the violation of Bell\\ninequalities. Unfortunately, however, their systematic derivation quickly\\nbecomes unfeasible as the scenario of interest grows in complexity. To cope\\nwith that, we propose here a machine learning approach for the detection and\\nquantification of non-locality. It consists of an ensemble of multilayer\\nperceptrons blended with genetic algorithms achieving a high performance in a\\nnumber of relevant Bell scenarios. Our results offer a novel method and a\\nproof-of-principle for the relevance of machine learning for understanding\\nnon-locality.\\n</summary>\\n    <author>\\n      <name>Askery Canabarro</name>\\n    </author>\\n    <author>\\n      <name>Samura\\xc3\\xad Brito</name>\\n    </author>\\n    <author>\\n      <name>Rafael Chaves</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.07069v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.07069v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.09420v1</id>\\n    <updated>2018-09-25T11:56:52Z</updated>\\n    <published>2018-09-25T11:56:52Z</published>\\n    <title>Co-Creative Level Design via Machine Learning</title>\\n    <summary>  Procedural Level Generation via Machine Learning (PLGML), the study of\\ngenerating game levels with machine learning, has received a large amount of\\nrecent academic attention. For certain measures these approaches have shown\\nsuccess at replicating the quality of existing game levels. However, it is\\nunclear the extent to which they might benefit human designers. In this paper\\nwe present a framework for co-creative level design with a PLGML agent. In\\nsupport of this framework we present results from a user study and results from\\na comparative study of PLGML approaches.\\n</summary>\\n    <author>\\n      <name>Matthew Guzdial</name>\\n    </author>\\n    <author>\\n      <name>Nicholas Liao</name>\\n    </author>\\n    <author>\\n      <name>Mark Riedl</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 2 figures, Fifth Experimental AI in Games Workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.09420v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.09420v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.10231v1</id>\\n    <updated>2018-09-26T21:00:58Z</updated>\\n    <published>2018-09-26T21:00:58Z</published>\\n    <title>A Kernel for Multi-Parameter Persistent Homology</title>\\n    <summary>  Topological data analysis and its main method, persistent homology, provide a\\ntoolkit for computing topological information of high-dimensional and noisy\\ndata sets. Kernels for one-parameter persistent homology have been established\\nto connect persistent homology with machine learning techniques. We contribute\\na kernel construction for multi-parameter persistence by integrating a\\none-parameter kernel weighted along straight lines. We prove that our kernel is\\nstable and efficiently computable, which establishes a theoretical connection\\nbetween topological data analysis and machine learning for multivariate data\\nanalysis.\\n</summary>\\n    <author>\\n      <name>Ren\\xc3\\xa9 Corbet</name>\\n    </author>\\n    <author>\\n      <name>Ulderico Fugacci</name>\\n    </author>\\n    <author>\\n      <name>Michael Kerber</name>\\n    </author>\\n    <author>\\n      <name>Claudia Landi</name>\\n    </author>\\n    <author>\\n      <name>Bei Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.10231v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.10231v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.02909v1</id>\\n    <updated>2018-10-05T23:29:55Z</updated>\\n    <published>2018-10-05T23:29:55Z</published>\\n    <title>On the Art and Science of Machine Learning Explanations</title>\\n    <summary>  This text discusses several explanatory methods that go beyond the error\\nmeasurements and plots traditionally used to assess machine learning models.\\nSome of the methods are tools of the trade while others are rigorously derived\\nand backed by long-standing theory. The methods, decision tree surrogate\\nmodels, individual conditional expectation (ICE) plots, local interpretable\\nmodel-agnostic explanations (LIME), partial dependence plots, and Shapley\\nexplanations, vary in terms of scope, fidelity, and suitable application\\ndomain. Along with descriptions of these methods, this text presents real-world\\nusage recommendations supported by a use case and in-depth software examples.\\n</summary>\\n    <author>\\n      <name>Patrick Hall</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This manuscript is a preprint of a similar text that will appear in\\n  the 2018 Joint Statistical Meetings (JSM) proceedings. Updates and\\n  corrections are available on GitHub:\\n  https://github.com/jphall663/jsm_2018_paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.02909v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.02909v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.08130v2</id>\\n    <updated>2018-10-23T08:13:06Z</updated>\\n    <published>2018-10-18T16:10:12Z</published>\\n    <title>Private Machine Learning in TensorFlow using Secure Computation</title>\\n    <summary>  We present a framework for experimenting with secure multi-party computation\\ndirectly in TensorFlow. By doing so we benefit from several properties valuable\\nto both researchers and practitioners, including tight integration with\\nordinary machine learning processes, existing optimizations for distributed\\ncomputation in TensorFlow, high-level abstractions for expressing complex\\nalgorithms and protocols, and an expanded set of familiar tooling. We give an\\nopen source implementation of a state-of-the-art protocol and report on\\nconcrete benchmarks using typical models from private machine learning.\\n</summary>\\n    <author>\\n      <name>Morten Dahl</name>\\n    </author>\\n    <author>\\n      <name>Jason Mancuso</name>\\n    </author>\\n    <author>\\n      <name>Yann Dupis</name>\\n    </author>\\n    <author>\\n      <name>Ben Decoste</name>\\n    </author>\\n    <author>\\n      <name>Morgan Giraud</name>\\n    </author>\\n    <author>\\n      <name>Ian Livingstone</name>\\n    </author>\\n    <author>\\n      <name>Justin Patriquin</name>\\n    </author>\\n    <author>\\n      <name>Gavin Uhma</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.08130v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.08130v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.08810v1</id>\\n    <updated>2018-10-20T14:24:05Z</updated>\\n    <published>2018-10-20T14:24:05Z</published>\\n    <title>The Frontiers of Fairness in Machine Learning</title>\\n    <summary>  The last few years have seen an explosion of academic and popular interest in\\nalgorithmic fairness. Despite this interest and the volume and velocity of work\\nthat has been produced recently, the fundamental science of fairness in machine\\nlearning is still in a nascent state. In March 2018, we convened a group of\\nexperts as part of a CCC visioning workshop to assess the state of the field,\\nand distill the most promising research directions going forward. This report\\nsummarizes the findings of that workshop. Along the way, it surveys recent\\ntheoretical work in the field and points towards promising directions for\\nresearch.\\n</summary>\\n    <author>\\n      <name>Alexandra Chouldechova</name>\\n    </author>\\n    <author>\\n      <name>Aaron Roth</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.08810v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.08810v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.06210v1</id>\\n    <updated>2018-11-15T07:26:19Z</updated>\\n    <published>2018-11-15T07:26:19Z</published>\\n    <title>Short-Term Wind-Speed Forecasting Using Kernel Spectral Hidden Markov\\n  Models</title>\\n    <summary>  In machine learning, a nonparametric forecasting algorithm for time series\\ndata has been proposed, called the kernel spectral hidden Markov model (KSHMM).\\nIn this paper, we propose a technique for short-term wind-speed prediction\\nbased on KSHMM. We numerically compared the performance of our KSHMM-based\\nforecasting technique to other techniques with machine learning, using\\nwind-speed data offered by the National Renewable Energy Laboratory. Our\\nresults demonstrate that, compared to these methods, the proposed technique\\noffers comparable or better performance.\\n</summary>\\n    <author>\\n      <name>Shunsuke Tsuzuki</name>\\n    </author>\\n    <author>\\n      <name>Yu Nishiyama</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.06210v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.06210v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.07192v1</id>\\n    <updated>2018-11-17T17:21:14Z</updated>\\n    <published>2018-11-17T17:21:14Z</published>\\n    <title>The Theory and Algorithm of Ergodic Inference</title>\\n    <summary>  Approximate inference algorithm is one of the fundamental research fields in\\nmachine learning. The two dominant theoretical inference frameworks in machine\\nlearning are variational inference (VI) and Markov chain Monte Carlo (MCMC).\\nHowever, because of the fundamental limitation in the theory, it is very\\nchallenging to improve existing VI and MCMC methods on both the computational\\nscalability and statistical efficiency. To overcome this obstacle, we propose a\\nnew theoretical inference framework called ergodic Inference based on the\\nfundamental property of ergodic transformations. The key contribution of this\\nwork is to establish the theoretical foundation of ergodic inference for the\\ndevelopment of practical algorithms in future work.\\n</summary>\\n    <author>\\n      <name>Yichuan Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ergodic inference, statistical inference, probability theory</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.07192v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.07192v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.10154v2</id>\\n    <updated>2018-12-05T04:09:42Z</updated>\\n    <published>2018-11-26T03:00:25Z</published>\\n    <title>Please Stop Explaining Black Box Models for High Stakes Decisions</title>\\n    <summary>  Black box machine learning models are currently being used for high stakes\\ndecision-making throughout society, causing problems throughout healthcare,\\ncriminal justice, and in other domains. People have hoped that creating methods\\nfor explaining these black box models will alleviate some of these problems,\\nbut trying to explain black box models, rather than creating models that are\\ninterpretable in the first place, is likely to perpetuate bad practices and can\\npotentially cause catastrophic harm to society. There is a way forward - it is\\nto design models that are inherently interpretable.\\n</summary>\\n    <author>\\n      <name>Cynthia Rudin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2018 Workshop on Critiquing and Correcting Trends in Machine\\n  Learning, Longer version. Expands also on NSF Statistics at a Crossroads\\n  Webinar</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.10154v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.10154v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.12323v1</id>\\n    <updated>2018-11-29T17:19:04Z</updated>\\n    <published>2018-11-29T17:19:04Z</published>\\n    <title>A Deep Latent-Variable Model Application to Select Treatment Intensity\\n  in Survival Analysis</title>\\n    <summary>  In the following short article we adapt a new and popular machine learning\\nmodel for inference on medical data sets. Our method is based on the\\nVariational AutoEncoder (VAE) framework that we adapt to survival analysis on\\nsmall data sets with missing values. In our model, the true health status\\nappears as a set of latent variables that affects the observed covariates and\\nthe survival chances. We show that this flexible model allows insightful\\ndecision-making using a predicted distribution and outperforms a classic\\nsurvival analysis model.\\n</summary>\\n    <author>\\n      <name>C\\xc3\\xa9dric Beaulac</name>\\n    </author>\\n    <author>\\n      <name>Jeffrey S. Rosenthal</name>\\n    </author>\\n    <author>\\n      <name>David Hodgson</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  arXiv:1811.07216</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.12323v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.12323v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.01495v1</id>\\n    <updated>2018-12-04T15:51:28Z</updated>\\n    <published>2018-12-04T15:51:28Z</published>\\n    <title>Expanding search in the space of empirical ML</title>\\n    <summary>  As researchers and practitioners of applied machine learning, we are given a\\nset of requirements on the problem to be solved, the plausibly obtainable data,\\nand the computational resources available. We aim to find (within those bounds)\\nreliably useful combinations of problem, data, and algorithm. An emphasis on\\nalgorithmic or technical novelty in ML conference publications leads to\\nexploration of one dimension of this space. Data collection and ML deployment\\nat scale in industry settings offers an environment for exploring the others.\\nOur conferences and reviewing criteria can better support empirical ML by\\nsoliciting and incentivizing experimentation and synthesis independent of\\nalgorithmic innovation.\\n</summary>\\n    <author>\\n      <name>Bronwyn Woods</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the Critiquing and Correcting Trends in Machine Learning\\n  workshop at NeurIPS 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.01495v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.01495v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.09245v2</id>\\n    <updated>2019-03-05T09:50:01Z</updated>\\n    <published>2018-12-21T16:38:39Z</published>\\n    <title>Persistence Bag-of-Words for Topological Data Analysis</title>\\n    <summary>  Persistent homology (PH) is a rigorous mathematical theory that provides a\\nrobust descriptor of data in the form of persistence diagrams (PDs). PDs\\nexhibit, however, complex structure and are difficult to integrate in today\\'s\\nmachine learning workflows. This paper introduces persistence bag-of-words: a\\nnovel and stable vectorized representation of PDs that enables the seamless\\nintegration with machine learning. Comprehensive experiments show that the new\\nrepresentation achieves state-of-the-art performance and beyond in much less\\ntime than alternative approaches.\\n</summary>\\n    <author>\\n      <name>Bartosz Zieli\\xc5\\x84ski</name>\\n    </author>\\n    <author>\\n      <name>Micha\\xc5\\x82 Lipi\\xc5\\x84ski</name>\\n    </author>\\n    <author>\\n      <name>Mateusz Juda</name>\\n    </author>\\n    <author>\\n      <name>Matthias Zeppelzauer</name>\\n    </author>\\n    <author>\\n      <name>Pawe\\xc5\\x82 D\\xc5\\x82otko</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">major changes</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.09245v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.09245v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.AT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.10793v1</id>\\n    <updated>2018-12-27T18:43:21Z</updated>\\n    <published>2018-12-27T18:43:21Z</published>\\n    <title>Generic adaptation strategies for automated machine learning</title>\\n    <summary>  Automation of machine learning model development is increasingly becoming an\\nestablished research area. While automated model selection and automated data\\npre-processing have been studied in depth, there is, however, a gap concerning\\nautomated model adaptation strategies when multiple strategies are available.\\nManually developing an adaptation strategy, including estimation of relevant\\nparameters can be time consuming and costly. In this paper we address this\\nissue by proposing generic adaptation strategies based on approaches from\\nearlier works. Experimental results after using the proposed strategies with\\nthree adaptive algorithms on 36 datasets confirm their viability. These\\nstrategies often achieve better or comparable performance with custom\\nadaptation strategies and naive methods such as repeatedly using only one\\nadaptive mechanism.\\n</summary>\\n    <author>\\n      <name>Rashid Bakirov</name>\\n    </author>\\n    <author>\\n      <name>Bogdan Gabrys</name>\\n    </author>\\n    <author>\\n      <name>Damien Fay</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.10793v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.10793v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.05051v1</id>\\n    <updated>2019-01-13T12:17:02Z</updated>\\n    <published>2019-01-13T12:17:02Z</published>\\n    <title>Machine-learning a virus assembly fitness landscape</title>\\n    <summary>  Realistic evolutionary fitness landscapes are notoriously difficult to\\nconstruct. A recent cutting-edge model of virus assembly consists of a\\ndodecahedral capsid with $12$ corresponding packaging signals in three affinity\\nbands. This whole genome/phenotype space consisting of $3^{12}$ genomes has\\nbeen explored via computationally expensive stochastic assembly models, giving\\na fitness landscape in terms of the assembly efficiency. Using latest\\nmachine-learning techniques by establishing a neural network, we show that the\\nintensive computation can be short-circuited in a matter of minutes to\\nastounding accuracy.\\n</summary>\\n    <author>\\n      <name>Pierre-Philippe Dechant</name>\\n    </author>\\n    <author>\\n      <name>Yang-Hui He</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.05051v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.05051v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Txx, 97R40, 92B20, 92Bxx, 82Dxx, 82D80\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.00641v1</id>\\n    <updated>2019-02-02T05:09:39Z</updated>\\n    <published>2019-02-02T05:09:39Z</published>\\n    <title>CodedPrivateML: A Fast and Privacy-Preserving Framework for Distributed\\n  Machine Learning</title>\\n    <summary>  How to train a machine learning model while keeping the data private and\\nsecure? We present CodedPrivateML, a fast and scalable approach to this\\ncritical problem. CodedPrivateML keeps both the data and the model\\ninformation-theoretically private, while allowing efficient parallelization of\\ntraining across distributed workers. We characterize CodedPrivateML\\'s privacy\\nthreshold and prove its convergence for logistic (and linear) regression.\\nFurthermore, via experiments over Amazon EC2, we demonstrate that\\nCodedPrivateML can provide an order of magnitude speedup (up to $\\\\sim\\n34\\\\times$) over the state-of-the-art cryptographic approaches.\\n</summary>\\n    <author>\\n      <name>Jinhyun So</name>\\n    </author>\\n    <author>\\n      <name>Basak Guler</name>\\n    </author>\\n    <author>\\n      <name>A. Salman Avestimehr</name>\\n    </author>\\n    <author>\\n      <name>Payman Mohassel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.00641v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.00641v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.03763v1</id>\\n    <updated>2019-03-09T08:32:32Z</updated>\\n    <published>2019-03-09T08:32:32Z</published>\\n    <title>A tractable ellipsoidal approximation for voltage regulation problems</title>\\n    <summary>  We present a machine learning approach to the solution of chance constrained\\noptimizations in the context of voltage regulation problems in power system\\noperation. The novelty of our approach resides in approximating the feasible\\nregion of uncertainty with an ellipsoid. We formulate this problem using a\\nlearning model similar to Support Vector Machines (SVM) and propose a sampling\\nalgorithm that efficiently trains the model. We demonstrate our approach on a\\nvoltage regulation problem using standard IEEE distribution test feeders.\\n</summary>\\n    <author>\\n      <name>Pan Li</name>\\n    </author>\\n    <author>\\n      <name>Baihong Jin</name>\\n    </author>\\n    <author>\\n      <name>Ruoxuan Xiong</name>\\n    </author>\\n    <author>\\n      <name>Dai Wang</name>\\n    </author>\\n    <author>\\n      <name>Alberto Sangiovanni-Vincentelli</name>\\n    </author>\\n    <author>\\n      <name>Baosen Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">accepted by ACC2019 http://acc2019.a2c2.org/</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.03763v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.03763v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.06047v1</id>\\n    <updated>2019-03-14T14:32:55Z</updated>\\n    <published>2019-03-14T14:32:55Z</published>\\n    <title>Inferring Personalized Bayesian Embeddings for Learning from\\n  Heterogeneous Demonstration</title>\\n    <summary>  For assistive robots and virtual agents to achieve ubiquity, machines will\\nneed to anticipate the needs of their human counterparts. The field of Learning\\nfrom Demonstration (LfD) has sought to enable machines to infer predictive\\nmodels of human behavior for autonomous robot control. However, humans exhibit\\nheterogeneity in decision-making, which traditional LfD approaches fail to\\ncapture. To overcome this challenge, we propose a Bayesian LfD framework to\\ninfer an integrated representation of all human task demonstrators by inferring\\nhuman-specific embeddings, thereby distilling their unique characteristics. We\\nvalidate our approach is able to outperform state-of-the-art techniques on both\\nsynthetic and real-world data sets.\\n</summary>\\n    <author>\\n      <name>Rohan Paleja</name>\\n    </author>\\n    <author>\\n      <name>Matthew Gombolay</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 Pages, 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.06047v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.06047v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.09732v1</id>\\n    <updated>2019-03-22T23:39:53Z</updated>\\n    <published>2019-03-22T23:39:53Z</published>\\n    <title>Time Series Imputation</title>\\n    <summary>  Multivariate time series is a very active topic in the research community and\\nmany machine learning tasks are being used in order to extract information from\\nthis type of data. However, in real-world problems data has missing values,\\nwhich may difficult the application of machine learning techniques to extract\\ninformation. In this paper we focus on the task of imputation of time series.\\nMany imputation methods for time series are based on regression methods.\\nUnfortunately, these methods perform poorly when the variables are categorical.\\nTo address this case, we propose a new imputation method based on Expectation\\nMaximization over dynamic Bayesian networks. The approach is assessed with\\nsynthetic and real data, and it outperforms several state-of-the art methods.\\n</summary>\\n    <author>\\n      <name>Samuel Arcadinho</name>\\n    </author>\\n    <author>\\n      <name>Paulo Mateus</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Master paper, draft to be submitted</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.09732v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.09732v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.04329v1</id>\\n    <updated>2019-04-08T19:54:27Z</updated>\\n    <published>2019-04-08T19:54:27Z</published>\\n    <title>Automated Monitoring Cropland Using Remote Sensing Data: Challenges and\\n  Opportunities for Machine Learning</title>\\n    <summary>  This paper provides an overview of how recent advances in machine learning\\nand the availability of data from earth observing satellites can dramatically\\nimprove our ability to automatically map croplands over long period and over\\nlarge regions. It discusses three applications in the domain of crop monitoring\\nwhere ML approaches are beginning to show great promise. For each application,\\nit highlights machine learning challenges, proposed approaches, and recent\\nresults. The paper concludes with discussion of major challenges that need to\\nbe addressed before ML approaches will reach their full potential for this\\nproblem of great societal relevance.\\n</summary>\\n    <author>\\n      <name>Xiaowei Jia</name>\\n    </author>\\n    <author>\\n      <name>Ankush Khandelwal</name>\\n    </author>\\n    <author>\\n      <name>Vipin Kumar</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.04329v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.04329v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1103.4204v1</id>\\n    <updated>2011-03-22T04:54:35Z</updated>\\n    <published>2011-03-22T04:54:35Z</published>\\n    <title>Parallel Online Learning</title>\\n    <summary>  In this work we study parallelization of online learning, a core primitive in\\nmachine learning. In a parallel environment all known approaches for parallel\\nonline learning lead to delayed updates, where the model is updated using\\nout-of-date information. In the worst case, or when examples are temporally\\ncorrelated, delay can have a very adverse effect on the learning algorithm.\\nHere, we analyze and present preliminary empirical results on a set of learning\\narchitectures based on a feature sharding approach that present various\\ntradeoffs between delay, degree of parallelism, representation power and\\nempirical performance.\\n</summary>\\n    <author>\\n      <name>Daniel Hsu</name>\\n    </author>\\n    <author>\\n      <name>Nikos Karampatziakis</name>\\n    </author>\\n    <author>\\n      <name>John Langford</name>\\n    </author>\\n    <author>\\n      <name>Alex Smola</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1103.4204v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1103.4204v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1201.2416v1</id>\\n    <updated>2012-01-11T21:03:55Z</updated>\\n    <published>2012-01-11T21:03:55Z</published>\\n    <title>Stochastic Low-Rank Kernel Learning for Regression</title>\\n    <summary>  We present a novel approach to learn a kernel-based regression function. It\\nis based on the useof conical combinations of data-based parameterized kernels\\nand on a new stochastic convex optimization procedure of which we establish\\nconvergence guarantees. The overall learning procedure has the nice properties\\nthat a) the learned conical combination is automatically designed to perform\\nthe regression task at hand and b) the updates implicated by the optimization\\nprocedure are quite inexpensive. In order to shed light on the appositeness of\\nour learning strategy, we present empirical results from experiments conducted\\non various benchmark datasets.\\n</summary>\\n    <author>\\n      <name>Pierre Machart</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LIF</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Thomas Peel</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LIF, LATP</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Liva Ralaivola</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LIF</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Sandrine Anthoine</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LATP</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Herv\\xc3\\xa9 Glotin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LSIS</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Machine Learning (ICML\\'11), Bellevue\\n  (Washington) : United States (2011)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1201.2416v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1201.2416v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1310.8243v1</id>\\n    <updated>2013-10-30T17:49:11Z</updated>\\n    <published>2013-10-30T17:49:11Z</published>\\n    <title>Para-active learning</title>\\n    <summary>  Training examples are not all equally informative. Active learning strategies\\nleverage this observation in order to massively reduce the number of examples\\nthat need to be labeled. We leverage the same observation to build a generic\\nstrategy for parallelizing learning algorithms. This strategy is effective\\nbecause the search for informative examples is highly parallelizable and\\nbecause we show that its performance does not deteriorate when the sifting\\nprocess relies on a slightly outdated model. Parallel active learning is\\nparticularly attractive to train nonlinear models with non-linear\\nrepresentations because there are few practical parallel learning algorithms\\nfor such models. We report preliminary experiments using both kernel SVMs and\\nSGD-trained neural networks.\\n</summary>\\n    <author>\\n      <name>Alekh Agarwal</name>\\n    </author>\\n    <author>\\n      <name>Leon Bottou</name>\\n    </author>\\n    <author>\\n      <name>Miroslav Dudik</name>\\n    </author>\\n    <author>\\n      <name>John Langford</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1310.8243v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.8243v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1412.6493v1</id>\\n    <updated>2014-12-19T19:27:21Z</updated>\\n    <published>2014-12-19T19:27:21Z</published>\\n    <title>A la Carte - Learning Fast Kernels</title>\\n    <summary>  Kernel methods have great promise for learning rich statistical\\nrepresentations of large modern datasets. However, compared to neural networks,\\nkernel methods have been perceived as lacking in scalability and flexibility.\\nWe introduce a family of fast, flexible, lightly parametrized and general\\npurpose kernel learning methods, derived from Fastfood basis function\\nexpansions. We provide mechanisms to learn the properties of groups of spectral\\nfrequencies in these expansions, which require only O(mlogd) time and O(m)\\nmemory, for m basis functions and d input dimensions. We show that the proposed\\nmethods can learn a wide class of kernels, outperforming the alternatives in\\naccuracy, speed, and memory consumption.\\n</summary>\\n    <author>\\n      <name>Zichao Yang</name>\\n    </author>\\n    <author>\\n      <name>Alexander J. Smola</name>\\n    </author>\\n    <author>\\n      <name>Le Song</name>\\n    </author>\\n    <author>\\n      <name>Andrew Gordon Wilson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1412.6493v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1412.6493v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1504.00083v1</id>\\n    <updated>2015-04-01T02:31:55Z</updated>\\n    <published>2015-04-01T02:31:55Z</published>\\n    <title>A Theory of Feature Learning</title>\\n    <summary>  Feature Learning aims to extract relevant information contained in data sets\\nin an automated fashion. It is driving force behind the current deep learning\\ntrend, a set of methods that have had widespread empirical success. What is\\nlacking is a theoretical understanding of different feature learning schemes.\\nThis work provides a theoretical framework for feature learning and then\\ncharacterizes when features can be learnt in an unsupervised fashion. We also\\nprovide means to judge the quality of features via rate-distortion theory and\\nits generalizations.\\n</summary>\\n    <author>\\n      <name>Brendan van Rooyen</name>\\n    </author>\\n    <author>\\n      <name>Robert C. Williamson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1504.00083v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1504.00083v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.4609v1</id>\\n    <updated>2012-06-18T14:45:17Z</updated>\\n    <published>2012-06-18T14:45:17Z</published>\\n    <title>On multi-view feature learning</title>\\n    <summary>  Sparse coding is a common approach to learning local features for object\\nrecognition. Recently, there has been an increasing interest in learning\\nfeatures from spatio-temporal, binocular, or other multi-observation data,\\nwhere the goal is to encode the relationship between images rather than the\\ncontent of a single image. We provide an analysis of multi-view feature\\nlearning, which shows that hidden variables encode transformations by detecting\\nrotation angles in the eigenspaces shared among multiple image warps. Our\\nanalysis helps explain recent experimental results showing that\\ntransformation-specific features emerge when training complex cell models on\\nvideos. Our analysis also shows that transformation-invariant features can\\nemerge as a by-product of learning representations of transformations.\\n</summary>\\n    <author>\\n      <name>Roland Memisevic</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Frankfurt</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.4609v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.4609v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.01776v1</id>\\n    <updated>2015-11-05T15:19:42Z</updated>\\n    <published>2015-11-05T15:19:42Z</published>\\n    <title>Computational Intractability of Dictionary Learning for Sparse\\n  Representation</title>\\n    <summary>  In this paper we consider the dictionary learning problem for sparse\\nrepresentation. We first show that this problem is NP-hard by polynomial time\\nreduction of the densest cut problem. Then, using successive convex\\napproximation strategies, we propose efficient dictionary learning schemes to\\nsolve several practical formulations of this problem to stationary points.\\nUnlike many existing algorithms in the literature, such as K-SVD, our proposed\\ndictionary learning scheme is theoretically guaranteed to converge to the set\\nof stationary points under certain mild assumptions. For the image denoising\\napplication, the performance and the efficiency of the proposed dictionary\\nlearning scheme are comparable to that of K-SVD algorithm in simulation.\\n</summary>\\n    <author>\\n      <name>Meisam Razaviyayn</name>\\n    </author>\\n    <author>\\n      <name>Hung-Wei Tseng</name>\\n    </author>\\n    <author>\\n      <name>Zhi-Quan Luo</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1511.01776v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.01776v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.7200v1</id>\\n    <updated>2014-11-26T12:14:22Z</updated>\\n    <published>2014-11-26T12:14:22Z</published>\\n    <title>Localized Complexities for Transductive Learning</title>\\n    <summary>  We show two novel concentration inequalities for suprema of empirical\\nprocesses when sampling without replacement, which both take the variance of\\nthe functions into account. While these inequalities may potentially have broad\\napplications in learning theory in general, we exemplify their significance by\\nstudying the transductive setting of learning theory. For which we provide the\\nfirst excess risk bounds based on the localized complexity of the hypothesis\\nclass, which can yield fast rates of convergence also in the transductive\\nlearning setting. We give a preliminary analysis of the localized complexities\\nfor the prominent case of kernel classes.\\n</summary>\\n    <author>\\n      <name>Ilya Tolstikhin</name>\\n    </author>\\n    <author>\\n      <name>Gilles Blanchard</name>\\n    </author>\\n    <author>\\n      <name>Marius Kloft</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appeared in Conference on Learning Theory 2014</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.7200v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.7200v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.05966v1</id>\\n    <updated>2016-07-20T14:14:49Z</updated>\\n    <published>2016-07-20T14:14:49Z</published>\\n    <title>Onsager-corrected deep learning for sparse linear inverse problems</title>\\n    <summary>  Deep learning has gained great popularity due to its widespread success on\\nmany inference problems. We consider the application of deep learning to the\\nsparse linear inverse problem encountered in compressive sensing, where one\\nseeks to recover a sparse signal from a small number of noisy linear\\nmeasurements. In this paper, we propose a novel neural-network architecture\\nthat decouples prediction errors across layers in the same way that the\\napproximate message passing (AMP) algorithm decouples them across iterations:\\nthrough Onsager correction. Numerical experiments suggest that our \"learned\\nAMP\" network significantly improves upon Gregor and LeCun\\'s \"learned ISTA\"\\nnetwork in both accuracy and complexity.\\n</summary>\\n    <author>\\n      <name>Mark Borgerding</name>\\n    </author>\\n    <author>\\n      <name>Philip Schniter</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1607.05966v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.05966v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.01230v1</id>\\n    <updated>2016-08-03T15:49:12Z</updated>\\n    <published>2016-08-03T15:49:12Z</published>\\n    <title>Learning a Driving Simulator</title>\\n    <summary>  Comma.ai\\'s approach to Artificial Intelligence for self-driving cars is based\\non an agent that learns to clone driver behaviors and plans maneuvers by\\nsimulating future events in the road. This paper illustrates one of our\\nresearch approaches for driving simulation. One where we learn to simulate.\\nHere we investigate variational autoencoders with classical and learned cost\\nfunctions using generative adversarial networks for embedding road frames.\\nAfterwards, we learn a transition model in the embedded space using action\\nconditioned Recurrent Neural Networks. We show that our approach can keep\\npredicting realistic looking video for several frames despite the transition\\nmodel being optimized without a cost function in the pixel space.\\n</summary>\\n    <author>\\n      <name>Eder Santana</name>\\n    </author>\\n    <author>\\n      <name>George Hotz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1608.01230v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.01230v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.03348v4</id>\\n    <updated>2017-01-14T05:54:29Z</updated>\\n    <published>2016-09-12T11:23:20Z</published>\\n    <title>A Threshold-based Scheme for Reinforcement Learning in Neural Networks</title>\\n    <summary>  A generic and scalable Reinforcement Learning scheme for Artificial Neural\\nNetworks is presented, providing a general purpose learning machine. By\\nreference to a node threshold three features are described 1) A mechanism for\\nPrimary Reinforcement, capable of solving linearly inseparable problems 2) The\\nlearning scheme is extended to include a mechanism for Conditioned\\nReinforcement, capable of forming long term strategy 3) The learning scheme is\\nmodified to use a threshold-based deep learning algorithm, providing a robust\\nand biologically inspired alternative to backpropagation. The model may be used\\nfor supervised as well as unsupervised training regimes.\\n</summary>\\n    <author>\\n      <name>Thomas H. Ward</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1609.03348v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.03348v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.01606v1</id>\\n    <updated>2016-11-05T05:42:40Z</updated>\\n    <published>2016-11-05T05:42:40Z</published>\\n    <title>Learning to Play in a Day: Faster Deep Reinforcement Learning by\\n  Optimality Tightening</title>\\n    <summary>  We propose a novel training algorithm for reinforcement learning which\\ncombines the strength of deep Q-learning with a constrained optimization\\napproach to tighten optimality and encourage faster reward propagation. Our\\nnovel technique makes deep reinforcement learning more practical by drastically\\nreducing the training time. We evaluate the performance of our approach on the\\n49 games of the challenging Arcade Learning Environment, and report significant\\nimprovements in both training time and accuracy.\\n</summary>\\n    <author>\\n      <name>Frank S. He</name>\\n    </author>\\n    <author>\\n      <name>Yang Liu</name>\\n    </author>\\n    <author>\\n      <name>Alexander G. Schwing</name>\\n    </author>\\n    <author>\\n      <name>Jian Peng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1611.01606v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.01606v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.04228v1</id>\\n    <updated>2016-11-14T02:28:13Z</updated>\\n    <published>2016-11-14T02:28:13Z</published>\\n    <title>Learning Sparse, Distributed Representations using the Hebbian Principle</title>\\n    <summary>  The \"fire together, wire together\" Hebbian model is a central principle for\\nlearning in neuroscience, but surprisingly, it has found limited applicability\\nin modern machine learning. In this paper, we take a first step towards\\nbridging this gap, by developing flavors of competitive Hebbian learning which\\nproduce sparse, distributed neural codes using online adaptation with minimal\\ntuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning\\n(AHL). We illustrate the distributed nature of the learned representations via\\noutput entropy computations for synthetic data, and demonstrate superior\\nperformance, compared to standard alternatives such as autoencoders, in\\ntraining a deep convolutional net on standard image datasets.\\n</summary>\\n    <author>\\n      <name>Aseem Wadhwa</name>\\n    </author>\\n    <author>\\n      <name>Upamanyu Madhow</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1611.04228v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.04228v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.06806v3</id>\\n    <updated>2017-07-28T09:40:37Z</updated>\\n    <published>2017-01-24T10:53:07Z</published>\\n    <title>A Survey of Quantum Learning Theory</title>\\n    <summary>  This paper surveys quantum learning theory: the theoretical aspects of\\nmachine learning using quantum computers. We describe the main results known\\nfor three models of learning: exact learning from membership queries, and\\nProbably Approximately Correct (PAC) and agnostic learning from classical or\\nquantum examples.\\n</summary>\\n    <author>\\n      <name>Srinivasan Arunachalam</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CWI</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Ronald de Wolf</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CWI and U of Amsterdam</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages LaTeX. v2: many small changes to improve the presentation.\\n  This version will appear as Complexity Theory Column in SIGACT News in June\\n  2017. v3: fixed a small ambiguity in the definition of gamma(C) and updated a\\n  reference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1701.06806v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.06806v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.08396v2</id>\\n    <updated>2017-06-09T17:19:15Z</updated>\\n    <published>2017-02-27T17:43:34Z</published>\\n    <title>Learning Hierarchical Features from Generative Models</title>\\n    <summary>  Deep neural networks have been shown to be very successful at learning\\nfeature hierarchies in supervised learning tasks. Generative models, on the\\nother hand, have benefited less from hierarchical models with multiple layers\\nof latent variables. In this paper, we prove that hierarchical latent variable\\nmodels do not take advantage of the hierarchical structure when trained with\\nexisting variational methods, and provide some limitations on the kind of\\nfeatures existing models can learn. Finally we propose an alternative\\narchitecture that do not suffer from these limitations. Our model is able to\\nlearn highly interpretable and disentangled hierarchical features on several\\nnatural image datasets with no task specific regularization or prior knowledge.\\n</summary>\\n    <author>\\n      <name>Shengjia Zhao</name>\\n    </author>\\n    <author>\\n      <name>Jiaming Song</name>\\n    </author>\\n    <author>\\n      <name>Stefano Ermon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML\\'2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1702.08396v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.08396v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.02155v2</id>\\n    <updated>2017-08-13T16:11:50Z</updated>\\n    <published>2017-03-07T00:03:32Z</published>\\n    <title>Model-Based Multiple Instance Learning</title>\\n    <summary>  While Multiple Instance (MI) data are point patterns -- sets or multi-sets of\\nunordered points -- appropriate statistical point pattern models have not been\\nused in MI learning. This article proposes a framework for model-based MI\\nlearning using point process theory. Likelihood functions for point pattern\\ndata derived from point process theory enable principled yet conceptually\\ntransparent extensions of learning tasks, such as classification, novelty\\ndetection and clustering, to point pattern data. Furthermore, tractable point\\npattern models as well as solutions for learning and decision making from point\\npattern data are developed.\\n</summary>\\n    <author>\\n      <name>Ba-Ngu Vo</name>\\n    </author>\\n    <author>\\n      <name>Dinh Phung</name>\\n    </author>\\n    <author>\\n      <name>Quang N. Tran</name>\\n    </author>\\n    <author>\\n      <name>Ba-Tuong Vo</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 15 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.02155v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.02155v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.05396v1</id>\\n    <updated>2017-05-15T18:07:31Z</updated>\\n    <published>2017-05-15T18:07:31Z</published>\\n    <title>Learning Probabilistic Programs Using Backpropagation</title>\\n    <summary>  Probabilistic modeling enables combining domain knowledge with learning from\\ndata, thereby supporting learning from fewer training instances than purely\\ndata-driven methods. However, learning probabilistic models is difficult and\\nhas not achieved the level of performance of methods such as deep neural\\nnetworks on many tasks. In this paper, we attempt to address this issue by\\npresenting a method for learning the parameters of a probabilistic program\\nusing backpropagation. Our approach opens the possibility to building deep\\nprobabilistic programming models that are trained in a similar way to neural\\nnetworks.\\n</summary>\\n    <author>\\n      <name>Avi Pfeffer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.05396v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.05396v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.00102v1</id>\\n    <updated>2017-07-31T23:36:18Z</updated>\\n    <published>2017-07-31T23:36:18Z</published>\\n    <title>Advantages and Limitations of using Successor Features for Transfer in\\n  Reinforcement Learning</title>\\n    <summary>  One question central to Reinforcement Learning is how to learn a feature\\nrepresentation that supports algorithm scaling and re-use of learned\\ninformation from different tasks. Successor Features approach this problem by\\nlearning a feature representation that satisfies a temporal constraint. We\\npresent an implementation of an approach that decouples the feature\\nrepresentation from the reward function, making it suitable for transferring\\nknowledge between domains. We then assess the advantages and limitations of\\nusing Successor Features for transfer.\\n</summary>\\n    <author>\\n      <name>Lucas Lehnert</name>\\n    </author>\\n    <author>\\n      <name>Stefanie Tellex</name>\\n    </author>\\n    <author>\\n      <name>Michael L. Littman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.00102v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.00102v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.04043v3</id>\\n    <updated>2018-02-20T16:52:36Z</updated>\\n    <published>2017-11-10T23:32:47Z</published>\\n    <title>Few-Shot Learning with Graph Neural Networks</title>\\n    <summary>  We propose to study the problem of few-shot learning with the prism of\\ninference on a partially observed graphical model, constructed from a\\ncollection of input images whose label can be either observed or not. By\\nassimilating generic message-passing inference algorithms with their\\nneural-network counterparts, we define a graph neural network architecture that\\ngeneralizes several of the recently proposed few-shot learning models. Besides\\nproviding improved numerical performance, our framework is easily extended to\\nvariants of few-shot learning, such as semi-supervised or active learning,\\ndemonstrating the ability of graph-based models to operate well on \\'relational\\'\\ntasks.\\n</summary>\\n    <author>\\n      <name>Victor Garcia</name>\\n    </author>\\n    <author>\\n      <name>Joan Bruna</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1711.04043v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.04043v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.10204v1</id>\\n    <updated>2017-11-28T09:47:51Z</updated>\\n    <published>2017-11-28T09:47:51Z</published>\\n    <title>Block Neural Network Avoids Catastrophic Forgetting When Learning\\n  Multiple Task</title>\\n    <summary>  In the present work we propose a Deep Feed Forward network architecture which\\ncan be trained according to a sequential learning paradigm, where tasks of\\nincreasing difficulty are learned sequentially, yet avoiding catastrophic\\nforgetting. The proposed architecture can re-use the features learned on\\nprevious tasks in a new task when the old tasks and the new one are related.\\nThe architecture needs fewer computational resources (neurons and connections)\\nand less data for learning the new task than a network trained from scratch\\n</summary>\\n    <author>\\n      <name>Guglielmo Montone</name>\\n    </author>\\n    <author>\\n      <name>J. Kevin O\\'Regan</name>\\n    </author>\\n    <author>\\n      <name>Alexander V. Terekhov</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1711.10204v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.10204v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.05016v2</id>\\n    <updated>2017-12-16T02:52:55Z</updated>\\n    <published>2017-12-13T21:41:56Z</published>\\n    <title>Deep Prior</title>\\n    <summary>  The recent literature on deep learning offers new tools to learn a rich\\nprobability distribution over high dimensional data such as images or sounds.\\nIn this work we investigate the possibility of learning the prior distribution\\nover neural network parameters using such tools. Our resulting variational\\nBayes algorithm generalizes well to new tasks, even when very few training\\nexamples are provided. Furthermore, this learned prior allows the model to\\nextrapolate correctly far from a given task\\'s training data on a meta-dataset\\nof periodic signals.\\n</summary>\\n    <author>\\n      <name>Alexandre Lacoste</name>\\n    </author>\\n    <author>\\n      <name>Thomas Boquet</name>\\n    </author>\\n    <author>\\n      <name>Negar Rostamzadeh</name>\\n    </author>\\n    <author>\\n      <name>Boris Oreshkin</name>\\n    </author>\\n    <author>\\n      <name>Wonchang Chung</name>\\n    </author>\\n    <author>\\n      <name>David Krueger</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Workshop paper, Accepted at Bayesian Deep Learning workshop, NIPS\\n  2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.05016v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.05016v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.00631v1</id>\\n    <updated>2018-01-02T12:49:35Z</updated>\\n    <published>2018-01-02T12:49:35Z</published>\\n    <title>Deep Learning: A Critical Appraisal</title>\\n    <summary>  Although deep learning has historical roots going back decades, neither the\\nterm \"deep learning\" nor the approach was popular just over five years ago,\\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\\nHinton\\'s now classic (2012) deep network model of Imagenet. What has the field\\ndiscovered in the five subsequent years? Against a background of considerable\\nprogress in areas such as speech recognition, image recognition, and game\\nplaying, and considerable enthusiasm in the popular press, I present ten\\nconcerns for deep learning, and suggest that deep learning must be supplemented\\nby other techniques if we are to reach artificial general intelligence.\\n</summary>\\n    <author>\\n      <name>Gary Marcus</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.00631v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.00631v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"97R40\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.0; I.2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.02961v1</id>\\n    <updated>2018-02-08T16:49:00Z</updated>\\n    <published>2018-02-08T16:49:00Z</published>\\n    <title>Learning Sparse Wavelet Representations</title>\\n    <summary>  In this work we propose a method for learning wavelet filters directly from\\ndata. We accomplish this by framing the discrete wavelet transform as a\\nmodified convolutional neural network. We introduce an autoencoder wavelet\\ntransform network that is trained using gradient descent. We show that the\\nmodel is capable of learning structured wavelet filters from synthetic and real\\ndata. The learned wavelets are shown to be similar to traditional wavelets that\\nare derived using Fourier methods. Our method is simple to implement and easily\\nincorporated into neural network architectures. A major advantage to our model\\nis that we can learn from raw audio data.\\n</summary>\\n    <author>\\n      <name>Daniel Recoskie</name>\\n    </author>\\n    <author>\\n      <name>Richard Mann</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.02961v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.02961v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.06293v2</id>\\n    <updated>2018-06-26T02:17:16Z</updated>\\n    <published>2018-02-17T20:59:02Z</published>\\n    <title>Black-Box Reductions for Parameter-free Online Learning in Banach Spaces</title>\\n    <summary>  We introduce several new black-box reductions that significantly improve the\\ndesign of adaptive and parameter-free online learning algorithms by simplifying\\nanalysis, improving regret guarantees, and sometimes even improving runtime. We\\nreduce parameter-free online learning to online exp-concave optimization, we\\nreduce optimization in a Banach space to one-dimensional optimization, and we\\nreduce optimization over a constrained domain to unconstrained optimization.\\nAll of our reductions run as fast as online gradient descent. We use our new\\ntechniques to improve upon the previously best regret bounds for parameter-free\\nlearning, and do so for arbitrary norms.\\n</summary>\\n    <author>\\n      <name>Ashok Cutkosky</name>\\n    </author>\\n    <author>\\n      <name>Francesco Orabona</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Conference on Learning Theory 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.06293v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.06293v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.08416v1</id>\\n    <updated>2018-03-22T15:49:32Z</updated>\\n    <published>2018-03-22T15:49:32Z</published>\\n    <title>Demystifying Deep Learning: A Geometric Approach to Iterative\\n  Projections</title>\\n    <summary>  Parametric approaches to Learning, such as deep learning (DL), are highly\\npopular in nonlinear regression, in spite of their extremely difficult training\\nwith their increasing complexity (e.g. number of layers in DL). In this paper,\\nwe present an alternative semi-parametric framework which foregoes the\\nordinarily required feedback, by introducing the novel idea of geometric\\nregularization. We show that certain deep learning techniques such as residual\\nnetwork (ResNet) architecture are closely related to our approach. Hence, our\\ntechnique can be used to analyze these types of deep learning. Moreover, we\\npresent preliminary results which confirm that our approach can be easily\\ntrained to obtain complex structures.\\n</summary>\\n    <author>\\n      <name>Ashkan Panahi</name>\\n    </author>\\n    <author>\\n      <name>Hamid Krim</name>\\n    </author>\\n    <author>\\n      <name>Liyi Dai</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To be appeared in the ICASSP 2018 proceedings</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.08416v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.08416v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.10689v2</id>\\n    <updated>2018-05-09T02:02:28Z</updated>\\n    <published>2018-04-27T21:16:40Z</published>\\n    <title>Decoupling Dynamics and Reward for Transfer Learning</title>\\n    <summary>  Current reinforcement learning (RL) methods can successfully learn single\\ntasks but often generalize poorly to modest perturbations in task domain or\\ntraining procedure. In this work, we present a decoupled learning strategy for\\nRL that creates a shared representation space where knowledge can be robustly\\ntransferred. We separate learning the task representation, the forward\\ndynamics, the inverse dynamics and the reward function of the domain, and show\\nthat this decoupling improves performance within the task, transfers well to\\nchanges in dynamics and reward, and can be effectively used for online\\nplanning. Empirical results show good performance in both continuous and\\ndiscrete RL domains.\\n</summary>\\n    <author>\\n      <name>Amy Zhang</name>\\n    </author>\\n    <author>\\n      <name>Harsh Satija</name>\\n    </author>\\n    <author>\\n      <name>Joelle Pineau</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.10689v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.10689v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.00980v1</id>\\n    <updated>2018-05-02T18:52:18Z</updated>\\n    <published>2018-05-02T18:52:18Z</published>\\n    <title>SaaS: Speed as a Supervisor for Semi-supervised Learning</title>\\n    <summary>  We introduce the SaaS Algorithm for semi-supervised learning, which uses\\nlearning speed during stochastic gradient descent in a deep neural network to\\nmeasure the quality of an iterative estimate of the posterior probability of\\nunknown labels. Training speed in supervised learning correlates strongly with\\nthe percentage of correct labels, so we use it as an inference criterion for\\nthe unknown labels, without attempting to infer the model parameters at first.\\nDespite its simplicity, SaaS achieves state-of-the-art results in\\nsemi-supervised learning benchmarks.\\n</summary>\\n    <author>\\n      <name>Safa Cicek</name>\\n    </author>\\n    <author>\\n      <name>Alhussein Fawzi</name>\\n    </author>\\n    <author>\\n      <name>Stefano Soatto</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.00980v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.00980v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.01867v1</id>\\n    <updated>2018-05-04T17:22:39Z</updated>\\n    <published>2018-05-04T17:22:39Z</published>\\n    <title>Bayesian active learning for choice models with deep Gaussian processes</title>\\n    <summary>  In this paper, we propose an active learning algorithm and models which can\\ngradually learn individual\\'s preference through pairwise comparisons. The\\nactive learning scheme aims at finding individual\\'s most preferred choice with\\nminimized number of pairwise comparisons. The pairwise comparisons are encoded\\ninto probabilistic models based on assumptions of choice models and deep\\nGaussian processes. The next-to-compare decision is determined by a novel\\nacquisition function. We benchmark the proposed algorithm and models using\\nfunctions with multiple local optima and one public airline itinerary dataset.\\nThe experiments indicate the effectiveness of our active learning algorithm and\\nmodels.\\n</summary>\\n    <author>\\n      <name>Jie Yang</name>\\n    </author>\\n    <author>\\n      <name>Diego Klabjan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.01867v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.01867v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07331v2</id>\\n    <updated>2019-01-25T10:55:46Z</updated>\\n    <published>2018-05-18T17:14:23Z</published>\\n    <title>Positive and Unlabeled Learning through Negative Selection and\\n  Imbalance-aware Classification</title>\\n    <summary>  Motivated by applications in protein function prediction, we consider a\\nchallenging supervised classification setting in which positive labels are\\nscarce and there are no explicit negative labels. The learning algorithm must\\nthus select which unlabeled examples to use as negative training points,\\npossibly ending up with an unbalanced learning problem. We address these issues\\nby proposing an algorithm that combines active learning (for selecting negative\\nexamples) with imbalance-aware learning (for mitigating the label imbalance).\\nIn our experiments we observe that these two techniques operate\\nsynergistically, outperforming state-of-the-art methods on standard protein\\nfunction prediction benchmarks.\\n</summary>\\n    <author>\\n      <name>Marco Frasca</name>\\n    </author>\\n    <author>\\n      <name>Nicol\\xc3\\xb2 Cesa-Bianchi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.07331v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07331v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07798v2</id>\\n    <updated>2018-06-01T23:29:24Z</updated>\\n    <published>2018-05-20T17:07:25Z</published>\\n    <title>Improved Learning of One-hidden-layer Convolutional Neural Networks with\\n  Overlaps</title>\\n    <summary>  We propose a new algorithm to learn a one-hidden-layer convolutional neural\\nnetwork where both the convolutional weights and the outputs weights are\\nparameters to be learned. Our algorithm works for a general class of\\n(potentially overlapping) patches, including commonly used structures for\\ncomputer vision tasks. Our algorithm draws ideas from (1) isotonic regression\\nfor learning neural networks and (2) landscape analysis of non-convex matrix\\nfactorization problems. We believe these findings may inspire further\\ndevelopment in designing provable algorithms for learning neural networks and\\nother complex models.\\n</summary>\\n    <author>\\n      <name>Simon S. Du</name>\\n    </author>\\n    <author>\\n      <name>Surbhi Goel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.07798v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07798v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08948v2</id>\\n    <updated>2018-12-16T08:23:55Z</updated>\\n    <published>2018-05-23T03:36:01Z</published>\\n    <title>Scalable Coordinated Exploration in Concurrent Reinforcement Learning</title>\\n    <summary>  We consider a team of reinforcement learning agents that concurrently operate\\nin a common environment, and we develop an approach to efficient coordinated\\nexploration that is suitable for problems of practical scale. Our approach\\nbuilds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value\\nfunction learning (Osband et al., 2016). We demonstrate that, for simple\\ntabular contexts, the approach is competitive with previously proposed tabular\\nmodel learning methods (Dimakopoulou and Van Roy, 2018). With a\\nhigher-dimensional problem and a neural network value function representation,\\nthe approach learns quickly with far fewer agents than alternative exploration\\nschemes.\\n</summary>\\n    <author>\\n      <name>Maria Dimakopoulou</name>\\n    </author>\\n    <author>\\n      <name>Ian Osband</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Van Roy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.08948v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08948v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.05076v1</id>\\n    <updated>2018-07-12T14:40:06Z</updated>\\n    <published>2018-07-12T14:40:06Z</published>\\n    <title>Metalearning with Hebbian Fast Weights</title>\\n    <summary>  We unify recent neural approaches to one-shot learning with older ideas of\\nassociative memory in a model for metalearning. Our model learns jointly to\\nrepresent data and to bind class labels to representations in a single shot. It\\nbuilds representations via slow weights, learned across tasks through SGD,\\nwhile fast weights constructed by a Hebbian learning rule implement one-shot\\nbinding for each new task. On the Omniglot, Mini-ImageNet, and Penn Treebank\\none-shot learning benchmarks, our model achieves state-of-the-art results.\\n</summary>\\n    <author>\\n      <name>Tsendsuren Munkhdalai</name>\\n    </author>\\n    <author>\\n      <name>Adam Trischler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures, 4 tables. arXiv admin note: text overlap with\\n  arXiv:1712.09926</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.05076v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.05076v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.09936v1</id>\\n    <updated>2018-07-26T03:21:49Z</updated>\\n    <published>2018-07-26T03:21:49Z</published>\\n    <title>Multi-Agent Generative Adversarial Imitation Learning</title>\\n    <summary>  Imitation learning algorithms can be used to learn a policy from expert\\ndemonstrations without access to a reward signal. However, most existing\\napproaches are not applicable in multi-agent settings due to the existence of\\nmultiple (Nash) equilibria and non-stationary environments. We propose a new\\nframework for multi-agent imitation learning for general Markov games, where we\\nbuild upon a generalized notion of inverse reinforcement learning. We further\\nintroduce a practical multi-agent actor-critic algorithm with good empirical\\nperformance. Our method can be used to imitate complex behaviors in\\nhigh-dimensional environments with multiple cooperative or competing agents.\\n</summary>\\n    <author>\\n      <name>Jiaming Song</name>\\n    </author>\\n    <author>\\n      <name>Hongyu Ren</name>\\n    </author>\\n    <author>\\n      <name>Dorsa Sadigh</name>\\n    </author>\\n    <author>\\n      <name>Stefano Ermon</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.09936v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.09936v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.06671v1</id>\\n    <updated>2018-08-20T19:53:19Z</updated>\\n    <published>2018-08-20T19:53:19Z</published>\\n    <title>Adversarial Sampling for Active Learning</title>\\n    <summary>  This paper describes ASAL a new active learning strategy that uses\\nuncertainty sampling, adversarial sample generation and sample matching.\\nCompared to traditional pool-based uncertainty sampling strategies, ASAL\\nsynthesizes uncertain samples instead of performing an exhaustive search in\\neach active learning cycle. Then, the sample matching efficiently selects\\nsimilar samples from the pool. We present a comprehensive set of experiments on\\nMNIST and CIFAR-10 and show that ASAL outperforms similar methods and clearly\\nexceeds passive learning. To the best of our knowledge this is the first\\npool-based adversarial active learning technique and the first that is applied\\nfor multi-label classification using deep convolutional classifiers.\\n</summary>\\n    <author>\\n      <name>Christoph Mayer</name>\\n    </author>\\n    <author>\\n      <name>Radu Timofte</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.06671v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.06671v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.09083v1</id>\\n    <updated>2018-11-22T10:15:52Z</updated>\\n    <published>2018-11-22T10:15:52Z</published>\\n    <title>Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement\\n  Learning</title>\\n    <summary>  In hierarchical reinforcement learning a major challenge is determining\\nappropriate low-level policies. We propose an unsupervised learning scheme,\\nbased on asymmetric self-play from Sukhbaatar et al. (2018), that automatically\\nlearns a good representation of sub-goals in the environment and a low-level\\npolicy that can execute them. A high-level policy can then direct the lower one\\nby generating a sequence of continuous sub-goal vectors. We evaluate our model\\nusing Mazebase and Mujoco environments, including the challenging AntGather\\ntask. Visualizations of the sub-goal embeddings reveal a logical decomposition\\nof tasks within the environment. Quantitatively, our approach obtains\\ncompelling performance gains over non-hierarchical approaches.\\n</summary>\\n    <author>\\n      <name>Sainbayar Sukhbaatar</name>\\n    </author>\\n    <author>\\n      <name>Emily Denton</name>\\n    </author>\\n    <author>\\n      <name>Arthur Szlam</name>\\n    </author>\\n    <author>\\n      <name>Rob Fergus</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.09083v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.09083v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00950v1</id>\\n    <updated>2018-12-03T18:21:18Z</updated>\\n    <published>2018-12-03T18:21:18Z</published>\\n    <title>Generative Adversarial Self-Imitation Learning</title>\\n    <summary>  This paper explores a simple regularizer for reinforcement learning by\\nproposing Generative Adversarial Self-Imitation Learning (GASIL), which\\nencourages the agent to imitate past good trajectories via generative\\nadversarial imitation learning framework. Instead of directly maximizing\\nrewards, GASIL focuses on reproducing past good trajectories, which can\\npotentially make long-term credit assignment easier when rewards are sparse and\\ndelayed. GASIL can be easily combined with any policy gradient objective by\\nusing GASIL as a learned shaped reward function. Our experimental results show\\nthat GASIL improves the performance of proximal policy optimization on 2D Point\\nMass and MuJoCo environments with delayed reward and stochastic dynamics.\\n</summary>\\n    <author>\\n      <name>Yijie Guo</name>\\n    </author>\\n    <author>\\n      <name>Junhyuk Oh</name>\\n    </author>\\n    <author>\\n      <name>Satinder Singh</name>\\n    </author>\\n    <author>\\n      <name>Honglak Lee</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.00950v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00950v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.03288v1</id>\\n    <updated>2018-12-08T08:54:37Z</updated>\\n    <published>2018-12-08T08:54:37Z</published>\\n    <title>No Peek: A Survey of private distributed deep learning</title>\\n    <summary>  We survey distributed deep learning models for training or inference without\\naccessing raw data from clients. These methods aim to protect confidential\\npatterns in data while still allowing servers to train models. The distributed\\ndeep learning methods of federated learning, split learning and large batch\\nstochastic gradient descent are compared in addition to private and secure\\napproaches of differential privacy, homomorphic encryption, oblivious transfer\\nand garbled circuits in the context of neural networks. We study their\\nbenefits, limitations and trade-offs with regards to computational resources,\\ndata leakage and communication efficiency and also share our anticipated future\\ntrends.\\n</summary>\\n    <author>\\n      <name>Praneeth Vepakomma</name>\\n    </author>\\n    <author>\\n      <name>Tristan Swedish</name>\\n    </author>\\n    <author>\\n      <name>Ramesh Raskar</name>\\n    </author>\\n    <author>\\n      <name>Otkrist Gupta</name>\\n    </author>\\n    <author>\\n      <name>Abhimanyu Dubey</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.03288v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.03288v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.10747v1</id>\\n    <updated>2018-12-27T15:48:10Z</updated>\\n    <published>2018-12-27T15:48:10Z</published>\\n    <title>Off-the-grid model based deep learning (O-MODL)</title>\\n    <summary>  We introduce a model based off-the-grid image reconstruction algorithm using\\ndeep learned priors. The main difference of the proposed scheme with current\\ndeep learning strategies is the learning of non-linear annihilation relations\\nin Fourier space. We rely on a model based framework, which allows us to use a\\nsignificantly smaller deep network, compared to direct approaches that also\\nlearn how to invert the forward model. Preliminary comparisons against image\\ndomain MoDL approach demonstrates the potential of the off-the-grid\\nformulation. The main benefit of the proposed scheme compared to structured\\nlow-rank methods is the quite significant reduction in computational\\ncomplexity.\\n</summary>\\n    <author>\\n      <name>Aniket Pramanik</name>\\n    </author>\\n    <author>\\n      <name>Hemant Kumar Aggarwal</name>\\n    </author>\\n    <author>\\n      <name>Mathews Jacob</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ISBI 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.10747v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.10747v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.11560v1</id>\\n    <updated>2018-12-30T15:38:25Z</updated>\\n    <published>2018-12-30T15:38:25Z</published>\\n    <title>Monte-Carlo Sampling applied to Multiple Instance Learning for\\n  Histological Image Classification</title>\\n    <summary>  We propose a patch sampling strategy based on a sequential Monte-Carlo method\\nfor high resolution image classification in the context of Multiple Instance\\nLearning. When compared with grid sampling and uniform sampling techniques, it\\nachieves higher generalization performance. We validate the strategy on two\\nartificial datasets and two histological datasets for breast cancer and sun\\nexposure classification.\\n</summary>\\n    <author>\\n      <name>Marc Combalia</name>\\n    </author>\\n    <author>\\n      <name>Veronica Vilaplana</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-030-00889-5</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-030-00889-5\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">accepted at 4th International Workshop on Deep Learning for Medical\\n  Image Analysis (DLMIA), MICCAI 2018, Deep Learning in Medical Image Analysis\\n  and Multimodal Learning for Clinical Decision Support, Springer International\\n  Publishing, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.11560v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.11560v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.03674v1</id>\\n    <updated>2019-01-11T17:54:47Z</updated>\\n    <published>2019-01-11T17:54:47Z</published>\\n    <title>On the Global Convergence of Imitation Learning: A Case for Linear\\n  Quadratic Regulator</title>\\n    <summary>  We study the global convergence of generative adversarial imitation learning\\nfor linear quadratic regulators, which is posed as minimax optimization. To\\naddress the challenges arising from non-convex-concave geometry, we analyze the\\nalternating gradient algorithm and establish its Q-linear rate of convergence\\nto a unique saddle point, which simultaneously recovers the globally optimal\\npolicy and reward function. We hope our results may serve as a small step\\ntowards understanding and taming the instability in imitation learning as well\\nas in more general non-convex-concave alternating minimax optimization that\\narises from reinforcement learning and generative adversarial learning.\\n</summary>\\n    <author>\\n      <name>Qi Cai</name>\\n    </author>\\n    <author>\\n      <name>Mingyi Hong</name>\\n    </author>\\n    <author>\\n      <name>Yongxin Chen</name>\\n    </author>\\n    <author>\\n      <name>Zhaoran Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.03674v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.03674v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.06622v1</id>\\n    <updated>2019-01-20T04:44:12Z</updated>\\n    <published>2019-01-20T04:44:12Z</published>\\n    <title>Mixed Formal Learning: A Path to Transparent Machine Learning</title>\\n    <summary>  This paper presents Mixed Formal Learning, a new architecture that learns\\nmodels based on formal mathematical representations of the domain of interest\\nand exposes latent variables. The second element in the architecture learns a\\nparticular skill, typically by using traditional prediction or classification\\nmechanisms. Our key findings include that this architecture: (1) Facilitates\\ntransparency by exposing key latent variables based on a learned mathematical\\nmodel; (2) Enables Low Shot and Zero Shot training of machine learning without\\nsacrificing accuracy or recall.\\n</summary>\\n    <author>\\n      <name>Sandra Carrico</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted IEEE ICSC 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.06622v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.06622v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.08649v3</id>\\n    <updated>2019-03-05T17:26:51Z</updated>\\n    <published>2019-01-24T21:46:39Z</published>\\n    <title>Learning Independently-Obtainable Reward Functions</title>\\n    <summary>  We present a novel method for learning a set of disentangled reward functions\\nthat sum to the original environment reward and are constrained to be\\nindependently obtainable. We define independent obtainability in terms of value\\nfunctions with respect to obtaining one learned reward while pursuing another\\nlearned reward. Empirically, we illustrate that our method can learn meaningful\\nreward decompositions in a variety of domains and that these decompositions\\nexhibit some form of generalization performance when the environment\\'s reward\\nis modified. Theoretically, we derive results about the effect of maximizing\\nour method\\'s objective on the resulting reward functions and their\\ncorresponding optimal policies.\\n</summary>\\n    <author>\\n      <name>Christopher Grimm</name>\\n    </author>\\n    <author>\\n      <name>Satinder Singh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.08649v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.08649v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.02948v1</id>\\n    <updated>2019-02-08T06:06:22Z</updated>\\n    <published>2019-02-08T06:06:22Z</published>\\n    <title>EILearn: Learning Incrementally Using Previous Knowledge Obtained From\\n  an Ensemble of Classifiers</title>\\n    <summary>  We propose an algorithm for incremental learning of classifiers. The proposed\\nmethod enables an ensemble of classifiers to learn incrementally by\\naccommodating new training data. We use an effective mechanism to overcome the\\nstability-plasticity dilemma. In incremental learning, the general convention\\nis to use only the knowledge acquired in the previous phase but not the\\npreviously seen data. We follow this convention by retaining the previously\\nacquired knowledge which is relevant and using it along with the current data.\\nThe performance of each classifier is monitored to eliminate the poorly\\nperforming classifiers in the subsequent phases. Experimental results show that\\nthe proposed approach outperforms the existing incremental learning approaches.\\n</summary>\\n    <author>\\n      <name>Shivang Agarwal</name>\\n    </author>\\n    <author>\\n      <name>C. Ravindranath Chowdary</name>\\n    </author>\\n    <author>\\n      <name>Shripriya Maheshwari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.02948v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.02948v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.03058v1</id>\\n    <updated>2019-03-07T17:32:32Z</updated>\\n    <published>2019-03-07T17:32:32Z</published>\\n    <title>Analysis Dictionary Learning: An Efficient and Discriminative Solution</title>\\n    <summary>  Discriminative Dictionary Learning (DL) methods have been widely advocated\\nfor image classification problems. To further sharpen their discriminative\\ncapabilities, most state-of-the-art DL methods have additional constraints\\nincluded in the learning stages. These various constraints, however, lead to\\nadditional computational complexity. We hence propose an efficient\\nDiscriminative Convolutional Analysis Dictionary Learning (DCADL) method, as a\\nlower cost Discriminative DL framework, to both characterize the image\\nstructures and refine the interclass structure representations. The proposed\\nDCADL jointly learns a convolutional analysis dictionary and a universal\\nclassifier, while greatly reducing the time complexity in both training and\\ntesting phases, and achieving a competitive accuracy, thus demonstrating great\\nperformance in many experiments with standard databases.\\n</summary>\\n    <author>\\n      <name>Wen Tang</name>\\n    </author>\\n    <author>\\n      <name>Ashkan Panahi</name>\\n    </author>\\n    <author>\\n      <name>Hamid Krim</name>\\n    </author>\\n    <author>\\n      <name>Liyi Dai</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICASSP 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.03058v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.03058v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.03614v1</id>\\n    <updated>2019-03-11T12:59:47Z</updated>\\n    <published>2019-03-11T12:59:47Z</published>\\n    <title>Gradient Descent based Optimization Algorithms for Deep Learning Models\\n  Training</title>\\n    <summary>  In this paper, we aim at providing an introduction to the gradient descent\\nbased optimization algorithms for learning deep neural network models. Deep\\nlearning models involving multiple nonlinear projection layers are very\\nchallenging to train. Nowadays, most of the deep learning model training still\\nrelies on the back propagation algorithm actually. In back propagation, the\\nmodel variables will be updated iteratively until convergence with gradient\\ndescent based optimization algorithms. Besides the conventional vanilla\\ngradient descent algorithm, many gradient descent variants have also been\\nproposed in recent years to improve the learning performance, including\\nMomentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this\\npaper respectively.\\n</summary>\\n    <author>\\n      <name>Jiawei Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1805.07500</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.03614v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.03614v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.09885v1</id>\\n    <updated>2019-03-23T21:29:49Z</updated>\\n    <published>2019-03-23T21:29:49Z</published>\\n    <title>Temporal Logic Guided Safe Reinforcement Learning Using Control Barrier\\n  Functions</title>\\n    <summary>  Using reinforcement learning to learn control policies is a challenge when\\nthe task is complex with potentially long horizons. Ensuring adequate but safe\\nexploration is also crucial for controlling physical systems. In this paper, we\\nuse temporal logic to facilitate specification and learning of complex tasks.\\nWe combine temporal logic with control Lyapunov functions to improve\\nexploration. We incorporate control barrier functions to safeguard the\\nexploration and deployment process. We develop a flexible and learnable system\\nthat allows users to specify task objectives and constraints in different forms\\nand at various levels. The framework is also able to take advantage of known\\nsystem dynamics and handle unknown environmental dynamics by integrating\\nmodel-free learning with model-based planning.\\n</summary>\\n    <author>\\n      <name>Xiao Li</name>\\n    </author>\\n    <author>\\n      <name>Calin Belta</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.09885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.09885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.12344v2</id>\\n    <updated>2019-04-02T03:35:15Z</updated>\\n    <published>2019-03-29T03:43:36Z</published>\\n    <title>Learning Good Representation via Continuous Attention</title>\\n    <summary>  In this paper we present our scientific discovery that good representation\\ncan be learned via continuous attention during the interaction between\\nUnsupervised Learning(UL) and Reinforcement Learning(RL) modules driven by\\nintrinsic motivation. Specifically, we designed intrinsic rewards generated\\nfrom UL modules for driving the RL agent to focus on objects for a period of\\ntime and to learn good representations of objects for later object recognition\\ntask. We evaluate our proposed algorithm in both with and without extrinsic\\nreward settings. Experiments with end-to-end training in simulated environments\\nwith applications to few-shot object recognition demonstrated the effectiveness\\nof the proposed algorithm.\\n</summary>\\n    <author>\\n      <name>Liang Zhao</name>\\n    </author>\\n    <author>\\n      <name>Wei Xu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.12344v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.12344v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.01033v1</id>\\n    <updated>2019-04-01T18:01:34Z</updated>\\n    <published>2019-04-01T18:01:34Z</published>\\n    <title>Multitask Soft Option Learning</title>\\n    <summary>  We present Multitask Soft Option Learning (MSOL), a hierarchical multitask\\nframework based on Planning as Inference. MSOL extends the concept of options,\\nusing separate variational posteriors for each task, regularized by a shared\\nprior. This allows fine-tuning of options for new tasks without forgetting\\ntheir learned policies, leading to faster training without reducing the\\nexpressiveness of the hierarchical policy. Additionally, MSOL avoids several\\ninstabilities during training in a multitask setting and provides a natural way\\nto not only learn intra-option policies, but also their terminations. We\\ndemonstrate empirically that MSOL significantly outperforms both hierarchical\\nand flat transfer-learning baselines in challenging multi-task environments.\\n</summary>\\n    <author>\\n      <name>Maximilian Igl</name>\\n    </author>\\n    <author>\\n      <name>Andrew Gambardella</name>\\n    </author>\\n    <author>\\n      <name>Nantas Nardelli</name>\\n    </author>\\n    <author>\\n      <name>N. Siddharth</name>\\n    </author>\\n    <author>\\n      <name>Wendelin B\\xc3\\xb6hmer</name>\\n    </author>\\n    <author>\\n      <name>Shimon Whiteson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.01033v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.01033v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.01139v1</id>\\n    <updated>2019-04-01T23:13:24Z</updated>\\n    <published>2019-04-01T23:13:24Z</published>\\n    <title>Generative predecessor models for sample-efficient imitation learning</title>\\n    <summary>  We propose Generative Predecessor Models for Imitation Learning (GPRIL), a\\nnovel imitation learning algorithm that matches the state-action distribution\\nto the distribution observed in expert demonstrations, using generative models\\nto reason probabilistically about alternative histories of demonstrated states.\\nWe show that this approach allows an agent to learn robust policies using only\\na small number of expert demonstrations and self-supervised interactions with\\nthe environment. We derive this approach from first principles and compare it\\nempirically to a state-of-the-art imitation learning method, showing that it\\noutperforms or matches its performance on two simulated robot manipulation\\ntasks and demonstrate significantly higher sample efficiency by applying the\\nalgorithm on a real robot.\\n</summary>\\n    <author>\\n      <name>Yannick Schroecker</name>\\n    </author>\\n    <author>\\n      <name>Mel Vecerik</name>\\n    </author>\\n    <author>\\n      <name>Jonathan Scholz</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.01139v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.01139v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6436v1</id>\\n    <updated>2012-06-27T19:59:59Z</updated>\\n    <published>2012-06-27T19:59:59Z</published>\\n    <title>Efficient Structured Prediction with Latent Variables for General\\n  Graphical Models</title>\\n    <summary>  In this paper we propose a unified framework for structured prediction with\\nlatent variables which includes hidden conditional random fields and latent\\nstructured support vector machines as special cases. We describe a local\\nentropy approximation for this general formulation using duality, and derive an\\nefficient message passing algorithm that is guaranteed to converge. We\\ndemonstrate its effectiveness in the tasks of image segmentation as well as 3D\\nindoor scene understanding from single images, showing that our approach is\\nsuperior to latent structured support vector machines and hidden conditional\\nrandom fields.\\n</summary>\\n    <author>\\n      <name>Alexander Schwing</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ETH Zurich</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Tamir Hazan</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">TTIC</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Marc Pollefeys</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ETH Zurich</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Raquel Urtasun</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">TTIC</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6436v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6436v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.3699v1</id>\\n    <updated>2013-09-14T21:06:22Z</updated>\\n    <published>2013-09-14T21:06:22Z</published>\\n    <title>Local Support Vector Machines:Formulation and Analysis</title>\\n    <summary>  We provide a formulation for Local Support Vector Machines (LSVMs) that\\ngeneralizes previous formulations, and brings out the explicit connections to\\nlocal polynomial learning used in nonparametric estimation literature. We\\ninvestigate the simplest type of LSVMs called Local Linear Support Vector\\nMachines (LLSVMs). For the first time we establish conditions under which\\nLLSVMs make Bayes consistent predictions at each test point $x_0$. We also\\nestablish rates at which the local risk of LLSVMs converges to the minimum\\nvalue of expected local risk at each point $x_0$. Using stability arguments we\\nestablish generalization error bounds for LLSVMs.\\n</summary>\\n    <author>\\n      <name>Ravi Ganti</name>\\n    </author>\\n    <author>\\n      <name>Alexander Gray</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1309.3699v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.3699v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1510.07526v3</id>\\n    <updated>2015-11-20T15:36:56Z</updated>\\n    <published>2015-10-26T16:03:27Z</published>\\n    <title>Empirical Study on Deep Learning Models for Question Answering</title>\\n    <summary>  In this paper we explore deep learning models with memory component or\\nattention mechanism for question answering task. We combine and compare three\\nmodels, Neural Machine Translation, Neural Turing Machine, and Memory Networks\\nfor a simulated QA data set. This paper is the first one that uses Neural\\nMachine Translation and Neural Turing Machines for solving QA tasks. Our\\nresults suggest that the combination of attention and memory have potential to\\nsolve certain QA problem.\\n</summary>\\n    <author>\\n      <name>Yang Yu</name>\\n    </author>\\n    <author>\\n      <name>Wei Zhang</name>\\n    </author>\\n    <author>\\n      <name>Chung-Wei Hang</name>\\n    </author>\\n    <author>\\n      <name>Bing Xiang</name>\\n    </author>\\n    <author>\\n      <name>Bowen Zhou</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1510.07526v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1510.07526v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.03284v2</id>\\n    <updated>2016-05-13T01:06:09Z</updated>\\n    <published>2016-05-11T05:05:05Z</published>\\n    <title>Machine Comprehension Based on Learning to Rank</title>\\n    <summary>  Machine comprehension plays an essential role in NLP and has been widely\\nexplored with dataset like MCTest. However, this dataset is too simple and too\\nsmall for learning true reasoning abilities. \\\\cite{hermann2015teaching}\\ntherefore release a large scale news article dataset and propose a deep LSTM\\nreader system for machine comprehension. However, the training process is\\nexpensive. We therefore try feature-engineered approach with semantics on the\\nnew dataset to see how traditional machine learning technique and semantics can\\nhelp with machine comprehension. Meanwhile, our proposed L2R reader system\\nachieves good performance with efficiency and less training data.\\n</summary>\\n    <author>\\n      <name>Tian Tian</name>\\n    </author>\\n    <author>\\n      <name>Yuezhang Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1605.03284v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.03284v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.06528v1</id>\\n    <updated>2017-03-19T22:23:01Z</updated>\\n    <published>2017-03-19T22:23:01Z</published>\\n    <title>Universal Consistency and Robustness of Localized Support Vector\\n  Machines</title>\\n    <summary>  The massive amount of available data potentially used to discover patters in\\nmachine learning is a challenge for kernel based algorithms with respect to\\nruntime and storage capacities. Local approaches might help to relieve these\\nissues. From a statistical point of view local approaches allow additionally to\\ndeal with different structures in the data in different ways. This paper\\nanalyses properties of localized kernel based, non-parametric statistical\\nmachine learning methods, in particular of support vector machines (SVMs) and\\nmethods close to them. We will show there that locally learnt kernel methods\\nare universal consistent. Furthermore, we give an upper bound for the maxbias\\nin order to show statistical robustness of the proposed method.\\n</summary>\\n    <author>\\n      <name>Florian Dumpert</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.06528v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.06528v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62G08, 62G20, 62G35\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.05506v1</id>\\n    <updated>2019-04-11T02:53:21Z</updated>\\n    <published>2019-04-11T02:53:21Z</published>\\n    <title>Membership Inference Attacks on Sequence-to-Sequence Models</title>\\n    <summary>  Data privacy is an important issue for \"machine learning as a service\"\\nproviders. We focus on the problem of membership inference attacks: given a\\ndata sample and black-box access to a model\\'s API, determine whether the sample\\nexisted in the model\\'s training data. Our contribution is an investigation of\\nthis problem in the context of sequence-to-sequence models, which are important\\nin applications such as machine translation and video captioning. We define the\\nmembership inference problem for sequence generation, provide an open dataset\\nbased on state-of-the-art machine translation models, and report initial\\nresults on whether these models leak private information against several kinds\\nof membership inference attacks.\\n</summary>\\n    <author>\\n      <name>Sorami Hisamoto</name>\\n    </author>\\n    <author>\\n      <name>Matt Post</name>\\n    </author>\\n    <author>\\n      <name>Kevin Duh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.05506v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.05506v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.03943v1</id>\\n    <updated>2017-09-11T07:23:57Z</updated>\\n    <published>2017-09-11T07:23:57Z</published>\\n    <title>Support Spinor Machine</title>\\n    <summary>  We generalize a support vector machine to a support spinor machine by using\\nthe mathematical structure of wedge product over vector machine in order to\\nextend field from vector field to spinor field. The separated hyperplane is\\nextended to Kolmogorov space in time series data which allow us to extend a\\nstructure of support vector machine to a support tensor machine and a support\\ntensor machine moduli space. Our performance test on support spinor machine is\\ndone over one class classification of end point in physiology state of time\\nseries data after empirical mode analysis and compared with support vector\\nmachine test. We implement algorithm of support spinor machine by using\\nHolo-Hilbert amplitude modulation for fully nonlinear and nonstationary time\\nseries data analysis.\\n</summary>\\n    <author>\\n      <name>Kabin Kanjamapornkul</name>\\n    </author>\\n    <author>\\n      <name>Richard Pin\\xc4\\x8d\\xc3\\xa1k</name>\\n    </author>\\n    <author>\\n      <name>Sanphet Chunithpaisan</name>\\n    </author>\\n    <author>\\n      <name>Erik Barto\\xc5\\xa1</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.dsp.2017.07.023</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.dsp.2017.07.023\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, 12 figures, 6 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Digital Signal Processing 70 (2017) 59-72</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1709.03943v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.03943v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.00681v1</id>\\n    <updated>2018-01-02T15:17:37Z</updated>\\n    <published>2018-01-02T15:17:37Z</published>\\n    <title>A novel improved fuzzy support vector machine based stock price trend\\n  forecast model</title>\\n    <summary>  Application of fuzzy support vector machine in stock price forecast. Support\\nvector machine is a new type of machine learning method proposed in 1990s. It\\ncan deal with classification and regression problems very successfully. Due to\\nthe excellent learning performance of support vector machine, the technology\\nhas become a hot research topic in the field of machine learning, and it has\\nbeen successfully applied in many fields. However, as a new technology, there\\nare many limitations to support vector machines. There is a large amount of\\nfuzzy information in the objective world. If the training of support vector\\nmachine contains noise and fuzzy information, the performance of the support\\nvector machine will become very weak and powerless. As the complexity of many\\nfactors influence the stock price prediction, the prediction results of\\ntraditional support vector machine cannot meet people with precision, this\\nstudy improved the traditional support vector machine fuzzy prediction\\nalgorithm is proposed to improve the new model precision. NASDAQ Stock Market,\\nStandard &amp; Poor\\'s (S&amp;P) Stock market are considered. Novel advanced- fuzzy\\nsupport vector machine (NA-FSVM) is the proposed methodology.\\n</summary>\\n    <author>\\n      <name>Shuheng Wang</name>\\n    </author>\\n    <author>\\n      <name>Guohao Li</name>\\n    </author>\\n    <author>\\n      <name>Yifan Bao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper is accepted by the International Conference on Innovations\\n  in Economic Management and Social Science (IEMSS 2017) and International\\n  Conference on Humanities, Management Engineering and Education Technology\\n  (HMEET 2017)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.00681v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.00681v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/cs/0212034v1</id>\\n    <updated>2002-12-11T19:42:14Z</updated>\\n    <published>2002-12-11T19:42:14Z</published>\\n    <title>Types of Cost in Inductive Concept Learning</title>\\n    <summary>  Inductive concept learning is the task of learning to assign cases to a\\ndiscrete set of classes. In real-world applications of concept learning, there\\nare many different types of cost involved. The majority of the machine learning\\nliterature ignores all types of cost (unless accuracy is interpreted as a type\\nof cost measure). A few papers have investigated the cost of misclassification\\nerrors. Very few papers have examined the many other types of cost. In this\\npaper, we attempt to create a taxonomy of the different types of cost that are\\ninvolved in inductive concept learning. This taxonomy may help to organize the\\nliterature on cost-sensitive learning. We hope that it will inspire researchers\\nto investigate all types of cost in inductive concept learning in more depth.\\n</summary>\\n    <author>\\n      <name>Peter D. Turney</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">National Research Council of Canada</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Workshop on Cost-Sensitive Learning at the Seventeenth\\n  International Conference on Machine Learning, (2000), Stanford University,\\n  California, 15-21</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cs/0212034v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cs/0212034v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6; I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1207.3772v3</id>\\n    <updated>2015-03-14T15:25:54Z</updated>\\n    <published>2012-07-16T19:26:24Z</published>\\n    <title>Surrogate Losses in Passive and Active Learning</title>\\n    <summary>  Active learning is a type of sequential design for supervised machine\\nlearning, in which the learning algorithm sequentially requests the labels of\\nselected instances from a large pool of unlabeled data points. The objective is\\nto produce a classifier of relatively low risk, as measured under the 0-1 loss,\\nideally using fewer label requests than the number of random labeled data\\npoints sufficient to achieve the same. This work investigates the potential\\nuses of surrogate loss functions in the context of active learning.\\nSpecifically, it presents an active learning algorithm based on an arbitrary\\nclassification-calibrated surrogate loss function, along with an analysis of\\nthe number of label requests sufficient for the classifier returned by the\\nalgorithm to achieve a given risk under the 0-1 loss. Interestingly, these\\nresults cannot be obtained by simply optimizing the surrogate risk via active\\nlearning to an extent sufficient to provide a guarantee on the 0-1 loss, as is\\ncommon practice in the analysis of surrogate losses for passive learning. Some\\nof the results have additional implications for the use of surrogate losses in\\npassive learning.\\n</summary>\\n    <author>\\n      <name>Steve Hanneke</name>\\n    </author>\\n    <author>\\n      <name>Liu Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1207.3772v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.3772v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.3877v1</id>\\n    <updated>2013-09-16T09:39:25Z</updated>\\n    <published>2013-09-16T09:39:25Z</published>\\n    <title>A Metric-learning based framework for Support Vector Machines and\\n  Multiple Kernel Learning</title>\\n    <summary>  Most metric learning algorithms, as well as Fisher\\'s Discriminant Analysis\\n(FDA), optimize some cost function of different measures of within-and\\nbetween-class distances. On the other hand, Support Vector Machines(SVMs) and\\nseveral Multiple Kernel Learning (MKL) algorithms are based on the SVM large\\nmargin theory. Recently, SVMs have been analyzed from SVM and metric learning,\\nand to develop new algorithms that build on the strengths of each. Inspired by\\nthe metric learning interpretation of SVM, we develop here a new\\nmetric-learning based SVM framework in which we incorporate metric learning\\nconcepts within SVM. We extend the optimization problem of SVM to include some\\nmeasure of the within-class distance and along the way we develop a new\\nwithin-class distance measure which is appropriate for SVM. In addition, we\\nadopt the same approach for MKL and show that it can be also formulated as a\\nMahalanobis metric learning problem. Our end result is a number of SVM/MKL\\nalgorithms that incorporate metric learning concepts. We experiment with them\\non a set of benchmark datasets and observe important predictive performance\\nimprovements.\\n</summary>\\n    <author>\\n      <name>Huyen Do</name>\\n    </author>\\n    <author>\\n      <name>Alexandros Kalousis</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1309.3877v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.3877v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.06237v1</id>\\n    <updated>2015-01-26T02:28:18Z</updated>\\n    <published>2015-01-26T02:28:18Z</published>\\n    <title>Deep Transductive Semi-supervised Maximum Margin Clustering</title>\\n    <summary>  Semi-supervised clustering is an very important topic in machine learning and\\ncomputer vision. The key challenge of this problem is how to learn a metric,\\nsuch that the instances sharing the same label are more likely close to each\\nother on the embedded space. However, little attention has been paid to learn\\nbetter representations when the data lie on non-linear manifold. Fortunately,\\ndeep learning has led to great success on feature learning recently. Inspired\\nby the advances of deep learning, we propose a deep transductive\\nsemi-supervised maximum margin clustering approach. More specifically, given\\npairwise constraints, we exploit both labeled and unlabeled data to learn a\\nnon-linear mapping under maximum margin framework for clustering analysis.\\nThus, our model unifies transductive learning, feature learning and maximum\\nmargin techniques in the semi-supervised clustering framework. We pretrain the\\ndeep network structure with restricted Boltzmann machines (RBMs) layer by layer\\ngreedily, and optimize our objective function with gradient descent. By\\nchecking the most violated constraints, our approach updates the model\\nparameters through error backpropagation, in which deep features are learned\\nautomatically. The experimental results shows that our model is significantly\\nbetter than the state of the art on semi-supervised clustering.\\n</summary>\\n    <author>\\n      <name>Gang Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1501.06237v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.06237v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T10\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1502.00363v1</id>\\n    <updated>2015-02-02T05:30:44Z</updated>\\n    <published>2015-02-02T05:30:44Z</published>\\n    <title>Iterated Support Vector Machines for Distance Metric Learning</title>\\n    <summary>  Distance metric learning aims to learn from the given training data a valid\\ndistance metric, with which the similarity between data samples can be more\\neffectively evaluated for classification. Metric learning is often formulated\\nas a convex or nonconvex optimization problem, while many existing metric\\nlearning algorithms become inefficient for large scale problems. In this paper,\\nwe formulate metric learning as a kernel classification problem, and solve it\\nby iterated training of support vector machines (SVM). The new formulation is\\neasy to implement, efficient in training, and tractable for large-scale\\nproblems. Two novel metric learning models, namely Positive-semidefinite\\nConstrained Metric Learning (PCML) and Nonnegative-coefficient Constrained\\nMetric Learning (NCML), are developed. Both PCML and NCML can guarantee the\\nglobal optimality of their solutions. Experimental results on UCI dataset\\nclassification, handwritten digit recognition, face verification and person\\nre-identification demonstrate that the proposed metric learning methods achieve\\nhigher classification accuracy than state-of-the-art methods and they are\\nsignificantly more efficient in training.\\n</summary>\\n    <author>\\n      <name>Wangmeng Zuo</name>\\n    </author>\\n    <author>\\n      <name>Faqiang Wang</name>\\n    </author>\\n    <author>\\n      <name>David Zhang</name>\\n    </author>\\n    <author>\\n      <name>Liang Lin</name>\\n    </author>\\n    <author>\\n      <name>Yuchi Huang</name>\\n    </author>\\n    <author>\\n      <name>Deyu Meng</name>\\n    </author>\\n    <author>\\n      <name>Lei Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 10 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1502.00363v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.00363v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.09847v4</id>\\n    <updated>2018-09-18T14:29:40Z</updated>\\n    <published>2017-05-27T17:34:15Z</published>\\n    <title>Lifelong Generative Modeling</title>\\n    <summary>  Lifelong learning is the problem of learning multiple consecutive tasks in a\\nsequential manner where knowledge gained from previous tasks is retained and\\nused for future learning. It is essential towards the development of\\nintelligent machines that can adapt to their surroundings. In this work we\\nfocus on a lifelong learning approach to generative modeling where we\\ncontinuously incorporate newly observed distributions into our learnt model. We\\ndo so through a student-teacher Variational Autoencoder architecture which\\nallows us to learn and preserve all the distributions seen so far without the\\nneed to retain the past data nor the past models. Through the introduction of a\\nnovel cross-model regularizer, inspired by a Bayesian update rule, the student\\nmodel leverages the information learnt by the teacher, which acts as a summary\\nof everything seen till now. The regularizer has the additional benefit of\\nreducing the effect of catastrophic interference that appears when we learn\\nover sequences of distributions. We demonstrate its efficacy in learning\\nsequentially observed distributions as well as its ability to learn a common\\nlatent representation across a complex transfer learning scenario.\\n</summary>\\n    <author>\\n      <name>Jason Ramapuram</name>\\n    </author>\\n    <author>\\n      <name>Magda Gregorova</name>\\n    </author>\\n    <author>\\n      <name>Alexandros Kalousis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages, 30 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.09847v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.09847v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.04465v1</id>\\n    <updated>2017-08-15T11:52:35Z</updated>\\n    <published>2017-08-15T11:52:35Z</published>\\n    <title>Actively Learning what makes a Discrete Sequence Valid</title>\\n    <summary>  Deep learning techniques have been hugely successful for traditional\\nsupervised and unsupervised machine learning problems. In large part, these\\ntechniques solve continuous optimization problems. Recently however, discrete\\ngenerative deep learning models have been successfully used to efficiently\\nsearch high-dimensional discrete spaces. These methods work by representing\\ndiscrete objects as sequences, for which powerful sequence-based deep models\\ncan be employed. Unfortunately, these techniques are significantly hindered by\\nthe fact that these generative models often produce invalid sequences. As a\\nstep towards solving this problem, we propose to learn a deep recurrent\\nvalidator model. Given a partial sequence, our model learns the probability of\\nthat sequence occurring as the beginning of a full valid sequence. Thus this\\nidentifies valid versus invalid sequences and crucially it also provides\\ninsight about how individual sequence elements influence the validity of\\ndiscrete objects. To learn this model we propose an approach inspired by\\nseminal work in Bayesian active learning. On a synthetic dataset, we\\ndemonstrate the ability of our model to distinguish valid and invalid\\nsequences. We believe this is a key step toward learning generative models that\\nfaithfully produce valid discrete objects.\\n</summary>\\n    <author>\\n      <name>David Janz</name>\\n    </author>\\n    <author>\\n      <name>Jos van der Westhuizen</name>\\n    </author>\\n    <author>\\n      <name>Jos\\xc3\\xa9 Miguel Hern\\xc3\\xa1ndez-Lobato</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1708.04465v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.04465v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.03850v1</id>\\n    <updated>2017-10-10T22:57:43Z</updated>\\n    <published>2017-10-10T22:57:43Z</published>\\n    <title>Using Task Descriptions in Lifelong Machine Learning for Improved\\n  Performance and Zero-Shot Transfer</title>\\n    <summary>  Knowledge transfer between tasks can improve the performance of learned\\nmodels, but requires an accurate estimate of the inter-task relationships to\\nidentify the relevant knowledge to transfer. These inter-task relationships are\\ntypically estimated based on training data for each task, which is inefficient\\nin lifelong learning settings where the goal is to learn each consecutive task\\nrapidly from as little data as possible. To reduce this burden, we develop a\\nlifelong learning method based on coupled dictionary learning that utilizes\\nhigh-level task descriptions to model the inter-task relationships. We show\\nthat using task descriptors improves the performance of the learned task\\npolicies, providing both theoretical justification for the benefit and\\nempirical demonstration of the improvement across a variety of learning\\nproblems. Given only the descriptor for a new task, the lifelong learner is\\nalso able to accurately predict a model for the new task through zero-shot\\nlearning using the coupled dictionary, eliminating the need to gather training\\ndata before addressing the task.\\n</summary>\\n    <author>\\n      <name>David Isele</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Rostami</name>\\n    </author>\\n    <author>\\n      <name>Eric Eaton</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">28 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1710.03850v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.03850v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.07759v2</id>\\n    <updated>2018-05-08T08:49:25Z</updated>\\n    <published>2018-04-20T08:04:32Z</published>\\n    <title>A Self-paced Regularization Framework for Partial-Label Learning</title>\\n    <summary>  Partial label learning (PLL) aims to solve the problem where each training\\ninstance is associated with a set of candidate labels, one of which is the\\ncorrect label. Most PLL algorithms try to disambiguate the candidate label set,\\nby either simply treating each candidate label equally or iteratively\\nidentifying the true label. Nonetheless, existing algorithms usually treat all\\nlabels and instances equally, and the complexities of both labels and instances\\nare not taken into consideration during the learning stage. Inspired by the\\nsuccessful application of self-paced learning strategy in machine learning\\nfield, we integrate the self-paced regime into the partial label learning\\nframework and propose a novel Self-Paced Partial-Label Learning (SP-PLL)\\nalgorithm, which could control the learning process to alleviate the problem by\\nranking the priorities of the training examples together with their candidate\\nlabels during each learning iteration. Extensive experiments and comparisons\\nwith other baseline methods demonstrate the effectiveness and robustness of the\\nproposed method.\\n</summary>\\n    <author>\\n      <name>Gengyu Lyu</name>\\n    </author>\\n    <author>\\n      <name>Songhe Feng</name>\\n    </author>\\n    <author>\\n      <name>Congyang Lang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.07759v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.07759v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.08096v1</id>\\n    <updated>2018-05-21T14:55:25Z</updated>\\n    <published>2018-05-21T14:55:25Z</published>\\n    <title>Understanding Self-Paced Learning under Concave Conjugacy Theory</title>\\n    <summary>  By simulating the easy-to-hard learning manners of humans/animals, the\\nlearning regimes called curriculum learning~(CL) and self-paced learning~(SPL)\\nhave been recently investigated and invoked broad interests. However, the\\nintrinsic mechanism for analyzing why such learning regimes can work has not\\nbeen comprehensively investigated. To this issue, this paper proposes a concave\\nconjugacy theory for looking into the insight of CL/SPL. Specifically, by using\\nthis theory, we prove the equivalence of the SPL regime and a latent concave\\nobjective, which is closely related to the known non-convex regularized penalty\\nwidely used in statistics and machine learning. Beyond the previous theory for\\nexplaining CL/SPL insights, this new theoretical framework on one hand\\nfacilitates two direct approaches for designing new SPL models for certain\\ntasks, and on the other hand can help conduct the latent objective of\\nself-paced curriculum learning, which is the advanced version of both CL/SPL\\nand possess advantages of both learning regimes to a certain extent. This\\nfurther facilitates a theoretical understanding for SPCL, instead of only\\nCL/SPL as conventional. Under this theory, we attempt to attain intrinsic\\nlatent objectives of two curriculum forms, the partial order and group\\ncurriculums, which easily follow the theoretical understanding of the\\ncorresponding SPCL regimes.\\n</summary>\\n    <author>\\n      <name>Shiqi Liu</name>\\n    </author>\\n    <author>\\n      <name>Zilu Ma</name>\\n    </author>\\n    <author>\\n      <name>Deyu Meng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.08096v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08096v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.05413v2</id>\\n    <updated>2018-07-29T12:27:31Z</updated>\\n    <published>2018-06-14T08:46:36Z</published>\\n    <title>Learning Dynamics of Linear Denoising Autoencoders</title>\\n    <summary>  Denoising autoencoders (DAEs) have proven useful for unsupervised\\nrepresentation learning, but a thorough theoretical understanding is still\\nlacking of how the input noise influences learning. Here we develop theory for\\nhow noise influences learning in DAEs. By focusing on linear DAEs, we are able\\nto derive analytic expressions that exactly describe their learning dynamics.\\nWe verify our theoretical predictions with simulations as well as experiments\\non MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise\\nallows DAEs to ignore low variance directions in the inputs while learning to\\nreconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs\\nto standard regularised autoencoders, we show that noise has a similar\\nregularisation effect to weight decay, but with faster training dynamics. We\\nalso show that our theoretical predictions approximate learning dynamics on\\nreal-world data and qualitatively match observed dynamics in nonlinear DAEs.\\n</summary>\\n    <author>\\n      <name>Arnu Pretorius</name>\\n    </author>\\n    <author>\\n      <name>Steve Kroon</name>\\n    </author>\\n    <author>\\n      <name>Herman Kamper</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 7 figures, accepted at the 35th International Conference on\\n  Machine Learning (ICML) 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.05413v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.05413v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.07218v1</id>\\n    <updated>2018-10-16T18:25:17Z</updated>\\n    <published>2018-10-16T18:25:17Z</published>\\n    <title>Incremental Few-Shot Learning with Attention Attractor Networks</title>\\n    <summary>  Machine learning classifiers are often trained to recognize a set of\\npre-defined classes. However, in many real applications, it is often desirable\\nto have the flexibility of learning additional concepts, without re-training on\\nthe full training set. This paper addresses this problem, incremental few-shot\\nlearning, where a regular classification network has already been trained to\\nrecognize a set of base classes; and several extra novel classes are being\\nconsidered, each with only a few labeled examples. After learning the novel\\nclasses, the model is then evaluated on the overall performance of both base\\nand novel classes. To this end, we propose a meta-learning model, the Attention\\nAttractor Network, which regularizes the learning of novel classes. In each\\nepisode, we train a set of new weights to recognize novel classes until they\\nconverge, and we show that the technique of recurrent back-propagation can\\nback-propagate through the optimization process and facilitate the learning of\\nthe attractor network regularizer. We demonstrate that the learned attractor\\nnetwork can recognize novel classes while remembering old classes without the\\nneed to review the original training set, outperforming baselines that do not\\nrely on an iterative optimization process.\\n</summary>\\n    <author>\\n      <name>Mengye Ren</name>\\n    </author>\\n    <author>\\n      <name>Renjie Liao</name>\\n    </author>\\n    <author>\\n      <name>Ethan Fetaya</name>\\n    </author>\\n    <author>\\n      <name>Richard S. Zemel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.07218v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.07218v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.08929v1</id>\\n    <updated>2018-11-21T20:03:05Z</updated>\\n    <published>2018-11-21T20:03:05Z</published>\\n    <title>Self-Adversarially Learned Bayesian Sampling</title>\\n    <summary>  Scalable Bayesian sampling is playing an important role in modern machine\\nlearning, especially in the fast-developed unsupervised-(deep)-learning models.\\nWhile tremendous progresses have been achieved via scalable Bayesian sampling\\nsuch as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient\\ndescent (SVGD), the generated samples are typically highly correlated.\\nMoreover, their sample-generation processes are often criticized to be\\ninefficient. In this paper, we propose a novel self-adversarial learning\\nframework that automatically learns a conditional generator to mimic the\\nbehavior of a Markov kernel (transition kernel). High-quality samples can be\\nefficiently generated by direct forward passes though a learned generator. Most\\nimportantly, the learning process adopts a self-learning paradigm, requiring no\\ninformation on existing Markov kernels, e.g., knowledge of how to draw samples\\nfrom them. Specifically, our framework learns to use current samples, either\\nfrom the generator or pre-provided training data, to update the generator such\\nthat the generated samples progressively approach a target distribution, thus\\nit is called self-learning. Experiments on both synthetic and real datasets\\nverify advantages of our framework, outperforming related methods in terms of\\nboth sampling efficiency and sample quality.\\n</summary>\\n    <author>\\n      <name>Yang Zhao</name>\\n    </author>\\n    <author>\\n      <name>Jianyi Zhang</name>\\n    </author>\\n    <author>\\n      <name>Changyou Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">AAAI 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.08929v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.08929v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.05944v1</id>\\n    <updated>2018-12-14T14:07:36Z</updated>\\n    <published>2018-12-14T14:07:36Z</published>\\n    <title>A Tutorial on Distance Metric Learning: Mathematical Foundations,\\n  Algorithms and Software</title>\\n    <summary>  This paper describes the discipline of distance metric learning, a branch of\\nmachine learning that aims to learn distances from the data. Distance metric\\nlearning can be useful to improve similarity learning algorithms, and also has\\napplications in dimensionality reduction. We describe the distance metric\\nlearning problem and analyze its main mathematical foundations. We discuss some\\nof the most popular distance metric learning techniques used in classification,\\nshowing their goals and the required information to understand and use them.\\nFurthermore, we present a Python package that collects a set of 17 distance\\nmetric learning techniques explained in this paper, with some experiments to\\nevaluate the performance of the different algorithms. Finally, we discuss\\nseveral possibilities of future work in this topic.\\n</summary>\\n    <author>\\n      <name>Juan Luis Su\\xc3\\xa1rez</name>\\n    </author>\\n    <author>\\n      <name>Salvador Garc\\xc3\\xada</name>\\n    </author>\\n    <author>\\n      <name>Francisco Herrera</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.05944v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.05944v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.03802v1</id>\\n    <updated>2019-01-12T05:32:05Z</updated>\\n    <published>2019-01-12T05:32:05Z</published>\\n    <title>ALiPy: Active Learning in Python</title>\\n    <summary>  Supervised machine learning methods usually require a large set of labeled\\nexamples for model training. However, in many real applications, there are\\nplentiful unlabeled data but limited labeled data; and the acquisition of\\nlabels is costly. Active learning (AL) reduces the labeling cost by iteratively\\nselecting the most valuable data to query their labels from the annotator. This\\narticle introduces a Python toobox ALiPy for active learning. ALiPy provides a\\nmodule based implementation of active learning framework, which allows users to\\nconveniently evaluate, compare and analyze the performance of active learning\\nmethods. In the toolbox, multiple options are available for each component of\\nthe learning framework, including data process, active selection, label query,\\nresults visualization, etc. In addition to the implementations of more than 20\\nstate-of-the-art active learning algorithms, ALiPy also supports users to\\neasily configure and implement their own approaches under different active\\nlearning settings, such as AL for multi-label data, AL with noisy annotators,\\nAL with different costs and so on. The toolbox is well-documented and\\nopen-source on Github, and can be easily installed through PyPI.\\n</summary>\\n    <author>\\n      <name>Ying-Peng Tang</name>\\n    </author>\\n    <author>\\n      <name>Guo-Xiang Li</name>\\n    </author>\\n    <author>\\n      <name>Sheng-Jun Huang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.03802v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.03802v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.08098v1</id>\\n    <updated>2019-01-23T19:43:59Z</updated>\\n    <published>2019-01-23T19:43:59Z</published>\\n    <title>Deep Mean Functions for Meta-Learning in Gaussian Processes</title>\\n    <summary>  Fitting machine learning models in the low-data limit is challenging. The\\nmain challenge is to obtain suitable prior knowledge and encode it into the\\nmodel, for instance in the form of a Gaussian process prior. Recent advances in\\nmeta-learning offer powerful methods for extracting such prior knowledge from\\ndata acquired in related tasks. When it comes to meta-learning in Gaussian\\nprocess models, approaches in this setting have mostly focused on learning the\\nkernel function of the prior, but not on learning its mean function. In this\\nwork, we propose to parameterize the mean function of a Gaussian process with a\\ndeep neural network and train it with a meta-learning procedure. We present\\nanalytical and empirical evidence that mean function learning can be superior\\nto kernel learning alone, particularly if data is scarce.\\n</summary>\\n    <author>\\n      <name>Vincent Fortuin</name>\\n    </author>\\n    <author>\\n      <name>Gunnar R\\xc3\\xa4tsch</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.08098v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.08098v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.12328v1</id>\\n    <updated>2019-03-29T02:27:54Z</updated>\\n    <published>2019-03-29T02:27:54Z</published>\\n    <title>Improved Reinforcement Learning with Curriculum</title>\\n    <summary>  Humans tend to learn complex abstract concepts faster if examples are\\npresented in a structured manner. For instance, when learning how to play a\\nboard game, usually one of the first concepts learned is how the game ends,\\ni.e. the actions that lead to a terminal state (win, lose or draw). The\\nadvantage of learning end-games first is that once the actions which lead to a\\nterminal state are understood, it becomes possible to incrementally learn the\\nconsequences of actions that are further away from a terminal state - we call\\nthis an end-game-first curriculum. Currently the state-of-the-art machine\\nlearning player for general board games, AlphaZero by Google DeepMind, does not\\nemploy a structured training curriculum; instead learning from the entire game\\nat all times. By employing an end-game-first training curriculum to train an\\nAlphaZero inspired player, we empirically show that the rate of learning of an\\nartificial player can be improved during the early stages of training when\\ncompared to a player not using a training curriculum.\\n</summary>\\n    <author>\\n      <name>Joseph West</name>\\n    </author>\\n    <author>\\n      <name>Frederic Maire</name>\\n    </author>\\n    <author>\\n      <name>Cameron Browne</name>\\n    </author>\\n    <author>\\n      <name>Simon Denman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Draft prior to submission to IEEE Trans on Games</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.12328v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.12328v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.03596v1</id>\\n    <updated>2018-02-10T14:18:08Z</updated>\\n    <published>2018-02-10T14:18:08Z</published>\\n    <title>Deep Meta-Learning: Learning to Learn in the Concept Space</title>\\n    <summary>  Few-shot learning remains challenging for meta-learning that learns a\\nlearning algorithm (meta-learner) from many related tasks. In this work, we\\nargue that this is due to the lack of a good representation for meta-learning,\\nand propose deep meta-learning to integrate the representation power of deep\\nlearning into meta-learning. The framework is composed of three modules, a\\nconcept generator, a meta-learner, and a concept discriminator, which are\\nlearned jointly. The concept generator, e.g. a deep residual net, extracts a\\nrepresentation for each instance that captures its high-level concept, on which\\nthe meta-learner performs few-shot learning, and the concept discriminator\\nrecognizes the concepts. By learning to learn in the concept space rather than\\nin the complicated instance space, deep meta-learning can substantially improve\\nvanilla meta-learning, which is demonstrated on various few-shot image\\nrecognition problems. For example, on 5-way-1-shot image recognition on\\nCIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to\\n58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%,\\nand improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%,\\nrespectively.\\n</summary>\\n    <author>\\n      <name>Fengwei Zhou</name>\\n    </author>\\n    <author>\\n      <name>Bin Wu</name>\\n    </author>\\n    <author>\\n      <name>Zhenguo Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.03596v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.03596v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.0824v1</id>\\n    <updated>2012-09-04T22:51:32Z</updated>\\n    <published>2012-09-04T22:51:32Z</published>\\n    <title>Proof of a Combinatorial Conjecture Coming from the PAC-Bayesian Machine\\n  Learning Theory</title>\\n    <summary>  We give a proof of a conjecture of A. Lacasse in his doctoral thesis which\\nhas applications in machine learning algorithms. The proof relies on some\\ninteresting binomial sums identities introduced by Abel (1839), and on their\\ngeneralization to the multinomial case by Hurwitz (1902).\\n</summary>\\n    <author>\\n      <name>Malik Younsi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1209.0824v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.0824v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1302.3668v1</id>\\n    <updated>2013-02-15T03:54:53Z</updated>\\n    <published>2013-02-15T03:54:53Z</published>\\n    <title>Bio-inspired data mining: Treating malware signatures as biosequences</title>\\n    <summary>  The application of machine learning to bioinformatics problems is well\\nestablished. Less well understood is the application of bioinformatics\\ntechniques to machine learning and, in particular, the representation of\\nnon-biological data as biosequences. The aim of this paper is to explore the\\neffects of giving amino acid representation to problematic machine learning\\ndata and to evaluate the benefits of supplementing traditional machine learning\\nwith bioinformatics tools and techniques. The signatures of 60 computer viruses\\nand 60 computer worms were converted into amino acid representations and first\\nmultiply aligned separately to identify conserved regions across different\\nfamilies within each class (virus and worm). This was followed by a second\\nalignment of all 120 aligned signatures together so that non-conserved regions\\nwere identified prior to input to a number of machine learning techniques.\\nDifferences in length between virus and worm signatures after the first\\nalignment were resolved by the second alignment. Our first set of experiments\\nindicates that representing computer malware signatures as amino acid sequences\\nfollowed by alignment leads to greater classification and prediction accuracy.\\nOur second set of experiments indicates that checking the results of data\\nmining from artificial virus and worm data against known proteins can lead to\\ngeneralizations being made from the domain of naturally occurring proteins to\\nmalware signatures. However, further work is needed to determine the advantages\\nand disadvantages of different representations and sequence alignment methods\\nfor handling problematic machine learning data.\\n</summary>\\n    <author>\\n      <name>Ajit Narayanan</name>\\n    </author>\\n    <author>\\n      <name>Yi Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1302.3668v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1302.3668v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1208.0806v1</id>\\n    <updated>2012-08-03T18:01:52Z</updated>\\n    <published>2012-08-03T18:01:52Z</published>\\n    <title>Cross-conformal predictors</title>\\n    <summary>  This note introduces the method of cross-conformal prediction, which is a\\nhybrid of the methods of inductive conformal prediction and cross-validation,\\nand studies its validity and predictive efficiency empirically.\\n</summary>\\n    <author>\\n      <name>Vladimir Vovk</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 2 figures, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1208.0806v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1208.0806v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62G15\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1301.4917v1</id>\\n    <updated>2013-01-21T16:27:17Z</updated>\\n    <published>2013-01-21T16:27:17Z</published>\\n    <title>Dirichlet draws are sparse with high probability</title>\\n    <summary>  This note provides an elementary proof of the folklore fact that draws from a\\nDirichlet distribution (with parameters less than 1) are typically sparse (most\\ncoordinates are small).\\n</summary>\\n    <author>\\n      <name>Matus Telgarsky</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1301.4917v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.4917v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.1674v1</id>\\n    <updated>2013-07-05T17:39:40Z</updated>\\n    <published>2013-07-05T17:39:40Z</published>\\n    <title>Stochastic Optimization of PCA with Capped MSG</title>\\n    <summary>  We study PCA as a stochastic optimization problem and propose a novel\\nstochastic approximation algorithm which we refer to as \"Matrix Stochastic\\nGradient\" (MSG), as well as a practical variant, Capped MSG. We study the\\nmethod both theoretically and empirically.\\n</summary>\\n    <author>\\n      <name>Raman Arora</name>\\n    </author>\\n    <author>\\n      <name>Andrew Cotter</name>\\n    </author>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1307.1674v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.1674v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1503.00036v2</id>\\n    <updated>2015-04-14T22:55:08Z</updated>\\n    <published>2015-02-27T23:50:22Z</published>\\n    <title>Norm-Based Capacity Control in Neural Networks</title>\\n    <summary>  We investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.\\n</summary>\\n    <author>\\n      <name>Behnam Neyshabur</name>\\n    </author>\\n    <author>\\n      <name>Ryota Tomioka</name>\\n    </author>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">29 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1503.00036v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1503.00036v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.01703v1</id>\\n    <updated>2016-05-05T19:34:08Z</updated>\\n    <published>2016-05-05T19:34:08Z</published>\\n    <title>A note on adjusting $R^2$ for using with cross-validation</title>\\n    <summary>  We show how to adjust the coefficient of determination ($R^2$) when used for\\nmeasuring predictive accuracy via leave-one-out cross-validation.\\n</summary>\\n    <author>\\n      <name>Indre Zliobaite</name>\\n    </author>\\n    <author>\\n      <name>Nikolaj Tatti</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1605.01703v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.01703v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.03862v1</id>\\n    <updated>2016-09-10T21:33:09Z</updated>\\n    <published>2016-09-10T21:33:09Z</published>\\n    <title>Induction and physical theory formation by Machine Learning</title>\\n    <summary>  Machine learning presents a general, systematic framework for the generation\\nof formal theoretical models for physical description and prediction.\\nTentatively standard linear modeling techniques are reviewed; followed by a\\nbrief discussion of generalizations to deep forward networks for approximating\\nnonlinear phenomena.\\n</summary>\\n    <author>\\n      <name>Alexander Svozil</name>\\n    </author>\\n    <author>\\n      <name>Karl Svozil</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1609.03862v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.03862v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.gen-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.gen-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.01256v2</id>\\n    <updated>2017-08-22T15:18:12Z</updated>\\n    <published>2016-10-05T02:16:48Z</published>\\n    <title>On the Safety of Machine Learning: Cyber-Physical Systems, Decision\\n  Sciences, and Data Products</title>\\n    <summary>  Machine learning algorithms increasingly influence our decisions and interact\\nwith us in all parts of our daily lives. Therefore, just as we consider the\\nsafety of power plants, highways, and a variety of other engineered\\nsocio-technical systems, we must also take into account the safety of systems\\ninvolving machine learning. Heretofore, the definition of safety has not been\\nformalized in a machine learning context. In this paper, we do so by defining\\nmachine learning safety in terms of risk, epistemic uncertainty, and the harm\\nincurred by unwanted outcomes. We then use this definition to examine safety in\\nall sorts of applications in cyber-physical systems, decision sciences, and\\ndata products. We find that the foundational principle of modern statistical\\nmachine learning, empirical risk minimization, is not always a sufficient\\nobjective. Finally, we discuss how four different categories of strategies for\\nachieving safety in engineering, including inherently safe design, safety\\nreserves, safe fail, and procedural safeguards can be mapped to a machine\\nlearning context. We then discuss example techniques that can be adopted in\\neach category, such as considering interpretability and causality of predictive\\nmodels, objective functions beyond expected prediction accuracy, human\\ninvolvement for labeling difficult or rare examples, and user experience design\\nof software and open data.\\n</summary>\\n    <author>\\n      <name>Kush R. Varshney</name>\\n    </author>\\n    <author>\\n      <name>Homa Alemzadeh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Big Data, 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.01256v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.01256v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.02696v1</id>\\n    <updated>2016-12-08T15:25:35Z</updated>\\n    <published>2016-12-08T15:25:35Z</published>\\n    <title>A note on the triangle inequality for the Jaccard distance</title>\\n    <summary>  Two simple proofs of the triangle inequality for the Jaccard distance in\\nterms of nonnegative, monotone, submodular functions are given and discussed.\\n</summary>\\n    <author>\\n      <name>Sven Kosub</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.02696v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.02696v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.04077v3</id>\\n    <updated>2017-01-27T09:11:59Z</updated>\\n    <published>2017-01-15T17:06:08Z</published>\\n    <title>Breeding electric zebras in the fields of Medicine</title>\\n    <summary>  A few notes on the use of machine learning in medicine and the related\\nunintended consequences.\\n</summary>\\n    <author>\\n      <name>Federico Cabitza</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Work-in-progress</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1701.04077v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.04077v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.04327v1</id>\\n    <updated>2017-07-13T21:17:14Z</updated>\\n    <published>2017-07-13T21:17:14Z</published>\\n    <title>Human-Level Intelligence or Animal-Like Abilities?</title>\\n    <summary>  The vision systems of the eagle and the snake outperform everything that we\\ncan make in the laboratory, but snakes and eagles cannot build an eyeglass or a\\ntelescope or a microscope. (Judea Pearl)\\n</summary>\\n    <author>\\n      <name>Adnan Darwiche</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.04327v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.04327v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.03038v1</id>\\n    <updated>2017-11-08T16:30:56Z</updated>\\n    <published>2017-11-08T16:30:56Z</published>\\n    <title>Recency-weighted Markovian inference</title>\\n    <summary>  We describe a Markov latent state space (MLSS) model, where the latent state\\ndistribution is a decaying mixture over multiple past states. We present a\\nsimple sampling algorithm that allows to approximate such high-order MLSS with\\nfixed time and memory costs.\\n</summary>\\n    <author>\\n      <name>Kristjan Kalm</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1711.03038v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.03038v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.03164v1</id>\\n    <updated>2018-01-09T22:33:51Z</updated>\\n    <published>2018-01-09T22:33:51Z</published>\\n    <title>Paranom: A Parallel Anomaly Dataset Generator</title>\\n    <summary>  In this paper, we present Paranom, a parallel anomaly dataset generator. We\\ndiscuss its design and provide brief experimental results demonstrating its\\nusefulness in improving the classification correctness of LSTM-AD, a\\nstate-of-the-art anomaly detection model.\\n</summary>\\n    <author>\\n      <name>Justin Gottschlich</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1801.03164v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.03164v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.08021v2</id>\\n    <updated>2018-10-02T04:05:22Z</updated>\\n    <published>2018-02-22T13:02:53Z</published>\\n    <title>SparCML: High-Performance Sparse Communication for Machine Learning</title>\\n    <summary>  One of the main drivers behind the rapid recent advances in machine learning\\nhas been the availability of efficient system support. Despite existing\\nprogress, scaling compute-intensive machine learning workloads to a large\\nnumber of compute nodes is still a challenging task. In this paper, we address\\nthis challenge, by proposing SparCML, a general, scalable communication layer\\nfor machine learning applications. SparCML is built on the observation that\\nmany distributed machine learning algorithms either have naturally sparse\\ncommunication patterns, or have updates which can be sparsified in a structured\\nway for improved performance, without loss of convergence or accuracy. To\\nexploit this insight, we analyze, design, and implement a set of\\ncommunication-efficient protocols for sparse input data, in conjunction with\\nefficient machine learning algorithms which can leverage these primitives. Our\\ncommunication protocols generalize standard collective operations, by allowing\\nprocesses to contribute sparse input data vectors, of heterogeneous sizes. Our\\ngeneric communication layer is enriched with additional features, such as\\nsupport for non-blocking (asynchronous) operations and support for\\nlow-precision data representations. We validate our algorithmic results\\nexperimentally on a range of large-scale machine learning applications and\\ntarget architectures, showing that we can leverage sparsity for\\norder-of-magnitude runtime savings, compared to existing methods and\\nframeworks.\\n</summary>\\n    <author>\\n      <name>C\\xc3\\xa9dric Renggli</name>\\n    </author>\\n    <author>\\n      <name>Dan Alistarh</name>\\n    </author>\\n    <author>\\n      <name>Torsten Hoefler</name>\\n    </author>\\n    <author>\\n      <name>Mehdi Aghagolzadeh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.08021v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.08021v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.00679v1</id>\\n    <updated>2018-03-02T01:27:58Z</updated>\\n    <published>2018-03-02T01:27:58Z</published>\\n    <title>Random perturbation and matrix sparsification and completion</title>\\n    <summary>  We discuss general perturbation inequalities when the perturbation is random.\\nAs applications, we obtain several new results concerning two important\\nproblems: matrix sparsification and matrix completion.\\n</summary>\\n    <author>\\n      <name>Sean O\\'Rourke</name>\\n    </author>\\n    <author>\\n      <name>Van Vu</name>\\n    </author>\\n    <author>\\n      <name>Ke Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">20 pages. Conference version</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.00679v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.00679v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.04779v1</id>\\n    <updated>2018-03-09T21:02:25Z</updated>\\n    <published>2018-03-09T21:02:25Z</published>\\n    <title>Hybrid Forecasting of Chaotic Processes: Using Machine Learning in\\n  Conjunction with a Knowledge-Based Model</title>\\n    <summary>  A model-based approach to forecasting chaotic dynamical systems utilizes\\nknowledge of the physical processes governing the dynamics to build an\\napproximate mathematical model of the system. In contrast, machine learning\\ntechniques have demonstrated promising results for forecasting chaotic systems\\npurely from past time series measurements of system state variables (training\\ndata), without prior knowledge of the system dynamics. The motivation for this\\npaper is the potential of machine learning for filling in the gaps in our\\nunderlying mechanistic knowledge that cause widely-used knowledge-based models\\nto be inaccurate. Thus we here propose a general method that leverages the\\nadvantages of these two approaches by combining a knowledge-based model and a\\nmachine learning technique to build a hybrid forecasting scheme. Potential\\napplications for such an approach are numerous (e.g., improving weather\\nforecasting). We demonstrate and test the utility of this approach using a\\nparticular illustrative version of a machine learning known as reservoir\\ncomputing, and we apply the resulting hybrid forecaster to a low-dimensional\\nchaotic system, as well as to a high-dimensional spatiotemporal chaotic system.\\nThese tests yield extremely promising results in that our hybrid technique is\\nable to accurately predict for a much longer period of time than either its\\nmachine-learning component or its model-based component alone.\\n</summary>\\n    <author>\\n      <name>Jaideep Pathak</name>\\n    </author>\\n    <author>\\n      <name>Alexander Wikner</name>\\n    </author>\\n    <author>\\n      <name>Rebeckah Fussell</name>\\n    </author>\\n    <author>\\n      <name>Sarthak Chandra</name>\\n    </author>\\n    <author>\\n      <name>Brian Hunt</name>\\n    </author>\\n    <author>\\n      <name>Michelle Girvan</name>\\n    </author>\\n    <author>\\n      <name>Edward Ott</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.5028373</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.5028373\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1803.04779v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.04779v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"nlin.CD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.00503v1</id>\\n    <updated>2018-04-30T03:49:22Z</updated>\\n    <published>2018-04-30T03:49:22Z</published>\\n    <title>Machine Learning for Exam Triage</title>\\n    <summary>  In this project, we extend the state-of-the-art CheXNet (Rajpurkar et al.\\n[2017]) by making use of the additional non-image features in the dataset. Our\\nmodel produced better AUROC scores than the original CheXNet.\\n</summary>\\n    <author>\\n      <name>Xinyu Guan</name>\\n    </author>\\n    <author>\\n      <name>Jessica Lee</name>\\n    </author>\\n    <author>\\n      <name>Peter Wu</name>\\n    </author>\\n    <author>\\n      <name>Yue Wu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.00503v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.00503v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.00875v1</id>\\n    <updated>2018-06-03T21:00:01Z</updated>\\n    <published>2018-06-03T21:00:01Z</published>\\n    <title>Deploying Customized Data Representation and Approximate Computing in\\n  Machine Learning Applications</title>\\n    <summary>  Major advancements in building general-purpose and customized hardware have\\nbeen one of the key enablers of versatility and pervasiveness of machine\\nlearning models such as deep neural networks. To sustain this ubiquitous\\ndeployment of machine learning models and cope with their computational and\\nstorage complexity, several solutions such as low-precision representation of\\nmodel parameters using fixed-point representation and deploying approximate\\narithmetic operations have been employed. Studying the potency of such\\nsolutions in different applications requires integrating them into existing\\nmachine learning frameworks for high-level simulations as well as implementing\\nthem in hardware to analyze their effects on power/energy dissipation,\\nthroughput, and chip area. Lop is a library for design space exploration that\\nbridges the gap between machine learning and efficient hardware realization. It\\ncomprises a Python module, which can be integrated with some of the existing\\nmachine learning frameworks and implements various customizable data\\nrepresentations including fixed-point and floating-point as well as approximate\\narithmetic operations.Furthermore, it includes a highly-parameterized Scala\\nmodule, which allows synthesizing hardware based on the said data\\nrepresentations and arithmetic operations. Lop allows researchers and designers\\nto quickly compare quality of their models using various data representations\\nand arithmetic operations in Python and contrast the hardware cost of viable\\nrepresentations by synthesizing them on their target platforms (e.g., FPGA or\\nASIC). To the best of our knowledge, Lop is the first library that allows both\\nsoftware simulation and hardware realization using customized data\\nrepresentations and approximate computing techniques.\\n</summary>\\n    <author>\\n      <name>Mahdi Nazemi</name>\\n    </author>\\n    <author>\\n      <name>Massoud Pedram</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.00875v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.00875v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.04270v2</id>\\n    <updated>2019-01-30T21:18:38Z</updated>\\n    <published>2018-07-10T16:43:30Z</published>\\n    <title>Attack and defence in cellular decision-making: lessons from machine\\n  learning</title>\\n    <summary>  Machine learning algorithms are sensitive to meaningless (or \"adversarial\")\\nperturbations. This is reminiscent of cellular decision-making where ligands\\n(called \"antagonists\") prevent correct signalling, like in early immune\\nrecognition. We draw a formal analogy between neural networks used in machine\\nlearning and models of cellular decision-making (adaptive proofreading). We\\napply attacks from machine learning to simple decision-making models, and show\\nexplicitly the correspondence to antagonism by weakly bound ligands. Such\\nantagonism is absent in more nonlinear models, which inspired us to implement a\\nbiomimetic defence in neural networks filtering out adversarial perturbations.\\nWe then apply a gradient-descent approach from machine learning to different\\ncellular decision-making models, and we reveal the existence of two regimes\\ncharacterized by the presence or absence of a critical point. The critical\\npoint causes the strongest antagonists to lie close to the threshold. This is\\nvalidated in the loss landscapes of robust neural networks and cellular\\ndecision-making models, and observed experimentally for immune cells. For both\\nregimes, we explain how associated defence mechanisms shape the geometry of the\\nloss landscape, and why different adversarial attacks are effective in\\ndifferent regimes. Our work connects evolved cellular decision-making to\\nmachine learning, and motivates the design of a general theory of adversarial\\nperturbations, both for in vivo and in silico systems.\\n</summary>\\n    <author>\\n      <name>Thomas J. Rademaker</name>\\n    </author>\\n    <author>\\n      <name>Emmanuel Bengio</name>\\n    </author>\\n    <author>\\n      <name>Paul Fran\\xc3\\xa7ois</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.04270v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.04270v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.bio-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.bio-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.OT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.09842v1</id>\\n    <updated>2018-11-24T14:21:50Z</updated>\\n    <published>2018-11-24T14:21:50Z</published>\\n    <title>OCLEP+: One-class Anomaly and Intrusion Detection Using Minimal Length\\n  of Emerging Patterns</title>\\n    <summary>  This paper presents a method called One-class Classification using Length\\nstatistics of Emerging Patterns Plus (OCLEP+).\\n</summary>\\n    <author>\\n      <name>Guozhu Dong</name>\\n    </author>\\n    <author>\\n      <name>Sai Kiran Pentukar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.09842v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.09842v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.02904v1</id>\\n    <updated>2019-02-08T01:15:09Z</updated>\\n    <published>2019-02-08T01:15:09Z</published>\\n    <title>Modeling Heterogeneity in Mode-Switching Behavior Under a\\n  Mobility-on-Demand Transit System: An Interpretable Machine Learning Approach</title>\\n    <summary>  Recent years have witnessed an increased focus on interpretability and the\\nuse of machine learning to inform policy analysis and decision making. This\\npaper applies machine learning to examine travel behavior and, in particular,\\non modeling changes in travel modes when individuals are presented with a novel\\n(on-demand) mobility option. It addresses the following question: Can machine\\nlearning be applied to model individual taste heterogeneity (preference\\nheterogeneity for travel modes and response heterogeneity to travel attributes)\\nin travel mode choice? This paper first develops a high-accuracy classifier to\\npredict mode-switching behavior under a hypothetical Mobility-on-Demand Transit\\nsystem (i.e., stated-preference data), which represents the case study\\nunderlying this research. We show that this classifier naturally captures\\nindividual heterogeneity available in the data. Moreover, the paper derives\\ninsights on heterogeneous switching behaviors through the generation of\\nmarginal effects and elasticities by current travel mode, partial dependence\\nplots, and individual conditional expectation plots. The paper also proposes\\ntwo new model-agnostic interpretation tools for machine learning, i.e.,\\nconditional partial dependence plots and conditional individual partial\\ndependence plots, specifically designed to examine response heterogeneity. The\\nresults on the case study show that the machine-learning classifier, together\\nwith model-agnostic interpretation tools, provides valuable insights on travel\\nmode switching behavior for different individuals and population segments. For\\nexample, the existing drivers are more sensitive to additional pickups than\\npeople using other travel modes, and current transit users are generally\\nwilling to share rides but reluctant to take any additional transfers.\\n</summary>\\n    <author>\\n      <name>Xilei Zhao</name>\\n    </author>\\n    <author>\\n      <name>Xiang Yan</name>\\n    </author>\\n    <author>\\n      <name>Pascal Van Hentenryck</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.02904v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.02904v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.07020v1</id>\\n    <updated>2019-03-17T03:02:32Z</updated>\\n    <published>2019-03-17T03:02:32Z</published>\\n    <title>Zeno++: robust asynchronous SGD with arbitrary number of Byzantine\\n  workers</title>\\n    <summary>  We propose Zeno++, a new robust asynchronous synchronous Stochastic Gradient\\nDescent~(SGD) under a general Byzantine failure model with unbounded number of\\nByzantine workers.\\n</summary>\\n    <author>\\n      <name>Cong Xie</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1805.10032</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.07020v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.07020v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.4635v1</id>\\n    <updated>2012-06-18T15:14:57Z</updated>\\n    <published>2012-06-18T15:14:57Z</published>\\n    <title>Deep Mixtures of Factor Analysers</title>\\n    <summary>  An efficient way to learn deep density models that have many layers of latent\\nvariables is to learn one layer at a time using a model that has only one layer\\nof latent variables. After learning each layer, samples from the posterior\\ndistributions for that layer are used as training data for learning the next\\nlayer. This approach is commonly used with Restricted Boltzmann Machines, which\\nare undirected graphical models with a single hidden layer, but it can also be\\nused with Mixtures of Factor Analysers (MFAs) which are directed graphical\\nmodels. In this paper, we present a greedy layer-wise learning algorithm for\\nDeep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted\\nto an equivalent shallow MFA by multiplying together the factor loading\\nmatrices at different levels, learning and inference are much more efficient in\\na DMFA and the sharing of each lower-level factor loading matrix by many\\ndifferent higher level MFAs prevents overfitting. We demonstrate empirically\\nthat DMFAs learn better density models than both MFAs and two types of\\nRestricted Boltzmann Machine on a wide variety of datasets.\\n</summary>\\n    <author>\\n      <name>Yichuan Tang</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Toronto</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Ruslan Salakhutdinov</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Toronto</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Geoffrey Hinton</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Toronto</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.4635v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.4635v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.6370v2</id>\\n    <updated>2017-03-01T14:07:26Z</updated>\\n    <published>2014-11-24T07:28:51Z</published>\\n    <title>Big Learning with Bayesian Methods</title>\\n    <summary>  Explosive growth in data and availability of cheap computing resources have\\nsparked increasing interest in Big learning, an emerging subfield that studies\\nscalable machine learning algorithms, systems, and applications with Big Data.\\nBayesian methods represent one important class of statistic methods for machine\\nlearning, with substantial recent developments on adaptive, flexible and\\nscalable Bayesian learning. This article provides a survey of the recent\\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\\nincluding nonparametric Bayesian methods for adaptively inferring model\\ncomplexity, regularized Bayesian inference for improving the flexibility via\\nposterior regularization, and scalable algorithms and systems based on\\nstochastic subsampling and distributed computing for dealing with large-scale\\napplications.\\n</summary>\\n    <author>\\n      <name>Jun Zhu</name>\\n    </author>\\n    <author>\\n      <name>Jianfei Chen</name>\\n    </author>\\n    <author>\\n      <name>Wenbo Hu</name>\\n    </author>\\n    <author>\\n      <name>Bo Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1411.6370v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.6370v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"F.1.2; G.3\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1509.08731v1</id>\\n    <updated>2015-09-29T13:04:03Z</updated>\\n    <published>2015-09-29T13:04:03Z</published>\\n    <title>Variational Information Maximisation for Intrinsically Motivated\\n  Reinforcement Learning</title>\\n    <summary>  The mutual information is a core statistical quantity that has applications\\nin all areas of machine learning, whether this is in training of density models\\nover multiple data modalities, in maximising the efficiency of noisy\\ntransmission channels, or when learning behaviour policies for exploration by\\nartificial agents. Most learning algorithms that involve optimisation of the\\nmutual information rely on the Blahut-Arimoto algorithm --- an enumerative\\nalgorithm with exponential complexity that is not suitable for modern machine\\nlearning applications. This paper provides a new approach for scalable\\noptimisation of the mutual information by merging techniques from variational\\ninference and deep learning. We develop our approach by focusing on the problem\\nof intrinsically-motivated learning, where the mutual information forms the\\ndefinition of a well-known internal drive known as empowerment. Using a\\nvariational lower bound on the mutual information, combined with convolutional\\nnetworks for handling visual input streams, we develop a stochastic\\noptimisation algorithm that allows for scalable information maximisation and\\nempowerment-based reasoning directly from pixels to actions.\\n</summary>\\n    <author>\\n      <name>Shakir Mohamed</name>\\n    </author>\\n    <author>\\n      <name>Danilo Jimenez Rezende</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 29th Conference on Neural Information Processing\\n  Systems (NIPS 2015)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1509.08731v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1509.08731v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.09083v1</id>\\n    <updated>2016-10-28T05:47:51Z</updated>\\n    <published>2016-10-28T05:47:51Z</published>\\n    <title>SOL: A Library for Scalable Online Learning Algorithms</title>\\n    <summary>  SOL is an open-source library for scalable online learning algorithms, and is\\nparticularly suitable for learning with high-dimensional data. The library\\nprovides a family of regular and sparse online learning algorithms for\\nlarge-scale binary and multi-class classification tasks with high efficiency,\\nscalability, portability, and extensibility. SOL was implemented in C++, and\\nprovided with a collection of easy-to-use command-line tools, python wrappers\\nand library calls for users and developers, as well as comprehensive documents\\nfor both beginners and advanced users. SOL is not only a practical machine\\nlearning toolbox, but also a comprehensive experimental platform for online\\nlearning research. Experiments demonstrate that SOL is highly efficient and\\nscalable for large-scale machine learning with high-dimensional data.\\n</summary>\\n    <author>\\n      <name>Yue Wu</name>\\n    </author>\\n    <author>\\n      <name>Steven C. H. Hoi</name>\\n    </author>\\n    <author>\\n      <name>Chenghao Liu</name>\\n    </author>\\n    <author>\\n      <name>Jing Lu</name>\\n    </author>\\n    <author>\\n      <name>Doyen Sahoo</name>\\n    </author>\\n    <author>\\n      <name>Nenghai Yu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.09083v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.09083v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.09827v2</id>\\n    <updated>2017-04-06T01:13:41Z</updated>\\n    <published>2016-11-29T20:26:00Z</published>\\n    <title>Learning Features of Music from Scratch</title>\\n    <summary>  This paper introduces a new large-scale music dataset, MusicNet, to serve as\\na source of supervision and evaluation of machine learning methods for music\\nresearch. MusicNet consists of hundreds of freely-licensed classical music\\nrecordings by 10 composers, written for 11 instruments, together with\\ninstrument/note annotations resulting in over 1 million temporal labels on 34\\nhours of chamber music performances under various studio and microphone\\nconditions.\\n  The paper defines a multi-label classification task to predict notes in\\nmusical recordings, along with an evaluation protocol, and benchmarks several\\nmachine learning architectures for this task: i) learning from spectrogram\\nfeatures; ii) end-to-end learning with a neural net; iii) end-to-end learning\\nwith a convolutional neural net. These experiments show that end-to-end models\\ntrained for note prediction learn frequency selective filters as a low-level\\nrepresentation of audio.\\n</summary>\\n    <author>\\n      <name>John Thickstun</name>\\n    </author>\\n    <author>\\n      <name>Zaid Harchaoui</name>\\n    </author>\\n    <author>\\n      <name>Sham Kakade</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages; camera-ready version; updated experiments and related\\n  works; additional MIR metrics (Appendix C)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.09827v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.09827v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.08531v1</id>\\n    <updated>2017-10-23T22:23:34Z</updated>\\n    <published>2017-10-23T22:23:34Z</published>\\n    <title>Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets</title>\\n    <summary>  Deep learning models (aka Deep Neural Networks) have revolutionized many\\nfields including computer vision, natural language processing, speech\\nrecognition, and is being increasingly used in clinical healthcare\\napplications. However, few works exist which have benchmarked the performance\\nof the deep learning models with respect to the state-of-the-art machine\\nlearning models and prognostic scoring systems on publicly available healthcare\\ndatasets. In this paper, we present the benchmarking results for several\\nclinical prediction tasks such as mortality prediction, length of stay\\nprediction, and ICD-9 code group prediction using Deep Learning models,\\nensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA\\nscores. We used the Medical Information Mart for Intensive Care III (MIMIC-III)\\n(v1.4) publicly available dataset, which includes all patients admitted to an\\nICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the\\nbenchmarking tasks. Our results show that deep learning models consistently\\noutperform all the other approaches especially when the `raw\\' clinical time\\nseries data is used as input features to the models.\\n</summary>\\n    <author>\\n      <name>Sanjay Purushotham</name>\\n    </author>\\n    <author>\\n      <name>Chuizheng Meng</name>\\n    </author>\\n    <author>\\n      <name>Zhengping Che</name>\\n    </author>\\n    <author>\\n      <name>Yan Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to Journal of Biomedical Informatics (JBI). First two\\n  authors have equal contributions</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1710.08531v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.08531v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.07966v2</id>\\n    <updated>2018-05-01T14:42:15Z</updated>\\n    <published>2018-02-22T10:22:58Z</published>\\n    <title>Incremental and Iterative Learning of Answer Set Programs from Mutually\\n  Distinct Examples</title>\\n    <summary>  Over the years the Artificial Intelligence (AI) community has produced\\nseveral datasets which have given the machine learning algorithms the\\nopportunity to learn various skills across various domains. However, a subclass\\nof these machine learning algorithms that aimed at learning logic programs,\\nnamely the Inductive Logic Programming algorithms, have often failed at the\\ntask due to the vastness of these datasets. This has impacted the usability of\\nknowledge representation and reasoning techniques in the development of AI\\nsystems. In this research, we try to address this scalability issue for the\\nalgorithms that learn answer set programs. We present a sound and complete\\nalgorithm which takes the input in a slightly different manner and performs an\\nefficient and more user controlled search for a solution. We show via\\nexperiments that our algorithm can learn from two popular datasets from machine\\nlearning community, namely bAbl (a question answering dataset) and MNIST (a\\ndataset for handwritten digit recognition), which to the best of our knowledge\\nwas not previously possible. The system is publicly available at\\nhttps://goo.gl/KdWAcV. This paper is under consideration for acceptance in\\nTPLP.\\n</summary>\\n    <author>\\n      <name>Arindam Mitra</name>\\n    </author>\\n    <author>\\n      <name>Chitta Baral</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.07966v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.07966v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.05621v2</id>\\n    <updated>2018-10-26T16:34:29Z</updated>\\n    <published>2018-03-15T07:38:50Z</published>\\n    <title>Proximal SCOPE for Distributed Sparse Learning: Better Data Partition\\n  Implies Faster Convergence Rate</title>\\n    <summary>  Distributed sparse learning with a cluster of multiple machines has attracted\\nmuch attention in machine learning, especially for large-scale applications\\nwith high-dimensional data. One popular way to implement sparse learning is to\\nuse $L_1$ regularization. In this paper, we propose a novel method, called\\nproximal \\\\mbox{SCOPE}~(\\\\mbox{pSCOPE}), for distributed sparse learning with\\n$L_1$ regularization. pSCOPE is based on a \\\\underline{c}ooperative\\n\\\\underline{a}utonomous \\\\underline{l}ocal \\\\underline{l}earning~(\\\\mbox{CALL})\\nframework. In the \\\\mbox{CALL} framework of \\\\mbox{pSCOPE}, we find that the data\\npartition affects the convergence of the learning procedure, and subsequently\\nwe define a metric to measure the goodness of a data partition. Based on the\\ndefined metric, we theoretically prove that pSCOPE is convergent with a linear\\nconvergence rate if the data partition is good enough. We also prove that\\nbetter data partition implies faster convergence rate. Furthermore, pSCOPE is\\nalso communication efficient. Experimental results on real data sets show that\\npSCOPE can outperform other state-of-the-art distributed methods for sparse\\nlearning.\\n</summary>\\n    <author>\\n      <name>Shen-Yi Zhao</name>\\n    </author>\\n    <author>\\n      <name>Gong-Duo Zhang</name>\\n    </author>\\n    <author>\\n      <name>Ming-Wei Li</name>\\n    </author>\\n    <author>\\n      <name>Wu-Jun Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.05621v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.05621v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.04950v1</id>\\n    <updated>2018-07-13T07:22:45Z</updated>\\n    <published>2018-07-13T07:22:45Z</published>\\n    <title>Deep Learning in the Wild</title>\\n    <summary>  Deep learning with neural networks is applied by an increasing number of\\npeople outside of classic research environments, due to the vast success of the\\nmethodology on a wide range of machine perception tasks. While this interest is\\nfueled by beautiful success stories, practical work in deep learning on novel\\ntasks without existing baselines remains challenging. This paper explores the\\nspecific challenges arising in the realm of real world tasks, based on case\\nstudies from research \\\\&amp; development in conjunction with industry, and extracts\\nlessons learned from them. It thus fills a gap between the publication of\\nlatest algorithmic and methodical developments, and the usually omitted\\nnitty-gritty of how to make them work. Specifically, we give insight into deep\\nlearning projects on face matching, print media monitoring, industrial quality\\ncontrol, music scanning, strategy game playing, and automated machine learning,\\nthereby providing best practices for deep learning in practice.\\n</summary>\\n    <author>\\n      <name>Thilo Stadelmann</name>\\n    </author>\\n    <author>\\n      <name>Mohammadreza Amirian</name>\\n    </author>\\n    <author>\\n      <name>Ismail Arabaci</name>\\n    </author>\\n    <author>\\n      <name>Marek Arnold</name>\\n    </author>\\n    <author>\\n      <name>Gilbert Fran\\xc3\\xa7ois Duivesteijn</name>\\n    </author>\\n    <author>\\n      <name>Ismail Elezi</name>\\n    </author>\\n    <author>\\n      <name>Melanie Geiger</name>\\n    </author>\\n    <author>\\n      <name>Stefan L\\xc3\\xb6rwald</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Bruno Meier</name>\\n    </author>\\n    <author>\\n      <name>Katharina Rombach</name>\\n    </author>\\n    <author>\\n      <name>Lukas Tuggener</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Invited paper on ANNPR 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.04950v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.04950v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.08372v1</id>\\n    <updated>2018-07-22T21:32:12Z</updated>\\n    <published>2018-07-22T21:32:12Z</published>\\n    <title>Knowledge-based Transfer Learning Explanation</title>\\n    <summary>  Machine learning explanation can significantly boost machine learning\\'s\\napplication in decision making, but the usability of current methods is limited\\nin human-centric explanation, especially for transfer learning, an important\\nmachine learning branch that aims at utilizing knowledge from one learning\\ndomain (i.e., a pair of dataset and prediction task) to enhance prediction\\nmodel training in another learning domain. In this paper, we propose an\\nontology-based approach for human-centric explanation of transfer learning.\\nThree kinds of knowledge-based explanatory evidence, with different\\ngranularities, including general factors, particular narrators and core\\ncontexts are first proposed and then inferred with both local ontologies and\\nexternal knowledge bases. The evaluation with US flight data and DBpedia has\\npresented their confidence and availability in explaining the transferability\\nof feature representation in flight departure delay forecasting.\\n</summary>\\n    <author>\\n      <name>Jiaoyan Chen</name>\\n    </author>\\n    <author>\\n      <name>Freddy Lecue</name>\\n    </author>\\n    <author>\\n      <name>Jeff Z. Pan</name>\\n    </author>\\n    <author>\\n      <name>Ian Horrocks</name>\\n    </author>\\n    <author>\\n      <name>Huajun Chen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by International Conference on Principles of Knowledge\\n  Representation and Reasoning, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.08372v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.08372v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.10013v2</id>\\n    <updated>2019-01-25T07:47:36Z</updated>\\n    <published>2018-08-29T18:55:22Z</published>\\n    <title>The implicit fairness criterion of unconstrained learning</title>\\n    <summary>  We clarify what fairness guarantees we can and cannot expect to follow from\\nunconstrained machine learning. Specifically, we characterize when\\nunconstrained learning on its own implies group calibration, that is, the\\noutcome variable is conditionally independent of group membership given the\\nscore. We show that under reasonable conditions, the deviation from satisfying\\ngroup calibration is upper bounded by the excess risk of the learned score\\nrelative to the Bayes optimal score function. A lower bound confirms the\\noptimality of our upper bound. Moreover, we prove that as the excess risk of\\nthe learned score decreases, it strongly violates separation and independence,\\ntwo other standard fairness criteria.\\n  Our results show that group calibration is the fairness criterion that\\nunconstrained learning implicitly favors. On the one hand, this means that\\ncalibration is often satisfied on its own without the need for active\\nintervention, albeit at the cost of violating other criteria that are at odds\\nwith calibration. On the other hand, it suggests that we should be satisfied\\nwith calibration as a fairness criterion only if we are at ease with the use of\\nunconstrained machine learning in a given application.\\n</summary>\\n    <author>\\n      <name>Lydia T. Liu</name>\\n    </author>\\n    <author>\\n      <name>Max Simchowitz</name>\\n    </author>\\n    <author>\\n      <name>Moritz Hardt</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">37 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.10013v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.10013v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.02069v1</id>\\n    <updated>2018-09-06T16:03:18Z</updated>\\n    <published>2018-09-06T16:03:18Z</published>\\n    <title>Deep learning for in vitro prediction of pharmaceutical formulations</title>\\n    <summary>  Current pharmaceutical formulation development still strongly relies on the\\ntraditional trial-and-error approach by individual experiences of\\npharmaceutical scientists, which is laborious, time-consuming and costly.\\nRecently, deep learning has been widely applied in many challenging domains\\nbecause of its important capability of automatic feature extraction. The aim of\\nthis research is to use deep learning to predict pharmaceutical formulations.\\nIn this paper, two different types of dosage forms were chosen as model\\nsystems. Evaluation criteria suitable for pharmaceutics were applied to\\nassessing the performance of the models. Moreover, an automatic dataset\\nselection algorithm was developed for selecting the representative data as\\nvalidation and test datasets. Six machine learning methods were compared with\\ndeep learning. The result shows the accuracies of both two deep neural networks\\nwere above 80% and higher than other machine learning models, which showed good\\nprediction in pharmaceutical formulations. In summary, deep learning with the\\nautomatic data splitting algorithm and the evaluation criteria suitable for\\npharmaceutical formulation data was firstly developed for the prediction of\\npharmaceutical formulations. The cross-disciplinary integration of\\npharmaceutics and artificial intelligence may shift the paradigm of\\npharmaceutical researches from experience-dependent studies to data-driven\\nmethodologies.\\n</summary>\\n    <author>\\n      <name>Yilong Yang</name>\\n    </author>\\n    <author>\\n      <name>Zhuyifan Ye</name>\\n    </author>\\n    <author>\\n      <name>Yan Su</name>\\n    </author>\\n    <author>\\n      <name>Qianqian Zhao</name>\\n    </author>\\n    <author>\\n      <name>Xiaoshan Li</name>\\n    </author>\\n    <author>\\n      <name>Defang Ouyang</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.apsb.2018.09.010</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.apsb.2018.09.010\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1809.02069v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.02069v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.01198v1</id>\\n    <updated>2018-11-03T12:15:42Z</updated>\\n    <published>2018-11-03T12:15:42Z</published>\\n    <title>Biconvex Landscape In SDP-Related Learning</title>\\n    <summary>  Many machine learning problems can be reduced to learning a low-rank positive\\nsemidefinite matrix (denoted as $Z$), which encounters semidefinite program\\n(SDP). Existing SDP solvers are often expensive for large-scale learning. To\\navoid directly solving SDP, some works convert SDP into a nonconvex program by\\nfactorizing $Z$ as $XX^\\\\top$. However, this would bring higher-order\\nnonlinearity, resulting in scarcity of structure in subsequent optimization. In\\nthis paper, we propose a novel surrogate for SDP-related learning, in which the\\nstructure of subproblem is exploited. More specifically, we surrogate\\nunconstrained SDP by a biconvex problem, through factorizing $Z$ as $XY^\\\\top$\\nand using a Courant penalty to penalize the difference of $X$ and $Y$, in which\\nthe resultant subproblems are convex. Furthermore, we provide a theoretical\\nbound for the associated penalty parameter under the assumption that the\\nobjective function is Lipschitz-smooth, such that the proposed surrogate will\\nsolve the original SDP when the penalty parameter is larger than this bound.\\nExperiments on two SDP-related machine learning applications demonstrate that\\nthe proposed algorithm is as accurate as the state-of-the-art, but is faster on\\nlarge-scale learning.\\n</summary>\\n    <author>\\n      <name>En-Liang Hu</name>\\n    </author>\\n    <author>\\n      <name>Bo Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.01198v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.01198v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.08840v1</id>\\n    <updated>2018-11-21T17:37:22Z</updated>\\n    <published>2018-11-21T17:37:22Z</published>\\n    <title>Integrating Reinforcement Learning to Self Training for Pulmonary Nodule\\n  Segmentation in Chest X-rays</title>\\n    <summary>  Machine learning applications in medical imaging are frequently limited by\\nthe lack of quality labeled data. In this paper, we explore the self training\\nmethod, a form of semi-supervised learning, to address the labeling burden. By\\nintegrating reinforcement learning, we were able to expand the application of\\nself training to complex segmentation networks without any further human\\nannotation. The proposed approach, reinforced self training (ReST), fine tunes\\na semantic segmentation networks by introducing a policy network that learns to\\ngenerate pseudolabels. We incorporate an expert demonstration network, based on\\ninverse reinforcement learning, to enhance clinical validity and convergence of\\nthe policy network. The model was tested on a pulmonary nodule segmentation\\ntask in chest X-rays and achieved the performance of a standard U-Net while\\nusing only 50% of the labeled data, by exploiting unlabeled data. When the same\\nnumber of labeled data was used, a moderate to significant cross validation\\naccuracy improvement was achieved depending on the absolute number of labels\\nused.\\n</summary>\\n    <author>\\n      <name>Sejin Park</name>\\n    </author>\\n    <author>\\n      <name>Woochan Hwang</name>\\n    </author>\\n    <author>\\n      <name>Kyu-Hwan Jung</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  arXiv:1811.07216</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.08840v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.08840v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.01063v1</id>\\n    <updated>2018-12-03T20:15:05Z</updated>\\n    <published>2018-12-03T20:15:05Z</published>\\n    <title>A Hybrid Instance-based Transfer Learning Method</title>\\n    <summary>  In recent years, supervised machine learning models have demonstrated\\ntremendous success in a variety of application domains. Despite the promising\\nresults, these successful models are data hungry and their performance relies\\nheavily on the size of training data. However, in many healthcare applications\\nit is difficult to collect sufficiently large training datasets. Transfer\\nlearning can help overcome this issue by transferring the knowledge from\\nreadily available datasets (source) to a new dataset (target). In this work, we\\npropose a hybrid instance-based transfer learning method that outperforms a set\\nof baselines including state-of-the-art instance-based transfer learning\\napproaches. Our method uses a probabilistic weighting strategy to fuse\\ninformation from the source domain to the model learned in the target domain.\\nOur method is generic, applicable to multiple source domains, and robust with\\nrespect to negative transfer. We demonstrate the effectiveness of our approach\\nthrough extensive experiments for two different applications.\\n</summary>\\n    <author>\\n      <name>Azin Asgarian</name>\\n    </author>\\n    <author>\\n      <name>Parinaz Sobhani</name>\\n    </author>\\n    <author>\\n      <name>Ji Chao Zhang</name>\\n    </author>\\n    <author>\\n      <name>Madalin Mihailescu</name>\\n    </author>\\n    <author>\\n      <name>Ariel Sibilia</name>\\n    </author>\\n    <author>\\n      <name>Ahmed Bilal Ashraf</name>\\n    </author>\\n    <author>\\n      <name>Babak Taati</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  arXiv:cs/0101200</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.01063v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.01063v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.07669v1</id>\\n    <updated>2018-12-18T22:25:54Z</updated>\\n    <published>2018-12-18T22:25:54Z</published>\\n    <title>Machine Learning for Molecular Dynamics on Long Timescales</title>\\n    <summary>  Molecular Dynamics (MD) simulation is widely used to analyze the properties\\nof molecules and materials. Most practical applications, such as comparison\\nwith experimental measurements, designing drug molecules, or optimizing\\nmaterials, rely on statistical quantities, which may be prohibitively expensive\\nto compute from direct long-time MD simulations. Classical Machine Learning\\n(ML) techniques have already had a profound impact on the field, especially for\\nlearning low-dimensional models of the long-time dynamics and for devising more\\nefficient sampling schemes for computing long-time statistics. Novel ML methods\\nhave the potential to revolutionize long-timescale MD and to obtain\\ninterpretable models. ML concepts such as statistical estimator theory,\\nend-to-end learning, representation learning and active learning are highly\\ninteresting for the MD researcher and will help to develop new solutions to\\nhard MD problems. With the aim of better connecting the MD and ML research\\nareas and spawning new research on this interface, we define the learning\\nproblems in long-timescale MD, present successful approaches and outline some\\nof the unsolved ML problems in this application field.\\n</summary>\\n    <author>\\n      <name>Frank No\\xc3\\xa9</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.07669v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.07669v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.07742v1</id>\\n    <updated>2019-02-20T19:22:00Z</updated>\\n    <published>2019-02-20T19:22:00Z</published>\\n    <title>From Language to Goals: Inverse Reinforcement Learning for Vision-Based\\n  Instruction Following</title>\\n    <summary>  Reinforcement learning is a promising framework for solving control problems,\\nbut its use in practical situations is hampered by the fact that reward\\nfunctions are often difficult to engineer. Specifying goals and tasks for\\nautonomous machines, such as robots, is a significant challenge:\\nconventionally, reward functions and goal states have been used to communicate\\nobjectives. But people can communicate objectives to each other simply by\\ndescribing or demonstrating them. How can we build learning algorithms that\\nwill allow us to tell machines what we want them to do? In this work, we\\ninvestigate the problem of grounding language commands as reward functions\\nusing inverse reinforcement learning, and argue that language-conditioned\\nrewards are more transferable than language-conditioned policies to new\\nenvironments. We propose language-conditioned reward learning (LC-RL), which\\ngrounds language commands as a reward function represented by a deep neural\\nnetwork. We demonstrate that our model learns rewards that transfer to novel\\ntasks and environments on realistic, high-dimensional visual environments with\\nnatural language commands, whereas directly learning a language-conditioned\\npolicy leads to poor performance.\\n</summary>\\n    <author>\\n      <name>Justin Fu</name>\\n    </author>\\n    <author>\\n      <name>Anoop Korattikara</name>\\n    </author>\\n    <author>\\n      <name>Sergey Levine</name>\\n    </author>\\n    <author>\\n      <name>Sergio Guadarrama</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.07742v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.07742v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.07947v1</id>\\n    <updated>2018-03-21T14:40:05Z</updated>\\n    <published>2018-03-21T14:40:05Z</published>\\n    <title>Crowd-Machine Collaboration for Item Screening</title>\\n    <summary>  In this paper we describe how crowd and machine classifier can be efficiently\\ncombined to screen items that satisfy a set of predicates. We show that this is\\na recurring problem in many domains, present machine-human (hybrid) algorithms\\nthat screen items efficiently and estimate the gain over human-only or\\nmachine-only screening in terms of performance and cost.\\n</summary>\\n    <author>\\n      <name>Evgeny Krivosheev</name>\\n    </author>\\n    <author>\\n      <name>Bahareh Harandizadeh</name>\\n    </author>\\n    <author>\\n      <name>Fabio Casati</name>\\n    </author>\\n    <author>\\n      <name>Boualem Benatallah</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.07947v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.07947v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.04340v1</id>\\n    <updated>2016-12-13T20:19:42Z</updated>\\n    <published>2016-12-13T20:19:42Z</published>\\n    <title>End-to-End Deep Reinforcement Learning for Lane Keeping Assist</title>\\n    <summary>  Reinforcement learning is considered to be a strong AI paradigm which can be\\nused to teach machines through interaction with the environment and learning\\nfrom their mistakes, but it has not yet been successfully used for automotive\\napplications. There has recently been a revival of interest in the topic,\\nhowever, driven by the ability of deep learning algorithms to learn good\\nrepresentations of the environment. Motivated by Google DeepMind\\'s successful\\ndemonstrations of learning for games from Breakout to Go, we will propose\\ndifferent methods for autonomous driving using deep reinforcement learning.\\nThis is of particular interest as it is difficult to pose autonomous driving as\\na supervised learning problem as it has a strong interaction with the\\nenvironment including other vehicles, pedestrians and roadworks. As this is a\\nrelatively new area of research for autonomous driving, we will formulate two\\nmain categories of algorithms: 1) Discrete actions category, and 2) Continuous\\nactions category. For the discrete actions category, we will deal with Deep\\nQ-Network Algorithm (DQN) while for the continuous actions category, we will\\ndeal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to\\nthat, We will also discover the performance of these two categories on an open\\nsource car simulator for Racing called (TORCS) which stands for The Open Racing\\ncar Simulator. Our simulation results demonstrate learning of autonomous\\nmaneuvering in a scenario of complex road curvatures and simple interaction\\nwith other vehicles. Finally, we explain the effect of some restricted\\nconditions, put on the car during the learning phase, on the convergence time\\nfor finishing its learning phase.\\n</summary>\\n    <author>\\n      <name>Ahmad El Sallab</name>\\n    </author>\\n    <author>\\n      <name>Mohammed Abdou</name>\\n    </author>\\n    <author>\\n      <name>Etienne Perot</name>\\n    </author>\\n    <author>\\n      <name>Senthil Yogamani</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the Machine Learning for Intelligent Transportation\\n  Systems Workshop, NIPS 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1612.04340v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.04340v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.10546v2</id>\\n    <updated>2018-06-18T10:29:41Z</updated>\\n    <published>2018-02-28T17:28:25Z</published>\\n    <title>Computational Theories of Curiosity-Driven Learning</title>\\n    <summary>  What are the functions of curiosity? What are the mechanisms of\\ncuriosity-driven learning? We approach these questions about the living using\\nconcepts and tools from machine learning and developmental robotics. We argue\\nthat curiosity-driven learning enables organisms to make discoveries to solve\\ncomplex problems with rare or deceptive rewards. By fostering exploration and\\ndiscovery of a diversity of behavioural skills, and ignoring these rewards,\\ncuriosity can be efficient to bootstrap learning when there is no information,\\nor deceptive information, about local improvement towards these problems. We\\nalso explain the key role of curiosity for efficient learning of world models.\\nWe review both normative and heuristic computational frameworks used to\\nunderstand the mechanisms of curiosity in humans, conceptualizing the child as\\na sense-making organism. These frameworks enable us to discuss the\\nbi-directional causal links between curiosity and learning, and to provide new\\nhypotheses about the fundamental role of curiosity in self-organizing\\ndevelopmental structures through curriculum learning. We present various\\ndevelopmental robotics experiments that study these mechanisms in action, both\\nsupporting these hypotheses to understand better curiosity in humans and\\nopening new research avenues in machine learning and artificial intelligence.\\nFinally, we discuss challenges for the design of experimental paradigms for\\nstudying curiosity in psychology and cognitive neuroscience.\\n  Keywords: Curiosity, intrinsic motivation, lifelong learning, predictions,\\nworld model, rewards, free-energy principle, learning progress, machine\\nlearning, AI, developmental robotics, development, curriculum learning,\\nself-organization.\\n</summary>\\n    <author>\\n      <name>Pierre-Yves Oudeyer</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in \"The New Science of Curiosity\", ed. G. Gordon, Nova\\n  Science Publishers</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.10546v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.10546v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.3341v1</id>\\n    <updated>2014-10-09T03:51:19Z</updated>\\n    <published>2014-10-09T03:51:19Z</published>\\n    <title>Generalization Analysis for Game-Theoretic Machine Learning</title>\\n    <summary>  For Internet applications like sponsored search, cautions need to be taken\\nwhen using machine learning to optimize their mechanisms (e.g., auction) since\\nself-interested agents in these applications may change their behaviors (and\\nthus the data distribution) in response to the mechanisms. To tackle this\\nproblem, a framework called game-theoretic machine learning (GTML) was recently\\nproposed, which first learns a Markov behavior model to characterize agents\\'\\nbehaviors, and then learns the optimal mechanism by simulating agents\\' behavior\\nchanges in response to the mechanism. While GTML has demonstrated practical\\nsuccess, its generalization analysis is challenging because the behavior data\\nare non-i.i.d. and dependent on the mechanism. To address this challenge,\\nfirst, we decompose the generalization error for GTML into the behavior\\nlearning error and the mechanism learning error; second, for the behavior\\nlearning error, we obtain novel non-asymptotic error bounds for both parametric\\nand non-parametric behavior learning methods; third, for the mechanism learning\\nerror, we derive a uniform convergence bound based on a new concept called\\nnested covering number of the mechanism space and the generalization analysis\\ntechniques developed for mixing sequences. To the best of our knowledge, this\\nis the first work on the generalization analysis of GTML, and we believe it has\\ngeneral implications to the theoretical analysis of other complicated machine\\nlearning problems.\\n</summary>\\n    <author>\\n      <name>Haifang Li</name>\\n    </author>\\n    <author>\\n      <name>Fei Tian</name>\\n    </author>\\n    <author>\\n      <name>Wei Chen</name>\\n    </author>\\n    <author>\\n      <name>Tao Qin</name>\\n    </author>\\n    <author>\\n      <name>Tie-Yan Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1410.3341v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.3341v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.01259v1</id>\\n    <updated>2018-06-04T17:56:29Z</updated>\\n    <published>2018-06-04T17:56:29Z</published>\\n    <title>Learning a Code: Machine Learning for Approximate Non-Linear Coded\\n  Computation</title>\\n    <summary>  Machine learning algorithms are typically run on large scale, distributed\\ncompute infrastructure that routinely face a number of unavailabilities such as\\nfailures and temporary slowdowns. Adding redundant computations using\\ncoding-theoretic tools called \"codes\" is an emerging technique to alleviate the\\nadverse effects of such unavailabilities. A code consists of an encoding\\nfunction that proactively introduces redundant computation and a decoding\\nfunction that reconstructs unavailable outputs using the available ones. Past\\nwork focuses on using codes to provide resilience for linear computations and\\nspecific iterative optimization algorithms. However, computations performed for\\na variety of applications including inference on state-of-the-art machine\\nlearning algorithms, such as neural networks, typically fall outside this\\nrealm. In this paper, we propose taking a learning-based approach to designing\\ncodes that can handle non-linear computations. We present carefully designed\\nneural network architectures and a training methodology for learning encoding\\nand decoding functions that produce approximate reconstructions of unavailable\\ncomputation results. We present extensive experimental results demonstrating\\nthe effectiveness of the proposed approach: we show that the our learned codes\\ncan accurately reconstruct $64 - 98\\\\%$ of the unavailable predictions from\\nneural-network based image classifiers on the MNIST, Fashion-MNIST, and\\nCIFAR-10 datasets. To the best of our knowledge, this work proposes the first\\nlearning-based approach for designing codes, and also presents the first\\ncoding-theoretic solution that can provide resilience for any non-linear\\n(differentiable) computation. Our results show that learning can be an\\neffective technique for designing codes, and that learned codes are a highly\\npromising approach for bringing the benefits of coding to non-linear\\ncomputations.\\n</summary>\\n    <author>\\n      <name>Jack Kosaian</name>\\n    </author>\\n    <author>\\n      <name>K. V. Rashmi</name>\\n    </author>\\n    <author>\\n      <name>Shivaram Venkataraman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.01259v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.01259v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.05507v1</id>\\n    <updated>2018-09-26T07:30:52Z</updated>\\n    <published>2018-09-26T07:30:52Z</published>\\n    <title>Dynamic Difficulty Awareness Training for Continuous Emotion Prediction</title>\\n    <summary>  Time-continuous emotion prediction has become an increasingly compelling task\\nin machine learning. Considerable efforts have been made to advance the\\nperformance of these systems. Nonetheless, the main focus has been the\\ndevelopment of more sophisticated models and the incorporation of different\\nexpressive modalities (e. g., speech, face, and physiology). In this paper,\\nmotivated by the benefit of difficulty awareness in a human learning procedure,\\nwe propose a novel machine learning framework, namely, Dynamic Difficulty\\nAwareness Training (DDAT), which sheds fresh light on the research -- directly\\nexploiting the difficulties in learning to boost the machine learning process.\\nThe DDAT framework consists of two stages: information retrieval and\\ninformation exploitation. In the first stage, we make use of the reconstruction\\nerror of input features or the annotation uncertainty to estimate the\\ndifficulty of learning specific information. The obtained difficulty level is\\nthen used in tandem with original features to update the model input in a\\nsecond learning stage with the expectation that the model can learn to focus on\\nhigh difficulty regions of the learning process. We perform extensive\\nexperiments on a benchmark database (RECOLA) to evaluate the effectiveness of\\nthe proposed framework. The experimental results show that our approach\\noutperforms related baselines as well as other well-established time-continuous\\nemotion prediction systems, which suggests that dynamically integrating the\\ndifficulty information for neural networks can help enhance the learning\\nprocess.\\n</summary>\\n    <author>\\n      <name>Zixing Zhang</name>\\n    </author>\\n    <author>\\n      <name>Jing Han</name>\\n    </author>\\n    <author>\\n      <name>Eduardo Coutinho</name>\\n    </author>\\n    <author>\\n      <name>Bj\\xc3\\xb6rn Schuller</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TMM.2018.2871949</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TMM.2018.2871949\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">accepted by IEEE T-MM</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.05507v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.05507v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1108.3476v2</id>\\n    <updated>2011-09-02T08:47:01Z</updated>\\n    <published>2011-08-17T13:36:11Z</published>\\n    <title>Structured Sparsity and Generalization</title>\\n    <summary>  We present a data dependent generalization bound for a large class of\\nregularized algorithms which implement structured sparsity constraints. The\\nbound can be applied to standard squared-norm regularization, the Lasso, the\\ngroup Lasso, some versions of the group Lasso with overlapping groups, multiple\\nkernel learning and other regularization schemes. In all these cases\\ncompetitive results are obtained. A novel feature of our bound is that it can\\nbe applied in an infinite dimensional setting such as the Lasso in a separable\\nHilbert space or multiple kernel learning with a countable number of kernels.\\n</summary>\\n    <author>\\n      <name>Andreas Maurer</name>\\n    </author>\\n    <author>\\n      <name>Massimiliano Pontil</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research, 13:671-690, 2012</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1108.3476v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1108.3476v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.7028v1</id>\\n    <updated>2013-07-26T13:24:31Z</updated>\\n    <published>2013-07-26T13:24:31Z</published>\\n    <title>Infinite Mixtures of Multivariate Gaussian Processes</title>\\n    <summary>  This paper presents a new model called infinite mixtures of multivariate\\nGaussian processes, which can be used to learn vector-valued functions and\\napplied to multitask learning. As an extension of the single multivariate\\nGaussian process, the mixture model has the advantages of modeling multimodal\\ndata and alleviating the computationally cubic complexity of the multivariate\\nGaussian process. A Dirichlet process prior is adopted to allow the (possibly\\ninfinite) number of mixture components to be automatically inferred from\\ntraining data, and Markov chain Monte Carlo sampling techniques are used for\\nparameter and latent variable inference. Preliminary experimental results on\\nmultivariate regression show the feasibility of the proposed model.\\n</summary>\\n    <author>\\n      <name>Shiliang Sun</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the International Conference on Machine Learning and\\n  Cybernetics, 2013, pages 1011-1016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1307.7028v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.7028v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.1489v1</id>\\n    <updated>2014-01-07T20:16:05Z</updated>\\n    <published>2014-01-07T20:16:05Z</published>\\n    <title>Key point selection and clustering of swimmer coordination through\\n  Sparse Fisher-EM</title>\\n    <summary>  To answer the existence of optimal swimmer learning/teaching strategies, this\\nwork introduces a two-level clustering in order to analyze temporal dynamics of\\nmotor learning in breaststroke swimming. Each level have been performed through\\nSparse Fisher-EM, a unsupervised framework which can be applied efficiently on\\nlarge and correlated datasets. The induced sparsity selects key points of the\\ncoordination phase without any prior knowledge.\\n</summary>\\n    <author>\\n      <name>John Komar</name>\\n    </author>\\n    <author>\\n      <name>Romain H\\xc3\\xa9rault</name>\\n    </author>\\n    <author>\\n      <name>Ludovic Seifert</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data\\n  Mining for Sports Analytics (MLSA2013)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1401.1489v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.1489v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1401.8008v2</id>\\n    <updated>2017-12-20T21:44:21Z</updated>\\n    <published>2014-01-30T21:49:16Z</published>\\n    <title>Support vector comparison machines</title>\\n    <summary>  In ranking problems, the goal is to learn a ranking function from labeled\\npairs of input points. In this paper, we consider the related comparison\\nproblem, where the label indicates which element of the pair is better, or if\\nthere is no significant difference. We cast the learning problem as a margin\\nmaximization, and show that it can be solved by converting it to a standard\\nSVM. We use simulated nonlinear patterns, a real learning to rank sushi data\\nset, and a chess data set to show that our proposed SVMcompare algorithm\\noutperforms SVMrank when there are equality pairs.\\n</summary>\\n    <author>\\n      <name>David Venuto</name>\\n    </author>\\n    <author>\\n      <name>Toby Dylan Hocking</name>\\n    </author>\\n    <author>\\n      <name>Lakjaree Sphanurattana</name>\\n    </author>\\n    <author>\\n      <name>Masashi Sugiyama</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1401.8008v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1401.8008v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.2986v1</id>\\n    <updated>2014-04-11T02:37:11Z</updated>\\n    <published>2014-04-11T02:37:11Z</published>\\n    <title>A Tutorial on Independent Component Analysis</title>\\n    <summary>  Independent component analysis (ICA) has become a standard data analysis\\ntechnique applied to an array of problems in signal processing and machine\\nlearning. This tutorial provides an introduction to ICA based on linear algebra\\nformulating an intuition for ICA from first principles. The goal of this\\ntutorial is to provide a solid foundation on this advanced topic so that one\\nmight learn the motivation behind ICA, learn why and when to apply this\\ntechnique and in the process gain an introduction to this exciting field of\\nactive research.\\n</summary>\\n    <author>\\n      <name>Jonathon Shlens</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1404.2986v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.2986v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.0042v2</id>\\n    <updated>2015-06-15T13:12:12Z</updated>\\n    <published>2014-04-30T21:48:34Z</published>\\n    <title>Learning with incremental iterative regularization</title>\\n    <summary>  Within a statistical learning setting, we propose and study an iterative\\nregularization algorithm for least squares defined by an incremental gradient\\nmethod. In particular, we show that, if all other parameters are fixed a\\npriori, the number of passes over the data (epochs) acts as a regularization\\nparameter, and prove strong universal consistency, i.e. almost sure convergence\\nof the risk, as well as sharp finite sample bounds for the iterates. Our\\nresults are a step towards understanding the effect of multiple epochs in\\nstochastic gradient techniques in machine learning and rely on integrating\\nstatistical and optimization results.\\n</summary>\\n    <author>\\n      <name>Lorenzo Rosasco</name>\\n    </author>\\n    <author>\\n      <name>Silvia Villa</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">30 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.0042v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.0042v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.3145v1</id>\\n    <updated>2014-10-12T20:43:04Z</updated>\\n    <published>2014-10-12T20:43:04Z</published>\\n    <title>Machine Learning Techniques in Cognitive Radio Networks</title>\\n    <summary>  Cognitive radio is an intelligent radio that can be programmed and configured\\ndynamically to fully use the frequency resources that are not used by licensed\\nusers. It defines the radio devices that are capable of learning and adapting\\nto their transmission to the external radio environment, which means it has\\nsome kind of intelligence for monitoring the radio environment, learning the\\nenvironment and make smart decisions. In this paper, we are reviewing some\\nexamples of the usage of machine learning techniques in cognitive radio\\nnetworks for implementing the intelligent radio.\\n</summary>\\n    <author>\\n      <name>Peter Hossain</name>\\n    </author>\\n    <author>\\n      <name>Adaulfo Komisarczuk</name>\\n    </author>\\n    <author>\\n      <name>Garin Pawetczak</name>\\n    </author>\\n    <author>\\n      <name>Sarah Van Dijk</name>\\n    </author>\\n    <author>\\n      <name>Isabella Axelsen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1410.3145v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.3145v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1410.5467v1</id>\\n    <updated>2014-10-20T21:16:52Z</updated>\\n    <published>2014-10-20T21:16:52Z</published>\\n    <title>Machine Learning of Coq Proof Guidance: First Experiments</title>\\n    <summary>  We report the results of the first experiments with learning proof\\ndependencies from the formalizations done with the Coq system. We explain the\\nprocess of obtaining the dependencies from the Coq proofs, the characterization\\nof formulas that is used for the learning, and the evaluation method. Various\\nmachine learning methods are compared on a dataset of 5021 toplevel Coq proofs\\ncoming from the CoRN repository. The best resulting method covers on average\\n75% of the needed proof dependencies among the first 100 predictions, which is\\na comparable performance of such initial experiments on other large-theory\\ncorpora.\\n</summary>\\n    <author>\\n      <name>Cezary Kaliszyk</name>\\n    </author>\\n    <author>\\n      <name>Lionel Mamane</name>\\n    </author>\\n    <author>\\n      <name>Josef Urban</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1410.5467v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1410.5467v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1111.1037v2</id>\\n    <updated>2012-02-17T08:51:38Z</updated>\\n    <published>2011-11-04T04:04:08Z</published>\\n    <title>Vector-valued Reproducing Kernel Banach Spaces with Applications to\\n  Multi-task Learning</title>\\n    <summary>  Motivated by multi-task machine learning with Banach spaces, we propose the\\nnotion of vector-valued reproducing kernel Banach spaces (RKBS). Basic\\nproperties of the spaces and the associated reproducing kernels are\\ninvestigated. We also present feature map constructions and several concrete\\nexamples of vector-valued RKBS. The theory is then applied to multi-task\\nmachine learning. Especially, the representer theorem and characterization\\nequations for the minimizer of regularized learning schemes in vector-valued\\nRKBS are established.\\n</summary>\\n    <author>\\n      <name>Haizhang Zhang</name>\\n    </author>\\n    <author>\\n      <name>Jun Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1111.1037v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1111.1037v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.FA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.FA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.06247v3</id>\\n    <updated>2016-05-26T11:53:55Z</updated>\\n    <published>2015-11-19T16:47:00Z</published>\\n    <title>Predicting online user behaviour using deep learning algorithms</title>\\n    <summary>  We propose a robust classifier to predict buying intentions based on user\\nbehaviour within a large e-commerce website. In this work we compare\\ntraditional machine learning techniques with the most advanced deep learning\\napproaches. We show that both Deep Belief Networks and Stacked Denoising\\nauto-Encoders achieved a substantial improvement by extracting features from\\nhigh dimensional data during the pre-train phase. They prove also to be more\\nconvenient to deal with severe class imbalance.\\n</summary>\\n    <author>\\n      <name>Armando Vieira</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages, 3 figures. arXiv admin note: text overlap with\\n  arXiv:1412.6601, arXiv:1406.1231, arXiv:1508.03856 by other authors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1511.06247v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.06247v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.7417v1</id>\\n    <updated>2014-07-28T13:44:25Z</updated>\\n    <published>2014-07-28T13:44:25Z</published>\\n    <title>\\'Almost Sure\\' Chaotic Properties of Machine Learning Methods</title>\\n    <summary>  It has been demonstrated earlier that universal computation is \\'almost\\nsurely\\' chaotic. Machine learning is a form of computational fixed point\\niteration, iterating over the computable function space. We showcase some\\nproperties of this iteration, and establish in general that the iteration is\\n\\'almost surely\\' of chaotic nature. This theory explains the observation in the\\ncounter intuitive properties of deep learning methods. This paper demonstrates\\nthat these properties are going to be universal to any learning method.\\n</summary>\\n    <author>\\n      <name>Nabarun Mondal</name>\\n    </author>\\n    <author>\\n      <name>Partha P. Ghosh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages : to be submitted to Theoretical Computer Science. arXiv\\n  admin note: text overlap with arXiv:1111.4949</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.7417v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.7417v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"Primary 03D10, Secondary 65P20, 68Q05, 68Q87, 68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1509.07179v1</id>\\n    <updated>2015-09-23T23:22:38Z</updated>\\n    <published>2015-09-23T23:22:38Z</published>\\n    <title>IllinoisSL: A JAVA Library for Structured Prediction</title>\\n    <summary>  IllinoisSL is a Java library for learning structured prediction models. It\\nsupports structured Support Vector Machines and structured Perceptron. The\\nlibrary consists of a core learning module and several applications, which can\\nbe executed from command-lines. Documentation is provided to guide users. In\\nComparison to other structured learning libraries, IllinoisSL is efficient,\\ngeneral, and easy to use.\\n</summary>\\n    <author>\\n      <name>Kai-Wei Chang</name>\\n    </author>\\n    <author>\\n      <name>Shyam Upadhyay</name>\\n    </author>\\n    <author>\\n      <name>Ming-Wei Chang</name>\\n    </author>\\n    <author>\\n      <name>Vivek Srikumar</name>\\n    </author>\\n    <author>\\n      <name>Dan Roth</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">http://cogcomp.cs.illinois.edu/software/illinois-sl</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1509.07179v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1509.07179v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1604.05242v2</id>\\n    <updated>2016-04-22T23:03:27Z</updated>\\n    <published>2016-04-18T17:05:00Z</published>\\n    <title>Can Boosting with SVM as Week Learners Help?</title>\\n    <summary>  Object recognition in images involves identifying objects with partial\\nocclusions, viewpoint changes, varying illumination, cluttered backgrounds.\\nRecent work in object recognition uses machine learning techniques SVM-KNN,\\nLocal Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, we\\nwant to utilize SVM as week learners in AdaBoost. Experiments are done with\\nclassifiers like near- est neighbor, k-nearest neighbor, Support vector\\nmachines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariant\\ndescriptors and Pyramid his- togram of gradient descriptors. AdaBoost is\\ntrained with set of week classifier as SVMs, each with kernel distance function\\non different descriptors. Results shows AdaBoost with SVM outperform other\\nmethods for Object Categorization dataset.\\n</summary>\\n    <author>\\n      <name>Dinesh Govindaraj</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Work done in 2009</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1604.05242v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1604.05242v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.01029v1</id>\\n    <updated>2016-05-03T19:16:12Z</updated>\\n    <published>2016-05-03T19:16:12Z</published>\\n    <title>Online Machine Learning Techniques for Predicting Operator Performance</title>\\n    <summary>  This thesis explores a number of online machine learning algorithms. From a\\ntheoret- ical perspective, it assesses their employability for a particular\\nfunction approximation problem where the analytical models fall short.\\nFurthermore, it discusses the applica- tion of theoretically suitable learning\\nalgorithms to the function approximation problem at hand through an efficient\\nimplementation that exploits various computational and mathematical shortcuts.\\nFinally, this thesis work evaluates the implemented learning algorithms\\naccording to various evaluation criteria through rigorous testing.\\n</summary>\\n    <author>\\n      <name>Ahmet Anil Pala</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Master Thesis defended at TU Berlin in Summer 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1605.01029v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.01029v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.09341v1</id>\\n    <updated>2016-09-29T13:53:26Z</updated>\\n    <published>2016-09-29T13:53:26Z</published>\\n    <title>Machine Learning Techniques for Stackelberg Security Games: a Survey</title>\\n    <summary>  The present survey aims at presenting the current machine learning techniques\\nemployed in security games domains. Specifically, we focused on papers and\\nworks developed by the Teamcore of University of Southern California, which\\ndeepened different directions in this field. After a brief introduction on\\nStackelberg Security Games (SSGs) and the poaching setting, the rest of the\\nwork presents how to model a boundedly rational attacker taking into account\\nher human behavior, then describes how to face the problem of having attacker\\'s\\npayoffs not defined and how to estimate them and, finally, presents how online\\nlearning techniques have been exploited to learn a model of the attacker.\\n</summary>\\n    <author>\\n      <name>Giuseppe De Nittis</name>\\n    </author>\\n    <author>\\n      <name>Francesco Trov\\xc3\\xb2</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1609.09341v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.09341v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.06996v1</id>\\n    <updated>2016-11-21T20:24:58Z</updated>\\n    <published>2016-11-21T20:24:58Z</published>\\n    <title>Spatial contrasting for deep unsupervised learning</title>\\n    <summary>  Convolutional networks have marked their place over the last few years as the\\nbest performing model for various visual tasks. They are, however, most suited\\nfor supervised learning from large amounts of labeled data. Previous attempts\\nhave been made to use unlabeled data to improve model performance by applying\\nunsupervised techniques. These attempts require different architectures and\\ntraining methods. In this work we present a novel approach for unsupervised\\ntraining of Convolutional networks that is based on contrasting between spatial\\nregions within images. This criterion can be employed within conventional\\nneural networks and trained using standard techniques such as SGD and\\nback-propagation, thus complementing supervised methods.\\n</summary>\\n    <author>\\n      <name>Elad Hoffer</name>\\n    </author>\\n    <author>\\n      <name>Itay Hubara</name>\\n    </author>\\n    <author>\\n      <name>Nir Ailon</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\\n  Complex Systems</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.06996v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.06996v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.03449v1</id>\\n    <updated>2017-01-12T18:36:47Z</updated>\\n    <published>2017-01-12T18:36:47Z</published>\\n    <title>Manifold Alignment Determination: finding correspondences across\\n  different data views</title>\\n    <summary>  We present Manifold Alignment Determination (MAD), an algorithm for learning\\nalignments between data points from multiple views or modalities. The approach\\nis capable of learning correspondences between views as well as correspondences\\nbetween individual data-points. The proposed method requires only a few aligned\\nexamples from which it is capable to recover a global alignment through a\\nprobabilistic model. The strong, yet flexible regularization provided by the\\ngenerative model is sufficient to align the views. We provide experiments on\\nboth synthetic and real data to highlight the benefit of the proposed approach.\\n</summary>\\n    <author>\\n      <name>Andreas Damianou</name>\\n    </author>\\n    <author>\\n      <name>Neil D. Lawrence</name>\\n    </author>\\n    <author>\\n      <name>Carl Henrik Ek</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS workshop on Multi-Modal Machine Learning, 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1701.03449v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.03449v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"60G15\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.3; G.1.2; I.2.6; I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.07552v2</id>\\n    <updated>2017-02-27T11:09:08Z</updated>\\n    <published>2017-02-24T12:06:47Z</published>\\n    <title>Learning Rates for Kernel-Based Expectile Regression</title>\\n    <summary>  Conditional expectiles are becoming an increasingly important tool in finance\\nas well as in other areas of applications. We analyse a support vector machine\\ntype approach for estimating conditional expectiles and establish learning\\nrates that are minimax optimal modulo a logarithmic factor if Gaussian RBF\\nkernels are used and the desired expectile is smooth in a Besov sense. As a\\nspecial case, our learning rates improve the best known rates for kernel-based\\nleast squares regression in this scenario. Key ingredients of our statistical\\nanalysis are a general calibration inequality for the asymmetric least squares\\nloss, a corresponding variance bound as well as an improved entropy number\\nbound for Gaussian RBF kernels.\\n</summary>\\n    <author>\\n      <name>Muhammad Farooq</name>\\n    </author>\\n    <author>\\n      <name>Ingo Steinwart</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1702.07552v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.07552v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.07780v1</id>\\n    <updated>2017-02-24T22:00:41Z</updated>\\n    <published>2017-02-24T22:00:41Z</published>\\n    <title>Changing Model Behavior at Test-Time Using Reinforcement Learning</title>\\n    <summary>  Machine learning models are often used at test-time subject to constraints\\nand trade-offs not present at training-time. For example, a computer vision\\nmodel operating on an embedded device may need to perform real-time inference,\\nor a translation model operating on a cell phone may wish to bound its average\\ncompute time in order to be power-efficient. In this work we describe a\\nmixture-of-experts model and show how to change its test-time resource-usage on\\na per-input basis using reinforcement learning. We test our method on a small\\nMNIST-based example.\\n</summary>\\n    <author>\\n      <name>Augustus Odena</name>\\n    </author>\\n    <author>\\n      <name>Dieterich Lawson</name>\\n    </author>\\n    <author>\\n      <name>Christopher Olah</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to ICLR 2017 Workshop Track</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1702.07780v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.07780v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.08056v1</id>\\n    <updated>2017-05-23T01:58:02Z</updated>\\n    <published>2017-05-23T01:58:02Z</published>\\n    <title>Ambiguity set and learning via Bregman and Wasserstein</title>\\n    <summary>  Construction of ambiguity set in robust optimization relies on the choice of\\ndivergences between probability distributions. In distribution learning,\\nchoosing appropriate probability distributions based on observed data is\\ncritical for approximating the true distribution. To improve the performance of\\nmachine learning models, there has recently been interest in designing\\nobjective functions based on Lp-Wasserstein distance rather than the classical\\nKullback-Leibler (KL) divergence. In this paper, we derive concentration and\\nasymptotic results using Bregman divergence. We propose a novel asymmetric\\nstatistical divergence called Wasserstein-Bregman divergence as a\\ngeneralization of L2-Wasserstein distance. We discuss how these results can be\\napplied to the construction of ambiguity set in robust optimization.\\n</summary>\\n    <author>\\n      <name>Xin Guo</name>\\n    </author>\\n    <author>\\n      <name>Johnny Hong</name>\\n    </author>\\n    <author>\\n      <name>Nan Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.08056v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.08056v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.01242v1</id>\\n    <updated>2017-06-05T09:04:07Z</updated>\\n    <published>2017-06-05T09:04:07Z</published>\\n    <title>Bayesian LSTMs in medicine</title>\\n    <summary>  The medical field stands to see significant benefits from the recent advances\\nin deep learning. Knowing the uncertainty in the decision made by any machine\\nlearning algorithm is of utmost importance for medical practitioners. This\\nstudy demonstrates the utility of using Bayesian LSTMs for classification of\\nmedical time series. Four medical time series datasets are used to show the\\naccuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we\\nshow cherry-picked examples of confident and uncertain classifications of the\\nmedical time series. With simple modifications of the common practice for deep\\nlearning, significant improvements can be made for the medical practitioner and\\npatient.\\n</summary>\\n    <author>\\n      <name>Jos van der Westhuizen</name>\\n    </author>\\n    <author>\\n      <name>Joan Lasenby</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1706.01242v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.01242v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.01439v1</id>\\n    <updated>2017-09-05T15:05:16Z</updated>\\n    <published>2017-09-05T15:05:16Z</published>\\n    <title>A Statistical Approach to Increase Classification Accuracy in Supervised\\n  Learning Algorithms</title>\\n    <summary>  Probabilistic mixture models have been widely used for different machine\\nlearning and pattern recognition tasks such as clustering, dimensionality\\nreduction, and classification. In this paper, we focus on trying to solve the\\nmost common challenges related to supervised learning algorithms by using\\nmixture probability distribution functions. With this modeling strategy, we\\nidentify sub-labels and generate synthetic data in order to reach better\\nclassification accuracy. It means we focus on increasing the training data\\nsynthetically to increase the classification accuracy.\\n</summary>\\n    <author>\\n      <name>Gustavo A Valencia-Zapata</name>\\n    </author>\\n    <author>\\n      <name>Daniel Mejia</name>\\n    </author>\\n    <author>\\n      <name>Gerhard Klimeck</name>\\n    </author>\\n    <author>\\n      <name>Michael Zentner</name>\\n    </author>\\n    <author>\\n      <name>Okan Ersoy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 9 figures, IPSI BgD Transactions</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PSI BGD TRANSACTIONS ON INTERNET RESEARCH 13.2 (2017)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1709.01439v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.01439v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.07314v1</id>\\n    <updated>2017-10-19T18:44:05Z</updated>\\n    <published>2017-10-19T18:44:05Z</published>\\n    <title>Power Plant Performance Modeling with Concept Drift</title>\\n    <summary>  Power plant is a complex and nonstationary system for which the traditional\\nmachine learning modeling approaches fall short of expectations. The\\nensemble-based online learning methods provide an effective way to continuously\\nlearn from the dynamic environment and autonomously update models to respond to\\nenvironmental changes. This paper proposes such an online ensemble regression\\napproach to model power plant performance, which is critically important for\\noperation optimization. The experimental results on both simulated and real\\ndata show that the proposed method can achieve performance with less than 1%\\nmean average percentage error, which meets the general expectations in field\\noperations.\\n</summary>\\n    <author>\\n      <name>Rui Xu</name>\\n    </author>\\n    <author>\\n      <name>Yunwen Xu</name>\\n    </author>\\n    <author>\\n      <name>Weizhong Yan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/IJCNN.2017.7966108</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/IJCNN.2017.7966108\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2017 International Joint Conference on Neural Networks (IJCNN),\\n  Anchorage, AK, 2017, pp. 2096-2103</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1710.07314v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.07314v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.07704v1</id>\\n    <updated>2017-12-20T20:56:12Z</updated>\\n    <published>2017-12-20T20:56:12Z</published>\\n    <title>Unsupervised learning of dynamical and molecular similarity using\\n  variance minimization</title>\\n    <summary>  In this report, we present an unsupervised machine learning method for\\ndetermining groups of molecular systems according to similarity in their\\ndynamics or structures using Ward\\'s minimum variance objective function. We\\nfirst apply the minimum variance clustering to a set of simulated tripeptides\\nusing the information theoretic Jensen-Shannon divergence between Markovian\\ntransition matrices in order to gain insight into how point mutations affect\\nprotein dynamics. Then, we extend the method to partition two chemoinformatic\\ndatasets according to structural similarity to motivate a train/validation/test\\nsplit for supervised learning that avoids overfitting.\\n</summary>\\n    <author>\\n      <name>Brooke E. Husic</name>\\n    </author>\\n    <author>\\n      <name>Vijay S. Pande</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2017 Workshop on Machine Learning for Molecules and Materials</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.07704v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.07704v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.bio-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.bio-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.04813v1</id>\\n    <updated>2018-01-15T14:11:57Z</updated>\\n    <published>2018-01-15T14:11:57Z</published>\\n    <title>Predicting Movie Genres Based on Plot Summaries</title>\\n    <summary>  This project explores several Machine Learning methods to predict movie\\ngenres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent\\nNeural Networks are used for text classification, while K-binary\\ntransformation, rank method and probabilistic classification with learned\\nprobability threshold are employed for the multi-label problem involved in the\\ngenre tagging task.Experiments with more than 250,000 movies show that\\nemploying the Gated Recurrent Units (GRU) neural networks for the probabilistic\\nclassification with learned probability threshold approach achieves the best\\nresult on the test set. The model attains a Jaccard Index of 50.0%, a F-score\\nof 0.56, and a hit rate of 80.5%.\\n</summary>\\n    <author>\\n      <name>Quan Hoang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1801.04813v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.04813v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.08054v1</id>\\n    <updated>2018-02-21T14:11:18Z</updated>\\n    <published>2018-02-21T14:11:18Z</published>\\n    <title>VBALD - Variational Bayesian Approximation of Log Determinants</title>\\n    <summary>  Evaluating the log determinant of a positive definite matrix is ubiquitous in\\nmachine learning. Applications thereof range from Gaussian processes,\\nminimum-volume ellipsoids, metric learning, kernel learning, Bayesian neural\\nnetworks, Determinental Point Processes, Markov random fields to partition\\nfunctions of discrete graphical models. In order to avoid the canonical, yet\\nprohibitive, Cholesky $\\\\mathcal{O}(n^{3})$ computational cost, we propose a\\nnovel approach, with complexity $\\\\mathcal{O}(n^{2})$, based on a constrained\\nvariational Bayes algorithm. We compare our method to Taylor, Chebyshev and\\nLanczos approaches and show state of the art performance on both synthetic and\\nreal-world datasets.\\n</summary>\\n    <author>\\n      <name>Diego Granziol</name>\\n    </author>\\n    <author>\\n      <name>Edward Wagstaff</name>\\n    </author>\\n    <author>\\n      <name>Bin Xin Ru</name>\\n    </author>\\n    <author>\\n      <name>Michael Osborne</name>\\n    </author>\\n    <author>\\n      <name>Stephen Roberts</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.08054v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.08054v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.07869v1</id>\\n    <updated>2018-05-21T02:23:49Z</updated>\\n    <published>2018-05-21T02:23:49Z</published>\\n    <title>Learning Device Models with Recurrent Neural Networks</title>\\n    <summary>  Recurrent neural networks (RNNs) are powerful constructs capable of modeling\\ncomplex systems, up to and including Turing Machines. However, learning such\\ncomplex models from finite training sets can be difficult. In this paper we\\nempirically show that RNNs can learn models of computer peripheral devices\\nthrough input and output state observation. This enables automated development\\nof functional software-only models of hardware devices. Such models are\\napplicable to any number of tasks, including device validation, driver\\ndevelopment, code de-obfuscation, and reverse engineering. We show that the\\nsame RNN structure successfully models six different devices from simple test\\ncircuits up to a 16550 UART serial port, and verify that these models are\\ncapable of producing equivalent output to real hardware.\\n</summary>\\n    <author>\\n      <name>John Clemens</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/IJCNN.2018.8489466</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/IJCNN.2018.8489466\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Under review for publication at IJCNN 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1805.07869v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.07869v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.10582v1</id>\\n    <updated>2018-05-27T06:12:50Z</updated>\\n    <published>2018-05-27T06:12:50Z</published>\\n    <title>Metric-Optimized Example Weights</title>\\n    <summary>  Real-world machine learning applications often have complex test metrics, and\\nmay have training and test data that follow different distributions. We propose\\naddressing these issues by using a weighted loss function with a standard\\nconvex loss, but with weights on the training examples that are learned to\\noptimize the test metric of interest on the validation set. These\\nmetric-optimized example weights can be learned for any test metric, including\\nblack box losses and customized metrics for specific applications. We\\nillustrate the performance of our proposal with public benchmark datasets and\\nreal-world applications with domain shift and custom loss functions that\\nbalance multiple objectives, impose fairness policies, and are non-convex and\\nnon-decomposable.\\n</summary>\\n    <author>\\n      <name>Sen Zhao</name>\\n    </author>\\n    <author>\\n      <name>Mahdi Milani Fard</name>\\n    </author>\\n    <author>\\n      <name>Maya Gupta</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.10582v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.10582v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.06003v1</id>\\n    <updated>2018-06-15T14:49:35Z</updated>\\n    <published>2018-06-15T14:49:35Z</published>\\n    <title>On Machine Learning and Structure for Mobile Robots</title>\\n    <summary>  Due to recent advances - compute, data, models - the role of learning in\\nautonomous systems has expanded significantly, rendering new applications\\npossible for the first time. While some of the most significant benefits are\\nobtained in the perception modules of the software stack, other aspects\\ncontinue to rely on known manual procedures based on prior knowledge on\\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\\nthese modules when data collection and curation become easier than manual rule\\ndesign. Building on this coarse and broad survey of current research, the final\\nsections aim to provide insights into future potentials and challenges as well\\nas the necessity of structure in current practical applications.\\n</summary>\\n    <author>\\n      <name>Markus Wulfmeier</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Informal Review</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.06003v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.06003v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.06365v1</id>\\n    <updated>2018-06-17T11:18:49Z</updated>\\n    <published>2018-06-17T11:18:49Z</published>\\n    <title>How Could Polyhedral Theory Harness Deep Learning?</title>\\n    <summary>  The holy grail of deep learning is to come up with an automatic method to\\ndesign optimal architectures for different applications. In other words, how\\ncan we effectively dimension and organize neurons along the network layers\\nbased on the computational resources, input size, and amount of training data?\\nWe outline promising research directions based on polyhedral theory and\\nmixed-integer representability that may offer an analytical approach to this\\nquestion, in contrast to the empirical techniques often employed.\\n</summary>\\n    <author>\\n      <name>Thiago Serra</name>\\n    </author>\\n    <author>\\n      <name>Christian Tjandraatmadja</name>\\n    </author>\\n    <author>\\n      <name>Srikumar Ramalingam</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Scientific Machine Learning Workshop, U.S. Department of Energy\\n  Office of Advanced Scientific Computing Research, January 30 -- February 1,\\n  2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1806.06365v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.06365v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.04752v1</id>\\n    <updated>2018-11-12T14:51:41Z</updated>\\n    <published>2018-11-12T14:51:41Z</published>\\n    <title>Learning Representations of Missing Data for Predicting Patient Outcomes</title>\\n    <summary>  Extracting actionable insight from Electronic Health Records (EHRs) poses\\nseveral challenges for traditional machine learning approaches. Patients are\\noften missing data relative to each other; the data comes in a variety of\\nmodalities, such as multivariate time series, free text, and categorical\\ndemographic information; important relationships among patients can be\\ndifficult to detect; and many others. In this work, we propose a novel approach\\nto address these first three challenges using a representation learning scheme\\nbased on message passing. We show that our proposed approach is competitive\\nwith or outperforms the state of the art for predicting in-hospital mortality\\n(binary classification), the length of hospital visits (regression) and the\\ndischarge destination (multiclass classification).\\n</summary>\\n    <author>\\n      <name>Brandon Malone</name>\\n    </author>\\n    <author>\\n      <name>Alberto Garcia-Duran</name>\\n    </author>\\n    <author>\\n      <name>Mathias Niepert</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.04752v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.04752v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.09602v1</id>\\n    <updated>2018-11-23T18:57:30Z</updated>\\n    <published>2018-11-23T18:57:30Z</published>\\n    <title>Model-Based Reinforcement Learning for Sepsis Treatment</title>\\n    <summary>  Sepsis is a dangerous condition that is a leading cause of patient mortality.\\nTreating sepsis is highly challenging, because individual patients respond very\\ndifferently to medical interventions and there is no universally agreed-upon\\ntreatment for sepsis. In this work, we explore the use of continuous\\nstate-space model-based reinforcement learning (RL) to discover high-quality\\ntreatment policies for sepsis patients. Our quantitative evaluation reveals\\nthat by blending the treatment strategy discovered with RL with what clinicians\\nfollow, we can obtain improved policies, potentially allowing for better\\nmedical treatment for sepsis.\\n</summary>\\n    <author>\\n      <name>Aniruddh Raghu</name>\\n    </author>\\n    <author>\\n      <name>Matthieu Komorowski</name>\\n    </author>\\n    <author>\\n      <name>Sumeetpal Singh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\\n  arXiv:1811.07216</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.09602v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.09602v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00884v1</id>\\n    <updated>2018-11-28T15:05:22Z</updated>\\n    <published>2018-11-28T15:05:22Z</published>\\n    <title>Cluster-Based Learning from Weakly Labeled Bags in Digital Pathology</title>\\n    <summary>  To alleviate the burden of gathering detailed expert annotations when\\ntraining deep neural networks, we propose a weakly supervised learning approach\\nto recognize metastases in microscopic images of breast lymph nodes. We\\ndescribe an alternative training loss which clusters weakly labeled bags in\\nlatent space to inform relevance of patch-instances during training of a\\nconvolutional neural network. We evaluate our method on the Camelyon dataset\\nwhich contains high-resolution digital slides of breast lymph nodes, where\\nlabels are provided at the image-level and only subsets of patches are made\\navailable during training.\\n</summary>\\n    <author>\\n      <name>Shazia Akbar</name>\\n    </author>\\n    <author>\\n      <name>Anne L. Martel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.00884v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00884v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.02885v1</id>\\n    <updated>2018-12-07T02:50:20Z</updated>\\n    <published>2018-12-07T02:50:20Z</published>\\n    <title>Adversarial Attacks, Regression, and Numerical Stability Regularization</title>\\n    <summary>  Adversarial attacks against neural networks in a regression setting are a\\ncritical yet understudied problem. In this work, we advance the state of the\\nart by investigating adversarial attacks against regression networks and by\\nformulating a more effective defense against these attacks. In particular, we\\ntake the perspective that adversarial attacks are likely caused by numerical\\ninstability in learned functions. We introduce a stability inducing,\\nregularization based defense against adversarial attacks in the regression\\nsetting. Our new and easy to implement defense is shown to outperform prior\\napproaches and to improve the numerical stability of learned functions.\\n</summary>\\n    <author>\\n      <name>Andre T. Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Edward Raff</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the AAAI 2019 Workshop on Engineering Dependable and\\n  Secure Machine Learning Systems</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.02885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.02885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.06591v1</id>\\n    <updated>2018-12-11T22:49:14Z</updated>\\n    <published>2018-12-11T22:49:14Z</published>\\n    <title>SMART: An Open Source Data Labeling Platform for Supervised Learning</title>\\n    <summary>  SMART is an open source web application designed to help data scientists and\\nresearch teams efficiently build labeled training data sets for supervised\\nmachine learning tasks. SMART provides users with an intuitive interface for\\ncreating labeled data sets, supports active learning to help reduce the\\nrequired amount of labeled data, and incorporates inter-rater reliability\\nstatistics to provide insight into label quality. SMART is designed to be\\nplatform agnostic and easily deployable to meet the needs of as many different\\nresearch teams as possible. The project website contains links to the code\\nrepository and extensive user documentation.\\n</summary>\\n    <author>\\n      <name>Rob Chew</name>\\n    </author>\\n    <author>\\n      <name>Michael Wenger</name>\\n    </author>\\n    <author>\\n      <name>Caroline Kery</name>\\n    </author>\\n    <author>\\n      <name>Jason Nance</name>\\n    </author>\\n    <author>\\n      <name>Keith Richards</name>\\n    </author>\\n    <author>\\n      <name>Emily Hadley</name>\\n    </author>\\n    <author>\\n      <name>Peter Baumgartner</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.06591v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.06591v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.09697v1</id>\\n    <updated>2019-01-28T14:37:17Z</updated>\\n    <published>2019-01-28T14:37:17Z</published>\\n    <title>Improved Accounting for Differentially Private Learning</title>\\n    <summary>  We consider the problem of differential privacy accounting, i.e. estimation\\nof privacy loss bounds, in machine learning in a broad sense. We propose two\\nversions of a generic privacy accountant suitable for a wide range of learning\\nalgorithms. Both versions are derived in a simple and principled way using\\nwell-known tools from probability theory, such as concentration inequalities.\\nWe demonstrate that our privacy accountant is able to achieve state-of-the-art\\nestimates of DP guarantees and can be applied to new areas like variational\\ninference. Moreover, we show that the latter enjoys differential privacy at\\nminor cost.\\n</summary>\\n    <author>\\n      <name>Aleksei Triastcyn</name>\\n    </author>\\n    <author>\\n      <name>Boi Faltings</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages without references, 5 figures, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.09697v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.09697v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.10040v1</id>\\n    <updated>2019-01-20T18:45:50Z</updated>\\n    <published>2019-01-20T18:45:50Z</published>\\n    <title>Towards Aggregating Weighted Feature Attributions</title>\\n    <summary>  Current approaches for explaining machine learning models fall into two\\ndistinct classes: antecedent event influence and value attribution. The former\\nleverages training instances to describe how much influence a training point\\nexerts on a test point, while the latter attempts to attribute value to the\\nfeatures most pertinent to a given prediction. In this work, we discuss an\\nalgorithm, AVA: Aggregate Valuation of Antecedents, that fuses these two\\nexplanation classes to form a new approach to feature attribution that not only\\nretrieves local explanations but also captures global patterns learned by a\\nmodel. Our experimentation convincingly favors weighting and aggregating\\nfeature attributions via AVA.\\n</summary>\\n    <author>\\n      <name>Umang Bhatt</name>\\n    </author>\\n    <author>\\n      <name>Pradeep Ravikumar</name>\\n    </author>\\n    <author>\\n      <name>Jose M. F. Moura</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In AAAI-19 Workshop on Network Interpretability for Deep Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.10040v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.10040v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.10691v1</id>\\n    <updated>2019-01-30T06:59:11Z</updated>\\n    <published>2019-01-30T06:59:11Z</published>\\n    <title>Probability Functional Descent: A Unifying Perspective on GANs,\\n  Variational Inference, and Reinforcement Learning</title>\\n    <summary>  The goal of this paper is to provide a unifying view of a wide range of\\nproblems of interest in machine learning by framing them as the minimization of\\nfunctionals defined on the space of probability measures. In particular, we\\nshow that generative adversarial networks, variational inference, and\\nactor-critic methods in reinforcement learning can all be seen through the lens\\nof our framework. We then discuss a generic optimization algorithm for our\\nformulation, called probability functional descent (PFD), and show how this\\nalgorithm recovers existing methods developed independently in the settings\\nmentioned earlier.\\n</summary>\\n    <author>\\n      <name>Casey Chu</name>\\n    </author>\\n    <author>\\n      <name>Jose Blanchet</name>\\n    </author>\\n    <author>\\n      <name>Peter Glynn</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.10691v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.10691v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.05401v1</id>\\n    <updated>2019-02-09T01:56:24Z</updated>\\n    <published>2019-02-09T01:56:24Z</published>\\n    <title>Improving Deep Image Clustering With Spatial Transformer Layers</title>\\n    <summary>  Image clustering is an important but challenging task in machine learning. As\\nin most image processing areas, the latest improvements came from models based\\non the deep learning approach. However, classical deep learning methods have\\nproblems to deal with spatial image transformations like scale and rotation. In\\nthis paper, we propose the use of visual attention techniques to reduce this\\nproblem in image clustering methods. We evaluate the combination of a deep\\nimage clustering model called Deep Adaptive Clustering (DAC) with the Visual\\nSpatial Transformer Networks (STN). The proposed model is evaluated in the\\ndatasets MNIST and FashionMNIST and outperformed the baseline model in\\nexperiments.\\n</summary>\\n    <author>\\n      <name>Thiago V. M. Souza</name>\\n    </author>\\n    <author>\\n      <name>Cleber Zanchettin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.05401v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.05401v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.06009v2</id>\\n    <updated>2019-03-15T01:09:45Z</updated>\\n    <published>2019-03-14T14:04:51Z</published>\\n    <title>On Learning from Ghost Imaging without Imaging</title>\\n    <summary>  Computational ghost imaging is an imaging technique with which an object is\\nimaged from light collected using a single-pixel detector with no spatial\\nresolution. Recently, ghost cytometry has been proposed for an ultrafast\\ncell-classification method that involves ghost imaging and machine learning in\\nflow cytometry. Ghost cytometry skipped the reconstruction of cell images from\\nsignals and directly used signals for cell-classification because this\\nreconstruction is the bottleneck in a high-speed analysis. In this paper, we\\nprovide a theoretical analysis for learning from ghost imaging without imaging.\\n</summary>\\n    <author>\\n      <name>Issei Sato</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.06009v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.06009v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"eess.IV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.09030v1</id>\\n    <updated>2019-03-21T14:38:45Z</updated>\\n    <published>2019-03-21T14:38:45Z</published>\\n    <title>Generative Models For Deep Learning with Very Scarce Data</title>\\n    <summary>  The goal of this paper is to deal with a data scarcity scenario where deep\\nlearning techniques use to fail. We compare the use of two well established\\ntechniques, Restricted Boltzmann Machines and Variational Auto-encoders, as\\ngenerative models in order to increase the training set in a classification\\nframework. Essentially, we rely on Markov Chain Monte Carlo (MCMC) algorithms\\nfor generating new samples. We show that generalization can be improved\\ncomparing this methodology to other state-of-the-art techniques, e.g.\\nsemi-supervised learning with ladder networks. Furthermore, we show that RBM is\\nbetter than VAE generating new samples for training a classifier with good\\ngeneralization capabilities.\\n</summary>\\n    <author>\\n      <name>Juan Maro\\xc3\\xb1as</name>\\n    </author>\\n    <author>\\n      <name>Roberto Paredes</name>\\n    </author>\\n    <author>\\n      <name>Daniel Ramos</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.09030v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.09030v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.05856v1</id>\\n    <updated>2019-04-11T17:23:41Z</updated>\\n    <published>2019-04-11T17:23:41Z</published>\\n    <title>Connections Between Adaptive Control and Optimization in Machine\\n  Learning</title>\\n    <summary>  This paper demonstrates many immediate connections between adaptive control\\nand optimization methods commonly employed in machine learning. Starting from\\ncommon output error formulations, similarities in update law modifications are\\nexamined. Concepts in stability, performance, and learning, common to both\\nfields are then discussed. Building on the similarities in update laws and\\ncommon concepts, new intersections and opportunities for improved algorithm\\nanalysis are provided. In particular, a specific problem related to higher\\norder learning is solved through insights obtained from these intersections.\\n</summary>\\n    <author>\\n      <name>Joseph E. Gaudio</name>\\n    </author>\\n    <author>\\n      <name>Travis E. Gibson</name>\\n    </author>\\n    <author>\\n      <name>Anuradha M. Annaswamy</name>\\n    </author>\\n    <author>\\n      <name>Michael A. Bolender</name>\\n    </author>\\n    <author>\\n      <name>Eugene Lavretsky</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.05856v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.05856v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1105.1951v2</id>\\n    <updated>2011-09-05T11:54:46Z</updated>\\n    <published>2011-05-10T14:01:41Z</published>\\n    <title>Self-configuration from a Machine-Learning Perspective</title>\\n    <summary>  The goal of machine learning is to provide solutions which are trained by\\ndata or by experience coming from the environment. Many training algorithms\\nexist and some brilliant successes were achieved. But even in structured\\nenvironments for machine learning (e.g. data mining or board games), most\\napplications beyond the level of toy problems need careful hand-tuning or human\\ningenuity (i.e. detection of interesting patterns) or both. We discuss several\\naspects how self-configuration can help to alleviate these problems. One aspect\\nis the self-configuration by tuning of algorithms, where recent advances have\\nbeen made in the area of SPO (Sequen- tial Parameter Optimization). Another\\naspect is the self-configuration by pattern detection or feature construction.\\nForming multiple features (e.g. random boolean functions) and using algorithms\\n(e.g. random forests) which easily digest many fea- tures can largely increase\\nlearning speed. However, a full-fledged theory of feature construction is not\\nyet available and forms a current barrier in machine learning. We discuss\\nseveral ideas for systematic inclusion of feature construction. This may lead\\nto partly self-configuring machine learning solutions which show robustness,\\nflexibility, and fast learning in potentially changing environments.\\n</summary>\\n    <author>\\n      <name>Wolfgang Konen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 5 figures, Dagstuhl seminar 11181 \"Organic Computing -\\n  Design of Self-Organizing Systems\", May 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1105.1951v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1105.1951v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"nlin.AO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"nlin.AO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.09158v1</id>\\n    <updated>2016-12-29T14:32:51Z</updated>\\n    <published>2016-12-29T14:32:51Z</published>\\n    <title>The interplay between system identification and machine learning</title>\\n    <summary>  Learning from examples is one of the key problems in science and engineering.\\nIt deals with function reconstruction from a finite set of direct and noisy\\nsamples. Regularization in reproducing kernel Hilbert spaces (RKHSs) is widely\\nused to solve this task and includes powerful estimators such as regularization\\nnetworks. Recent achievements include the proof of the statistical consistency\\nof these kernel- based approaches. Parallel to this, many different system\\nidentification techniques have been developed but the interaction with machine\\nlearning does not appear so strong yet. One reason is that the RKHSs usually\\nemployed in machine learning do not embed the information available on dynamic\\nsystems, e.g. BIBO stability. In addition, in system identification the\\nindependent data assumptions routinely adopted in machine learning are never\\nsatisfied in practice. This paper provides new results which strengthen the\\nconnection between system identification and machine learning. Our starting\\npoint is the introduction of RKHSs of dynamic systems. They contain functionals\\nover spaces defined by system inputs and allow to interpret system\\nidentification as learning from examples. In both linear and nonlinear\\nsettings, it is shown that this perspective permits to derive in a relatively\\nsimple way conditions on RKHS stability (i.e. the property of containing only\\nBIBO stable systems or predictors), also facilitating the design of new kernels\\nfor system identification. Furthermore, we prove the convergence of the\\nregularized estimator to the optimal predictor under conditions typical of\\ndynamic systems.\\n</summary>\\n    <author>\\n      <name>Gianluigi Pillonetto</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.09158v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.09158v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.00564v3</id>\\n    <updated>2018-10-26T00:52:38Z</updated>\\n    <published>2017-03-02T00:39:53Z</published>\\n    <title>MoleculeNet: A Benchmark for Molecular Machine Learning</title>\\n    <summary>  Molecular machine learning has been maturing rapidly over the last few years.\\nImproved methods and the presence of larger datasets have enabled machine\\nlearning algorithms to make increasingly accurate predictions about molecular\\nproperties. However, algorithmic progress has been limited due to the lack of a\\nstandard benchmark to compare the efficacy of proposed methods; most new\\nalgorithms are benchmarked on different datasets making it challenging to gauge\\nthe quality of proposed methods. This work introduces MoleculeNet, a large\\nscale benchmark for molecular machine learning. MoleculeNet curates multiple\\npublic datasets, establishes metrics for evaluation, and offers high quality\\nopen-source implementations of multiple previously proposed molecular\\nfeaturization and learning algorithms (released as part of the DeepChem open\\nsource library). MoleculeNet benchmarks demonstrate that learnable\\nrepresentations are powerful tools for molecular machine learning and broadly\\noffer the best performance. However, this result comes with caveats. Learnable\\nrepresentations still struggle to deal with complex tasks under data scarcity\\nand highly imbalanced classification. For quantum mechanical and biophysical\\ndatasets, the use of physics-aware featurizations can be more important than\\nchoice of particular learning algorithm.\\n</summary>\\n    <author>\\n      <name>Zhenqin Wu</name>\\n    </author>\\n    <author>\\n      <name>Bharath Ramsundar</name>\\n    </author>\\n    <author>\\n      <name>Evan N. Feinberg</name>\\n    </author>\\n    <author>\\n      <name>Joseph Gomes</name>\\n    </author>\\n    <author>\\n      <name>Caleb Geniesse</name>\\n    </author>\\n    <author>\\n      <name>Aneesh S. Pappu</name>\\n    </author>\\n    <author>\\n      <name>Karl Leswing</name>\\n    </author>\\n    <author>\\n      <name>Vijay Pande</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.00564v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.00564v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.07947v1</id>\\n    <updated>2019-01-23T15:25:48Z</updated>\\n    <published>2019-01-23T15:25:48Z</published>\\n    <title>Machine Learning for Wireless Communications in the Internet of Things:\\n  A Comprehensive Survey</title>\\n    <summary>  The Internet of Things (IoT) is expected to require more effective and\\nefficient wireless communications than ever before. For this reason, techniques\\nsuch as spectrum sharing and dynamic spectrum access will soon become essential\\ncomponents of the IoT wireless communication process. In this vision, IoT\\ndevices must be able to not only learn to autonomously extract spectrum\\nknowledge on-the-fly from the network but also leverage such knowledge to\\ndynamically change appropriate wireless parameters (e.g., frequency band,\\nsymbol modulation, coding rate, route selection etc) to reach the network\\'s\\noptimal operating point. To address the above challenges, much research has\\nbeen devoted to exploring the use of machine learning to address problems in\\nthe IoT wireless communications domain. The reason behind machine learning\\'s\\npopularity is that it provides a general framework to solve very complex\\nproblems where a model of the phenomenon being learned is too complex to derive\\nor too dynamic to be summarized in mathematical terms. This work provides a\\ncomprehensive survey of the state of the art in the application of machine\\nlearning techniques to address key problems in IoT wireless communications with\\nan emphasis on its ad hoc networking aspect. First, we present extensive\\nbackground notions on machine learning techniques. Then, by adopting a\\nbottom-up approach, we examine existing work on machine learning for the IoT at\\nthe physical, data-link and network layer of the protocol stack. Thereafter, we\\ndiscuss directions taken by the community towards hardware implementation to\\nensure the feasibility of these techniques. Finally, we provide a series of\\nresearch challenges associated with the applications of machine learning\\ntechniques for IoT wireless communications.\\n</summary>\\n    <author>\\n      <name>Jithin Jagannath</name>\\n    </author>\\n    <author>\\n      <name>Nicholas Polosky</name>\\n    </author>\\n    <author>\\n      <name>Anu Jagannath</name>\\n    </author>\\n    <author>\\n      <name>Francesco Restuccia</name>\\n    </author>\\n    <author>\\n      <name>Tommaso Melodia</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ad-Hoc Networks Journal</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.07947v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.07947v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.11303v3</id>\\n    <updated>2019-03-22T00:53:31Z</updated>\\n    <published>2019-01-31T11:10:29Z</published>\\n    <title>Hyperbox based machine learning algorithms: A comprehensive survey</title>\\n    <summary>  With the rapid development of digital information, the data volume generated\\nby humans and machines is growing exponentially. Along with this trend, machine\\nlearning algorithms have been formed and evolved continuously to discover new\\ninformation and knowledge from different data sources. Learning algorithms\\nusing hyperboxes as fundamental representational and building blocks are a\\nbranch of machine learning methods. These algorithms have enormous potential\\nfor high scalability and online adaptation of predictors built using hyperbox\\ndata representations to the dynamically changing environments and streaming\\ndata. This paper aims to give a comprehensive survey of literature on\\nhyperbox-based machine learning models. In general, according to the\\narchitecture and characteristic features of the resulting models, the existing\\nhyperbox-based learning algorithms may be grouped into three major categories:\\nfuzzy min-max neural networks, hyperbox-based hybrid models, and other\\nalgorithms based on hyperbox representations. Within each of these groups, this\\npaper shows a brief description of the structure of models, associated learning\\nalgorithms, and an analysis of their advantages and drawbacks. Main\\napplications of these hyperbox-based models to the real-world problems are also\\ndescribed in this paper. Finally, we discuss some open problems and identify\\npotential future research directions in this field.\\n</summary>\\n    <author>\\n      <name>Thanh Tung Khuat</name>\\n    </author>\\n    <author>\\n      <name>Dymitr Ruta</name>\\n    </author>\\n    <author>\\n      <name>Bogdan Gabrys</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.11303v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.11303v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T30, 68T20, 68T37, 68W27\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.1; I.2.6; I.2.m; I.5.0; I.5.1; I.5.2; I.5.3; I.5.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.01418v1</id>\\n    <updated>2017-06-05T16:51:36Z</updated>\\n    <published>2017-06-05T16:51:36Z</published>\\n    <title>Learning Whenever Learning is Possible: Universal Learning under General\\n  Stochastic Processes</title>\\n    <summary>  This work initiates a general study of learning and generalization without\\nthe i.i.d. assumption, starting from first principles. While the standard\\napproach to statistical learning theory is based on assumptions chosen largely\\nfor their convenience (e.g., i.i.d. or stationary ergodic), in this work we are\\ninterested in developing a theory of learning based only on the most\\nfundamental and natural assumptions implicit in the requirements of the\\nlearning problem itself. We specifically study universally consistent function\\nlearning, where the objective is to obtain low long-run average loss for any\\ntarget function, when the data follow a given stochastic process. We are then\\ninterested in the question of whether there exist learning rules guaranteed to\\nbe universally consistent given only the assumption that universally consistent\\nlearning is possible for the given data process. The reasoning that motivates\\nthis criterion emanates from a kind of optimist\\'s decision theory, and so we\\nrefer to such learning rules as being optimistically universal. We study this\\nquestion in three natural learning settings: inductive, self-adaptive, and\\nonline. Remarkably, as our strongest positive result, we find that\\noptimistically universal learning rules do indeed exist in the self-adaptive\\nlearning setting. Establishing this fact requires us to develop new approaches\\nto the design of learning algorithms. Along the way, we also identify concise\\ncharacterizations of the family of processes under which universally consistent\\nlearning is possible in the inductive and self-adaptive settings. We\\nadditionally pose a number of enticing open problems, particularly for the\\nonline learning setting.\\n</summary>\\n    <author>\\n      <name>Steve Hanneke</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.01418v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.01418v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.2312v1</id>\\n    <updated>2013-07-09T00:58:10Z</updated>\\n    <published>2013-07-09T00:58:10Z</published>\\n    <title>Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning</title>\\n    <summary>  Bayesian network structure learning algorithms with limited data are being\\nused in domains such as systems biology and neuroscience to gain insight into\\nthe underlying processes that produce observed data. Learning reliable networks\\nfrom limited data is difficult, therefore transfer learning can improve the\\nrobustness of learned networks by leveraging data from related tasks. Existing\\ntransfer learning algorithms for Bayesian network structure learning give a\\nsingle maximum a posteriori estimate of network models. Yet, many other models\\nmay be equally likely, and so a more informative result is provided by Bayesian\\nstructure discovery. Bayesian structure discovery algorithms estimate posterior\\nprobabilities of structural features, such as edges. We present transfer\\nlearning for Bayesian structure discovery which allows us to explore the shared\\nand unique structural features among related tasks. Efficient computation\\nrequires that our transfer learning objective factors into local calculations,\\nwhich we prove is given by a broad class of transfer biases. Theoretically, we\\nshow the efficiency of our approach. Empirically, we show that compared to\\nsingle task learning, transfer learning is better able to positively identify\\ntrue edges. We apply the method to whole-brain neuroimaging data.\\n</summary>\\n    <author>\\n      <name>Diane Oyen</name>\\n    </author>\\n    <author>\\n      <name>Terran Lane</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1307.2312v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.2312v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.03130v3</id>\\n    <updated>2016-10-28T13:37:46Z</updated>\\n    <published>2016-03-10T02:53:52Z</published>\\n    <title>Theoretical Comparisons of Positive-Unlabeled Learning against\\n  Positive-Negative Learning</title>\\n    <summary>  In PU learning, a binary classifier is trained from positive (P) and\\nunlabeled (U) data without negative (N) data. Although N data is missing, it\\nsometimes outperforms PN learning (i.e., ordinary supervised learning).\\nHitherto, neither theoretical nor experimental analysis has been given to\\nexplain this phenomenon. In this paper, we theoretically compare PU (and NU)\\nlearning against PN learning based on the upper bounds on estimation errors. We\\nfind simple conditions when PU and NU learning are likely to outperform PN\\nlearning, and we prove that, in terms of the upper bounds, either PU or NU\\nlearning (depending on the class-prior probability and the sizes of P and N\\ndata) given infinite U data will improve on PN learning. Our theoretical\\nfindings well agree with the experimental results on artificial and benchmark\\ndata even when the experimental setup does not match the theoretical\\nassumptions exactly.\\n</summary>\\n    <author>\\n      <name>Gang Niu</name>\\n    </author>\\n    <author>\\n      <name>Marthinus Christoffel du Plessis</name>\\n    </author>\\n    <author>\\n      <name>Tomoya Sakai</name>\\n    </author>\\n    <author>\\n      <name>Yao Ma</name>\\n    </author>\\n    <author>\\n      <name>Masashi Sugiyama</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2016 camera-ready version</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1603.03130v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.03130v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1611.08331v1</id>\\n    <updated>2016-11-25T00:53:37Z</updated>\\n    <published>2016-11-25T00:53:37Z</published>\\n    <title>An Overview on Data Representation Learning: From Traditional Feature\\n  Learning to Recent Deep Learning</title>\\n    <summary>  Since about 100 years ago, to learn the intrinsic structure of data, many\\nrepresentation learning approaches have been proposed, including both linear\\nones and nonlinear ones, supervised ones and unsupervised ones. Particularly,\\ndeep architectures are widely applied for representation learning in recent\\nyears, and have delivered top results in many tasks, such as image\\nclassification, object detection and speech recognition. In this paper, we\\nreview the development of data representation learning methods. Specifically,\\nwe investigate both traditional feature learning algorithms and\\nstate-of-the-art deep learning models. The history of data representation\\nlearning is introduced, while available resources (e.g. online course, tutorial\\nand book information) and toolboxes are provided. Finally, we conclude this\\npaper with remarks and some interesting research directions on data\\nrepresentation learning.\\n</summary>\\n    <author>\\n      <name>Guoqiang Zhong</name>\\n    </author>\\n    <author>\\n      <name>Li-Na Wang</name>\\n    </author>\\n    <author>\\n      <name>Junyu Dong</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">About 20 pages. Submitted to Journal of Finance and Data Science as\\n  an invited paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1611.08331v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1611.08331v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.06859v1</id>\\n    <updated>2017-06-20T04:19:57Z</updated>\\n    <published>2017-06-20T04:19:57Z</published>\\n    <title>Analysis of dropout learning regarded as ensemble learning</title>\\n    <summary>  Deep learning is the state-of-the-art in fields such as visual object\\nrecognition and speech recognition. This learning uses a large number of\\nlayers, huge number of units, and connections. Therefore, overfitting is a\\nserious problem. To avoid this problem, dropout learning is proposed. Dropout\\nlearning neglects some inputs and hidden units in the learning process with a\\nprobability, p, and then, the neglected inputs and hidden units are combined\\nwith the learned network to express the final output. We find that the process\\nof combining the neglected hidden units with the learned network can be\\nregarded as ensemble learning, so we analyze dropout learning from this point\\nof view.\\n</summary>\\n    <author>\\n      <name>Kazuyuki Hara</name>\\n    </author>\\n    <author>\\n      <name>Daisuke Saitoh</name>\\n    </author>\\n    <author>\\n      <name>Hayaru Shouno</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-319-44781-0_9</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-319-44781-0_9\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 8 figures, submitted to Conference</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A. E. P. VIlla et al. (Eds.): ICANN 2016 ( Part II, LNCS 9887, pp.\\n  1-8, 2016)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1706.06859v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.06859v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.05866v2</id>\\n    <updated>2017-09-28T21:51:43Z</updated>\\n    <published>2017-08-19T15:55:31Z</published>\\n    <title>A Brief Survey of Deep Reinforcement Learning</title>\\n    <summary>  Deep reinforcement learning is poised to revolutionise the field of AI and\\nrepresents a step towards building autonomous systems with a higher level\\nunderstanding of the visual world. Currently, deep learning is enabling\\nreinforcement learning to scale to problems that were previously intractable,\\nsuch as learning to play video games directly from pixels. Deep reinforcement\\nlearning algorithms are also applied to robotics, allowing control policies for\\nrobots to be learned directly from camera inputs in the real world. In this\\nsurvey, we begin with an introduction to the general field of reinforcement\\nlearning, then progress to the main streams of value-based and policy-based\\nmethods. Our survey will cover central algorithms in deep reinforcement\\nlearning, including the deep $Q$-network, trust region policy optimisation, and\\nasynchronous advantage actor-critic. In parallel, we highlight the unique\\nadvantages of deep neural networks, focusing on visual understanding via\\nreinforcement learning. To conclude, we describe several current areas of\\nresearch within the field.\\n</summary>\\n    <author>\\n      <name>Kai Arulkumaran</name>\\n    </author>\\n    <author>\\n      <name>Marc Peter Deisenroth</name>\\n    </author>\\n    <author>\\n      <name>Miles Brundage</name>\\n    </author>\\n    <author>\\n      <name>Anil Anthony Bharath</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/MSP.2017.2743240</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/MSP.2017.2743240\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Signal Processing Magazine, Special Issue on Deep Learning for\\n  Image Understanding (arXiv extended version)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1708.05866v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.05866v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.03577v1</id>\\n    <updated>2017-11-06T23:00:13Z</updated>\\n    <published>2017-11-06T23:00:13Z</published>\\n    <title>What Really is Deep Learning Doing?</title>\\n    <summary>  Deep learning has achieved a great success in many areas, from computer\\nvision to natural language processing, to game playing, and much more. Yet,\\nwhat deep learning is really doing is still an open question. There are a lot\\nof works in this direction. For example, [5] tried to explain deep learning by\\ngroup renormalization, and [6] tried to explain deep learning from the view of\\nfunctional approximation. In order to address this very crucial question, here\\nwe see deep learning from perspective of mechanical learning and learning\\nmachine (see [1], [2]). From this particular angle, we can see deep learning\\nmuch better and answer with confidence: What deep learning is really doing? why\\nit works well, how it works, and how much data is necessary for learning. We\\nalso will discuss advantages and disadvantages of deep learning at the end of\\nthis work.\\n</summary>\\n    <author>\\n      <name>Chuyu Xiong</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1711.03577v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.03577v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.09859v1</id>\\n    <updated>2018-04-26T02:28:48Z</updated>\\n    <published>2018-04-26T02:28:48Z</published>\\n    <title>Competitive Learning Enriches Learning Representation and Accelerates\\n  the Fine-tuning of CNNs</title>\\n    <summary>  In this study, we propose the integration of competitive learning into\\nconvolutional neural networks (CNNs) to improve the representation learning and\\nefficiency of fine-tuning. Conventional CNNs use back propagation learning, and\\nit enables powerful representation learning by a discrimination task. However,\\nit requires huge amount of labeled data, and acquisition of labeled data is\\nmuch harder than that of unlabeled data. Thus, efficient use of unlabeled data\\nis getting crucial for DNNs. To address the problem, we introduce unsupervised\\ncompetitive learning into the convolutional layer, and utilize unlabeled data\\nfor effective representation learning. The results of validation experiments\\nusing a toy model demonstrated that strong representation learning effectively\\nextracted bases of images into convolutional filters using unlabeled data, and\\naccelerated the speed of the fine-tuning of subsequent supervised back\\npropagation learning. The leverage was more apparent when the number of filters\\nwas sufficiently large, and, in such a case, the error rate steeply decreased\\nin the initial phase of fine-tuning. Thus, the proposed method enlarged the\\nnumber of filters in CNNs, and enabled a more detailed and generalized\\nrepresentation. It could provide a possibility of not only deep but broad\\nneural networks.\\n</summary>\\n    <author>\\n      <name>Takashi Shinozaki</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appeared at NIPS 2017 Workshop: Deep Learning: Bridging Theory and\\n  Practice</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1804.09859v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.09859v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.02686v2</id>\\n    <updated>2018-09-17T21:51:57Z</updated>\\n    <published>2018-05-07T18:33:43Z</published>\\n    <title>Holarchic Structures for Decentralized Deep Learning - A Performance\\n  Analysis</title>\\n    <summary>  Structure plays a key role in learning performance. In centralized\\ncomputational systems, hyperparameter optimization and regularization\\ntechniques such as dropout are computational means to enhance learning\\nperformance by adjusting the deep hierarchical structure. However, in\\ndecentralized deep learning by the Internet of Things, the structure is an\\nactual network of autonomous interconnected devices such as smart phones that\\ninteract via complex network protocols. Self-adaptation of the learning\\nstructure is a challenge. Uncertainties such as network latency, node and link\\nfailures or even bottlenecks by limited processing capacity and energy\\navailability can signif- icantly downgrade learning performance. Network\\nself-organization and self-management is complex, while it requires additional\\ncomputational and network resources that hinder the feasibility of\\ndecentralized deep learning. In contrast, this paper introduces a self-adaptive\\nlearning approach based on holarchic learning structures for exploring,\\nmitigating and boosting learning performance in distributed environments with\\nuncertainties. A large-scale performance analysis with 864000 experiments fed\\nwith synthetic and real-world data from smart grid and smart city pilot\\nprojects confirm the cost-effectiveness of holarchic structures for\\ndecentralized deep learning.\\n</summary>\\n    <author>\\n      <name>Evangelos Pournaras</name>\\n    </author>\\n    <author>\\n      <name>Srivatsan Yadhunathan</name>\\n    </author>\\n    <author>\\n      <name>Ada Diaconescu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1805.02686v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.02686v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.06404v3</id>\\n    <updated>2019-02-22T23:32:23Z</updated>\\n    <published>2018-09-17T18:47:47Z</published>\\n    <title>Adversarial Imitation via Variational Inverse Reinforcement Learning</title>\\n    <summary>  We consider a problem of learning the reward and policy from expert examples\\nunder unknown dynamics. Our proposed method builds on the framework of\\ngenerative adversarial networks and introduces the empowerment-regularized\\nmaximum-entropy inverse reinforcement learning to learn near-optimal rewards\\nand policies. Empowerment-based regularization prevents the policy from\\noverfitting to expert demonstrations, which advantageously leads to more\\ngeneralized behaviors that result in learning near-optimal rewards. Our method\\nsimultaneously learns empowerment through variational information maximization\\nalong with the reward and policy under the adversarial learning formulation. We\\nevaluate our approach on various high-dimensional complex control tasks. We\\nalso test our learned rewards in challenging transfer learning problems where\\ntraining and testing environments are made to be different from each other in\\nterms of dynamics or structure. The results show that our proposed method not\\nonly learns near-optimal rewards and policies that are matching expert behavior\\nbut also performs significantly better than state-of-the-art inverse\\nreinforcement learning algorithms.\\n</summary>\\n    <author>\\n      <name>Ahmed H. Qureshi</name>\\n    </author>\\n    <author>\\n      <name>Byron Boots</name>\\n    </author>\\n    <author>\\n      <name>Michael C. Yip</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Paper published at ICLR 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.06404v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.06404v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.08904v1</id>\\n    <updated>2018-12-21T01:10:06Z</updated>\\n    <published>2018-12-21T01:10:06Z</published>\\n    <title>Pre-training with Non-expert Human Demonstration for Deep Reinforcement\\n  Learning</title>\\n    <summary>  Deep reinforcement learning (deep RL) has achieved superior performance in\\ncomplex sequential tasks by using deep neural networks as function\\napproximators to learn directly from raw input images. However, learning\\ndirectly from raw images is data inefficient. The agent must learn feature\\nrepresentation of complex states in addition to learning a policy. As a result,\\ndeep RL typically suffers from slow learning speeds and often requires a\\nprohibitively large amount of training time and data to reach reasonable\\nperformance, making it inapplicable to real-world settings where data is\\nexpensive. In this work, we improve data efficiency in deep RL by addressing\\none of the two learning goals, feature learning. We leverage supervised\\nlearning to pre-train on a small set of non-expert human demonstrations and\\nempirically evaluate our approach using the asynchronous advantage actor-critic\\nalgorithms (A3C) in the Atari domain. Our results show significant improvements\\nin learning speed, even when the provided demonstration is noisy and of low\\nquality.\\n</summary>\\n    <author>\\n      <name>Gabriel V. de la Cruz</name>\\n    </author>\\n    <author>\\n      <name>Yunshu Du</name>\\n    </author>\\n    <author>\\n      <name>Matthew E. Taylor</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1812.08904v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.08904v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1509.07450v2</id>\\n    <updated>2015-09-27T07:39:03Z</updated>\\n    <published>2015-09-22T06:30:16Z</published>\\n    <title>A 128 channel Extreme Learning Machine based Neural Decoder for Brain\\n  Machine Interfaces</title>\\n    <summary>  Currently, state-of-the-art motor intention decoding algorithms in\\nbrain-machine interfaces are mostly implemented on a PC and consume significant\\namount of power. A machine learning co-processor in 0.35um CMOS for motor\\nintention decoding in brain-machine interfaces is presented in this paper.\\nUsing Extreme Learning Machine algorithm and low-power analog processing, it\\nachieves an energy efficiency of 290 GMACs/W at a classification rate of 50 Hz.\\nThe learning in second stage and corresponding digitally stored coefficients\\nare used to increase robustness of the core analog processor. The chip is\\nverified with neural data recorded in monkey finger movements experiment,\\nachieving a decoding accuracy of 99.3% for movement type. The same co-processor\\nis also used to decode time of movement from asynchronous neural spikes. With\\ntime-delayed feature dimension enhancement, the classification accuracy can be\\nincreased by 5% with limited number of input channels. Further, a sparsity\\npromoting training scheme enables reduction of number of programmable weights\\nby ~2X.\\n</summary>\\n    <author>\\n      <name>Yi Chen</name>\\n    </author>\\n    <author>\\n      <name>Enyi Yao</name>\\n    </author>\\n    <author>\\n      <name>Arindam Basu</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TBCAS.2015.2483618</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TBCAS.2015.2483618\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 17 figures, accepted by IEEE Transactions on Biomedical\\n  Circuits and Systems, 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1509.07450v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1509.07450v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.07424v1</id>\\n    <updated>2018-09-19T22:53:46Z</updated>\\n    <published>2018-09-19T22:53:46Z</published>\\n    <title>Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing\\n  System Failure</title>\\n    <summary>  As machine learning systems move from computer-science laboratories into the\\nopen world, their accountability becomes a high priority problem.\\nAccountability requires deep understanding of system behavior and its failures.\\nCurrent evaluation methods such as single-score error metrics and confusion\\nmatrices provide aggregate views of system performance that hide important\\nshortcomings. Understanding details about failures is important for identifying\\npathways for refinement, communicating the reliability of systems in different\\nsettings, and for specifying appropriate human oversight and engagement.\\nCharacterization of failures and shortcomings is particularly complex for\\nsystems composed of multiple machine learned components. For such systems,\\nexisting evaluation methods have limited expressiveness in describing and\\nexplaining the relationship among input content, the internal states of system\\ncomponents, and final output quality. We present Pandora, a set of hybrid\\nhuman-machine methods and tools for describing and explaining system failures.\\nPandora leverages both human and system-generated observations to summarize\\nconditions of system malfunction with respect to the input content and system\\narchitecture. We share results of a case study with a machine learning pipeline\\nfor image captioning that show how detailed performance views can be beneficial\\nfor analysis and debugging.\\n</summary>\\n    <author>\\n      <name>Besmira Nushi</name>\\n    </author>\\n    <author>\\n      <name>Ece Kamar</name>\\n    </author>\\n    <author>\\n      <name>Eric Horvitz</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">AAAI Conference on Human Computation and Crowdsourcing 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1809.07424v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.07424v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0802.1412v1</id>\\n    <updated>2008-02-11T11:12:06Z</updated>\\n    <published>2008-02-11T11:12:06Z</published>\\n    <title>Extreme Learning Machine for land cover classification</title>\\n    <summary>  This paper explores the potential of extreme learning machine based\\nsupervised classification algorithm for land cover classification. In\\ncomparison to a backpropagation neural network, which requires setting of\\nseveral user-defined parameters and may produce local minima, extreme learning\\nmachine require setting of one parameter and produce a unique solution. ETM+\\nmultispectral data set (England) was used to judge the suitability of extreme\\nlearning machine for remote sensing classifications. A back propagation neural\\nnetwork was used to compare its performance in term of classification accuracy\\nand computational cost. Results suggest that the extreme learning machine\\nperform equally well to back propagation neural network in term of\\nclassification accuracy with this data set. The computational cost using\\nextreme learning machine is very small in comparison to back propagation neural\\nnetwork.\\n</summary>\\n    <author>\\n      <name>Mahesh Pal</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1080/01431160902788636</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1080/01431160902788636\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, mapindia 2008 conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0802.1412v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0802.1412v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1104.5061v2</id>\\n    <updated>2014-03-13T01:07:49Z</updated>\\n    <published>2011-04-27T01:21:05Z</published>\\n    <title>On Combining Machine Learning with Decision Making</title>\\n    <summary>  We present a new application and covering number bound for the framework of\\n\"Machine Learning with Operational Costs (MLOC),\" which is an exploratory form\\nof decision theory. The MLOC framework incorporates knowledge about how a\\npredictive model will be used for a subsequent task, thus combining machine\\nlearning with the decision that is made afterwards. In this work, we use the\\nMLOC framework to study a problem that has implications for power grid\\nreliability and maintenance, called the Machine Learning and Traveling\\nRepairman Problem ML&amp;TRP. The goal of the ML&amp;TRP is to determine a route for a\\n\"repair crew,\" which repairs nodes on a graph. The repair crew aims to minimize\\nthe cost of failures at the nodes, but as in many real situations, the failure\\nprobabilities are not known and must be estimated. The MLOC framework allows us\\nto understand how this uncertainty influences the repair route. We also present\\nnew covering number generalization bounds for the MLOC framework.\\n</summary>\\n    <author>\\n      <name>Theja Tulabandhula</name>\\n    </author>\\n    <author>\\n      <name>Cynthia Rudin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">35 pages, 16 figures, longer version of a paper appearing in\\n  Algorithmic Decision Theory 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1104.5061v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.5061v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.4152v1</id>\\n    <updated>2013-06-18T11:42:03Z</updated>\\n    <published>2013-06-18T11:42:03Z</published>\\n    <title>Bioclimating Modelling: A Machine Learning Perspective</title>\\n    <summary>  Many machine learning (ML) approaches are widely used to generate bioclimatic\\nmodels for prediction of geographic range of organism as a function of climate.\\nApplications such as prediction of range shift in organism, range of invasive\\nspecies influenced by climate change are important parameters in understanding\\nthe impact of climate change. However, success of machine learning-based\\napproaches depends on a number of factors. While it can be safely said that no\\nparticular ML technique can be effective in all applications and success of a\\ntechnique is predominantly dependent on the application or the type of the\\nproblem, it is useful to understand their behaviour to ensure informed choice\\nof techniques. This paper presents a comprehensive review of machine\\nlearning-based bioclimatic model generation and analyses the factors\\ninfluencing success of such models. Considering the wide use of statistical\\ntechniques, in our discussion we also include conventional statistical\\ntechniques used in bioclimatic modelling.\\n</summary>\\n    <author>\\n      <name>Maumita Bhattacharya</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, In the Proceedings of the 2012 International Joint\\n  Conferences on Computer, Information, and Systems Sciences, and Engineering\\n  (CISSE 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.4152v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.4152v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.0160v2</id>\\n    <updated>2012-03-02T10:14:58Z</updated>\\n    <published>2012-03-01T11:43:43Z</published>\\n    <title>Scaling Datalog for Machine Learning on Big Data</title>\\n    <summary>  In this paper, we present the case for a declarative foundation for\\ndata-intensive machine learning systems. Instead of creating a new system for\\neach specific flavor of machine learning task, or hardcoding new optimizations,\\nwe argue for the use of recursive queries to program a variety of machine\\nlearning systems. By taking this approach, database query optimization\\ntechniques can be utilized to identify effective execution plans, and the\\nresulting runtime plans can be executed on a single unified data-parallel query\\nprocessing engine. As a proof of concept, we consider two programming\\nmodels--Pregel and Iterative Map-Reduce-Update---from the machine learning\\ndomain, and show how they can be captured in Datalog, tuned for a specific\\ntask, and then compiled into an optimized physical plan. Experiments performed\\non a large computing cluster with real data demonstrate that this declarative\\napproach can provide very good performance while offering both increased\\ngenerality and programming ease.\\n</summary>\\n    <author>\\n      <name>Yingyi Bu</name>\\n    </author>\\n    <author>\\n      <name>Vinayak Borkar</name>\\n    </author>\\n    <author>\\n      <name>Michael J. Carey</name>\\n    </author>\\n    <author>\\n      <name>Joshua Rosen</name>\\n    </author>\\n    <author>\\n      <name>Neoklis Polyzotis</name>\\n    </author>\\n    <author>\\n      <name>Tyson Condie</name>\\n    </author>\\n    <author>\\n      <name>Markus Weimer</name>\\n    </author>\\n    <author>\\n      <name>Raghu Ramakrishnan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1203.0160v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.0160v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.PF\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1404.7456v1</id>\\n    <updated>2014-04-28T17:19:25Z</updated>\\n    <published>2014-04-28T17:19:25Z</published>\\n    <title>Automatic Differentiation of Algorithms for Machine Learning</title>\\n    <summary>  Automatic differentiation---the mechanical transformation of numeric computer\\nprograms to calculate derivatives efficiently and accurately---dates to the\\norigin of the computer age. Reverse mode automatic differentiation both\\nantedates and generalizes the method of backwards propagation of errors used in\\nmachine learning. Despite this, practitioners in a variety of fields, including\\nmachine learning, have been little influenced by automatic differentiation, and\\nmake scant use of available tools. Here we review the technique of automatic\\ndifferentiation, describe its two main modes, and explain how it can benefit\\nmachine learning practitioners. To reach the widest possible audience our\\ntreatment assumes only elementary differential calculus, and does not assume\\nany knowledge of linear algebra.\\n</summary>\\n    <author>\\n      <name>Atilim Gunes Baydin</name>\\n    </author>\\n    <author>\\n      <name>Barak A. Pearlmutter</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 1 figure</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1404.7456v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1404.7456v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68W30, 65D25, 68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1.4; I.2.6\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.4463v2</id>\\n    <updated>2015-03-19T15:15:04Z</updated>\\n    <published>2014-05-18T06:28:47Z</published>\\n    <title>Machine Learning in Wireless Sensor Networks: Algorithms, Strategies,\\n  and Applications</title>\\n    <summary>  Wireless sensor networks monitor dynamic environments that change rapidly\\nover time. This dynamic behavior is either caused by external factors or\\ninitiated by the system designers themselves. To adapt to such conditions,\\nsensor networks often adopt machine learning techniques to eliminate the need\\nfor unnecessary redesign. Machine learning also inspires many practical\\nsolutions that maximize resource utilization and prolong the lifespan of the\\nnetwork. In this paper, we present an extensive literature review over the\\nperiod 2002-2013 of machine learning methods that were used to address common\\nissues in wireless sensor networks (WSNs). The advantages and disadvantages of\\neach proposed algorithm are evaluated against the corresponding problem. We\\nalso provide a comparative guide to aid WSN designers in developing suitable\\nmachine learning solutions for their specific application challenges.\\n</summary>\\n    <author>\\n      <name>Mohammad Abu Alsheikh</name>\\n    </author>\\n    <author>\\n      <name>Shaowei Lin</name>\\n    </author>\\n    <author>\\n      <name>Dusit Niyato</name>\\n    </author>\\n    <author>\\n      <name>Hwee-Pink Tan</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/COMST.2014.2320099</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/COMST.2014.2320099\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for publication in IEEE Communications Surveys and Tutorials</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Communications Surveys &amp; Tutorials, vol. 16, no. 4, pp.\\n  1996-2018, Fourthquarter 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1405.4463v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.4463v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.2620v1</id>\\n    <updated>2014-09-09T07:35:40Z</updated>\\n    <published>2014-09-09T07:35:40Z</published>\\n    <title>Learning Machines Implemented on Non-Deterministic Hardware</title>\\n    <summary>  This paper highlights new opportunities for designing large-scale machine\\nlearning systems as a consequence of blurring traditional boundaries that have\\nallowed algorithm designers and application-level practitioners to stay -- for\\nthe most part -- oblivious to the details of the underlying hardware-level\\nimplementations. The hardware/software co-design methodology advocated here\\nhinges on the deployment of compute-intensive machine learning kernels onto\\ncompute platforms that trade-off determinism in the computation for improvement\\nin speed and/or energy efficiency. To achieve this, we revisit digital\\nstochastic circuits for approximating matrix computations that are ubiquitous\\nin machine learning algorithms. Theoretical and empirical evaluation is\\nundertaken to assess the impact of the hardware-induced computational noise on\\nalgorithm performance. As a proof-of-concept, a stochastic hardware simulator\\nis employed for training deep neural networks for image recognition problems.\\n</summary>\\n    <author>\\n      <name>Suyog Gupta</name>\\n    </author>\\n    <author>\\n      <name>Vikas Sindhwani</name>\\n    </author>\\n    <author>\\n      <name>Kailash Gopalakrishnan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.2620v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.2620v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1406.3896v1</id>\\n    <updated>2014-06-16T03:43:20Z</updated>\\n    <published>2014-06-16T03:43:20Z</published>\\n    <title>Freeze-Thaw Bayesian Optimization</title>\\n    <summary>  In this paper we develop a dynamic form of Bayesian optimization for machine\\nlearning models with the goal of rapidly finding good hyperparameter settings.\\nOur method uses the partial information gained during the training of a machine\\nlearning model in order to decide whether to pause training and start a new\\nmodel, or resume the training of a previously-considered model. We specifically\\ntailor our method to machine learning problems by developing a novel\\npositive-definite covariance kernel to capture a variety of training curves.\\nFurthermore, we develop a Gaussian process prior that scales gracefully with\\nadditional temporal observations. Finally, we provide an information-theoretic\\nframework to automate the decision process. Experiments on several common\\nmachine learning models show that our approach is extremely effective in\\npractice.\\n</summary>\\n    <author>\\n      <name>Kevin Swersky</name>\\n    </author>\\n    <author>\\n      <name>Jasper Snoek</name>\\n    </author>\\n    <author>\\n      <name>Ryan Prescott Adams</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1406.3896v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1406.3896v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1602.03943v5</id>\\n    <updated>2017-11-30T18:38:02Z</updated>\\n    <published>2016-02-12T01:38:05Z</published>\\n    <title>Second-Order Stochastic Optimization for Machine Learning in Linear Time</title>\\n    <summary>  First-order stochastic methods are the state-of-the-art in large-scale\\nmachine learning optimization owing to efficient per-iteration complexity.\\nSecond-order methods, while able to provide faster convergence, have been much\\nless explored due to the high cost of computing the second-order information.\\nIn this paper we develop second-order stochastic methods for optimization\\nproblems in machine learning that match the per-iteration cost of gradient\\nbased methods, and in certain settings improve upon the overall running time\\nover popular first-order methods. Furthermore, our algorithm has the desirable\\nproperty of being implementable in time linear in the sparsity of the input\\ndata.\\n</summary>\\n    <author>\\n      <name>Naman Agarwal</name>\\n    </author>\\n    <author>\\n      <name>Brian Bullins</name>\\n    </author>\\n    <author>\\n      <name>Elad Hazan</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research 18(116) (2017) 1-40</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1602.03943v5\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.03943v5\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.07163v1</id>\\n    <updated>2016-06-23T02:08:58Z</updated>\\n    <published>2016-06-23T02:08:58Z</published>\\n    <title>Interpretable Machine Learning Models for the Digital Clock Drawing Test</title>\\n    <summary>  The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular\\nneuropsychological screening tool for cognitive conditions. The Digital Clock\\nDrawing Test (dCDT) uses novel software to analyze data from a digitizing\\nballpoint pen that reports its position with considerable spatial and temporal\\nprecision, making possible the analysis of both the drawing process and final\\nproduct. We developed methodology to analyze pen stroke data from these\\ndrawings, and computed a large collection of features which were then analyzed\\nwith a variety of machine learning techniques. The resulting scoring systems\\nwere designed to be more accurate than the systems currently used by\\nclinicians, but just as interpretable and easy to use. The systems also allow\\nus to quantify the tradeoff between accuracy and interpretability. We created\\nautomated versions of the CDT scoring systems currently used by clinicians,\\nallowing us to benchmark our models, which indicated that our machine learning\\nmodels substantially outperformed the existing scoring systems.\\n</summary>\\n    <author>\\n      <name>William Souillard-Mandar</name>\\n    </author>\\n    <author>\\n      <name>Randall Davis</name>\\n    </author>\\n    <author>\\n      <name>Cynthia Rudin</name>\\n    </author>\\n    <author>\\n      <name>Rhoda Au</name>\\n    </author>\\n    <author>\\n      <name>Dana Penney</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at 2016 ICML Workshop on Human Interpretability in Machine\\n  Learning (WHI 2016), New York, NY</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.07163v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.07163v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1106.4509v1</id>\\n    <updated>2011-06-22T17:12:42Z</updated>\\n    <published>2011-06-22T17:12:42Z</published>\\n    <title>Machine Learning Markets</title>\\n    <summary>  Prediction markets show considerable promise for developing flexible\\nmechanisms for machine learning. Here, machine learning markets for\\nmultivariate systems are defined, and a utility-based framework is established\\nfor their analysis. This differs from the usual approach of defining static\\nbetting functions. It is shown that such markets can implement model\\ncombination methods used in machine learning, such as product of expert and\\nmixture of expert approaches as equilibrium pricing models, by varying agent\\nutility functions. They can also implement models composed of local potentials,\\nand message passing methods. Prediction markets also allow for more flexible\\ncombinations, by combining multiple different utility functions. Conversely,\\nthe market mechanisms implement inference in the relevant probabilistic models.\\nThis means that market mechanism can be utilized for implementing parallelized\\nmodel building and inference for probabilistic modelling.\\n</summary>\\n    <author>\\n      <name>Amos Storkey</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the Fourteenth International Conference on Artificial\\n  Intelligence and Statistics 2011</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research W&amp;CP 15(AISTATS):716-724,\\n  2011</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1106.4509v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1106.4509v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.TR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6443v2</id>\\n    <updated>2012-09-04T17:50:18Z</updated>\\n    <published>2012-06-27T19:59:59Z</published>\\n    <title>Isoelastic Agents and Wealth Updates in Machine Learning Markets</title>\\n    <summary>  Recently, prediction markets have shown considerable promise for developing\\nflexible mechanisms for machine learning. In this paper, agents with isoelastic\\nutilities are considered. It is shown that the costs associated with\\nhomogeneous markets of agents with isoelastic utilities produce equilibrium\\nprices corresponding to alpha-mixtures, with a particular form of mixing\\ncomponent relating to each agent\\'s wealth. We also demonstrate that wealth\\naccumulation for logarithmic and other isoelastic agents (through payoffs on\\nprediction of training targets) can implement both Bayesian model updates and\\nmixture weight updates by imposing different market payoff structures. An\\niterative algorithm is given for market equilibrium computation. We demonstrate\\nthat inhomogeneous markets of agents with isoelastic utilities outperform state\\nof the art aggregate classifiers such as random forests, as well as single\\nclassifiers (neural networks, decision trees) on a number of machine learning\\nbenchmarks, and show that isoelastic combination methods are generally better\\nthan their logarithmic counterparts.\\n</summary>\\n    <author>\\n      <name>Amos Storkey</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Edinburgh</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Jono Millin</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Edinburgh</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Krzysztof Geras</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Edinburgh</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6443v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6443v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1403.0648v1</id>\\n    <updated>2014-03-04T01:14:40Z</updated>\\n    <published>2014-03-04T01:14:40Z</published>\\n    <title>Multi-period Trading Prediction Markets with Connections to Machine\\n  Learning</title>\\n    <summary>  We present a new model for prediction markets, in which we use risk measures\\nto model agents and introduce a market maker to describe the trading process.\\nThis specific choice on modelling tools brings us mathematical convenience. The\\nanalysis shows that the whole market effectively approaches a global objective,\\ndespite that the market is designed such that each agent only cares about its\\nown goal. Additionally, the market dynamics provides a sensible algorithm for\\noptimising the global objective. An intimate connection between machine\\nlearning and our markets is thus established, such that we could 1) analyse a\\nmarket by applying machine learning methods to the global objective, and 2)\\nsolve machine learning problems by setting up and running certain markets.\\n</summary>\\n    <author>\\n      <name>Jinli Hu</name>\\n    </author>\\n    <author>\\n      <name>Amos Storkey</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1403.0648v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1403.0648v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-fin.TR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.05263v4</id>\\n    <updated>2016-02-24T18:58:32Z</updated>\\n    <published>2015-11-17T03:14:46Z</published>\\n    <title>The Use of Machine Learning Algorithms in Recommender Systems: A\\n  Systematic Review</title>\\n    <summary>  Recommender systems use algorithms to provide users with product or service\\nrecommendations. Recently, these systems have been using machine learning\\nalgorithms from the field of artificial intelligence. However, choosing a\\nsuitable machine learning algorithm for a recommender system is difficult\\nbecause of the number of algorithms described in the literature. Researchers\\nand practitioners developing recommender systems are left with little\\ninformation about the current approaches in algorithm usage. Moreover, the\\ndevelopment of a recommender system using a machine learning algorithm often\\nhas problems and open questions that must be evaluated, so software engineers\\nknow where to focus research efforts. This paper presents a systematic review\\nof the literature that analyzes the use of machine learning algorithms in\\nrecommender systems and identifies research opportunities for software\\nengineering research. The study concludes that Bayesian and decision tree\\nalgorithms are widely used in recommender systems because of their relative\\nsimplicity, and that requirement and design phases of recommender system\\ndevelopment appear to offer opportunities for further research.\\n</summary>\\n    <author>\\n      <name>Ivens Portugal</name>\\n    </author>\\n    <author>\\n      <name>Paulo Alencar</name>\\n    </author>\\n    <author>\\n      <name>Donald Cowan</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1511.05263v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.05263v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.06212v1</id>\\n    <updated>2016-03-20T13:32:27Z</updated>\\n    <published>2016-03-20T13:32:27Z</published>\\n    <title>Evaluation of a Tree-based Pipeline Optimization Tool for Automating\\n  Data Science</title>\\n    <summary>  As the field of data science continues to grow, there will be an\\never-increasing demand for tools that make machine learning accessible to\\nnon-experts. In this paper, we introduce the concept of tree-based pipeline\\noptimization for automating one of the most tedious parts of machine\\nlearning---pipeline design. We implement an open source Tree-based Pipeline\\nOptimization Tool (TPOT) in Python and demonstrate its effectiveness on a\\nseries of simulated and real-world benchmark data sets. In particular, we show\\nthat TPOT can design machine learning pipelines that provide a significant\\nimprovement over a basic machine learning analysis while requiring little to no\\ninput nor prior knowledge from the user. We also address the tendency for TPOT\\nto design overly complex pipelines by integrating Pareto optimization, which\\nproduces compact pipelines without sacrificing classification accuracy. As\\nsuch, this work represents an important step toward fully automating machine\\nlearning pipeline design.\\n</summary>\\n    <author>\\n      <name>Randal S. Olson</name>\\n    </author>\\n    <author>\\n      <name>Nathan Bartley</name>\\n    </author>\\n    <author>\\n      <name>Ryan J. Urbanowicz</name>\\n    </author>\\n    <author>\\n      <name>Jason H. Moore</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 5 figures, preprint to appear in GECCO 2016, edits not yet\\n  made from reviewer comments</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1603.06212v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.06212v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1402.6076v1</id>\\n    <updated>2014-02-25T07:50:50Z</updated>\\n    <published>2014-02-25T07:50:50Z</published>\\n    <title>Machine Learning at Scale</title>\\n    <summary>  It takes skill to build a meaningful predictive model even with the abundance\\nof implementations of modern machine learning algorithms and readily available\\ncomputing resources. Building a model becomes challenging if hundreds of\\nterabytes of data need to be processed to produce the training data set. In a\\ndigital advertising technology setting, we are faced with the need to build\\nthousands of such models that predict user behavior and power advertising\\ncampaigns in a 24/7 chaotic real-time production environment. As data\\nscientists, we also have to convince other internal departments critical to\\nimplementation success, our management, and our customers that our machine\\nlearning system works. In this paper, we present the details of the design and\\nimplementation of an automated, robust machine learning platform that impacts\\nbillions of advertising impressions monthly. This platform enables us to\\ncontinuously optimize thousands of campaigns over hundreds of millions of\\nusers, on multiple continents, against varying performance objectives.\\n</summary>\\n    <author>\\n      <name>Sergei Izrailev</name>\\n    </author>\\n    <author>\\n      <name>Jeremy M. Stanley</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to KDD\\'14</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1402.6076v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.6076v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1604.08275v1</id>\\n    <updated>2016-04-28T00:35:32Z</updated>\\n    <published>2016-04-28T00:35:32Z</published>\\n    <title>Crafting Adversarial Input Sequences for Recurrent Neural Networks</title>\\n    <summary>  Machine learning models are frequently used to solve complex security\\nproblems, as well as to make decisions in sensitive situations like guiding\\nautonomous vehicles or predicting financial market behaviors. Previous efforts\\nhave shown that numerous machine learning models were vulnerable to adversarial\\nmanipulations of their inputs taking the form of adversarial samples. Such\\ninputs are crafted by adding carefully selected perturbations to legitimate\\ninputs so as to force the machine learning model to misbehave, for instance by\\noutputting a wrong class if the machine learning task of interest is\\nclassification. In fact, to the best of our knowledge, all previous work on\\nadversarial samples crafting for neural network considered models used to solve\\nclassification tasks, most frequently in computer vision applications. In this\\npaper, we contribute to the field of adversarial machine learning by\\ninvestigating adversarial input sequences for recurrent neural networks\\nprocessing sequential data. We show that the classes of algorithms introduced\\npreviously to craft adversarial samples misclassified by feed-forward neural\\nnetworks can be adapted to recurrent neural networks. In a experiment, we show\\nthat adversaries can craft adversarial sequences misleading both categorical\\nand sequential recurrent neural networks.\\n</summary>\\n    <author>\\n      <name>Nicolas Papernot</name>\\n    </author>\\n    <author>\\n      <name>Patrick McDaniel</name>\\n    </author>\\n    <author>\\n      <name>Ananthram Swami</name>\\n    </author>\\n    <author>\\n      <name>Richard Harang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1604.08275v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1604.08275v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.01224v1</id>\\n    <updated>2016-07-05T12:42:01Z</updated>\\n    <published>2016-07-05T12:42:01Z</published>\\n    <title>Machine Learning for Antimicrobial Resistance</title>\\n    <summary>  Biological datasets amenable to applied machine learning are more available\\ntoday than ever before, yet they lack adequate representation in the\\nData-for-Good community. Here we present a work in progress case study\\nperforming analysis on antimicrobial resistance (AMR) using standard ensemble\\nmachine learning techniques and note the successes and pitfalls such work\\nentails. Broadly, applied machine learning (AML) techniques are well suited to\\nAMR, with classification accuracies ranging from mid-90% to low- 80% depending\\non sample size. Additionally, these techniques prove successful at identifying\\ngene regions known to be associated with the AMR phenotype. We believe that the\\nextensive amount of biological data available, the plethora of problems\\npresented, and the global impact of such work merits the consideration of the\\nData- for-Good community.\\n</summary>\\n    <author>\\n      <name>John W. Santerre</name>\\n    </author>\\n    <author>\\n      <name>James J. Davis</name>\\n    </author>\\n    <author>\\n      <name>Fangfang Xia</name>\\n    </author>\\n    <author>\\n      <name>Rick Stevens</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\\n  Social Good Applications, New York, NY</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1607.01224v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.01224v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.05820v2</id>\\n    <updated>2017-03-31T22:17:07Z</updated>\\n    <published>2016-10-18T22:38:33Z</published>\\n    <title>Membership Inference Attacks against Machine Learning Models</title>\\n    <summary>  We quantitatively investigate how machine learning models leak information\\nabout the individual data records on which they were trained. We focus on the\\nbasic membership inference attack: given a data record and black-box access to\\na model, determine if the record was in the model\\'s training dataset. To\\nperform membership inference against a target model, we make adversarial use of\\nmachine learning and train our own inference model to recognize differences in\\nthe target model\\'s predictions on the inputs that it trained on versus the\\ninputs that it did not train on.\\n  We empirically evaluate our inference techniques on classification models\\ntrained by commercial \"machine learning as a service\" providers such as Google\\nand Amazon. Using realistic datasets and classification tasks, including a\\nhospital discharge dataset whose membership is sensitive from the privacy\\nperspective, we show that these models can be vulnerable to membership\\ninference attacks. We then investigate the factors that influence this leakage\\nand evaluate mitigation strategies.\\n</summary>\\n    <author>\\n      <name>Reza Shokri</name>\\n    </author>\\n    <author>\\n      <name>Marco Stronati</name>\\n    </author>\\n    <author>\\n      <name>Congzheng Song</name>\\n    </author>\\n    <author>\\n      <name>Vitaly Shmatikov</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In the proceedings of the IEEE Symposium on Security and Privacy,\\n  2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.05820v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.05820v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1701.08431v4</id>\\n    <updated>2017-09-06T19:31:51Z</updated>\\n    <published>2017-01-29T21:18:22Z</published>\\n    <title>Source localization in an ocean waveguide using supervised machine\\n  learning</title>\\n    <summary>  Source localization in ocean acoustics is posed as a machine learning problem\\nin which data-driven methods learn source ranges directly from observed\\nacoustic data. The pressure received by a vertical linear array is preprocessed\\nby constructing a normalized sample covariance matrix (SCM) and used as the\\ninput. Three machine learning methods (feed-forward neural networks (FNN),\\nsupport vector machines (SVM) and random forests (RF)) are investigated in this\\npaper, with focus on the FNN. The range estimation problem is solved both as a\\nclassification problem and as a regression problem by these three machine\\nlearning algorithms. The results of range estimation for the Noise09 experiment\\nare compared for FNN, SVM, RF and conventional matched-field processing and\\ndemonstrate the potential of machine learning for underwater source\\nlocalization..\\n</summary>\\n    <author>\\n      <name>Haiqiang Niu</name>\\n    </author>\\n    <author>\\n      <name>Emma Reeves</name>\\n    </author>\\n    <author>\\n      <name>Peter Gerstoft</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1121/1.5000165</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1121/1.5000165\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to The Journal of the Acoustical Society of America</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The Journal of the Acoustical Society of America 142, 1176 (2017)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1701.08431v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1701.08431v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.ao-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.ao-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.geo-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1702.04018v1</id>\\n    <updated>2017-02-13T23:20:22Z</updated>\\n    <published>2017-02-13T23:20:22Z</published>\\n    <title>Intercomparison of Machine Learning Methods for Statistical Downscaling:\\n  The Case of Daily and Extreme Precipitation</title>\\n    <summary>  Statistical downscaling of global climate models (GCMs) allows researchers to\\nstudy local climate change effects decades into the future. A wide range of\\nstatistical models have been applied to downscaling GCMs but recent advances in\\nmachine learning have not been explored. In this paper, we compare four\\nfundamental statistical methods, Bias Correction Spatial Disaggregation (BCSD),\\nOrdinary Least Squares, Elastic-Net, and Support Vector Machine, with three\\nmore advanced machine learning methods, Multi-task Sparse Structure Learning\\n(MSSL), BCSD coupled with MSSL, and Convolutional Neural Networks to downscale\\ndaily precipitation in the Northeast United States. Metrics to evaluate of each\\nmethod\\'s ability to capture daily anomalies, large scale climate shifts, and\\nextremes are analyzed. We find that linear methods, led by BCSD, consistently\\noutperform non-linear approaches. The direct application of state-of-the-art\\nmachine learning methods to statistical downscaling does not provide\\nimprovements over simpler, longstanding approaches.\\n</summary>\\n    <author>\\n      <name>Thomas Vandal</name>\\n    </author>\\n    <author>\\n      <name>Evan Kodra</name>\\n    </author>\\n    <author>\\n      <name>Auroop R Ganguly</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">20 pages, 6 figures, 3 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1702.04018v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1702.04018v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.00426v1</id>\\n    <updated>2017-03-01T18:20:19Z</updated>\\n    <published>2017-03-01T18:20:19Z</published>\\n    <title>HolStep: A Machine Learning Dataset for Higher-order Logic Theorem\\n  Proving</title>\\n    <summary>  Large computer-understandable proofs consist of millions of intermediate\\nlogical steps. The vast majority of such steps originate from manually selected\\nand manually guided heuristics applied to intermediate goals. So far, machine\\nlearning has generally not been used to filter or generate these steps. In this\\npaper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for\\nthe purpose of developing new machine learning-based theorem-proving\\nstrategies. We make this dataset publicly available under the BSD license. We\\npropose various machine learning tasks that can be performed on this dataset,\\nand discuss their significance for theorem proving. We also benchmark a set of\\nsimple baseline machine learning models suited for the tasks (including\\nlogistic regression, convolutional neural networks and recurrent neural\\nnetworks). The results of our baseline models show the promise of applying\\nmachine learning to HOL theorem proving.\\n</summary>\\n    <author>\\n      <name>Cezary Kaliszyk</name>\\n    </author>\\n    <author>\\n      <name>Fran\\xc3\\xa7ois Chollet</name>\\n    </author>\\n    <author>\\n      <name>Christian Szegedy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.00426v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.00426v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.00512v1</id>\\n    <updated>2017-03-01T21:20:11Z</updated>\\n    <published>2017-03-01T21:20:11Z</published>\\n    <title>PMLB: A Large Benchmark Suite for Machine Learning Evaluation and\\n  Comparison</title>\\n    <summary>  The selection, development, or comparison of machine learning methods in data\\nmining can be a difficult task based on the target problem and goals of a\\nparticular study. Numerous publicly available real-world and simulated\\nbenchmark datasets have emerged from different sources, but their organization\\nand adoption as standards have been inconsistent. As such, selecting and\\ncurating specific benchmarks remains an unnecessary burden on machine learning\\npractitioners and data scientists. The present study introduces an accessible,\\ncurated, and developing public benchmark resource to facilitate identification\\nof the strengths and weaknesses of different machine learning methodologies. We\\ncompare meta-features among the current set of benchmark datasets in this\\nresource to characterize the diversity of available data. Finally, we apply a\\nnumber of established machine learning methods to the entire benchmark suite\\nand analyze how datasets and algorithms cluster in terms of performance. This\\nwork is an important first step towards understanding the limitations of\\npopular benchmarking suites and developing a resource that connects existing\\nbenchmarking standards to more diverse and efficient standards in the future.\\n</summary>\\n    <author>\\n      <name>Randal S. Olson</name>\\n    </author>\\n    <author>\\n      <name>William La Cava</name>\\n    </author>\\n    <author>\\n      <name>Patryk Orzechowski</name>\\n    </author>\\n    <author>\\n      <name>Ryan J. Urbanowicz</name>\\n    </author>\\n    <author>\\n      <name>Jason H. Moore</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 5 figures, submitted for review to JMLR</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.00512v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.00512v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.06777v1</id>\\n    <updated>2017-03-20T14:42:27Z</updated>\\n    <published>2017-03-20T14:42:27Z</published>\\n    <title>On the Use of Default Parameter Settings in the Empirical Evaluation of\\n  Classification Algorithms</title>\\n    <summary>  We demonstrate that, for a range of state-of-the-art machine learning\\nalgorithms, the differences in generalisation performance obtained using\\ndefault parameter settings and using parameters tuned via cross-validation can\\nbe similar in magnitude to the differences in performance observed between\\nstate-of-the-art and uncompetitive learning systems. This means that fair and\\nrigorous evaluation of new learning algorithms requires performance comparison\\nagainst benchmark methods with best-practice model selection procedures, rather\\nthan using default parameter settings. We investigate the sensitivity of three\\nkey machine learning algorithms (support vector machine, random forest and\\nrotation forest) to their default parameter settings, and provide guidance on\\ndetermining sensible default parameter values for implementations of these\\nalgorithms. We also conduct an experimental comparison of these three\\nalgorithms on 121 classification problems and find that, perhaps surprisingly,\\nrotation forest is significantly more accurate on average than both random\\nforest and a support vector machine.\\n</summary>\\n    <author>\\n      <name>Anthony Bagnall</name>\\n    </author>\\n    <author>\\n      <name>Gavin C. Cawley</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1703.06777v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.06777v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.07915v1</id>\\n    <updated>2017-03-23T03:17:14Z</updated>\\n    <published>2017-03-23T03:17:14Z</published>\\n    <title>Perspective: Energy Landscapes for Machine Learning</title>\\n    <summary>  Machine learning techniques are being increasingly used as flexible\\nnon-linear fitting and prediction tools in the physical sciences. Fitting\\nfunctions that exhibit multiple solutions as local minima can be analysed in\\nterms of the corresponding machine learning landscape. Methods to explore and\\nvisualise molecular potential energy landscapes can be applied to these machine\\nlearning landscapes to gain new insight into the solution space involved in\\ntraining and the nature of the corresponding predictions. In particular, we can\\ndefine quantities analogous to molecular structure, thermodynamics, and\\nkinetics, and relate these emergent properties to the structure of the\\nunderlying landscape. This Perspective aims to describe these analogies with\\nexamples from recent applications, and suggest avenues for new\\ninterdisciplinary research.\\n</summary>\\n    <author>\\n      <name>Andrew J. Ballard</name>\\n    </author>\\n    <author>\\n      <name>Ritankar Das</name>\\n    </author>\\n    <author>\\n      <name>Stefano Martiniani</name>\\n    </author>\\n    <author>\\n      <name>Dhagash Mehta</name>\\n    </author>\\n    <author>\\n      <name>Levent Sagun</name>\\n    </author>\\n    <author>\\n      <name>Jacob D. Stevenson</name>\\n    </author>\\n    <author>\\n      <name>David J. Wales</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1039/C7CP01108C</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1039/C7CP01108C\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">41 pages, 25 figures. Accepted for publication in Physical Chemistry\\n  Chemical Physics, 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1703.07915v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.07915v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"hep-th\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.08339v2</id>\\n    <updated>2017-11-16T05:25:30Z</updated>\\n    <published>2017-09-25T06:37:09Z</published>\\n    <title>Machine Learning for Networking: Workflow, Advances and Opportunities</title>\\n    <summary>  Recently, machine learning has been used in every possible field to leverage\\nits amazing power. For a long time, the net-working and distributed computing\\nsystem is the key infrastructure to provide efficient computational resource\\nfor machine learning. Networking itself can also benefit from this promising\\ntechnology. This article focuses on the application of Machine Learning\\ntechniques for Networking (MLN), which can not only help solve the intractable\\nold network questions but also stimulate new network applications. In this\\narticle, we summarize the basic workflow to explain how to apply the machine\\nlearning technology in the networking domain. Then we provide a selective\\nsurvey of the latest representative advances with explanations on their design\\nprinciples and benefits. These advances are divided into several network design\\nobjectives and the detailed information of how they perform in each step of MLN\\nworkflow is presented. Finally, we shed light on the new opportunities on\\nnetworking design and community building of this new inter-discipline. Our goal\\nis to provide a broad research guideline on networking with machine learning to\\nhelp and motivate researchers to develop innovative algorithms, standards and\\nframeworks.\\n</summary>\\n    <author>\\n      <name>Mowei Wang</name>\\n    </author>\\n    <author>\\n      <name>Yong Cui</name>\\n    </author>\\n    <author>\\n      <name>Xin Wang</name>\\n    </author>\\n    <author>\\n      <name>Shihan Xiao</name>\\n    </author>\\n    <author>\\n      <name>Junchen Jiang</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/MNET.2017.1700200</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/MNET.2017.1700200\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1709.08339v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.08339v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.10824v2</id>\\n    <updated>2018-03-10T09:03:36Z</updated>\\n    <published>2017-10-30T09:37:20Z</published>\\n    <title>Rough extreme learning machine: a new classification method based on\\n  uncertainty measure</title>\\n    <summary>  Extreme learning machine (ELM) is a new single hidden layer feedback neural\\nnetwork. The weights of the input layer and the biases of neurons in hidden\\nlayer are randomly generated, the weights of the output layer can be\\nanalytically determined. ELM has been achieved good results for a large number\\nof classification tasks. In this paper, a new extreme learning machine called\\nrough extreme learning machine (RELM) was proposed. RELM uses rough set to\\ndivide data into upper approximation set and lower approximation set, and the\\ntwo approximation sets are utilized to train upper approximation neurons and\\nlower approximation neurons. In addition, an attribute reduction is executed in\\nthis algorithm to remove redundant attributes. The experimental results showed,\\ncomparing with the comparison algorithms, RELM can get a better accuracy and\\nrepeatability in most cases, RELM can not only maintain the advantages of fast\\nspeed, but also effectively cope with the classification task for\\nhigh-dimensional data.\\n</summary>\\n    <author>\\n      <name>Lin Feng</name>\\n    </author>\\n    <author>\\n      <name>Shuliang Xu</name>\\n    </author>\\n    <author>\\n      <name>Feilong Wang</name>\\n    </author>\\n    <author>\\n      <name>Shenglan Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">23 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1710.10824v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.10824v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.00644v1</id>\\n    <updated>2017-12-02T17:35:40Z</updated>\\n    <published>2017-12-02T17:35:40Z</published>\\n    <title>Short-term Mortality Prediction for Elderly Patients Using Medicare\\n  Claims Data</title>\\n    <summary>  Risk prediction is central to both clinical medicine and public health. While\\nmany machine learning models have been developed to predict mortality, they are\\nrarely applied in the clinical literature, where classification tasks typically\\nrely on logistic regression. One reason for this is that existing machine\\nlearning models often seek to optimize predictions by incorporating features\\nthat are not present in the databases readily available to providers and policy\\nmakers, limiting generalizability and implementation. Here we tested a number\\nof machine learning classifiers for prediction of six-month mortality in a\\npopulation of elderly Medicare beneficiaries, using an administrative claims\\ndatabase of the kind available to the majority of health care payers and\\nproviders. We show that machine learning classifiers substantially outperform\\ncurrent widely-used methods of risk prediction but only when used with an\\nimproved feature set incorporating insights from clinical medicine, developed\\nfor this study. Our work has applications to supporting patient and provider\\ndecision making at the end of life, as well as population health-oriented\\nefforts to identify patients at high risk of poor outcomes.\\n</summary>\\n    <author>\\n      <name>Maggie Makar</name>\\n    </author>\\n    <author>\\n      <name>Marzyeh Ghassemi</name>\\n    </author>\\n    <author>\\n      <name>David Cutler</name>\\n    </author>\\n    <author>\\n      <name>Ziad Obermeyer</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1712.00644v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.00644v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.09208v1</id>\\n    <updated>2017-12-26T08:37:39Z</updated>\\n    <published>2017-12-26T08:37:39Z</published>\\n    <title>Machine Learning Cosmic Expansion History</title>\\n    <summary>  We use the machine learning techniques, for the first time, to study the\\nbackground evolution of the universe in light of 30 cosmic chronometers. From 7\\nmachine learning algorithms, using the principle of mean squared error\\nminimization on testing set, we find that Bayesian ridge regression is the\\noptimal method to extract the information from cosmic chronometers. By use of a\\npower-law polynomial expansion, we obtain the first Hubble constant estimation\\n$H_0=65.95^{+6.98}_{-6.36}$ km s$^{-1}$ Mpc$^{-1}$ from machine learning. From\\nthe view of machine learning, we may rule out a large number of cosmological\\nmodels, the number of physical parameters of which containing $H_0$ is larger\\nthan 3. Very importantly and interestingly, we find that the parameter spaces\\nof 3 specific cosmological models can all be clearly compressed by considering\\nboth their explanation and generalization abilities.\\n</summary>\\n    <author>\\n      <name>Deng Wang</name>\\n    </author>\\n    <author>\\n      <name>Wei Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4.5 pages, 7 figures. This is the first work using machine learning\\n  algorithms to study the dark energy</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.09208v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.09208v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"astro-ph.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph.HE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"gr-qc\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.00779v1</id>\\n    <updated>2017-12-31T02:06:30Z</updated>\\n    <published>2017-12-31T02:06:30Z</published>\\n    <title>Machine Learning for Building Energy and Indoor Environment: A\\n  Perspective</title>\\n    <summary>  Machine learning is a promising technique for many practical applications. In\\nthis perspective, we illustrate the development and application for machine\\nlearning. It is indicated that the theories and applications of machine\\nlearning method in the field of energy conservation and indoor environment are\\nnot mature, due to the difficulty of the determination for model structure with\\nbetter prediction. In order to significantly contribute to the problems, we\\nutilize the ANN model to predict the indoor culturable fungi concentration,\\nwhich achieves the better accuracy and convenience. The proposal of hybrid\\nmethod is further expand the application fields of machine learning method.\\nFurther, ANN model based on HTS was successfully applied for the optimization\\nof building energy system. We hope that this novel method could capture more\\nattention from investigators via our introduction and perspective, due to its\\npotential development with accuracy and reliability. However, its feasibility\\nin other fields needs to be promoted further.\\n</summary>\\n    <author>\\n      <name>Zhijian Liu</name>\\n    </author>\\n    <author>\\n      <name>Di Wu</name>\\n    </author>\\n    <author>\\n      <name>Hongyu Wei</name>\\n    </author>\\n    <author>\\n      <name>Guoqing Cao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to a Interdisciplinary Journal</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.00779v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.00779v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.07295v1</id>\\n    <updated>2018-02-20T19:20:31Z</updated>\\n    <published>2018-02-20T19:20:31Z</published>\\n    <title>Attack Strength vs. Detectability Dilemma in Adversarial Machine\\n  Learning</title>\\n    <summary>  As the prevalence and everyday use of machine learning algorithms, along with\\nour reliance on these algorithms grow dramatically, so do the efforts to attack\\nand undermine these algorithms with malicious intent, resulting in a growing\\ninterest in adversarial machine learning. A number of approaches have been\\ndeveloped that can render a machine learning algorithm ineffective through\\npoisoning or other types of attacks. Most attack algorithms typically use\\nsophisticated optimization approaches, whose objective function is designed to\\ncause maximum damage with respect to accuracy and performance of the algorithm\\nwith respect to some task. In this effort, we show that while such an objective\\nfunction is indeed brutally effective in causing maximum damage on an embedded\\nfeature selection task, it often results in an attack mechanism that can be\\neasily detected with an embarrassingly simple novelty or outlier detection\\nalgorithm. We then propose an equally simple yet elegant solution by adding a\\nregularization term to the attacker\\'s objective function that penalizes\\noutlying attack points.\\n</summary>\\n    <author>\\n      <name>Christopher Frederickson</name>\\n    </author>\\n    <author>\\n      <name>Michael Moore</name>\\n    </author>\\n    <author>\\n      <name>Glenn Dawson</name>\\n    </author>\\n    <author>\\n      <name>Robi Polikar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 9 figures, submitted to IJCNN 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1802.07295v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.07295v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.07954v1</id>\\n    <updated>2018-02-22T09:48:56Z</updated>\\n    <published>2018-02-22T09:48:56Z</published>\\n    <title>The State of the Art in Integrating Machine Learning into Visual\\n  Analytics</title>\\n    <summary>  Visual analytics systems combine machine learning or other analytic\\ntechniques with interactive data visualization to promote sensemaking and\\nanalytical reasoning. It is through such techniques that people can make sense\\nof large, complex data. While progress has been made, the tactful combination\\nof machine learning and data visualization is still under-explored. This\\nstate-of-the-art report presents a summary of the progress that has been made\\nby highlighting and synthesizing select research advances. Further, it presents\\nopportunities and challenges to enhance the synergy between machine learning\\nand visual analytics for impactful future research directions.\\n</summary>\\n    <author>\\n      <name>A. Endert</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>W. Ribarsky</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>C. Turkay</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>W Wong</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>I. Nabney</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>I D\\xc3\\xadaz Blanco</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Fabrice Rossi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">SAMM</arxiv:affiliation>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1111/cgf.13092</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1111/cgf.13092\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Computer Graphics Forum, Wiley, 2017, 36 (8), pp.458 - 486</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1802.07954v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.07954v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.10185v1</id>\\n    <updated>2018-02-27T22:03:56Z</updated>\\n    <published>2018-02-27T22:03:56Z</published>\\n    <title>Trustless Machine Learning Contracts; Evaluating and Exchanging Machine\\n  Learning Models on the Ethereum Blockchain</title>\\n    <summary>  Using blockchain technology, it is possible to create contracts that offer a\\nreward in exchange for a trained machine learning model for a particular data\\nset. This would allow users to train machine learning models for a reward in a\\ntrustless manner. The smart contract will use the blockchain to automatically\\nvalidate the solution, so there would be no debate about whether the solution\\nwas correct or not. Users who submit the solutions won\\'t have counterparty risk\\nthat they won\\'t get paid for their work. Contracts can be created easily by\\nanyone with a dataset, even programmatically by software agents. This creates a\\nmarket where parties who are good at solving machine learning problems can\\ndirectly monetize their skillset, and where any organization or software agent\\nthat has a problem to solve with AI can solicit solutions from all over the\\nworld. This will incentivize the creation of better machine learning models,\\nand make AI more accessible to companies and software agents.\\n</summary>\\n    <author>\\n      <name>A. Besir Kurtulmus</name>\\n    </author>\\n    <author>\\n      <name>Kenny Daniel</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.10185v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.10185v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.07433v2</id>\\n    <updated>2018-06-12T12:54:48Z</updated>\\n    <published>2018-04-20T02:43:41Z</published>\\n    <title>Two Use Cases of Machine Learning for SDN-Enabled IP/Optical Networks:\\n  Traffic Matrix Prediction and Optical Path Performance Prediction</title>\\n    <summary>  We describe two applications of machine learning in the context of IP/Optical\\nnetworks. The first one allows agile management of resources at a core\\nIP/Optical network by using machine learning for short-term and long-term\\nprediction of traffic flows and joint global optimization of IP and optical\\nlayers using colorless/directionless (CD) flexible ROADMs. Multilayer\\ncoordination allows for significant cost savings, flexible new services to meet\\ndynamic capacity needs, and improved robustness by being able to proactively\\nadapt to new traffic patterns and network conditions. The second application is\\nimportant as we migrate our metro networks to Open ROADM networks, to allow\\nphysical routing without the need for detailed knowledge of optical parameters.\\nWe discuss a proof-of-concept study, where detailed performance data for\\nwavelengths on a current flexible ROADM network is used for machine learning to\\npredict the optical performance of each wavelength. Both applications can be\\nefficiently implemented by using a SDN (Software Defined Network) controller.\\n</summary>\\n    <author>\\n      <name>Gagan Choudhury</name>\\n    </author>\\n    <author>\\n      <name>David Lynch</name>\\n    </author>\\n    <author>\\n      <name>Gaurav Thakur</name>\\n    </author>\\n    <author>\\n      <name>Simon Tse</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.07433v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.07433v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.00878v1</id>\\n    <updated>2018-05-02T15:48:11Z</updated>\\n    <published>2018-05-02T15:48:11Z</published>\\n    <title>Modelling tourism demand to Spain with machine learning techniques. The\\n  impact of forecast horizon on model selection</title>\\n    <summary>  This study assesses the influence of the forecast horizon on the forecasting\\nperformance of several machine learning techniques. We compare the fo recast\\naccuracy of Support Vector Regression (SVR) to Neural Network (NN) models,\\nusing a linear model as a benchmark. We focus on international tourism demand\\nto all seventeen regions of Spain. The SVR with a Gaussian radial basis\\nfunction kernel outperforms the rest of the models for the longest forecast\\nhorizons. We also find that machine learning methods improve their forecasting\\naccuracy with respect to linear models as forecast horizons increase. This\\nresult shows the suitability of SVR for medium and long term forecasting.\\n</summary>\\n    <author>\\n      <name>Oscar Claveria</name>\\n    </author>\\n    <author>\\n      <name>Enric Monte</name>\\n    </author>\\n    <author>\\n      <name>Salvador Torra</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">24 pages, 3 figures, 6 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Claveria, O., Monte, E., and Torra, S. (2016): Modelling tourism\\n  demand to Spain with machine learning techniques. The impact of forecast\\n  horizon on model selection. Revista de Economia Aplicada, 24 (72), 109-132</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1805.00878v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.00878v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1805.04058v1</id>\\n    <updated>2018-05-10T16:51:40Z</updated>\\n    <published>2018-05-10T16:51:40Z</published>\\n    <title>Ariadne: Analysis for Machine Learning Program</title>\\n    <summary>  Machine learning has transformed domains like vision and translation, and is\\nnow increasingly used in science, where the correctness of such code is vital.\\nPython is popular for machine learning, in part because of its wealth of\\nmachine learning libraries, and is felt to make development faster; however,\\nthis dynamic language has less support for error detection at code creation\\ntime than tools like Eclipse. This is especially problematic for machine\\nlearning: given its statistical nature, code with subtle errors may run and\\nproduce results that look plausible but are meaningless. This can vitiate\\nscientific results. We report on Ariadne: applying a static framework, WALA, to\\nmachine learning code that uses TensorFlow. We have created static analysis for\\nPython, a type system for tracking tensors---Tensorflow\\'s core data\\nstructures---and a data flow analysis to track their usage. We report on how it\\nwas built and present some early results.\\n</summary>\\n    <author>\\n      <name>Julian Dolby</name>\\n    </author>\\n    <author>\\n      <name>Avraham Shinnar</name>\\n    </author>\\n    <author>\\n      <name>Allison Allain</name>\\n    </author>\\n    <author>\\n      <name>Jenna Reinen</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3211346.3211349</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3211346.3211349\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1805.04058v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.04058v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.03461v1</id>\\n    <updated>2018-06-09T11:40:22Z</updated>\\n    <published>2018-06-09T11:40:22Z</published>\\n    <title>TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service</title>\\n    <summary>  Machine learning methods are widely used for a variety of prediction\\nproblems. \\\\emph{Prediction as a service} is a paradigm in which service\\nproviders with technological expertise and computational resources may perform\\npredictions for clients. However, data privacy severely restricts the\\napplicability of such services, unless measures to keep client data private\\n(even from the service provider) are designed. Equally important is to minimize\\nthe amount of computation and communication required between client and server.\\nFully homomorphic encryption offers a possible way out, whereby clients may\\nencrypt their data, and on which the server may perform arithmetic\\ncomputations. The main drawback of using fully homomorphic encryption is the\\namount of time required to evaluate large machine learning models on encrypted\\ndata. We combine ideas from the machine learning literature, particularly work\\non binarization and sparsification of neural networks, together with\\nalgorithmic tools to speed-up and parallelize computation using encrypted data.\\n</summary>\\n    <author>\\n      <name>Amartya Sanyal</name>\\n    </author>\\n    <author>\\n      <name>Matt J. Kusner</name>\\n    </author>\\n    <author>\\n      <name>Adri\\xc3\\xa0 Gasc\\xc3\\xb3n</name>\\n    </author>\\n    <author>\\n      <name>Varun Kanade</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted at International Conference in Machine Learning (ICML), 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.03461v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.03461v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.00243v3</id>\\n    <updated>2018-07-12T03:02:31Z</updated>\\n    <published>2018-06-30T23:17:41Z</published>\\n    <title>chemmodlab: A Cheminformatics Modeling Laboratory for Fitting and\\n  Assessing Machine Learning Models</title>\\n    <summary>  The goal of chemmodlab is to streamline the fitting and assessment pipeline\\nfor many machine learning models in R, making it easy for researchers to\\ncompare the utility of new models. While focused on implementing methods for\\nmodel fitting and assessment that have been accepted by experts in the\\ncheminformatics field, all of the methods in chemmodlab have broad utility for\\nthe machine learning community. chemmodlab contains several assessment\\nutilities including a plotting function that constructs accumulation curves and\\na function that computes many performance measures. The most novel feature of\\nchemmodlab is the ease with which statistically significant performance\\ndifferences for many machine learning models is presented by means of the\\nmultiple comparisons similarity plot. Differences are assessed using repeated\\nk-fold cross validation where blocking increases precision and multiplicity\\nadjustments are applied.\\n</summary>\\n    <author>\\n      <name>Jeremy R. Ash Jacqueline M. Hughes-Oliver</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages, 10 figures, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.00243v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.00243v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.07215v3</id>\\n    <updated>2019-02-01T05:28:56Z</updated>\\n    <published>2018-07-19T02:01:36Z</published>\\n    <title>Machine Learning Classifiers Do Not Improve the Prediction of Academic\\n  Risk: Evidence from Australia</title>\\n    <summary>  Machine learning methods tend to outperform traditional statistical models at\\nprediction. In the prediction of academic achievement, ML models have not shown\\nsubstantial improvement over linear and logistic regression. So far, these\\nresults have almost entirely focused on college achievement, due to the\\navailability of administrative datasets, and have contained relatively small\\nsample sizes by ML standards. In this article we apply popular machine learning\\nmodels to a large dataset ($n=2.2$ million) containing primary and middle\\nschool performance on NAPLAN, a test given annually to all Australian students\\nin grades 3, 5, 7, and 9. We show that machine learning models do not\\noutperform logistic regression for detecting students who will perform in the\\n`below standard\\' band of achievement upon sitting their next test.\\n</summary>\\n    <author>\\n      <name>Sarah Cornell-Farrow</name>\\n    </author>\\n    <author>\\n      <name>Robert Garrard</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">20 pages, 6 tables, 6 figures. Note that previous versions of this\\n  paper contained an error in our codes. The error has been rectified and the\\n  paper substantially rewritten</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.07215v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.07215v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.03601v1</id>\\n    <updated>2018-08-10T15:59:31Z</updated>\\n    <published>2018-08-10T15:59:31Z</published>\\n    <title>Using Randomness to Improve Robustness of Machine-Learning Models\\n  Against Evasion Attacks</title>\\n    <summary>  Machine learning models have been widely used in security applications such\\nas intrusion detection, spam filtering, and virus or malware detection.\\nHowever, it is well-known that adversaries are always trying to adapt their\\nattacks to evade detection. For example, an email spammer may guess what\\nfeatures spam detection models use and modify or remove those features to avoid\\ndetection. There has been some work on making machine learning models more\\nrobust to such attacks. However, one simple but promising approach called {\\\\em\\nrandomization} is underexplored. This paper proposes a novel\\nrandomization-based approach to improve robustness of machine learning models\\nagainst evasion attacks. The proposed approach incorporates randomization into\\nboth model training time and model application time (meaning when the model is\\nused to detect attacks). We also apply this approach to random forest, an\\nexisting ML method which already has some degree of randomness. Experiments on\\nintrusion detection and spam filtering data show that our approach further\\nimproves robustness of random-forest method. We also discuss how this approach\\ncan be applied to other ML models.\\n</summary>\\n    <author>\\n      <name>Fan Yang</name>\\n    </author>\\n    <author>\\n      <name>Zhiyuan Chen</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.03601v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.03601v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.08067v1</id>\\n    <updated>2018-09-21T12:49:27Z</updated>\\n    <published>2018-09-21T12:49:27Z</published>\\n    <title>Learning of Tree-Structured Gaussian Graphical Models on Distributed\\n  Data under Communication Constraints</title>\\n    <summary>  In this paper, learning of tree-structured Gaussian graphical models from\\ndistributed data is addressed. In our model, samples are stored in a set of\\ndistributed machines where each machine has access to only a subset of\\nfeatures. A central machine is then responsible for learning the structure\\nbased on received messages from the other nodes. We present a set of\\ncommunication efficient strategies, which are theoretically proved to convey\\nsufficient information for reliable learning of the structure. In particular,\\nour analyses show that even if each machine sends only the signs of its local\\ndata samples to the central node, the tree structure can still be recovered\\nwith high accuracy. Our simulation results on both synthetic and real-world\\ndatasets show that our strategies achieve a desired accuracy in inferring the\\nunderlying structure, while spending a small budget on communication.\\n</summary>\\n    <author>\\n      <name>Mostafa Tavassolipour</name>\\n    </author>\\n    <author>\\n      <name>Seyed Abolfazl Motahari</name>\\n    </author>\\n    <author>\\n      <name>Mohammad-Taghi Manzuri Shalmani</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TSP.2018.2876325</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TSP.2018.2876325\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1809.08067v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.08067v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.00024v1</id>\\n    <updated>2018-09-28T18:13:26Z</updated>\\n    <published>2018-09-28T18:13:26Z</published>\\n    <title>Explainable Black-Box Attacks Against Model-based Authentication</title>\\n    <summary>  Establishing unique identities for both humans and end systems has been an\\nactive research problem in the security community, giving rise to innovative\\nmachine learning-based authentication techniques. Although such techniques\\noffer an automated method to establish identity, they have not been vetted\\nagainst sophisticated attacks that target their core machine learning\\ntechnique. This paper demonstrates that mimicking the unique signatures\\ngenerated by host fingerprinting and biometric authentication systems is\\npossible. We expose the ineffectiveness of underlying machine learning\\nclassification models by constructing a blind attack based around the query\\nsynthesis framework and utilizing Explainable-AI (XAI) techniques. We launch an\\nattack in under 130 queries on a state-of-the-art face authentication system,\\nand under 100 queries on a host authentication system. We examine how these\\nattacks can be defended against and explore their limitations. XAI provides an\\neffective means for adversaries to infer decision boundaries and provides a new\\nway forward in constructing attacks against systems using machine learning\\nmodels for authentication.\\n</summary>\\n    <author>\\n      <name>Washington Garcia</name>\\n    </author>\\n    <author>\\n      <name>Joseph I. Choi</name>\\n    </author>\\n    <author>\\n      <name>Suman K. Adari</name>\\n    </author>\\n    <author>\\n      <name>Somesh Jha</name>\\n    </author>\\n    <author>\\n      <name>Kevin R. B. Butler</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.00024v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.00024v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.06992v2</id>\\n    <updated>2019-01-17T17:38:12Z</updated>\\n    <published>2018-10-13T01:54:08Z</published>\\n    <title>Topographic Representation for Quantum Machine Learning</title>\\n    <summary>  This paper proposes a brain-inspired approach to quantum machine learning\\nwith the goal of circumventing many of the complications of other approaches.\\nThe fact that quantum processes are unitary presents both opportunities and\\nchallenges. A principal opportunity is that a large number of computations can\\nbe carried out in parallel in linear superposition, that is, quantum\\nparallelism. The challenge is that the process is linear, and most approaches\\nto machine learning depend significantly on nonlinear processes. Fortunately,\\nthe situation is not hopeless, for we know that nonlinear processes can be\\nembedded in unitary processes, as is familiar from the circuit model of quantum\\ncomputation. This paper explores an approach to the quantum implementation of\\nmachine learning involving nonlinear functions operating on information\\nrepresented topographically (by computational maps), as common in neural\\ncortex.\\n</summary>\\n    <author>\\n      <name>Bruce MacLennan</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.06992v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.06992v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"81P68\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"B.2; I.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.10731v3</id>\\n    <updated>2018-12-05T02:02:46Z</updated>\\n    <published>2018-10-25T06:17:34Z</published>\\n    <title>Law and Adversarial Machine Learning</title>\\n    <summary>  When machine learning systems fail because of adversarial manipulation, how\\nshould society expect the law to respond? Through scenarios grounded in\\nadversarial ML literature, we explore how some aspects of computer crime,\\ncopyright, and tort law interface with perturbation, poisoning, model stealing\\nand model inversion attacks to show how some attacks are more likely to result\\nin liability than others. We end with a call for action to ML researchers to\\ninvest in transparent benchmarks of attacks and defenses; architect ML systems\\nwith forensics in mind and finally, think more about adversarial machine\\nlearning in the context of civil liberties. The paper is targeted towards ML\\nresearchers who have no legal background.\\n</summary>\\n    <author>\\n      <name>Ram Shankar Siva Kumar</name>\\n    </author>\\n    <author>\\n      <name>David R. O\\'Brien</name>\\n    </author>\\n    <author>\\n      <name>Kendra Albert</name>\\n    </author>\\n    <author>\\n      <name>Salome Vilojen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Minor edits. Corrected typos, Added references. 4 pages, submitted to\\n  NIPS 2018 Workshop on Security in Machine Learning</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.10731v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.10731v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.07051v1</id>\\n    <updated>2018-11-16T22:25:10Z</updated>\\n    <published>2018-11-16T22:25:10Z</published>\\n    <title>Symmetry constrained machine learning</title>\\n    <summary>  Symmetry, a central concept in understanding the laws of nature, has been\\nused for centuries in physics, mathematics, and chemistry, to help make\\nmathematical models tractable. Yet, despite its power, symmetry has not been\\nused extensively in machine learning, until rather recently. In this article we\\nshow a general way to incorporate symmetries into machine learning models. We\\ndemonstrate this with a detailed analysis on a rather simple real world machine\\nlearning system - a neural network for classifying handwritten digits, lacking\\nbias terms for every neuron. We demonstrate that ignoring symmetries can have\\ndire over-fitting consequences, and that incorporating symmetry into the model\\nreduces over-fitting, while at the same time reducing complexity, ultimately\\nrequiring less training data, and taking less time and resources to train.\\n</summary>\\n    <author>\\n      <name>Doron L. Bergman</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.07051v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.07051v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.data-an\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.01105v1</id>\\n    <updated>2018-11-29T19:30:08Z</updated>\\n    <published>2018-11-29T19:30:08Z</published>\\n    <title>Correspondence Analysis of Government Expenditure Patterns</title>\\n    <summary>  We analyze expenditure patterns of discretionary funds by Brazilian congress\\nmembers. This analysis is based on a large dataset containing over $7$ million\\nexpenses made publicly available by the Brazilian government. This dataset has,\\nup to now, remained widely untouched by machine learning methods. Our main\\ncontributions are two-fold: (i) we provide a novel dataset benchmark for\\nmachine learning-based efforts for government transparency to the broader\\nresearch community, and (ii) introduce a neural network-based approach for\\nanalyzing and visualizing outlying expense patterns. Our hope is that the\\napproach presented here can inspire new machine learning methodologies for\\ngovernment transparency applicable to other developing nations.\\n</summary>\\n    <author>\\n      <name>Hsiang Hsu</name>\\n    </author>\\n    <author>\\n      <name>Flavio P. Calmon</name>\\n    </author>\\n    <author>\\n      <name>Jos\\xc3\\xa9 C\\xc3\\xa2ndido Silveira Santos Filho</name>\\n    </author>\\n    <author>\\n      <name>Andre P. Calmon</name>\\n    </author>\\n    <author>\\n      <name>Salman Salamatian</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at NIPS 2018 Workshop on Machine Learning for the\\n  Developing World</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.01105v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.01105v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.00774v1</id>\\n    <updated>2019-01-02T04:48:19Z</updated>\\n    <published>2019-01-02T04:48:19Z</published>\\n    <title>A New Strategy in Applying the Learning Machine to Study Phase\\n  Transitions</title>\\n    <summary>  In this Letter, we present a new strategy for applying the learning machine\\nto study phase transitions. We train the learning machine with samples only\\nobtained at a non-critical parameter point, aiming to establish intrinsic\\ncorrelations between the learning machine and the target system. Then, we find\\nthat the accuracy of the learning machine, which is the most important\\nperformance index in conventional learning machines, is no longer a key goal of\\nthe training in our approach. Instead, relatively low accuracy of identifying\\nunlabeled data category can help to determine the critical point with greater\\nprecision, manifesting the singularity around the critical point. It thus\\nprovides a robust tool to study the phase transition. The classical\\nferromagnetic and percolation phase transitions are employed as illustrative\\nexamples.\\n</summary>\\n    <author>\\n      <name>Rongxing Xu</name>\\n    </author>\\n    <author>\\n      <name>Weicheng Fu</name>\\n    </author>\\n    <author>\\n      <name>Hong Zhao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.00774v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.00774v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.class-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.08013v1</id>\\n    <updated>2018-11-20T08:42:41Z</updated>\\n    <published>2018-11-20T08:42:41Z</published>\\n    <title>DarwinML: A Graph-based Evolutionary Algorithm for Automated Machine\\n  Learning</title>\\n    <summary>  As an emerging field, Automated Machine Learning (AutoML) aims to reduce or\\neliminate manual operations that require expertise in machine learning. In this\\npaper, a graph-based architecture is employed to represent flexible\\ncombinations of ML models, which provides a large searching space compared to\\ntree-based and stacking-based architectures. Based on this, an evolutionary\\nalgorithm is proposed to search for the best architecture, where the mutation\\nand heredity operators are the key for architecture evolution. With Bayesian\\nhyper-parameter optimization, the proposed approach can automate the workflow\\nof machine learning. On the PMLB dataset, the proposed approach shows the\\nstate-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn.\\nSome of the optimized models are with complex structures which are difficult to\\nobtain in manual design.\\n</summary>\\n    <author>\\n      <name>Fei Qi</name>\\n    </author>\\n    <author>\\n      <name>Zhaohui Xia</name>\\n    </author>\\n    <author>\\n      <name>Gaoyang Tang</name>\\n    </author>\\n    <author>\\n      <name>Hang Yang</name>\\n    </author>\\n    <author>\\n      <name>Yu Song</name>\\n    </author>\\n    <author>\\n      <name>Guangrui Qian</name>\\n    </author>\\n    <author>\\n      <name>Xiong An</name>\\n    </author>\\n    <author>\\n      <name>Chunhuan Lin</name>\\n    </author>\\n    <author>\\n      <name>Guangming Shi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 7 figures, 3 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.08013v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.08013v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.10002v1</id>\\n    <updated>2019-01-28T21:00:20Z</updated>\\n    <published>2019-01-28T21:00:20Z</published>\\n    <title>A Framework for Understanding Unintended Consequences of Machine\\n  Learning</title>\\n    <summary>  As machine learning increasingly affects people and society, it is important\\nthat we strive for a comprehensive and unified understanding of how and why\\nunwanted consequences arise. For instance, downstream harms to particular\\ngroups are often blamed on \"biased data,\" but this concept encompass too many\\nissues to be useful in developing solutions. In this paper, we provide a\\nframework that partitions sources of downstream harm in machine learning into\\nfive distinct categories spanning the data generation and machine learning\\npipeline. We describe how these issues arise, how they are relevant to\\nparticular applications, and how they motivate different solutions. In doing\\nso, we aim to facilitate the development of solutions that stem from an\\nunderstanding of application-specific populations and data generation\\nprocesses, rather than relying on general claims about what may or may not be\\n\"fair.\"\\n</summary>\\n    <author>\\n      <name>Harini Suresh</name>\\n    </author>\\n    <author>\\n      <name>John V. Guttag</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.10002v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.10002v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.03501v1</id>\\n    <updated>2019-02-09T21:49:36Z</updated>\\n    <published>2019-02-09T21:49:36Z</published>\\n    <title>Assessing the Local Interpretability of Machine Learning Models</title>\\n    <summary>  The increasing adoption of machine learning tools has led to calls for\\naccountability via model interpretability. But what does it mean for a machine\\nlearning model to be interpretable by humans, and how can this be assessed? We\\nfocus on two definitions of interpretability that have been introduced in the\\nmachine learning literature: simulatability (a user\\'s ability to run a model on\\na given input) and \"what if\" local explainability (a user\\'s ability to\\ncorrectly indicate the outcome to a model under local changes to the input).\\nThrough a user study with 1000 participants, we test whether humans perform\\nwell on tasks that mimic the definitions of simulatability and \"what if\" local\\nexplainability on models that are typically considered locally interpretable.\\nWe find evidence consistent with the common intuition that decision trees and\\nlogistic regression models are interpretable and are more interpretable than\\nneural networks. We propose a metric - the runtime operation count on the\\nsimulatability task - to indicate the relative interpretability of models and\\nshow that as the number of operations increases the users\\' accuracy on the\\nlocal interpretability tasks decreases.\\n</summary>\\n    <author>\\n      <name>Sorelle A. Friedler</name>\\n    </author>\\n    <author>\\n      <name>Chitradeep Dutta Roy</name>\\n    </author>\\n    <author>\\n      <name>Carlos Scheidegger</name>\\n    </author>\\n    <author>\\n      <name>Dylan Slack</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.03501v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.03501v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.05009v1</id>\\n    <updated>2019-02-13T17:03:33Z</updated>\\n    <published>2019-02-13T17:03:33Z</published>\\n    <title>ATMSeer: Increasing Transparency and Controllability in Automated\\n  Machine Learning</title>\\n    <summary>  To relieve the pain of manually selecting machine learning algorithms and\\ntuning hyperparameters, automated machine learning (AutoML) methods have been\\ndeveloped to automatically search for good models. Due to the huge model search\\nspace, it is impossible to try all models. Users tend to distrust automatic\\nresults and increase the search budget as much as they can, thereby undermining\\nthe efficiency of AutoML. To address these issues, we design and implement\\nATMSeer, an interactive visualization tool that supports users in refining the\\nsearch space of AutoML and analyzing the results. To guide the design of\\nATMSeer, we derive a workflow of using AutoML based on interviews with machine\\nlearning experts. A multi-granularity visualization is proposed to enable users\\nto monitor the AutoML process, analyze the searched models, and refine the\\nsearch space in real time. We demonstrate the utility and usability of ATMSeer\\nthrough two case studies, expert interviews, and a user study with 13 end\\nusers.\\n</summary>\\n    <author>\\n      <name>Qianwen Wang</name>\\n    </author>\\n    <author>\\n      <name>Yao Ming</name>\\n    </author>\\n    <author>\\n      <name>Zhihua Jin</name>\\n    </author>\\n    <author>\\n      <name>Qiaomu Shen</name>\\n    </author>\\n    <author>\\n      <name>Dongyu Liu</name>\\n    </author>\\n    <author>\\n      <name>Micah J. Smith</name>\\n    </author>\\n    <author>\\n      <name>Kalyan Veeramachaneni</name>\\n    </author>\\n    <author>\\n      <name>Huamin Qu</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3290605.3300911</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3290605.3300911\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in the ACM Conference on Human Factors in Computing Systems\\n  (CHI), 2019, Glasgow, Scotland UK</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.05009v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.05009v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.00278v1</id>\\n    <updated>2019-03-01T13:15:55Z</updated>\\n    <published>2019-03-01T13:15:55Z</published>\\n    <title>Continuous Integration of Machine Learning Models with ease.ml/ci:\\n  Towards a Rigorous Yet Practical Treatment</title>\\n    <summary>  Continuous integration is an indispensable step of modern software\\nengineering practices to systematically manage the life cycles of system\\ndevelopment. Developing a machine learning model is no difference - it is an\\nengineering process with a life cycle, including design, implementation,\\ntuning, testing, and deployment. However, most, if not all, existing continuous\\nintegration engines do not support machine learning as first-class citizens.\\n  In this paper, we present ease.ml/ci, to our best knowledge, the first\\ncontinuous integration system for machine learning. The challenge of building\\nease.ml/ci is to provide rigorous guarantees, e.g., single accuracy point error\\ntolerance with 0.999 reliability, with a practical amount of labeling effort,\\ne.g., 2K labels per test. We design a domain specific language that allows\\nusers to specify integration conditions with reliability constraints, and\\ndevelop simple novel optimizations that can lower the number of labels required\\nby up to two orders of magnitude for test conditions popularly used in real\\nproduction systems.\\n</summary>\\n    <author>\\n      <name>Cedric Renggli</name>\\n    </author>\\n    <author>\\n      <name>Bojan Karla\\xc5\\xa1</name>\\n    </author>\\n    <author>\\n      <name>Bolin Ding</name>\\n    </author>\\n    <author>\\n      <name>Feng Liu</name>\\n    </author>\\n    <author>\\n      <name>Kevin Schawinski</name>\\n    </author>\\n    <author>\\n      <name>Wentao Wu</name>\\n    </author>\\n    <author>\\n      <name>Ce Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.00278v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.00278v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.02175v1</id>\\n    <updated>2019-03-06T05:07:58Z</updated>\\n    <published>2019-03-06T05:07:58Z</published>\\n    <title>Materials development by interpretable machine learning</title>\\n    <summary>  Machine learning technologies are expected to be great tools for scientific\\ndiscoveries. In particular, materials development (which has brought a lot of\\ninnovation by finding new and better functional materials) is one of the most\\nattractive scientific fields. To apply machine learning to actual materials\\ndevelopment, collaboration between scientists and machine learning is becoming\\ninevitable. However, such collaboration has been restricted so far due to black\\nbox machine learning, in which it is difficult for scientists to interpret the\\ndata-driven model from the viewpoint of material science and physics. Here, we\\nshow a material development success story that was achieved by good\\ncollaboration between scientists and one type of interpretable (explainable)\\nmachine learning called factorized asymptotic Bayesian inference hierarchical\\nmixture of experts (FAB/HMEs). Based on material science and physics, we\\ninterpreted the data-driven model constructed by the FAB/HMEs, so that we\\ndiscovered surprising correlation and knowledge about thermoelectric material.\\nGuided by this, we carried out actual material synthesis that led to\\nidentification of a novel spin-driven thermoelectric material with the largest\\nthermopower to date.\\n</summary>\\n    <author>\\n      <name>Yuma Iwasaki</name>\\n    </author>\\n    <author>\\n      <name>Ryoto Sawada</name>\\n    </author>\\n    <author>\\n      <name>Valentin Stanev</name>\\n    </author>\\n    <author>\\n      <name>Masahiko Ishida</name>\\n    </author>\\n    <author>\\n      <name>Akihiro Kirihara</name>\\n    </author>\\n    <author>\\n      <name>Yasutomo Omori</name>\\n    </author>\\n    <author>\\n      <name>Hiroko Someya</name>\\n    </author>\\n    <author>\\n      <name>Ichiro Takeuchi</name>\\n    </author>\\n    <author>\\n      <name>Eiji Saitoh</name>\\n    </author>\\n    <author>\\n      <name>Yorozu Shinichi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.02175v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.02175v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.08061v2</id>\\n    <updated>2019-03-20T14:07:30Z</updated>\\n    <published>2019-03-19T15:44:27Z</published>\\n    <title>Few-shot machine learning in the three-dimensional Ising model</title>\\n    <summary>  We investigate theoretically the phase transition in three dimensional cubic\\nIsing model utilizing state-of-the-art machine learning algorithms. Supervised\\nmachine learning models show high accuracies (~99\\\\%) in phase classification\\nand very small relative errors ($&lt; 10^{-4}$) of the energies in different spin\\nconfigurations. Unsupervised machine learning models are introduced to study\\nthe spin configuration reconstructions and reductions, and the phases of\\nreconstructed spin configurations can be accurately classified by a linear\\nlogistic algorithm. Based on the comparison between various machine learning\\nmodels, we develop a few-shot strategy to predict phase transitions in larger\\nlattices from trained sample in smaller lattices. The few-shot machine learning\\nstrategy for three dimensional(3D) Ising model enable us to study 3D ising\\nmodel efficiently and provides a new integrated and highly accurate approach to\\nother spin models.\\n</summary>\\n    <author>\\n      <name>Rui Zhang</name>\\n    </author>\\n    <author>\\n      <name>Bin Wei</name>\\n    </author>\\n    <author>\\n      <name>Dong Zhang</name>\\n    </author>\\n    <author>\\n      <name>Jia-Ji Zhu</name>\\n    </author>\\n    <author>\\n      <name>Kai Chang</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevB.99.094427</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevB.99.094427\" rel=\"related\"/>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Phys. Rev. B 99, 094427(2019)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1903.08061v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.08061v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.10563v1</id>\\n    <updated>2019-03-25T19:34:24Z</updated>\\n    <published>2019-03-25T19:34:24Z</published>\\n    <title>Machine learning and the physical sciences</title>\\n    <summary>  Machine learning encompasses a broad range of algorithms and modeling tools\\nused for a vast array of data processing tasks, which has entered most\\nscientific disciplines in recent years. We review in a selective way the recent\\nresearch on the interface between machine learning and physical sciences.This\\nincludes conceptual developments in machine learning (ML) motivated by physical\\ninsights, applications of machine learning techniques to several domains in\\nphysics, and cross-fertilization between the two fields. After giving basic\\nnotion of machine learning methods and principles, we describe examples of how\\nstatistical physics is used to understand methods in ML. We then move to\\ndescribe applications of ML methods in particle physics and cosmology, quantum\\nmany body physics, quantum computing, and chemical and material physics. We\\nalso highlight research and development into novel computing architectures\\naimed at accelerating ML. In each of the sections we describe recent successes\\nas well as domain-specific methodology and challenges.\\n</summary>\\n    <author>\\n      <name>Giuseppe Carleo</name>\\n    </author>\\n    <author>\\n      <name>Ignacio Cirac</name>\\n    </author>\\n    <author>\\n      <name>Kyle Cranmer</name>\\n    </author>\\n    <author>\\n      <name>Laurent Daudet</name>\\n    </author>\\n    <author>\\n      <name>Maria Schuld</name>\\n    </author>\\n    <author>\\n      <name>Naftali Tishby</name>\\n    </author>\\n    <author>\\n      <name>Leslie Vogt-Maranto</name>\\n    </author>\\n    <author>\\n      <name>Lenka Zdeborov\\xc3\\xa1</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.10563v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.10563v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"astro-ph.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"hep-th\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.10572v1</id>\\n    <updated>2019-03-25T19:52:17Z</updated>\\n    <published>2019-03-25T19:52:17Z</published>\\n    <title>On the Functional Equivalence of TSK Fuzzy Systems to Neural Networks,\\n  Mixture of Experts, CART, and Stacking Ensemble Regression</title>\\n    <summary>  Fuzzy systems have achieved great success in numerous applications. However,\\nthere are still many challenges in designing an optimal fuzzy system, e.g., how\\nto efficiently train its parameters, how to improve its performance without\\nadding too many parameters, how to balance the trade-off between cooperations\\nand competitions among the rules, how to overcome the curse of dimensionality,\\netc. Literature has shown that by making appropriate connections between fuzzy\\nsystems and other machine learning approaches, good practices from other\\ndomains may be used to improve the fuzzy systems, and vice versa. This paper\\ngives an overview on the functional equivalence between Takagi-Sugeno-Kang\\nfuzzy systems and four classic machine learning approaches -- neural networks,\\nmixture of experts, classification and regression trees, and stacking ensemble\\nregression -- for regression problems. We also point out some promising new\\nresearch directions, inspired by the functional equivalence, that could lead to\\nsolutions to the aforementioned problems. To our knowledge, this is so far the\\nmost comprehensive overview on the connections between fuzzy systems and other\\npopular machine learning approaches, and hopefully will stimulate more\\nhybridization between different machine learning algorithms.\\n</summary>\\n    <author>\\n      <name>Dongrui Wu</name>\\n    </author>\\n    <author>\\n      <name>Chin-Teng Lin</name>\\n    </author>\\n    <author>\\n      <name>Jian Huang</name>\\n    </author>\\n    <author>\\n      <name>Zhigang Zeng</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.10572v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.10572v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.10735v1</id>\\n    <updated>2019-03-26T08:43:38Z</updated>\\n    <published>2019-03-26T08:43:38Z</published>\\n    <title>Interoperability and machine-to-machine translation model with mappings\\n  to machine learning tasks</title>\\n    <summary>  Modern large-scale automation systems integrate thousands to hundreds of\\nthousands of physical sensors and actuators. Demands for more flexible\\nreconfiguration of production systems and optimization across different\\ninformation models, standards and legacy systems challenge current system\\ninteroperability concepts. Automatic semantic translation across information\\nmodels and standards is an increasingly important problem that needs to be\\naddressed to fulfill these demands in a cost-efficient manner under constraints\\nof human capacity and resources in relation to timing requirements and system\\ncomplexity. Here we define a translator-based operational interoperability\\nmodel for interacting cyber-physical systems in mathematical terms, which\\nincludes system identification and ontology-based translation as special cases.\\nWe present alternative mathematical definitions of the translator learning task\\nand mappings to similar machine learning tasks and solutions based on recent\\ndevelopments in machine learning. Possibilities to learn translators between\\nartefacts without a common physical context, for example in simulations of\\ndigital twins and across layers of the automation pyramid are briefly\\ndiscussed.\\n</summary>\\n    <author>\\n      <name>Jacob Nilsson</name>\\n    </author>\\n    <author>\\n      <name>Fredrik Sandin</name>\\n    </author>\\n    <author>\\n      <name>Jerker Delsing</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 2 figures, 1 table, 1 listing. Submitted to the IEEE\\n  International Conference on Industrial Informatics 2019, INDIN\\'19</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.10735v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.10735v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.00753v1</id>\\n    <updated>2019-02-10T23:31:04Z</updated>\\n    <published>2019-02-10T23:31:04Z</published>\\n    <title>SCADA System Testbed for Cybersecurity Research Using Machine Learning\\n  Approach</title>\\n    <summary>  This paper presents the development of a Supervisory Control and Data\\nAcquisition (SCADA) system testbed used for cybersecurity research. The testbed\\nconsists of a water storage tank\\'s control system, which is a stage in the\\nprocess of water treatment and distribution. Sophisticated cyber-attacks were\\nconducted against the testbed. During the attacks, the network traffic was\\ncaptured, and features were extracted from the traffic to build a dataset for\\ntraining and testing different machine learning algorithms. Five traditional\\nmachine learning algorithms were trained to detect the attacks: Random Forest,\\nDecision Tree, Logistic Regression, Naive Bayes and KNN. Then, the trained\\nmachine learning models were built and deployed in the network, where new tests\\nwere made using online network traffic. The performance obtained during the\\ntraining and testing of the machine learning models was compared to the\\nperformance obtained during the online deployment of these models in the\\nnetwork. The results show the efficiency of the machine learning models in\\ndetecting the attacks in real time. The testbed provides a good understanding\\nof the effects and consequences of attacks on real SCADA environments\\n</summary>\\n    <author>\\n      <name>Marcio Andrey Teixeira</name>\\n    </author>\\n    <author>\\n      <name>Tara Salman</name>\\n    </author>\\n    <author>\\n      <name>Maede Zolanvari</name>\\n    </author>\\n    <author>\\n      <name>Raj Jain</name>\\n    </author>\\n    <author>\\n      <name>Nader Meskin</name>\\n    </author>\\n    <author>\\n      <name>Mohammed Samaka</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.3390/fi10080076</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.3390/fi10080076\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">E-Preprint</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Future Internet 2018, 10(8), 76</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1904.00753v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.00753v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.02773v1</id>\\n    <updated>2019-04-04T20:03:46Z</updated>\\n    <published>2019-04-04T20:03:46Z</published>\\n    <title>Adaptive Sequential Machine Learning</title>\\n    <summary>  A framework previously introduced in [3] for solving a sequence of stochastic\\noptimization problems with bounded changes in the minimizers is extended and\\napplied to machine learning problems such as regression and classification. The\\nstochastic optimization problems arising in these machine learning problems is\\nsolved using algorithms such as stochastic gradient descent (SGD). A method\\nbased on estimates of the change in the minimizers and properties of the\\noptimization algorithm is introduced for adaptively selecting the number of\\nsamples at each time step to ensure that the excess risk, i.e., the expected\\ngap between the loss achieved by the approximate minimizer produced by the\\noptimization algorithm and the exact minimizer, does not exceed a target level.\\nA bound is developed to show that the estimate of the change in the minimizers\\nis non-trivial provided that the excess risk is small enough. Extensions\\nrelevant to the machine learning setting are considered, including a cost-based\\napproach to select the number of samples with a cost budget over a fixed\\nhorizon, and an approach to applying cross-validation for model selection.\\nFinally, experiments with synthetic and real data are used to validate the\\nalgorithms.\\n</summary>\\n    <author>\\n      <name>Craig Wilson</name>\\n    </author>\\n    <author>\\n      <name>Yuheng Bu</name>\\n    </author>\\n    <author>\\n      <name>Venugopal Veeravalli</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1509.07422</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1904.02773v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.02773v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.03673v1</id>\\n    <updated>2019-04-07T15:43:59Z</updated>\\n    <published>2019-04-07T15:43:59Z</published>\\n    <title>Every Local Minimum is a Global Minimum of an Induced Model</title>\\n    <summary>  For non-convex optimization in machine learning, this paper proves that every\\nlocal minimum achieves the global optimality of the perturbable gradient basis\\nmodel at any differentiable point. As a result, non-convex machine learning is\\ntheoretically as supported as convex machine learning with a hand-crafted basis\\nin terms of the loss at differentiable local minima, except in the case when a\\npreference is given to the hand-crafted basis over the perturbable gradient\\nbasis. The proofs of these results are derived under mild assumptions.\\nAccordingly, the proven results are directly applicable to many machine\\nlearning models, including practical deep neural networks, without any\\nmodification of practical methods. Furthermore, as special cases of our general\\nresults, this paper improves or complements several state-of-the-art\\ntheoretical results in the literature with a simple and unified proof\\ntechnique.\\n</summary>\\n    <author>\\n      <name>Kenji Kawaguchi</name>\\n    </author>\\n    <author>\\n      <name>Leslie Pack Kaelbling</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.03673v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.03673v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1512.01914v1</id>\\n    <updated>2015-12-07T05:20:30Z</updated>\\n    <published>2015-12-07T05:20:30Z</published>\\n    <title>Rademacher Complexity of the Restricted Boltzmann Machine</title>\\n    <summary>  Boltzmann machine, as a fundamental construction block of deep belief network\\nand deep Boltzmann machines, is widely used in deep learning community and\\ngreat success has been achieved. However, theoretical understanding of many\\naspects of it is still far from clear. In this paper, we studied the Rademacher\\ncomplexity of both the asymptotic restricted Boltzmann machine and the\\npractical implementation with single-step contrastive divergence (CD-1)\\nprocedure. Our results disclose the fact that practical implementation training\\nprocedure indeed increased the Rademacher complexity of restricted Boltzmann\\nmachines. A further research direction might be the investigation of the VC\\ndimension of a compositional function used in the CD-1 procedure.\\n</summary>\\n    <author>\\n      <name>Xiao Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1512.01914v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.01914v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.06008v2</id>\\n    <updated>2019-01-18T10:35:56Z</updated>\\n    <published>2017-08-20T19:29:44Z</published>\\n    <title>Boltzmann machines and energy-based models</title>\\n    <summary>  We review Boltzmann machines and energy-based models. A Boltzmann machine\\ndefines a probability distribution over binary-valued patterns. One can learn\\nparameters of a Boltzmann machine via gradient based approaches in a way that\\nlog likelihood of data is increased. The gradient and Hessian of a Boltzmann\\nmachine admit beautiful mathematical representations, although computing them\\nis in general intractable. This intractability motivates approximate methods,\\nincluding Gibbs sampler and contrastive divergence, and tractable alternatives,\\nnamely energy-based models.\\n</summary>\\n    <author>\\n      <name>Takayuki Osogami</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">36 pages. The topics covered in this paper are presented in Part I of\\n  IJCAI-17 tutorial on energy-based machine learning.\\n  https://researcher.watson.ibm.com/researcher/view_group.php?id=7834</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1708.06008v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.06008v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.09626v1</id>\\n    <updated>2018-01-29T17:03:34Z</updated>\\n    <published>2018-01-29T17:03:34Z</published>\\n    <title>Human-Machine Inference Networks For Smart Decision Making:\\n  Opportunities and Challenges</title>\\n    <summary>  The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines\\ncomplementary cognitive strengths of humans and machines in an intelligent\\nmanner to tackle various inference tasks and achieves higher performance than\\neither humans or machines by themselves. While inference performance\\noptimization techniques for human-only or sensor-only networks are quite\\nmature, HuMaINs require novel signal processing and machine learning solutions.\\nIn this paper, we present an overview of the HuMaINs architecture with a focus\\non three main issues that include architecture design, inference algorithms\\nincluding security/privacy challenges, and application areas/use cases.\\n</summary>\\n    <author>\\n      <name>Aditya Vempaty</name>\\n    </author>\\n    <author>\\n      <name>Bhavya Kailkhura</name>\\n    </author>\\n    <author>\\n      <name>Pramod K. Varshney</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1801.09626v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.09626v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.04890v1</id>\\n    <updated>2017-10-13T12:22:03Z</updated>\\n    <published>2017-10-13T12:22:03Z</published>\\n    <title>Knowledge is at the Edge! How to Search in Distributed Machine Learning\\n  Models</title>\\n    <summary>  With the advent of the Internet of Things and Industry 4.0 an enormous amount\\nof data is produced at the edge of the network. Due to a lack of computing\\npower, this data is currently send to the cloud where centralized machine\\nlearning models are trained to derive higher level knowledge. With the recent\\ndevelopment of specialized machine learning hardware for mobile devices, a new\\nera of distributed learning is about to begin that raises a new research\\nquestion: How can we search in distributed machine learning models? Machine\\nlearning at the edge of the network has many benefits, such as low-latency\\ninference and increased privacy. Such distributed machine learning models can\\nalso learn personalized for a human user, a specific context, or application\\nscenario. As training data stays on the devices, control over possibly\\nsensitive data is preserved as it is not shared with a third party. This new\\nform of distributed learning leads to the partitioning of knowledge between\\nmany devices which makes access difficult. In this paper we tackle the problem\\nof finding specific knowledge by forwarding a search request (query) to a\\ndevice that can answer it best. To that end, we use a entropy based quality\\nmetric that takes the context of a query and the learning quality of a device\\ninto account. We show that our forwarding strategy can achieve over 95%\\naccuracy in a urban mobility scenario where we use data from 30 000 people\\ncommuting in the city of Trento, Italy.\\n</summary>\\n    <author>\\n      <name>Thomas Bach</name>\\n    </author>\\n    <author>\\n      <name>Muhammad Adnan Tariq</name>\\n    </author>\\n    <author>\\n      <name>Ruben Mayer</name>\\n    </author>\\n    <author>\\n      <name>Kurt Rothermel</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-319-69462-7_27</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-319-69462-7_27\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in CoopIS 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1710.04890v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.04890v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.03476v1</id>\\n    <updated>2016-06-10T20:51:29Z</updated>\\n    <published>2016-06-10T20:51:29Z</published>\\n    <title>Generative Adversarial Imitation Learning</title>\\n    <summary>  Consider learning a policy from example expert behavior, without interaction\\nwith the expert or access to reinforcement signal. One approach is to recover\\nthe expert\\'s cost function with inverse reinforcement learning, then extract a\\npolicy from that cost function with reinforcement learning. This approach is\\nindirect and can be slow. We propose a new general framework for directly\\nextracting a policy from data, as if it were obtained by reinforcement learning\\nfollowing inverse reinforcement learning. We show that a certain instantiation\\nof our framework draws an analogy between imitation learning and generative\\nadversarial networks, from which we derive a model-free imitation learning\\nalgorithm that obtains significant performance gains over existing model-free\\nmethods in imitating complex behaviors in large, high-dimensional environments.\\n</summary>\\n    <author>\\n      <name>Jonathan Ho</name>\\n    </author>\\n    <author>\\n      <name>Stefano Ermon</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1606.03476v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.03476v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.10009v1</id>\\n    <updated>2018-08-29T18:40:26Z</updated>\\n    <published>2018-08-29T18:40:26Z</published>\\n    <title>Learning a Policy for Opportunistic Active Learning</title>\\n    <summary>  Active learning identifies data points to label that are expected to be the\\nmost useful in improving a supervised model. Opportunistic active learning\\nincorporates active learning into interactive tasks that constrain possible\\nqueries during interactions. Prior work has shown that opportunistic active\\nlearning can be used to improve grounding of natural language descriptions in\\nan interactive object retrieval task. In this work, we use reinforcement\\nlearning for such an object retrieval task, to learn a policy that effectively\\ntrades off task completion with model improvement that would benefit future\\ntasks.\\n</summary>\\n    <author>\\n      <name>Aishwarya Padmakumar</name>\\n    </author>\\n    <author>\\n      <name>Peter Stone</name>\\n    </author>\\n    <author>\\n      <name>Raymond J. Mooney</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">EMNLP 2018 Camera Ready</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">EMNLP 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1808.10009v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.10009v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.02718v1</id>\\n    <updated>2016-06-08T20:00:01Z</updated>\\n    <published>2016-06-08T20:00:01Z</published>\\n    <title>Learning Thermodynamics with Boltzmann Machines</title>\\n    <summary>  A Boltzmann machine is a stochastic neural network that has been extensively\\nused in the layers of deep architectures for modern machine learning\\napplications. In this paper, we develop a Boltzmann machine that is capable of\\nmodelling thermodynamic observables for physical systems in thermal\\nequilibrium. Through unsupervised learning, we train the Boltzmann machine on\\ndata sets constructed with spin configurations importance-sampled from the\\npartition function of an Ising Hamiltonian at different temperatures using\\nMonte Carlo (MC) methods. The trained Boltzmann machine is then used to\\ngenerate spin states, for which we compare thermodynamic observables to those\\ncomputed by direct MC sampling. We demonstrate that the Boltzmann machine can\\nfaithfully reproduce the observables of the physical system. Further, we\\nobserve that the number of neurons required to obtain accurate results\\nincreases as the system is brought close to criticality.\\n</summary>\\n    <author>\\n      <name>Giacomo Torlai</name>\\n    </author>\\n    <author>\\n      <name>Roger G. Melko</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1103/PhysRevB.94.165134</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1103/PhysRevB.94.165134\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 5 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Phys. Rev. B 94, 165134 (2016)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1606.02718v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.02718v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1409.4814v1</id>\\n    <updated>2014-09-16T21:45:22Z</updated>\\n    <published>2014-09-16T21:45:22Z</published>\\n    <title>ICE: Enabling Non-Experts to Build Models Interactively for Large-Scale\\n  Lopsided Problems</title>\\n    <summary>  Quick interaction between a human teacher and a learning machine presents\\nnumerous benefits and challenges when working with web-scale data. The human\\nteacher guides the machine towards accomplishing the task of interest. The\\nlearning machine leverages big data to find examples that maximize the training\\nvalue of its interaction with the teacher. When the teacher is restricted to\\nlabeling examples selected by the machine, this problem is an instance of\\nactive learning. When the teacher can provide additional information to the\\nmachine (e.g., suggestions on what examples or predictive features should be\\nused) as the learning task progresses, then the problem becomes one of\\ninteractive learning.\\n  To accommodate the two-way communication channel needed for efficient\\ninteractive learning, the teacher and the machine need an environment that\\nsupports an interaction language. The machine can access, process, and\\nsummarize more examples than the teacher can see in a lifetime. Based on the\\nmachine\\'s output, the teacher can revise the definition of the task or make it\\nmore precise. Both the teacher and the machine continuously learn and benefit\\nfrom the interaction.\\n  We have built a platform to (1) produce valuable and deployable models and\\n(2) support research on both the machine learning and user interface challenges\\nof the interactive learning problem. The platform relies on a dedicated,\\nlow-latency, distributed, in-memory architecture that allows us to construct\\nweb-scale learning machines with quick interaction speed. The purpose of this\\npaper is to describe this architecture and demonstrate how it supports our\\nresearch efforts. Preliminary results are presented as illustrations of the\\narchitecture but are not the primary focus of the paper.\\n</summary>\\n    <author>\\n      <name>Patrice Simard</name>\\n    </author>\\n    <author>\\n      <name>David Chickering</name>\\n    </author>\\n    <author>\\n      <name>Aparna Lakshmiratan</name>\\n    </author>\\n    <author>\\n      <name>Denis Charles</name>\\n    </author>\\n    <author>\\n      <name>Leon Bottou</name>\\n    </author>\\n    <author>\\n      <name>Carlos Garcia Jurado Suarez</name>\\n    </author>\\n    <author>\\n      <name>David Grangier</name>\\n    </author>\\n    <author>\\n      <name>Saleema Amershi</name>\\n    </author>\\n    <author>\\n      <name>Johan Verwey</name>\\n    </author>\\n    <author>\\n      <name>Jina Suh</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1409.4814v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.4814v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0704.3453v1</id>\\n    <updated>2007-04-25T21:23:31Z</updated>\\n    <published>2007-04-25T21:23:31Z</published>\\n    <title>An Adaptive Strategy for the Classification of G-Protein Coupled\\n  Receptors</title>\\n    <summary>  One of the major problems in computational biology is the inability of\\nexisting classification models to incorporate expanding and new domain\\nknowledge. This problem of static classification models is addressed in this\\npaper by the introduction of incremental learning for problems in\\nbioinformatics. Many machine learning tools have been applied to this problem\\nusing static machine learning structures such as neural networks or support\\nvector machines that are unable to accommodate new information into their\\nexisting models. We utilize the fuzzy ARTMAP as an alternate machine learning\\nsystem that has the ability of incrementally learning new data as it becomes\\navailable. The fuzzy ARTMAP is found to be comparable to many of the widespread\\nmachine learning systems. The use of an evolutionary strategy in the selection\\nand combination of individual classifiers into an ensemble system, coupled with\\nthe incremental learning ability of the fuzzy ARTMAP is proven to be suitable\\nas a pattern classifier. The algorithm presented is tested using data from the\\nG-Coupled Protein Receptors Database and shows good accuracy of 83%. The system\\npresented is also generally applicable, and can be used in problems in genomics\\nand proteomics.\\n</summary>\\n    <author>\\n      <name>S. Mohamed</name>\\n    </author>\\n    <author>\\n      <name>D. Rubin</name>\\n    </author>\\n    <author>\\n      <name>T. Marwala</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 5 tables, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/0704.3453v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0704.3453v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.7093v3</id>\\n    <updated>2013-04-08T14:26:49Z</updated>\\n    <published>2013-03-28T11:01:53Z</published>\\n    <title>Relevance As a Metric for Evaluating Machine Learning Algorithms</title>\\n    <summary>  In machine learning, the choice of a learning algorithm that is suitable for\\nthe application domain is critical. The performance metric used to compare\\ndifferent algorithms must also reflect the concerns of users in the application\\ndomain under consideration. In this work, we propose a novel probability-based\\nperformance metric called Relevance Score for evaluating supervised learning\\nalgorithms. We evaluate the proposed metric through empirical analysis on a\\ndataset gathered from an intelligent lighting pilot installation. In comparison\\nto the commonly used Classification Accuracy metric, the Relevance Score proves\\nto be more appropriate for a certain class of applications.\\n</summary>\\n    <author>\\n      <name>Aravind Kota Gopalakrishna</name>\\n    </author>\\n    <author>\\n      <name>Tanir Ozcelebi</name>\\n    </author>\\n    <author>\\n      <name>Antonio Liotta</name>\\n    </author>\\n    <author>\\n      <name>Johan J. Lukkien</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-642-39712-7_15</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-642-39712-7_15\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To Appear at International Conference on Machine Learning and Data\\n  Mining (MLDM 2013), 14 pages, 6 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1303.7093v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.7093v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1106.1770v3</id>\\n    <updated>2011-10-04T06:02:16Z</updated>\\n    <published>2011-06-09T10:40:08Z</published>\\n    <title>Reinforcement learning based sensing policy optimization for energy\\n  efficient cognitive radio networks</title>\\n    <summary>  This paper introduces a machine learning based collaborative multi-band\\nspectrum sensing policy for cognitive radios. The proposed sensing policy\\nguides secondary users to focus the search of unused radio spectrum to those\\nfrequencies that persistently provide them high data rate. The proposed policy\\nis based on machine learning, which makes it adaptive with the temporally and\\nspatially varying radio spectrum. Furthermore, there is no need for dynamic\\nmodeling of the primary activity since it is implicitly learned over time.\\nEnergy efficiency is achieved by minimizing the number of assigned sensors per\\neach subband under a constraint on miss detection probability. It is important\\nto control the missed detections because they cause collisions with primary\\ntransmissions and lead to retransmissions at both the primary and secondary\\nuser. Simulations show that the proposed machine learning based sensing policy\\nimproves the overall throughput of the secondary network and improves the\\nenergy efficiency while controlling the miss detection probability.\\n</summary>\\n    <author>\\n      <name>Jan Oksanen</name>\\n    </author>\\n    <author>\\n      <name>Jarmo Lund\\xc3\\xa9n</name>\\n    </author>\\n    <author>\\n      <name>Visa Koivunen</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 13 figures, Accepted to Neurocomputing special issue:\\n  Machine learning for signal processing, 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1106.1770v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1106.1770v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1508.06845v1</id>\\n    <updated>2015-08-27T13:06:55Z</updated>\\n    <published>2015-08-27T13:06:55Z</published>\\n    <title>Encrypted statistical machine learning: new privacy preserving methods</title>\\n    <summary>  We present two new statistical machine learning methods designed to learn on\\nfully homomorphic encrypted (FHE) data. The introduction of FHE schemes\\nfollowing Gentry (2009) opens up the prospect of privacy preserving statistical\\nmachine learning analysis and modelling of encrypted data without compromising\\nsecurity constraints. We propose tailored algorithms for applying extremely\\nrandom forests, involving a new cryptographic stochastic fraction estimator,\\nand na\\\\\"{i}ve Bayes, involving a semi-parametric model for the class decision\\nboundary, and show how they can be used to learn and predict from encrypted\\ndata. We demonstrate that these techniques perform competitively on a variety\\nof classification data sets and provide detailed information about the\\ncomputational practicalities of these and other FHE methods.\\n</summary>\\n    <author>\\n      <name>Louis J. M. Aslett</name>\\n    </author>\\n    <author>\\n      <name>Pedro M. Esperan\\xc3\\xa7a</name>\\n    </author>\\n    <author>\\n      <name>Chris C. Holmes</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">39 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1508.06845v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.06845v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.03530v1</id>\\n    <updated>2016-08-11T16:52:03Z</updated>\\n    <published>2016-08-11T16:52:03Z</published>\\n    <title>Semi-Supervised Prediction of Gene Regulatory Networks Using Machine\\n  Learning Algorithms</title>\\n    <summary>  Use of computational methods to predict gene regulatory networks (GRNs) from\\ngene expression data is a challenging task. Many studies have been conducted\\nusing unsupervised methods to fulfill the task; however, such methods usually\\nyield low prediction accuracies due to the lack of training data. In this\\narticle, we propose semi-supervised methods for GRN prediction by utilizing two\\nmachine learning algorithms, namely support vector machines (SVM) and random\\nforests (RF). The semi-supervised methods make use of unlabeled data for\\ntraining. We investigate inductive and transductive learning approaches, both\\nof which adopt an iterative procedure to obtain reliable negative training data\\nfrom the unlabeled data. We then apply our semi-supervised methods to gene\\nexpression data of Escherichia coli and Saccharomyces cerevisiae, and evaluate\\nthe performance of our methods using the expression data. Our analysis\\nindicated that the transductive learning approach outperformed the inductive\\nlearning approach for both organisms. However, there was no conclusive\\ndifference identified in the performance of SVM and RF. Experimental results\\nalso showed that the proposed semi-supervised methods performed better than\\nexisting supervised methods for both organisms.\\n</summary>\\n    <author>\\n      <name>Nihir Patel</name>\\n    </author>\\n    <author>\\n      <name>Jason T. L. Wang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">25 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1608.03530v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.03530v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.02815v3</id>\\n    <updated>2017-06-15T08:19:46Z</updated>\\n    <published>2016-09-09T14:45:48Z</published>\\n    <title>By-passing the Kohn-Sham equations with machine learning</title>\\n    <summary>  Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of\\ndensity functional theory to solve electronic structure problems in a wide\\nvariety of scientific fields, ranging from materials science to biochemistry to\\nastrophysics. Machine learning holds the promise of learning the kinetic energy\\nfunctional via examples, by-passing the need to solve the Kohn-Sham equations.\\nThis should yield substantial savings in computer time, allowing either larger\\nsystems or longer time-scales to be tackled, but attempts to machine-learn this\\nfunctional have been limited by the need to find its derivative. The present\\nwork overcomes this difficulty by directly learning the density-potential and\\nenergy-density maps for test systems and various molecules. Both improved\\naccuracy and lower computational cost with this method are demonstrated by\\nreproducing DFT energies for a range of molecular geometries generated during\\nmolecular dynamics simulations. Moreover, the methodology could be applied\\ndirectly to quantum chemical calculations, allowing construction of density\\nfunctionals of quantum-chemical accuracy.\\n</summary>\\n    <author>\\n      <name>Felix Brockherde</name>\\n    </author>\\n    <author>\\n      <name>Leslie Vogt</name>\\n    </author>\\n    <author>\\n      <name>Li Li</name>\\n    </author>\\n    <author>\\n      <name>Mark E. Tuckerman</name>\\n    </author>\\n    <author>\\n      <name>Kieron Burke</name>\\n    </author>\\n    <author>\\n      <name>Klaus-Robert M\\xc3\\xbcller</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1038/s41467-017-00839-3</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1038/s41467-017-00839-3\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1609.02815v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.02815v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.05236v1</id>\\n    <updated>2016-12-15T20:44:50Z</updated>\\n    <published>2016-12-15T20:44:50Z</published>\\n    <title>Private Learning on Networks</title>\\n    <summary>  Continual data collection and widespread deployment of machine learning\\nalgorithms, particularly the distributed variants, have raised new privacy\\nchallenges. In a distributed machine learning scenario, the dataset is stored\\namong several machines and they solve a distributed optimization problem to\\ncollectively learn the underlying model. We present a secure multi-party\\ncomputation inspired privacy preserving distributed algorithm for optimizing a\\nconvex function consisting of several possibly non-convex functions. Each\\nindividual objective function is privately stored with an agent while the\\nagents communicate model parameters with neighbor machines connected in a\\nnetwork. We show that our algorithm can correctly optimize the overall\\nobjective function and learn the underlying model accurately. We further prove\\nthat under a vertex connectivity condition on the topology, our algorithm\\npreserves privacy of individual objective functions. We establish limits on the\\nwhat a coalition of adversaries can learn by observing the messages and states\\nshared over a network.\\n</summary>\\n    <author>\\n      <name>Shripad Gade</name>\\n    </author>\\n    <author>\\n      <name>Nitin H. Vaidya</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.05236v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.05236v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.10207v1</id>\\n    <updated>2017-06-30T14:09:44Z</updated>\\n    <published>2017-06-30T14:09:44Z</published>\\n    <title>Optimization Methods for Supervised Machine Learning: From Linear Models\\n  to Deep Learning</title>\\n    <summary>  The goal of this tutorial is to introduce key models, algorithms, and open\\nquestions related to the use of optimization methods for solving problems\\narising in machine learning. It is written with an INFORMS audience in mind,\\nspecifically those readers who are familiar with the basics of optimization\\nalgorithms, but less familiar with machine learning. We begin by deriving a\\nformulation of a supervised learning problem and show how it leads to various\\noptimization problems, depending on the context and underlying assumptions. We\\nthen discuss some of the distinctive features of these optimization problems,\\nfocusing on the examples of logistic regression and the training of deep neural\\nnetworks. The latter half of the tutorial focuses on optimization algorithms,\\nfirst for convex logistic regression, for which we discuss the use of\\nfirst-order methods, the stochastic gradient method, variance reducing\\nstochastic methods, and second-order methods. Finally, we discuss how these\\napproaches can be employed to the training of deep neural networks, emphasizing\\nthe difficulties that arise from the complex, nonconvex structure of these\\nmodels.\\n</summary>\\n    <author>\\n      <name>Frank E. Curtis</name>\\n    </author>\\n    <author>\\n      <name>Katya Scheinberg</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.10207v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.10207v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.06613v1</id>\\n    <updated>2017-07-20T17:08:48Z</updated>\\n    <published>2017-07-20T17:08:48Z</published>\\n    <title>Decoupled classifiers for fair and efficient machine learning</title>\\n    <summary>  When it is ethical and legal to use a sensitive attribute (such as gender or\\nrace) in machine learning systems, the question remains how to do so. We show\\nthat the naive application of machine learning algorithms using sensitive\\nfeatures leads to an inherent tradeoff in accuracy between groups. We provide a\\nsimple and efficient decoupling technique, that can be added on top of any\\nblack-box machine learning algorithm, to learn different classifiers for\\ndifferent groups. Transfer learning is used to mitigate the problem of having\\ntoo little data on any one group.\\n  The method can apply to a range of fairness criteria. In particular, we\\nrequire the application designer to specify as joint loss function that makes\\nexplicit the trade-off between fairness and accuracy. Our reduction is shown to\\nefficiently find the minimum loss as long as the objective has a certain\\nnatural monotonicity property which may be of independent interest in the study\\nof fairness in algorithms.\\n</summary>\\n    <author>\\n      <name>Cynthia Dwork</name>\\n    </author>\\n    <author>\\n      <name>Nicole Immorlica</name>\\n    </author>\\n    <author>\\n      <name>Adam Tauman Kalai</name>\\n    </author>\\n    <author>\\n      <name>Max Leiserson</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1707.06613v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.06613v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.07082v2</id>\\n    <updated>2018-04-11T13:57:18Z</updated>\\n    <published>2017-09-18T22:52:52Z</published>\\n    <title>Machine learning of molecular properties: locality and active learning</title>\\n    <summary>  In recent years the machine learning techniques have shown a great potential\\nin various problems from a multitude of disciplines, including materials design\\nand drug discovery. The high computational speed on the one hand and the\\naccuracy comparable to that of DFT on another hand make machine learning\\nalgorithms efficient for high-throughput screening through chemical and\\nconfigurational space. However, the machine learning algorithms available in\\nthe literature require large training datasets to reach the chemical accuracy\\nand also show large errors for the so-called outliers - the out-of-sample\\nmolecules, not well-represented in the training set. In the present paper we\\npropose a new machine learning algorithm for predicting molecular properties\\nthat addresses these two issues: it is based on a local model of interatomic\\ninteractions providing high accuracy when trained on relatively small training\\nsets and an active learning algorithm of optimally choosing the training set\\nthat significantly reduces the errors for the outliers. We compare our model to\\nthe other state-of-the-art algorithms from the literature on the widely used\\nbenchmark tests.\\n</summary>\\n    <author>\\n      <name>Konstantin Gubaev</name>\\n    </author>\\n    <author>\\n      <name>Evgeny V. Podryabinkin</name>\\n    </author>\\n    <author>\\n      <name>Alexander V. Shapeev</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.5005095</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.5005095\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">revised version</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1709.07082v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.07082v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.chem-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.mtrl-sci\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.05208v2</id>\\n    <updated>2018-07-10T20:57:22Z</updated>\\n    <published>2018-06-13T18:27:32Z</published>\\n    <title>Enabling End-To-End Machine Learning Replicability: A Case Study in\\n  Educational Data Mining</title>\\n    <summary>  The use of machine learning techniques has expanded in education research,\\ndriven by the rich data from digital learning environments and institutional\\ndata warehouses. However, replication of machine learned models in the domain\\nof the learning sciences is particularly challenging due to a confluence of\\nexperimental, methodological, and data barriers. We discuss the challenges of\\nend-to-end machine learning replication in this context, and present an\\nopen-source software toolkit, the MOOC Replication Framework (MORF), to address\\nthem. We demonstrate the use of MORF by conducting a replication at scale, and\\nprovide a complete executable container, with unique DOIs documenting the\\nconfigurations of each individual trial, for replication or future extension at\\nhttps://github.com/educational-technology-collective/fy2015-replication. This\\nwork demonstrates an approach to end-to-end machine learning replication which\\nis relevant to any domain with large, complex or multi-format,\\nprivacy-protected data with a consistent schema.\\n</summary>\\n    <author>\\n      <name>Josh Gardner</name>\\n    </author>\\n    <author>\\n      <name>Yuming Yang</name>\\n    </author>\\n    <author>\\n      <name>Ryan Baker</name>\\n    </author>\\n    <author>\\n      <name>Christopher Brooks</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1801.05236</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.05208v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.05208v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.AP\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.10569v1</id>\\n    <updated>2018-07-10T01:49:50Z</updated>\\n    <published>2018-07-10T01:49:50Z</published>\\n    <title>The Helmholtz Method: Using Perceptual Compression to Reduce Machine\\n  Learning Complexity</title>\\n    <summary>  This paper proposes a fundamental answer to a frequently asked question in\\nmultimedia computing and machine learning: Do artifacts from perceptual\\ncompression contribute to error in the machine learning process and if so, how\\nmuch? Our approach to the problem is a reinterpretation of the Helmholtz Free\\nEnergy formula from physics to explain the relationship between content and\\nnoise when using sensors (such as cameras or microphones) to capture multimedia\\ndata. The reinterpretation allows a bit-measurement of the noise contained in\\nimages, audio, and video by combining a classifier with perceptual compression,\\nsuch as JPEG or MP3. Our experiments on CIFAR-10 as well as Fraunhofer\\'s\\nIDMT-SMT-Audio-Effects dataset indicate that, at the right quality level,\\nperceptual compression is actually not harmful but contributes to a significant\\nreduction of complexity of the machine learning process. That is, our noise\\nquantification method can be used to speed up the training of deep learning\\nclassifiers significantly while maintaining, or sometimes even improving,\\noverall classification accuracy. Moreover, our results provide insights into\\nthe reasons for the success of deep learning.\\n</summary>\\n    <author>\\n      <name>Gerald Friedland</name>\\n    </author>\\n    <author>\\n      <name>Jingkang Wang</name>\\n    </author>\\n    <author>\\n      <name>Ruoxi Jia</name>\\n    </author>\\n    <author>\\n      <name>Bo Li</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1807.10569v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.10569v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.app-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.05770v1</id>\\n    <updated>2018-08-17T06:34:53Z</updated>\\n    <published>2018-08-17T06:34:53Z</published>\\n    <title>Reinforcement Learning for Autonomous Defence in Software-Defined\\n  Networking</title>\\n    <summary>  Despite the successful application of machine learning (ML) in a wide range\\nof domains, adaptability---the very property that makes machine learning\\ndesirable---can be exploited by adversaries to contaminate training and evade\\nclassification. In this paper, we investigate the feasibility of applying a\\nspecific class of machine learning algorithms, namely, reinforcement learning\\n(RL) algorithms, for autonomous cyber defence in software-defined networking\\n(SDN). In particular, we focus on how an RL agent reacts towards different\\nforms of causative attacks that poison its training process, including\\nindiscriminate and targeted, white-box and black-box attacks. In addition, we\\nalso study the impact of the attack timing, and explore potential\\ncountermeasures such as adversarial training.\\n</summary>\\n    <author>\\n      <name>Yi Han</name>\\n    </author>\\n    <author>\\n      <name>Benjamin I. P. Rubinstein</name>\\n    </author>\\n    <author>\\n      <name>Tamas Abraham</name>\\n    </author>\\n    <author>\\n      <name>Tansu Alpcan</name>\\n    </author>\\n    <author>\\n      <name>Olivier De Vel</name>\\n    </author>\\n    <author>\\n      <name>Sarah Erfani</name>\\n    </author>\\n    <author>\\n      <name>David Hubczenko</name>\\n    </author>\\n    <author>\\n      <name>Christopher Leckie</name>\\n    </author>\\n    <author>\\n      <name>Paul Montague</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">20 pages, 8 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.05770v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.05770v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.01018v2</id>\\n    <updated>2018-09-14T14:40:22Z</updated>\\n    <published>2018-09-04T14:24:19Z</published>\\n    <title>Parameter Transfer Extreme Learning Machine based on Projective Model</title>\\n    <summary>  Recent years, transfer learning has attracted much attention in the community\\nof machine learning. In this paper, we mainly focus on the tasks of parameter\\ntransfer under the framework of extreme learning machine (ELM). Unlike the\\nexisting parameter transfer approaches, which incorporate the source model\\ninformation into the target by regularizing the di erence between the source\\nand target domain parameters, an intuitively appealing projective-model is\\nproposed to bridge the source and target model parameters. Specifically, we\\nformulate the parameter transfer in the ELM networks by the means of parameter\\nprojection, and train the model by optimizing the projection matrix and\\nclassifier parameters jointly. Further more, the `L2,1-norm structured sparsity\\npenalty is imposed on the source domain parameters, which encourages the joint\\nfeature selection and parameter transfer. To evaluate the e ectiveness of the\\nproposed method, comprehensive experiments on several commonly used domain\\nadaptation datasets are presented. The results show that the proposed method\\nsignificantly outperforms the non-transfer ELM networks and other classical\\ntransfer learning methods.\\n</summary>\\n    <author>\\n      <name>Chao Chen</name>\\n    </author>\\n    <author>\\n      <name>Boyuan Jiang</name>\\n    </author>\\n    <author>\\n      <name>Xinyu Jin</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/IJCNN.2018.8489244</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/IJCNN.2018.8489244\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper was accepted as an oral paper by IJCNN 2018</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2018 International Joint Conference on Neural Networks (IJCNN)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1809.01018v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.01018v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.05889v1</id>\\n    <updated>2018-09-16T14:40:16Z</updated>\\n    <published>2018-09-16T14:40:16Z</published>\\n    <title>Comparison of Deep Learning and the Classical Machine Learning Algorithm\\n  for the Malware Detection</title>\\n    <summary>  Recently, Deep Learning has been showing promising results in various\\nArtificial Intelligence applications like image recognition, natural language\\nprocessing, language modeling, neural machine translation, etc. Although, in\\ngeneral, it is computationally more expensive as compared to classical machine\\nlearning techniques, their results are found to be more effective in some\\ncases. Therefore, in this paper, we investigated and compared one of the Deep\\nLearning Architecture called Deep Neural Network (DNN) with the classical\\nRandom Forest (RF) machine learning algorithm for the malware classification.\\nWe studied the performance of the classical RF and DNN with 2, 4 &amp; 7 layers\\narchitectures with the four different feature sets, and found that irrespective\\nof the features inputs, the classical RF accuracy outperforms the DNN.\\n</summary>\\n    <author>\\n      <name>Mohit Sewak</name>\\n    </author>\\n    <author>\\n      <name>Sanjay K. Sahay</name>\\n    </author>\\n    <author>\\n      <name>Hemant Rathore</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/SNPD.2018.8441123</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/SNPD.2018.8441123\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 Pages, 1 figure</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE, pp. 293-296, 19th IEEE/ACIS International Conference on\\n  Software Engineering, Artificial Intelligence, Networking and\\n  Parallel/Distributed Computing (SNPD), 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1809.05889v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.05889v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.02659v2</id>\\n    <updated>2019-02-20T16:50:47Z</updated>\\n    <published>2018-10-24T15:34:18Z</published>\\n    <title>Machine Learning Algorithms for Classification of Microcirculation\\n  Images from Septic and Non-Septic Patients</title>\\n    <summary>  Sepsis is a life-threatening disease and one of the major causes of death in\\nhospitals. Imaging of microcirculatory dysfunction is a promising approach for\\nautomated diagnosis of sepsis. We report a machine learning classifier capable\\nof distinguishing non-septic and septic images from dark field microcirculation\\nvideos of patients. The classifier achieves an accuracy of 89.45%. The area\\nunder the receiver operating characteristics of the classifier was 0.92, the\\nprecision was 0.92 and the recall was 0.84. Codes representing the learned\\nfeature space of trained classifier were visualized using t-SNE embedding and\\nwere separable and distinguished between images from critically ill and\\nnon-septic patients. Using an unsupervised convolutional autoencoder,\\nindependent of the clinical diagnosis, we also report clustering of learned\\nfeatures from a compressed representation associated with healthy images and\\nthose with microcirculatory dysfunction. The feature space used by our trained\\nclassifier to distinguish between images from septic and non-septic patients\\nhas potential diagnostic application.\\n</summary>\\n    <author>\\n      <name>Perikumar Javia</name>\\n    </author>\\n    <author>\\n      <name>Aman Rana</name>\\n    </author>\\n    <author>\\n      <name>Nathan Shapiro</name>\\n    </author>\\n    <author>\\n      <name>Pratik Shah</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/ICMLA.2018.00097</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/ICMLA.2018.00097\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for publication at 2018 IEEE International Conference on\\n  Machine Learning and Applications (IEEE ICMLA)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.02659v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.02659v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.09296v1</id>\\n    <updated>2019-03-22T01:49:30Z</updated>\\n    <published>2019-03-22T01:49:30Z</published>\\n    <title>Patient Clustering Improves Efficiency of Federated Machine Learning to\\n  predict mortality and hospital stay time using distributed Electronic Medical\\n  Records</title>\\n    <summary>  Electronic medical records (EMRs) supports the development of machine\\nlearning algorithms for predicting disease incidence, patient response to\\ntreatment, and other healthcare events. But insofar most algorithms have been\\ncentralized, taking little account of the decentralized, non-identically\\nindependently distributed (non-IID), and privacy-sensitive characteristics of\\nEMRs that can complicate data collection, sharing and learning. To address this\\nchallenge, we introduced a community-based federated machine learning (CBFL)\\nalgorithm and evaluated it on non-IID ICU EMRs. Our algorithm clustered the\\ndistributed data into clinically meaningful communities that captured similar\\ndiagnoses and geological locations, and learnt one model for each community.\\nThroughout the learning process, the data was kept local on hospitals, while\\nlocally-computed results were aggregated on a server. Evaluation results show\\nthat CBFL outperformed the baseline FL algorithm in terms of Area Under the\\nReceiver Operating Characteristic Curve (ROC AUC), Area Under the\\nPrecision-Recall Curve (PR AUC), and communication cost between hospitals and\\nthe server. Furthermore, communities\\' performance difference could be explained\\nby how dissimilar one community was to others.\\n</summary>\\n    <author>\\n      <name>Li Huang</name>\\n    </author>\\n    <author>\\n      <name>Dianbo Liu</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.09296v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.09296v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.01049v1</id>\\n    <updated>2019-04-01T18:19:11Z</updated>\\n    <published>2019-04-01T18:19:11Z</published>\\n    <title>Bayesian Optimization for Policy Search via Online-Offline\\n  Experimentation</title>\\n    <summary>  Online field experiments are the gold-standard way of evaluating changes to\\nreal-world interactive machine learning systems. Yet our ability to explore\\ncomplex, multi-dimensional policy spaces - such as those found in\\nrecommendation and ranking problems - is often constrained by the limited\\nnumber of experiments that can be run simultaneously. To alleviate these\\nconstraints, we augment online experiments with an offline simulator and apply\\nmulti-task Bayesian optimization to tune live machine learning systems. We\\ndescribe practical issues that arise in these types of applications, including\\nbiases that arise from using a simulator and assumptions for the multi-task\\nkernel. We measure empirical learning curves which show substantial gains from\\nincluding data from biased offline experiments, and show how these learning\\ncurves are consistent with theoretical results for multi-task Gaussian process\\ngeneralization. We find that improved kernel inference is a significant driver\\nof multi-task generalization. Finally, we show several examples of Bayesian\\noptimization efficiently tuning a live machine learning system by combining\\noffline and online experiments.\\n</summary>\\n    <author>\\n      <name>Benjamin Letham</name>\\n    </author>\\n    <author>\\n      <name>Eytan Bakshy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.01049v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.01049v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.06709v2</id>\\n    <updated>2018-03-23T19:22:19Z</updated>\\n    <published>2017-09-20T03:18:54Z</published>\\n    <title>Online Learning of a Memory for Learning Rates</title>\\n    <summary>  The promise of learning to learn for robotics rests on the hope that by\\nextracting some information about the learning process itself we can speed up\\nsubsequent similar learning tasks. Here, we introduce a computationally\\nefficient online meta-learning algorithm that builds and optimizes a memory\\nmodel of the optimal learning rate landscape from previously observed gradient\\nbehaviors. While performing task specific optimization, this memory of learning\\nrates predicts how to scale currently observed gradients. After applying the\\ngradient scaling our meta-learner updates its internal memory based on the\\nobserved effect its prediction had. Our meta-learner can be combined with any\\ngradient-based optimizer, learns on the fly and can be transferred to new\\noptimization tasks. In our evaluations we show that our meta-learning algorithm\\nspeeds up learning of MNIST classification and a variety of learning control\\ntasks, either in batch or online learning settings.\\n</summary>\\n    <author>\\n      <name>Franziska Meier</name>\\n    </author>\\n    <author>\\n      <name>Daniel Kappler</name>\\n    </author>\\n    <author>\\n      <name>Stefan Schaal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">accepted to ICRA 2018, code available:\\n  https://github.com/fmeier/online-meta-learning ; video pitch available:\\n  https://youtu.be/9PzQ25FPPOM</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1709.06709v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.06709v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.08006v1</id>\\n    <updated>2019-02-21T12:44:25Z</updated>\\n    <published>2019-02-21T12:44:25Z</published>\\n    <title>Limit Learning Equivalence Structures</title>\\n    <summary>  While most research in Gold-style learning focuses on learning formal\\nlanguages, we consider the identification of computable structures,\\nspecifically equivalence structures. In our core model the learner gets more\\nand more information about which pairs of elements of a structure are related\\nand which are not. The aim of the learner is to find (an effective description\\nof) the isomorphism type of the structure presented in the limit. In accordance\\nwith language learning we call this learning criterion InfEx-learning\\n(explanatory learning from informant).\\n  Our main contribution is a complete characterization of which families of\\nequivalence structures are InfEx-learnable. This characterization allows us to\\nderive a bound of $\\\\mathbf{0\\'\\'}$ on the computational complexity required to\\nlearn uniformly enumerable families of equivalence structures. We also\\ninvestigate variants of InfEx-learning, including learning from text (where the\\nonly information provided is which elements are related, and not which elements\\nare not related) and finite learning (where the first actual conjecture of the\\nlearner has to be correct). Finally, we show how learning families of\\nstructures relates to learning classes of languages by mapping learning tasks\\nfor structures to equivalent learning tasks for languages.\\n</summary>\\n    <author>\\n      <name>Ekaterina Fokina</name>\\n    </author>\\n    <author>\\n      <name>Timo K\\xc3\\xb6tzing</name>\\n    </author>\\n    <author>\\n      <name>Luca San Mauro</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">22 pages, forthcoming in Proceedings of Machine Learning Research</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.08006v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.08006v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Q32, 03C57\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.12556v2</id>\\n    <updated>2019-03-02T21:10:11Z</updated>\\n    <published>2018-11-30T00:36:34Z</published>\\n    <title>How to Organize your Deep Reinforcement Learning Agents: The Importance\\n  of Communication Topology</title>\\n    <summary>  In this empirical paper, we investigate how learning agents can be arranged\\nin more efficient communication topologies for improved learning. This is an\\nimportant problem because a common technique to improve speed and robustness of\\nlearning in deep reinforcement learning and many other machine learning\\nalgorithms is to run multiple learning agents in parallel. The standard\\ncommunication architecture typically involves all agents intermittently\\ncommunicating with each other (fully connected topology) or with a centralized\\nserver (star topology). Unfortunately, optimizing the topology of communication\\nover the space of all possible graphs is a hard problem, so we borrow results\\nfrom the networked optimization and collective intelligence literatures which\\nsuggest that certain families of network topologies can lead to strong\\nimprovements over fully-connected networks. We start by introducing alternative\\nnetwork topologies to DRL benchmark tasks under the Evolution Strategies\\nparadigm which we call Network Evolution Strategies. We explore the relative\\nperformance of the four main graph families and observe that one such family\\n(Erdos-Renyi random graphs) empirically outperforms all other families,\\nincluding the de facto fully-connected communication topologies. Additionally,\\nthe use of alternative network topologies has a multiplicative performance\\neffect: we observe that when 1000 learning agents are arranged in a carefully\\ndesigned communication topology, they can compete with 3000 agents arranged in\\nthe de facto fully-connected topology. Overall, our work suggests that\\ndistributed machine learning algorithms would learn more efficiently if the\\ncommunication topology between learning agents was optimized.\\n</summary>\\n    <author>\\n      <name>Dhaval Adjodah</name>\\n    </author>\\n    <author>\\n      <name>Dan Calacci</name>\\n    </author>\\n    <author>\\n      <name>Abhimanyu Dubey</name>\\n    </author>\\n    <author>\\n      <name>Peter Krafft</name>\\n    </author>\\n    <author>\\n      <name>Esteban Moro</name>\\n    </author>\\n    <author>\\n      <name>Alex `Sandy\\' Pentland</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">please refer to arXiv:1902.06740 for updated paper</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.12556v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.12556v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1607.00446v2</id>\\n    <updated>2016-10-24T19:25:51Z</updated>\\n    <published>2016-07-02T01:33:00Z</published>\\n    <title>A Greedy Approach to Adapting the Trace Parameter for Temporal\\n  Difference Learning</title>\\n    <summary>  One of the main obstacles to broad application of reinforcement learning\\nmethods is the parameter sensitivity of our core learning algorithms. In many\\nlarge-scale applications, online computation and function approximation\\nrepresent key strategies in scaling up reinforcement learning algorithms. In\\nthis setting, we have effective and reasonably well understood algorithms for\\nadapting the learning-rate parameter, online during learning. Such\\nmeta-learning approaches can improve robustness of learning and enable\\nspecialization to current task, improving learning speed. For\\ntemporal-difference learning algorithms which we study here, there is yet\\nanother parameter, $\\\\lambda$, that similarly impacts learning speed and\\nstability in practice. Unfortunately, unlike the learning-rate parameter,\\n$\\\\lambda$ parametrizes the objective function that temporal-difference methods\\noptimize. Different choices of $\\\\lambda$ produce different fixed-point\\nsolutions, and thus adapting $\\\\lambda$ online and characterizing the\\noptimization is substantially more complex than adapting the learning-rate\\nparameter. There are no meta-learning method for $\\\\lambda$ that can achieve (1)\\nincremental updating, (2) compatibility with function approximation, and (3)\\nmaintain stability of learning under both on and off-policy sampling. In this\\npaper we contribute a novel objective function for optimizing $\\\\lambda$ as a\\nfunction of state rather than time. We derive a new incremental, linear\\ncomplexity $\\\\lambda$-adaption algorithm that does not require offline batch\\nupdating or access to a model of the world, and present a suite of experiments\\nillustrating the practicality of our new algorithm in three different settings.\\nTaken together, our contributions represent a concrete step towards black-box\\napplication of temporal-difference learning methods in real world problems.\\n</summary>\\n    <author>\\n      <name>Martha White</name>\\n    </author>\\n    <author>\\n      <name>Adam White</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1607.00446v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.00446v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.05629v1</id>\\n    <updated>2017-08-18T14:36:29Z</updated>\\n    <published>2017-08-18T14:36:29Z</published>\\n    <title>Learning to Transfer</title>\\n    <summary>  Transfer learning borrows knowledge from a source domain to facilitate\\nlearning in a target domain. Two primary issues to be addressed in transfer\\nlearning are what and how to transfer. For a pair of domains, adopting\\ndifferent transfer learning algorithms results in different knowledge\\ntransferred between them. To discover the optimal transfer learning algorithm\\nthat maximally improves the learning performance in the target domain,\\nresearchers have to exhaustively explore all existing transfer learning\\nalgorithms, which is computationally intractable. As a trade-off, a sub-optimal\\nalgorithm is selected, which requires considerable expertise in an ad-hoc way.\\nMeanwhile, it is widely accepted in educational psychology that human beings\\nimprove transfer learning skills of deciding what to transfer through\\nmeta-cognitive reflection on inductive transfer learning practices. Motivated\\nby this, we propose a novel transfer learning framework known as Learning to\\nTransfer (L2T) to automatically determine what and how to transfer are the best\\nby leveraging previous transfer learning experiences. We establish the L2T\\nframework in two stages: 1) we first learn a reflection function encrypting\\ntransfer learning skills from experiences; and 2) we infer what and how to\\ntransfer for a newly arrived pair of domains by optimizing the reflection\\nfunction. Extensive experiments demonstrate the L2T\\'s superiority over several\\nstate-of-the-art transfer learning algorithms and its effectiveness on\\ndiscovering more transferable knowledge.\\n</summary>\\n    <author>\\n      <name>Ying Wei</name>\\n    </author>\\n    <author>\\n      <name>Yu Zhang</name>\\n    </author>\\n    <author>\\n      <name>Qiang Yang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 8 figures, conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1708.05629v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.05629v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1209.0738v3</id>\\n    <updated>2014-06-16T15:06:48Z</updated>\\n    <published>2012-09-04T19:06:51Z</published>\\n    <title>Sparse coding for multitask and transfer learning</title>\\n    <summary>  We investigate the use of sparse coding and dictionary learning in the\\ncontext of multitask and transfer learning. The central assumption of our\\nlearning method is that the tasks parameters are well approximated by sparse\\nlinear combinations of the atoms of a dictionary on a high or infinite\\ndimensional space. This assumption, together with the large quantity of\\navailable data in the multitask and transfer learning settings, allows a\\nprincipled choice of the dictionary. We provide bounds on the generalization\\nerror of this approach, for both settings. Numerical experiments on one\\nsynthetic and two real datasets show the advantage of our method over single\\ntask learning, a previous method based on orthogonal and dense representation\\nof the tasks and a related method learning task grouping.\\n</summary>\\n    <author>\\n      <name>Andreas Maurer</name>\\n    </author>\\n    <author>\\n      <name>Massimiliano Pontil</name>\\n    </author>\\n    <author>\\n      <name>Bernardino Romera-Paredes</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Machine Learning 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1209.0738v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.0738v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68Q32, 68T05, 97C30, 46N30\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.3108v2</id>\\n    <updated>2013-08-29T15:38:27Z</updated>\\n    <published>2013-06-13T13:47:51Z</published>\\n    <title>Guaranteed Classification via Regularized Similarity Learning</title>\\n    <summary>  Learning an appropriate (dis)similarity function from the available data is a\\ncentral problem in machine learning, since the success of many machine learning\\nalgorithms critically depends on the choice of a similarity function to compare\\nexamples. Despite many approaches for similarity metric learning have been\\nproposed, there is little theoretical study on the links between similarity\\nmet- ric learning and the classification performance of the result classifier.\\nIn this paper, we propose a regularized similarity learning formulation\\nassociated with general matrix-norms, and establish their generalization\\nbounds. We show that the generalization error of the resulting linear separator\\ncan be bounded by the derived generalization bound of similarity learning. This\\nshows that a good gen- eralization of the learnt similarity function guarantees\\na good classification of the resulting linear classifier. Our results extend\\nand improve those obtained by Bellet at al. [3]. Due to the techniques\\ndependent on the notion of uniform stability [6], the bound obtained there\\nholds true only for the Frobenius matrix- norm regularization. Our techniques\\nusing the Rademacher complexity [5] and its related Khinchin-type inequality\\nenable us to establish bounds for regularized similarity learning formulations\\nassociated with general matrix-norms including sparse L 1 -norm and mixed\\n(2,1)-norm.\\n</summary>\\n    <author>\\n      <name>Zheng-Chu Guo</name>\\n    </author>\\n    <author>\\n      <name>Yiming Ying</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1306.3108v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.3108v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1203.3495v1</id>\\n    <updated>2012-03-15T11:17:56Z</updated>\\n    <published>2012-03-15T11:17:56Z</published>\\n    <title>Parameter-Free Spectral Kernel Learning</title>\\n    <summary>  Due to the growing ubiquity of unlabeled data, learning with unlabeled data\\nis attracting increasing attention in machine learning. In this paper, we\\npropose a novel semi-supervised kernel learning method which can seamlessly\\ncombine manifold structure of unlabeled data and Regularized Least-Squares\\n(RLS) to learn a new kernel. Interestingly, the new kernel matrix can be\\nobtained analytically with the use of spectral decomposition of graph Laplacian\\nmatrix. Hence, the proposed algorithm does not require any numerical\\noptimization solvers. Moreover, by maximizing kernel target alignment on\\nlabeled data, we can also learn model parameters automatically with a\\nclosed-form solution. For a given graph Laplacian matrix, our proposed method\\ndoes not need to tune any model parameter including the tradeoff parameter in\\nRLS and the balance parameter for unlabeled data. Extensive experiments on ten\\nbenchmark datasets show that our proposed two-stage parameter-free spectral\\nkernel learning algorithm can obtain comparable performance with fine-tuned\\nmanifold regularization methods in transductive setting, and outperform\\nmultiple kernel learning in supervised setting.\\n</summary>\\n    <author>\\n      <name>Qi Mao</name>\\n    </author>\\n    <author>\\n      <name>Ivor W. Tsang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\\n  in Artificial Intelligence (UAI2010)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1203.3495v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1203.3495v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1602.04511v2</id>\\n    <updated>2016-06-11T23:47:23Z</updated>\\n    <published>2016-02-14T21:14:07Z</published>\\n    <title>Learning Granger Causality for Hawkes Processes</title>\\n    <summary>  Learning Granger causality for general point processes is a very challenging\\ntask. In this paper, we propose an effective method, learning Granger\\ncausality, for a special but significant type of point processes --- Hawkes\\nprocess. We reveal the relationship between Hawkes process\\'s impact function\\nand its Granger causality graph. Specifically, our model represents impact\\nfunctions using a series of basis functions and recovers the Granger causality\\ngraph via group sparsity of the impact functions\\' coefficients. We propose an\\neffective learning algorithm combining a maximum likelihood estimator (MLE)\\nwith a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility of\\nour model allows to incorporate the clustering structure event types into\\nlearning framework. We analyze our learning algorithm and propose an adaptive\\nprocedure to select basis functions. Experiments on both synthetic and\\nreal-world data show that our method can learn the Granger causality graph and\\nthe triggering patterns of the Hawkes processes simultaneously.\\n</summary>\\n    <author>\\n      <name>Hongteng Xu</name>\\n    </author>\\n    <author>\\n      <name>Mehrdad Farajtabar</name>\\n    </author>\\n    <author>\\n      <name>Hongyuan Zha</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Machine Learning, 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1602.04511v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.04511v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.04080v2</id>\\n    <updated>2017-12-29T17:45:19Z</updated>\\n    <published>2016-06-13T19:34:22Z</published>\\n    <title>Matching Networks for One Shot Learning</title>\\n    <summary>  Learning from a few examples remains a key challenge in machine learning.\\nDespite recent advances in important domains such as vision and language, the\\nstandard supervised deep learning paradigm does not offer a satisfactory\\nsolution for learning new concepts rapidly from little data. In this work, we\\nemploy ideas from metric learning based on deep neural features and from recent\\nadvances that augment neural networks with external memories. Our framework\\nlearns a network that maps a small labelled support set and an unlabelled\\nexample to its label, obviating the need for fine-tuning to adapt to new class\\ntypes. We then define one-shot learning problems on vision (using Omniglot,\\nImageNet) and language tasks. Our algorithm improves one-shot accuracy on\\nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to\\ncompeting approaches. We also demonstrate the usefulness of the same model on\\nlanguage modeling by introducing a one-shot task on the Penn Treebank.\\n</summary>\\n    <author>\\n      <name>Oriol Vinyals</name>\\n    </author>\\n    <author>\\n      <name>Charles Blundell</name>\\n    </author>\\n    <author>\\n      <name>Timothy Lillicrap</name>\\n    </author>\\n    <author>\\n      <name>Koray Kavukcuoglu</name>\\n    </author>\\n    <author>\\n      <name>Daan Wierstra</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1606.04080v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.04080v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1309.5823v1</id>\\n    <updated>2013-09-23T14:39:48Z</updated>\\n    <published>2013-09-23T14:39:48Z</published>\\n    <title>A Kernel Classification Framework for Metric Learning</title>\\n    <summary>  Learning a distance metric from the given training samples plays a crucial\\nrole in many machine learning tasks, and various models and optimization\\nalgorithms have been proposed in the past decade. In this paper, we generalize\\nseveral state-of-the-art metric learning methods, such as large margin nearest\\nneighbor (LMNN) and information theoretic metric learning (ITML), into a kernel\\nclassification framework. First, doublets and triplets are constructed from the\\ntraining samples, and a family of degree-2 polynomial kernel functions are\\nproposed for pairs of doublets or triplets. Then, a kernel classification\\nframework is established, which can not only generalize many popular metric\\nlearning methods such as LMNN and ITML, but also suggest new metric learning\\nmethods, which can be efficiently implemented, interestingly, by using the\\nstandard support vector machine (SVM) solvers. Two novel metric learning\\nmethods, namely doublet-SVM and triplet-SVM, are then developed under the\\nproposed framework. Experimental results show that doublet-SVM and triplet-SVM\\nachieve competitive classification accuracies with state-of-the-art metric\\nlearning methods such as ITML and LMNN but with significantly less training\\ntime.\\n</summary>\\n    <author>\\n      <name>Faqiang Wang</name>\\n    </author>\\n    <author>\\n      <name>Wangmeng Zuo</name>\\n    </author>\\n    <author>\\n      <name>Lei Zhang</name>\\n    </author>\\n    <author>\\n      <name>Deyu Meng</name>\\n    </author>\\n    <author>\\n      <name>David Zhang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 7 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1309.5823v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1309.5823v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.5.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1312.5465v3</id>\\n    <updated>2014-09-25T02:31:30Z</updated>\\n    <published>2013-12-19T10:10:02Z</published>\\n    <title>Learning rates of $l^q$ coefficient regularization learning with\\n  Gaussian kernel</title>\\n    <summary>  Regularization is a well recognized powerful strategy to improve the\\nperformance of a learning machine and $l^q$ regularization schemes with\\n$0&lt;q&lt;\\\\infty$ are central in use. It is known that different $q$ leads to\\ndifferent properties of the deduced estimators, say, $l^2$ regularization leads\\nto smooth estimators while $l^1$ regularization leads to sparse estimators.\\nThen, how does the generalization capabilities of $l^q$ regularization learning\\nvary with $q$? In this paper, we study this problem in the framework of\\nstatistical learning theory and show that implementing $l^q$ coefficient\\nregularization schemes in the sample dependent hypothesis space associated with\\nGaussian kernel can attain the same almost optimal learning rates for all\\n$0&lt;q&lt;\\\\infty$. That is, the upper and lower bounds of learning rates for $l^q$\\nregularization learning are asymptotically identical for all $0&lt;q&lt;\\\\infty$. Our\\nfinding tentatively reveals that, in some modeling contexts, the choice of $q$\\nmight not have a strong impact with respect to the generalization capability.\\nFrom this perspective, $q$ can be arbitrarily specified, or specified merely by\\nother no generalization criteria like smoothness, computational complexity,\\nsparsity, etc..\\n</summary>\\n    <author>\\n      <name>Shaobo Lin</name>\\n    </author>\\n    <author>\\n      <name>Jinshan Zeng</name>\\n    </author>\\n    <author>\\n      <name>Jian Fang</name>\\n    </author>\\n    <author>\\n      <name>Zongben Xu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1312.5465v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.5465v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68T05\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"F.2.1\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1507.00066v1</id>\\n    <updated>2015-06-30T23:30:28Z</updated>\\n    <published>2015-06-30T23:30:28Z</published>\\n    <title>Fast Cross-Validation for Incremental Learning</title>\\n    <summary>  Cross-validation (CV) is one of the main tools for performance estimation and\\nparameter tuning in machine learning. The general recipe for computing CV\\nestimate is to run a learning algorithm separately for each CV fold, a\\ncomputationally expensive process. In this paper, we propose a new approach to\\nreduce the computational burden of CV-based performance estimation. As opposed\\nto all previous attempts, which are specific to a particular learning model or\\nproblem domain, we propose a general method applicable to a large class of\\nincremental learning algorithms, which are uniquely fitted to big data\\nproblems. In particular, our method applies to a wide range of supervised and\\nunsupervised learning tasks with different performance criteria, as long as the\\nbase learning algorithm is incremental. We show that the running time of the\\nalgorithm scales logarithmically, rather than linearly, in the number of CV\\nfolds. Furthermore, the algorithm has favorable properties for parallel and\\ndistributed implementation. Experiments with state-of-the-art incremental\\nlearning algorithms confirm the practicality of the proposed method.\\n</summary>\\n    <author>\\n      <name>Pooria Joulani</name>\\n    </author>\\n    <author>\\n      <name>Andr\\xc3\\xa1s Gy\\xc3\\xb6rgy</name>\\n    </author>\\n    <author>\\n      <name>Csaba Szepesv\\xc3\\xa1ri</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appearing in the International Joint Conference on Artificial\\n  Intelligence (IJCAI-2015), Buenos Aires, Argentina, July 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1507.00066v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1507.00066v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.06570v1</id>\\n    <updated>2016-09-21T14:16:14Z</updated>\\n    <published>2016-09-21T14:16:14Z</published>\\n    <title>Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced\\n  Datasets in Machine Learning</title>\\n    <summary>  Imbalanced-learn is an open-source python toolbox aiming at providing a wide\\nrange of methods to cope with the problem of imbalanced dataset frequently\\nencountered in machine learning and pattern recognition. The implemented\\nstate-of-the-art methods can be categorized into 4 groups: (i) under-sampling,\\n(ii) over-sampling, (iii) combination of over- and under-sampling, and (iv)\\nensemble learning methods. The proposed toolbox only depends on numpy, scipy,\\nand scikit-learn and is distributed under MIT license. Furthermore, it is fully\\ncompatible with scikit-learn and is part of the scikit-learn-contrib supported\\nproject. Documentation, unit tests as well as integration tests are provided to\\nease usage and contribution. The toolbox is publicly available in GitHub:\\nhttps://github.com/scikit-learn-contrib/imbalanced-learn.\\n</summary>\\n    <author>\\n      <name>Guillaume Lemaitre</name>\\n    </author>\\n    <author>\\n      <name>Fernando Nogueira</name>\\n    </author>\\n    <author>\\n      <name>Christos K. Aridas</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1609.06570v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.06570v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1703.03121v2</id>\\n    <updated>2018-05-25T05:07:40Z</updated>\\n    <published>2017-03-09T03:45:42Z</published>\\n    <title>Coordinated Multi-Agent Imitation Learning</title>\\n    <summary>  We study the problem of imitation learning from demonstrations of multiple\\ncoordinating agents. One key challenge in this setting is that learning a good\\nmodel of coordination can be difficult, since coordination is often implicit in\\nthe demonstrations and must be inferred as a latent variable. We propose a\\njoint approach that simultaneously learns a latent coordination model along\\nwith the individual policies. In particular, our method integrates unsupervised\\nstructure learning with conventional imitation learning. We illustrate the\\npower of our approach on a difficult problem of learning multiple policies for\\nfine-grained behavior modeling in team sports, where different players occupy\\ndifferent roles in the coordinated team strategy. We show that having a\\ncoordination model to infer the roles of players yields substantially improved\\nimitation loss compared to conventional baselines.\\n</summary>\\n    <author>\\n      <name>Hoang M. Le</name>\\n    </author>\\n    <author>\\n      <name>Yisong Yue</name>\\n    </author>\\n    <author>\\n      <name>Peter Carr</name>\\n    </author>\\n    <author>\\n      <name>Patrick Lucey</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Machine Learning 2017</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Hoang M. Le, Yisong Yue, Peter Carr, Patrick Lucey ; Proceedings\\n  of the 34th International Conference on Machine Learning, PMLR 70:1995-2003,\\n  2017</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1703.03121v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1703.03121v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.00470v2</id>\\n    <updated>2017-08-08T13:50:49Z</updated>\\n    <published>2017-05-01T11:06:04Z</published>\\n    <title>Learning Multimodal Transition Dynamics for Model-Based Reinforcement\\n  Learning</title>\\n    <summary>  In this paper we study how to learn stochastic, multimodal transition\\ndynamics in reinforcement learning (RL) tasks. We focus on evaluating\\ntransition function estimation, while we defer planning over this model to\\nfuture work. Stochasticity is a fundamental property of many task environments.\\nHowever, discriminative function approximators have difficulty estimating\\nmultimodal stochasticity. In contrast, deep generative models do capture\\ncomplex high-dimensional outcome distributions. First we discuss why, amongst\\nsuch models, conditional variational inference (VI) is theoretically most\\nappealing for model-based RL. Subsequently, we compare different VI models on\\ntheir ability to learn complex stochasticity on simulated functions, as well as\\non a typical RL gridworld with multimodal dynamics. Results show VI\\nsuccessfully predicts multimodal outcomes, but also robustly ignores these for\\ndeterministic parts of the transition dynamics. In summary, we show a robust\\nmethod to learn multimodal transitions using function approximation, which is a\\nkey preliminary for model-based RL in stochastic domains.\\n</summary>\\n    <author>\\n      <name>Thomas M. Moerland</name>\\n    </author>\\n    <author>\\n      <name>Joost Broekens</name>\\n    </author>\\n    <author>\\n      <name>Catholijn M. Jonker</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Scaling Up Reinforcement Learning (SURL) Workshop @ European Machine\\n  Learning Conference (ECML)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.00470v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.00470v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.02464v3</id>\\n    <updated>2018-07-31T16:55:11Z</updated>\\n    <published>2018-04-06T21:43:13Z</published>\\n    <title>Differentiable plasticity: training plastic neural networks with\\n  backpropagation</title>\\n    <summary>  How can we build agents that keep learning from experience, quickly and\\nefficiently, after their initial training? Here we take inspiration from the\\nmain mechanism of learning in biological brains: synaptic plasticity, carefully\\ntuned by evolution to produce efficient lifelong learning. We show that\\nplasticity, just like connection weights, can be optimized by gradient descent\\nin large (millions of parameters) recurrent networks with Hebbian plastic\\nconnections. First, recurrent plastic networks with more than two million\\nparameters can be trained to memorize and reconstruct sets of novel,\\nhigh-dimensional 1000+ pixels natural images not seen during training.\\nCrucially, traditional non-plastic recurrent networks fail to solve this task.\\nFurthermore, trained plastic networks can also solve generic meta-learning\\ntasks such as the Omniglot task, with competitive results and little parameter\\noverhead. Finally, in reinforcement learning settings, plastic networks\\noutperform a non-plastic equivalent in a maze exploration task. We conclude\\nthat differentiable plasticity may provide a powerful novel approach to the\\nlearning-to-learn problem.\\n</summary>\\n    <author>\\n      <name>Thomas Miconi</name>\\n    </author>\\n    <author>\\n      <name>Jeff Clune</name>\\n    </author>\\n    <author>\\n      <name>Kenneth O. Stanley</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at ICML 2018</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 35th International Conference on Machine\\n  Learning (ICML2018), Stockholm, Sweden, PMLR 80, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1804.02464v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.02464v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.08894v1</id>\\n    <updated>2018-06-23T02:18:26Z</updated>\\n    <published>2018-06-23T02:18:26Z</published>\\n    <title>Deep Reinforcement Learning: An Overview</title>\\n    <summary>  In recent years, a specific machine learning method called deep learning has\\ngained huge attraction, as it has obtained astonishing results in broad\\napplications such as pattern recognition, speech recognition, computer vision,\\nand natural language processing. Recent research has also been shown that deep\\nlearning techniques can be combined with reinforcement learning methods to\\nlearn useful representations for the problems with high dimensional raw data\\ninput. This chapter reviews the recent advances in deep reinforcement learning\\nwith a focus on the most used deep architectures such as autoencoders,\\nconvolutional neural networks and recurrent neural networks which have\\nsuccessfully been come together with the reinforcement learning framework.\\n</summary>\\n    <author>\\n      <name>Seyed Sajad Mousavi</name>\\n    </author>\\n    <author>\\n      <name>Michael Schukat</name>\\n    </author>\\n    <author>\\n      <name>Enda Howley</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-319-56991-8_32</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-319-56991-8_32\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of SAI Intelligent Systems Conference (IntelliSys) 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.08894v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.08894v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.03953v1</id>\\n    <updated>2018-07-11T05:34:32Z</updated>\\n    <published>2018-07-11T05:34:32Z</published>\\n    <title>Adaptive Learning Method of Recurrent Temporal Deep Belief Network to\\n  Analyze Time Series Data</title>\\n    <summary>  Deep Learning has the hierarchical network architecture to represent the\\ncomplicated features of input patterns. Such architecture is well known to\\nrepresent higher learning capability compared with some conventional models if\\nthe best set of parameters in the optimal network structure is found. We have\\nbeen developing the adaptive learning method that can discover the optimal\\nnetwork structure in Deep Belief Network (DBN). The learning method can\\nconstruct the network structure with the optimal number of hidden neurons in\\neach Restricted Boltzmann Machine and with the optimal number of layers in the\\nDBN during learning phase. The network structure of the learning method can be\\nself-organized according to given input patterns of big data set. In this\\npaper, we embed the adaptive learning method into the recurrent temporal RBM\\nand the self-generated layer into DBN. In order to verify the effectiveness of\\nour proposed method, the experimental results are higher classification\\ncapability than the conventional methods in this paper.\\n</summary>\\n    <author>\\n      <name>Takumi Ichimura</name>\\n    </author>\\n    <author>\\n      <name>Shin Kamada</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/IJCNN.2017.7966140</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/IJCNN.2017.7966140\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 9 figures. arXiv admin note: text overlap with\\n  arXiv:1807.03487, arXiv:1807.03486</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proc. of The International Joint Conference on Neural Networks\\n  (IJCNN 2017)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1807.03953v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.03953v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.09347v2</id>\\n    <updated>2018-11-03T07:19:39Z</updated>\\n    <published>2018-08-28T15:04:32Z</published>\\n    <title>Joint Domain Alignment and Discriminative Feature Learning for\\n  Unsupervised Deep Domain Adaptation</title>\\n    <summary>  Recently, considerable effort has been devoted to deep domain adaptation in\\ncomputer vision and machine learning communities. However, most of existing\\nwork only concentrates on learning shared feature representation by minimizing\\nthe distribution discrepancy across different domains. Due to the fact that all\\nthe domain alignment approaches can only reduce, but not remove the domain\\nshift. Target domain samples distributed near the edge of the clusters, or far\\nfrom their corresponding class centers are easily to be misclassified by the\\nhyperplane learned from the source domain. To alleviate this issue, we propose\\nto joint domain alignment and discriminative feature learning, which could\\nbenefit both domain alignment and final classification. Specifically, an\\ninstance-based discriminative feature learning method and a center-based\\ndiscriminative feature learning method are proposed, both of which guarantee\\nthe domain invariant features with better intra-class compactness and\\ninter-class separability. Extensive experiments show that learning the\\ndiscriminative features in the shared feature space can significantly boost the\\nperformance of deep domain adaptation methods.\\n</summary>\\n    <author>\\n      <name>Chao Chen</name>\\n    </author>\\n    <author>\\n      <name>Zhihong Chen</name>\\n    </author>\\n    <author>\\n      <name>Boyuan Jiang</name>\\n    </author>\\n    <author>\\n      <name>Xinyu Jin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been accepted by AAAI-2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.09347v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.09347v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.10941v1</id>\\n    <updated>2018-09-28T10:06:36Z</updated>\\n    <published>2018-09-28T10:06:36Z</published>\\n    <title>Deep learning systems as complex networks</title>\\n    <summary>  Thanks to the availability of large scale digital datasets and massive\\namounts of computational power, deep learning algorithms can learn\\nrepresentations of data by exploiting multiple levels of abstraction. These\\nmachine learning methods have greatly improved the state-of-the-art in many\\nchallenging cognitive tasks, such as visual object recognition, speech\\nprocessing, natural language understanding and automatic translation. In\\nparticular, one class of deep learning models, known as deep belief networks,\\ncan discover intricate statistical structure in large data sets in a completely\\nunsupervised fashion, by learning a generative model of the data using\\nHebbian-like learning mechanisms. Although these self-organizing systems can be\\nconveniently formalized within the framework of statistical mechanics, their\\ninternal functioning remains opaque, because their emergent dynamics cannot be\\nsolved analytically. In this article we propose to study deep belief networks\\nusing techniques commonly employed in the study of complex networks, in order\\nto gain some insights into the structural and functional properties of the\\ncomputational graph resulting from the learning process.\\n</summary>\\n    <author>\\n      <name>Alberto Testolin</name>\\n    </author>\\n    <author>\\n      <name>Michele Piccolini</name>\\n    </author>\\n    <author>\\n      <name>Samir Suweis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">20 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.10941v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.10941v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.04451v2</id>\\n    <updated>2018-11-17T13:46:04Z</updated>\\n    <published>2018-11-11T18:59:21Z</published>\\n    <title>Multi-Source Neural Variational Inference</title>\\n    <summary>  Learning from multiple sources of information is an important problem in\\nmachine-learning research. The key challenges are learning representations and\\nformulating inference methods that take into account the complementarity and\\nredundancy of various information sources. In this paper we formulate a\\nvariational autoencoder based multi-source learning framework in which each\\nencoder is conditioned on a different information source. This allows us to\\nrelate the sources via the shared latent variables by computing divergence\\nmeasures between individual source\\'s posterior approximations. We explore a\\nvariety of options to learn these encoders and to integrate the beliefs they\\ncompute into a consistent posterior approximation. We visualise learned beliefs\\non a toy dataset and evaluate our methods for learning shared representations\\nand structured output prediction, showing trade-offs of learning separate\\nencoders for each information source. Furthermore, we demonstrate how conflict\\ndetection and redundancy can increase robustness of inference in a multi-source\\nsetting.\\n</summary>\\n    <author>\\n      <name>Richard Kurle</name>\\n    </author>\\n    <author>\\n      <name>Stephan G\\xc3\\xbcnnemann</name>\\n    </author>\\n    <author>\\n      <name>Patrick van der Smagt</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">AAAI 2019, Association for the Advancement of Artificial Intelligence\\n  (AAAI) 2019</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.04451v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.04451v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.04355v1</id>\\n    <updated>2019-01-14T15:01:49Z</updated>\\n    <published>2019-01-14T15:01:49Z</published>\\n    <title>Iterative Deep Learning Based Unbiased Stereology With Human-in-the-Loop</title>\\n    <summary>  Lack of enough labeled data is a major problem in building machine learning\\nbased models when the manual annotation (labeling) is error-prone, expensive,\\ntedious, and time-consuming. In this paper, we introduce an iterative deep\\nlearning based method to improve segmentation and counting of cells based on\\nunbiased stereology applied to regions of interest of extended depth of field\\n(EDF) images. This method uses an existing machine learning algorithm called\\nthe adaptive segmentation algorithm (ASA) to generate masks (verified by a\\nuser) for EDF images to train deep learning models. Then an iterative deep\\nlearning approach is used to feed newly predicted and accepted deep learning\\nmasks/images (verified by a user) to the training set of the deep learning\\nmodel. The error rate in unbiased stereology count of cells on an unseen test\\nset reduced from about 3 % to less than 1 % after 5 iterations of the iterative\\ndeep learning based unbiased stereology process.\\n</summary>\\n    <author>\\n      <name>Saeed S. Alahmari</name>\\n    </author>\\n    <author>\\n      <name>Dmitry Goldgof</name>\\n    </author>\\n    <author>\\n      <name>Lawrence O. Hall</name>\\n    </author>\\n    <author>\\n      <name>Palak Dave</name>\\n    </author>\\n    <author>\\n      <name>Hady Ahmady Phoulady</name>\\n    </author>\\n    <author>\\n      <name>Peter R. Mouton</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by ICMLA18 conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1901.04355v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.04355v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.06494v1</id>\\n    <updated>2019-02-18T10:22:13Z</updated>\\n    <published>2019-02-18T10:22:13Z</published>\\n    <title>A Unifying Bayesian View of Continual Learning</title>\\n    <summary>  Some machine learning applications require continual learning - where data\\ncomes in a sequence of datasets, each is used for training and then permanently\\ndiscarded. From a Bayesian perspective, continual learning seems\\nstraightforward: Given the model posterior one would simply use this as the\\nprior for the next task. However, exact posterior evaluation is intractable\\nwith many models, especially with Bayesian neural networks (BNNs). Instead,\\nposterior approximations are often sought. Unfortunately, when posterior\\napproximations are used, prior-focused approaches do not succeed in evaluations\\ndesigned to capture properties of realistic continual learning use cases. As an\\nalternative to prior-focused methods, we introduce a new approximate Bayesian\\nderivation of the continual learning loss. Our loss does not rely on the\\nposterior from earlier tasks, and instead adapts the model itself by changing\\nthe likelihood term. We call these approaches likelihood-focused. We then\\ncombine prior- and likelihood-focused methods into one objective, tying the two\\nviews together under a single unifying framework of approximate Bayesian\\ncontinual learning.\\n</summary>\\n    <author>\\n      <name>Sebastian Farquhar</name>\\n    </author>\\n    <author>\\n      <name>Yarin Gal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at the Bayesian Deep Learning Workshop at Neural\\n  Information Processing Systems December 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.06494v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.06494v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.10214v1</id>\\n    <updated>2019-02-26T20:47:56Z</updated>\\n    <published>2019-02-26T20:47:56Z</published>\\n    <title>Implicit Kernel Learning</title>\\n    <summary>  Kernels are powerful and versatile tools in machine learning and statistics.\\nAlthough the notion of universal kernels and characteristic kernels has been\\nstudied, kernel selection still greatly influences the empirical performance.\\nWhile learning the kernel in a data driven way has been investigated, in this\\npaper we explore learning the spectral distribution of kernel via implicit\\ngenerative models parametrized by deep neural networks. We called our method\\nImplicit Kernel Learning (IKL). The proposed framework is simple to train and\\ninference is performed via sampling random Fourier features. We investigate two\\napplications of the proposed IKL as examples, including generative adversarial\\nnetworks with MMD (MMD GAN) and standard supervised learning. Empirically, MMD\\nGAN with IKL outperforms vanilla predefined kernels on both image and text\\ngeneration benchmarks; using IKL with Random Kitchen Sinks also leads to\\nsubstantial improvement over existing state-of-the-art kernel learning\\nalgorithms on popular supervised learning benchmarks. Theory and conditions for\\nusing IKL in both applications are also studied as well as connections to\\nprevious state-of-the-art methods.\\n</summary>\\n    <author>\\n      <name>Chun-Liang Li</name>\\n    </author>\\n    <author>\\n      <name>Wei-Cheng Chang</name>\\n    </author>\\n    <author>\\n      <name>Youssef Mroueh</name>\\n    </author>\\n    <author>\\n      <name>Yiming Yang</name>\\n    </author>\\n    <author>\\n      <name>Barnab\\xc3\\xa1s P\\xc3\\xb3czos</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In the Proceedings of the 22nd International Conference on Artificial\\n  Intelligence and Statistics (AISTATS 2019)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.10214v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.10214v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1904.02580v1</id>\\n    <updated>2019-04-04T14:32:19Z</updated>\\n    <published>2019-04-04T14:32:19Z</published>\\n    <title>Online Convex Dictionary Learning</title>\\n    <summary>  Dictionary learning is a dimensionality reduction technique widely used in\\ndata mining, machine learning and signal processing alike. Nevertheless, many\\ndictionary learning algorithms such as variants of Matrix Factorization (MF) do\\nnot adequately scale with the size of available datasets. Furthermore, scalable\\ndictionary learning methods lack interpretability of the derived dictionary\\nmatrix. To mitigate these two issues, we propose a novel low-complexity, batch\\nonline convex dictionary learning algorithm. The algorithm sequentially\\nprocesses small batches of data maintained in a fixed amount of storage space,\\nand produces meaningful dictionaries that satisfy convexity constraints. Our\\nanalytical results are two-fold. First, we establish convergence guarantees for\\nthe proposed online learning scheme. Second, we show that a subsequence of the\\ngenerated dictionaries converges to a stationary point of the\\napproximation-error function. Experimental results on synthetic and real world\\ndatasets demonstrate both the computational savings of the proposed online\\nmethod with respect to convex non-negative MF, and performance guarantees\\ncomparable to those of online non-convex learning.\\n</summary>\\n    <author>\\n      <name>Abhishek Agarwal</name>\\n    </author>\\n    <author>\\n      <name>Jianhao Peng</name>\\n    </author>\\n    <author>\\n      <name>Olgica Milenkovic</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1904.02580v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1904.02580v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1604.00289v3</id>\\n    <updated>2016-11-02T17:26:50Z</updated>\\n    <published>2016-04-01T15:37:57Z</published>\\n    <title>Building Machines That Learn and Think Like People</title>\\n    <summary>  Recent progress in artificial intelligence (AI) has renewed interest in\\nbuilding systems that learn and think like people. Many advances have come from\\nusing deep neural networks trained end-to-end in tasks such as object\\nrecognition, video games, and board games, achieving performance that equals or\\neven beats humans in some respects. Despite their biological inspiration and\\nperformance achievements, these systems differ from human intelligence in\\ncrucial ways. We review progress in cognitive science suggesting that truly\\nhuman-like learning and thinking machines will have to reach beyond current\\nengineering trends in both what they learn, and how they learn it.\\nSpecifically, we argue that these machines should (a) build causal models of\\nthe world that support explanation and understanding, rather than merely\\nsolving pattern recognition problems; (b) ground learning in intuitive theories\\nof physics and psychology, to support and enrich the knowledge that is learned;\\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\\ngeneralize knowledge to new tasks and situations. We suggest concrete\\nchallenges and promising routes towards these goals that can combine the\\nstrengths of recent neural network advances with more structured cognitive\\nmodels.\\n</summary>\\n    <author>\\n      <name>Brenden M. Lake</name>\\n    </author>\\n    <author>\\n      <name>Tomer D. Ullman</name>\\n    </author>\\n    <author>\\n      <name>Joshua B. Tenenbaum</name>\\n    </author>\\n    <author>\\n      <name>Samuel J. Gershman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In press at Behavioral and Brain Sciences. Open call for commentary\\n  proposals (until Nov. 22, 2016).\\n  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1604.00289v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1604.00289v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.04022v3</id>\\n    <updated>2017-06-20T12:00:03Z</updated>\\n    <published>2016-12-13T04:22:10Z</published>\\n    <title>Distributed Multi-Task Relationship Learning</title>\\n    <summary>  Multi-task learning aims to learn multiple tasks jointly by exploiting their\\nrelatedness to improve the generalization performance for each task.\\nTraditionally, to perform multi-task learning, one needs to centralize data\\nfrom all the tasks to a single machine. However, in many real-world\\napplications, data of different tasks may be geo-distributed over different\\nlocal machines. Due to heavy communication caused by transmitting the data and\\nthe issue of data privacy and security, it is impossible to send data of\\ndifferent task to a master machine to perform multi-task learning. Therefore,\\nin this paper, we propose a distributed multi-task learning framework that\\nsimultaneously learns predictive models for each task as well as task\\nrelationships between tasks alternatingly in the parameter server paradigm. In\\nour framework, we first offer a general dual form for a family of regularized\\nmulti-task relationship learning methods. Subsequently, we propose a\\ncommunication-efficient primal-dual distributed optimization algorithm to solve\\nthe dual problem by carefully designing local subproblems to make the dual\\nproblem decomposable. Moreover, we provide a theoretical convergence analysis\\nfor the proposed algorithm, which is specific for distributed multi-task\\nrelationship learning. We conduct extensive experiments on both synthetic and\\nreal-world datasets to evaluate our proposed framework in terms of\\neffectiveness and convergence.\\n</summary>\\n    <author>\\n      <name>Sulin Liu</name>\\n    </author>\\n    <author>\\n      <name>Sinno Jialin Pan</name>\\n    </author>\\n    <author>\\n      <name>Qirong Ho</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in KDD 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1612.04022v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.04022v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.08121v1</id>\\n    <updated>2018-08-23T00:38:14Z</updated>\\n    <published>2018-08-23T00:38:14Z</published>\\n    <title>An Improvement of Data Classification Using Random Multimodel Deep\\n  Learning (RMDL)</title>\\n    <summary>  The exponential growth in the number of complex datasets every year requires\\nmore enhancement in machine learning methods to provide robust and accurate\\ndata classification. Lately, deep learning approaches have achieved surpassing\\nresults in comparison to previous machine learning algorithms. However, finding\\nthe suitable structure for these models has been a challenge for researchers.\\nThis paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,\\ndeep learning approach for classification. RMDL solves the problem of finding\\nthe best deep learning structure and architecture while simultaneously\\nimproving robustness and accuracy through ensembles of deep learning\\narchitectures. In short, RMDL trains multiple randomly generated models of Deep\\nNeural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural\\nNetwork (RNN) in parallel and combines their results to produce better result\\nof any of those models individually. In this paper, we describe RMDL model and\\ncompare the results for image and text classification as well as face\\nrecognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for\\nimage classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text\\nclassification. Lastly, we used ORL dataset to compare the model performance on\\nface recognition task.\\n</summary>\\n    <author>\\n      <name>Mojtaba Heidarysafa</name>\\n    </author>\\n    <author>\\n      <name>Kamran Kowsari</name>\\n    </author>\\n    <author>\\n      <name>Donald E. Brown</name>\\n    </author>\\n    <author>\\n      <name>Kiana Jafari Meimandi</name>\\n    </author>\\n    <author>\\n      <name>Laura E. Barnes</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.18178/ijmlc.2018.8.4.703</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.18178/ijmlc.2018.8.4.703\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">published in International Journal of Machine Learning and Computing\\n  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1808.08121v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.08121v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1311.4639v1</id>\\n    <updated>2013-11-19T07:39:58Z</updated>\\n    <published>2013-11-19T07:39:58Z</published>\\n    <title>Post-Proceedings of the First International Workshop on Learning and\\n  Nonmonotonic Reasoning</title>\\n    <summary>  Knowledge Representation and Reasoning and Machine Learning are two important\\nfields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming\\n(ASP) provide formal languages for representing and reasoning with commonsense\\nknowledge and realize declarative problem solving in AI. On the other side,\\nInductive Logic Programming (ILP) realizes Machine Learning in logic\\nprogramming, which provides a formal background to inductive learning and the\\ntechniques have been applied to the fields of relational learning and data\\nmining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning while\\nlack the ability of learning. By contrast, ILP realizes inductive learning\\nwhile most techniques have been developed under the classical monotonic logic.\\nWith this background, some researchers attempt to combine techniques in the\\ncontext of nonmonotonic ILP. Such combination will introduce a learning\\nmechanism to programs and would exploit new applications on the NMLP side,\\nwhile on the ILP side it will extend the representation language and enable us\\nto use existing solvers. Cross-fertilization between learning and nonmonotonic\\nreasoning can also occur in such as the use of answer set solvers for ILP,\\nspeed-up learning while running answer set solvers, learning action theories,\\nlearning transition rules in dynamical systems, abductive learning, learning\\nbiological networks with inhibition, and applications involving default and\\nnegation. This workshop is the first attempt to provide an open forum for the\\nidentification of problems and discussion of possible collaborations among\\nresearchers with complementary expertise. The workshop was held on September\\n15th of 2013 in Corunna, Spain. This post-proceedings contains five technical\\npapers (out of six accepted papers) and the abstract of the invited talk by Luc\\nDe Raedt.\\n</summary>\\n    <author>\\n      <name>Katsumi Inoue</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Editors</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Chiaki Sakama</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Editors</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">67 pages, 5 papers, 1 abstract, 1 cover</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1311.4639v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.4639v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1804.01900v1</id>\\n    <updated>2018-04-05T15:11:11Z</updated>\\n    <published>2018-04-05T15:11:11Z</published>\\n    <title>Large Scale Local Online Similarity/Distance Learning Framework based on\\n  Passive/Aggressive</title>\\n    <summary>  Similarity/Distance measures play a key role in many machine learning,\\npattern recognition, and data mining algorithms, which leads to the emergence\\nof metric learning field. Many metric learning algorithms learn a global\\ndistance function from data that satisfy the constraints of the problem.\\nHowever, in many real-world datasets that the discrimination power of features\\nvaries in the different regions of input space, a global metric is often unable\\nto capture the complexity of the task. To address this challenge, local metric\\nlearning methods are proposed that learn multiple metrics across the different\\nregions of input space. Some advantages of these methods are high flexibility\\nand the ability to learn a nonlinear mapping but typically achieves at the\\nexpense of higher time requirement and overfitting problem. To overcome these\\nchallenges, this research presents an online multiple metric learning\\nframework. Each metric in the proposed framework is composed of a global and a\\nlocal component learned simultaneously. Adding a global component to a local\\nmetric efficiently reduce the problem of overfitting. The proposed framework is\\nalso scalable with both sample size and the dimension of input data. To the\\nbest of our knowledge, this is the first local online similarity/distance\\nlearning framework based on PA (Passive/Aggressive). In addition, for\\nscalability with the dimension of input data, DRP (Dual Random Projection) is\\nextended for local online learning in the present work. It enables our methods\\nto be run efficiently on high-dimensional datasets, while maintains their\\npredictive performance. The proposed framework provides a straightforward local\\nextension to any global online similarity/distance learning algorithm based on\\nPA.\\n</summary>\\n    <author>\\n      <name>Baida Hamdan</name>\\n    </author>\\n    <author>\\n      <name>Davood Zabihzadeh</name>\\n    </author>\\n    <author>\\n      <name>Monsefi Reza</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1804.01900v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1804.01900v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1708.07308v1</id>\\n    <updated>2017-08-24T08:21:28Z</updated>\\n    <published>2017-08-24T08:21:28Z</published>\\n    <title>Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning\\n  Workloads</title>\\n    <summary>  We present ease.ml, a declarative machine learning service platform we built\\nto support more than ten research groups outside the computer science\\ndepartments at ETH Zurich for their machine learning needs. With ease.ml, a\\nuser defines the high-level schema of a machine learning application and\\nsubmits the task via a Web interface. The system automatically deals with the\\nrest, such as model selection and data movement. In this paper, we describe the\\nease.ml architecture and focus on a novel technical problem introduced by\\nease.ml regarding resource allocation. We ask, as a \"service provider\" that\\nmanages a shared cluster of machines among all our users running machine\\nlearning workloads, what is the resource allocation strategy that maximizes the\\nglobal satisfaction of all our users?\\n  Resource allocation is a critical yet subtle issue in this multi-tenant\\nscenario, as we have to balance between efficiency and fairness. We first\\nformalize the problem that we call multi-tenant model selection, aiming for\\nminimizing the total regret of all users running automatic model selection\\ntasks. We then develop a novel algorithm that combines multi-armed bandits with\\nBayesian optimization and prove a regret bound under the multi-tenant setting.\\nFinally, we report our evaluation of ease.ml on synthetic data and on one\\nservice we are providing to our users, namely, image classification with deep\\nneural networks. Our experimental evaluation results show that our proposed\\nsolution can be up to 9.8x faster in achieving the same global quality for all\\nusers as the two popular heuristics used by our users before ease.ml.\\n</summary>\\n    <author>\\n      <name>Tian Li</name>\\n    </author>\\n    <author>\\n      <name>Jie Zhong</name>\\n    </author>\\n    <author>\\n      <name>Ji Liu</name>\\n    </author>\\n    <author>\\n      <name>Wentao Wu</name>\\n    </author>\\n    <author>\\n      <name>Ce Zhang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1708.07308v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1708.07308v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1710.05476v3</id>\\n    <updated>2017-11-13T19:01:55Z</updated>\\n    <published>2017-10-16T02:49:07Z</published>\\n    <title>Calibrated Boosting-Forest</title>\\n    <summary>  Excellent ranking power along with well calibrated probability estimates are\\nneeded in many classification tasks. In this paper, we introduce a technique,\\nCalibrated Boosting-Forest that captures both. This novel technique is an\\nensemble of gradient boosting machines that can support both continuous and\\nbinary labels. While offering superior ranking power over any individual\\nregression or classification model, Calibrated Boosting-Forest is able to\\npreserve well calibrated posterior probabilities. Along with these benefits, we\\nprovide an alternative to the tedious step of tuning gradient boosting\\nmachines. We demonstrate that tuning Calibrated Boosting-Forest can be reduced\\nto a simple hyper-parameter selection. We further establish that increasing\\nthis hyper-parameter improves the ranking performance under a diminishing\\nreturn. We examine the effectiveness of Calibrated Boosting-Forest on\\nligand-based virtual screening where both continuous and binary labels are\\navailable and compare the performance of Calibrated Boosting-Forest with\\nlogistic regression, gradient boosting machine and deep learning. Calibrated\\nBoosting-Forest achieved an approximately 48% improvement compared to a\\nstate-of-art deep learning model. Moreover, it achieved around 95% improvement\\non probability quality measurement compared to the best individual gradient\\nboosting machine. Calibrated Boosting-Forest offers a benchmark demonstration\\nthat in the field of ligand-based virtual screening, deep learning is not the\\nuniversally dominant machine learning model and good calibrated probabilities\\ncan better facilitate virtual screening process.\\n</summary>\\n    <author>\\n      <name>Haozhen Wu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 3 figures, 4 tables, NIPS 2017 Workshop on Machine Learning\\n  for Molecules and Materials</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1710.05476v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1710.05476v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.00025v1</id>\\n    <updated>2017-12-29T19:46:09Z</updated>\\n    <published>2017-12-29T19:46:09Z</published>\\n    <title>A Deep Belief Network Based Machine Learning System for Risky Host\\n  Detection</title>\\n    <summary>  To assure cyber security of an enterprise, typically SIEM (Security\\nInformation and Event Management) system is in place to normalize security\\nevent from different preventive technologies and flag alerts. Analysts in the\\nsecurity operation center (SOC) investigate the alerts to decide if it is truly\\nmalicious or not. However, generally the number of alerts is overwhelming with\\nmajority of them being false positive and exceeding the SOC\\'s capacity to\\nhandle all alerts. There is a great need to reduce the false positive rate as\\nmuch as possible. While most previous research focused on network intrusion\\ndetection, we focus on risk detection and propose an intelligent Deep Belief\\nNetwork machine learning system. The system leverages alert information,\\nvarious security logs and analysts\\' investigation results in a real enterprise\\nenvironment to flag hosts that have high likelihood of being compromised. Text\\nmining and graph based method are used to generate targets and create features\\nfor machine learning. In the experiment, Deep Belief Network is compared with\\nother machine learning algorithms, including multi-layer neural network, random\\nforest, support vector machine and logistic regression. Results on real\\nenterprise data indicate that the deep belief network machine learning system\\nperforms better than other algorithms for our problem and is six times more\\neffective than current rule-based system. We also implement the whole system\\nfrom data collection, label creation, feature engineering to host score\\ngeneration in a real enterprise production environment.\\n</summary>\\n    <author>\\n      <name>Wangyan Feng</name>\\n    </author>\\n    <author>\\n      <name>Shuning Wu</name>\\n    </author>\\n    <author>\\n      <name>Xiaodan Li</name>\\n    </author>\\n    <author>\\n      <name>Kevin Kunkle</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 10 figures. The paper is accepted by IEEE Conference on\\n  Communications and Network Security 2017. However, it is not published\\n  because either of the authors showed up in the conference</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.00025v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.00025v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.08971v1</id>\\n    <updated>2018-03-23T20:39:04Z</updated>\\n    <published>2018-03-23T20:39:04Z</published>\\n    <title>Computational Power and the Social Impact of Artificial Intelligence</title>\\n    <summary>  Machine learning is a computational process. To that end, it is inextricably\\ntied to computational power - the tangible material of chips and semiconductors\\nthat the algorithms of machine intelligence operate on. Most obviously,\\ncomputational power and computing architectures shape the speed of training and\\ninference in machine learning, and therefore influence the rate of progress in\\nthe technology. But, these relationships are more nuanced than that: hardware\\nshapes the methods used by researchers and engineers in the design and\\ndevelopment of machine learning models. Characteristics such as the power\\nconsumption of chips also define where and how machine learning can be used in\\nthe real world.\\n  Despite this, many analyses of the social impact of the current wave of\\nprogress in AI have not substantively brought the dimension of hardware into\\ntheir accounts. While a common trope in both the popular press and scholarly\\nliterature is to highlight the massive increase in computational power that has\\nenabled the recent breakthroughs in machine learning, the analysis frequently\\ngoes no further than this observation around magnitude. This paper aims to dig\\nmore deeply into the relationship between computational power and the\\ndevelopment of machine learning. Specifically, it examines how changes in\\ncomputing architectures, machine learning methodologies, and supply chains\\nmight influence the future of AI. In doing so, it seeks to trace a set of\\nspecific relationships between this underlying hardware layer and the broader\\nsocial impacts and risks around AI.\\n</summary>\\n    <author>\\n      <name>Tim Hwang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.08971v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.08971v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.08159v1</id>\\n    <updated>2018-11-20T10:09:02Z</updated>\\n    <published>2018-11-20T10:09:02Z</published>\\n    <title>Machine Learning Distinguishes Neurosurgical Skill Levels in a Virtual\\n  Reality Tumor Resection Task</title>\\n    <summary>  Background: Virtual reality simulators and machine learning have the\\npotential to augment understanding, assessment and training of psychomotor\\nperformance in neurosurgery residents. Objective: This study outlines the first\\napplication of machine learning to distinguish \"skilled\" and \"novice\"\\npsychomotor performance during a virtual reality neurosurgical task. Methods:\\nTwenty-three neurosurgeons and senior neurosurgery residents comprising the\\n\"skilled\" group and 92 junior neurosurgery residents and medical students the\\n\"novice\" group. The task involved removing a series of virtual brain tumors\\nwithout causing injury to surrounding tissue. Over 100 features were extracted\\nand 68 selected using t-test analysis. These features were provided to 4\\nclassifiers: K-Nearest Neighbors, Parzen Window, Support Vector Machine, and\\nFuzzy K-Nearest Neighbors. Equal Error Rate was used to assess classifier\\nperformance. Results: Ratios of train set size to test set size from 10% to 90%\\nand 5 to 30 features, chosen by the forward feature selection algorithm, were\\nemployed. A working point of 50% train to test set size ratio and 15 features\\nresulted in an equal error rates as low as 8.3% using the Fuzzy K-Nearest\\nNeighbors classifier. Conclusion: Machine learning may be one component helping\\nrealign the traditional apprenticeship educational paradigm to a more objective\\nmodel based on proven performance standards.\\n  Keywords: Artificial intelligence, Classifiers, Machine learning,\\nNeurosurgery skill assessment, Surgical education, Tumor resection, Virtual\\nreality simulation\\n</summary>\\n    <author>\\n      <name>Samaneh Siyar</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Neurosurgical Simulation Research and Training Centre, McGill University, Canada</arxiv:affiliation>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Biomedical Engineering, Amirkabir University of Technology, Iran</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Hamed Azarnoush</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Neurosurgical Simulation Research and Training Centre, McGill University, Canada</arxiv:affiliation>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Biomedical Engineering, Amirkabir University of Technology, Iran</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Saeid Rashidi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Science and Research Branch, Islamic Azad University, Iran</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Alexandre Winkler-Schwartz</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Neurosurgical Simulation Research and Training Centre, McGill University, Canada</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Vincent Bissonnette</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Neurosurgical Simulation Research and Training Centre, McGill University, Canada</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Nirros Ponnudurai</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Neurosurgical Simulation Research and Training Centre, McGill University, Canada</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Rolando F. Del Maestro</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Neurosurgical Simulation Research and Training Centre, McGill University, Canada</arxiv:affiliation>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1811.08159v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.08159v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.04209v1</id>\\n    <updated>2019-03-11T10:37:05Z</updated>\\n    <published>2019-03-11T10:37:05Z</published>\\n    <title>Shapley regressions: A framework for statistical inference on machine\\n  learning models</title>\\n    <summary>  Machine learning models often excel in the accuracy of their predictions but\\nare opaque due to their non-linear and non-parametric structure. This makes\\nstatistical inference challenging and disqualifies them from many applications\\nwhere model interpretability is crucial. This paper proposes the Shapley\\nregression framework as an approach for statistical inference on non-linear or\\nnon-parametric models. Inference is performed based on the Shapley value\\ndecomposition of a model, a pay-off concept from cooperative game theory. I\\nshow that universal approximators from machine learning are estimation\\nconsistent and introduce hypothesis tests for individual variable\\ncontributions, model bias and parametric functional forms. The inference\\nproperties of state-of-the-art machine learning models - like artificial neural\\nnetworks, support vector machines and random forests - are investigated using\\nnumerical simulations and real-world data. The proposed framework is unique in\\nthe sense that it is identical to the conventional case of statistical\\ninference on a linear model if the model is linear in parameters. This makes it\\na well-motivated extension to more general models and strengthens the case for\\nthe use of machine learning to inform decisions.\\n</summary>\\n    <author>\\n      <name>Andreas Joseph</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">43 pages, 8 figures, 6 tables, 1 box</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1903.04209v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.04209v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"econ.EM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"62G10, 62G20, 62-07, 91-08, 91A12\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"G.1; G.2; G.3; I.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.12081v1</id>\\n    <updated>2018-10-29T13:03:38Z</updated>\\n    <published>2018-10-29T13:03:38Z</published>\\n    <title>Learning to Teach with Dynamic Loss Functions</title>\\n    <summary>  Teaching is critical to human society: it is with teaching that prospective\\nstudents are educated and human civilization can be inherited and advanced. A\\ngood teacher not only provides his/her students with qualified teaching\\nmaterials (e.g., textbooks), but also sets up appropriate learning objectives\\n(e.g., course projects and exams) considering different situations of a\\nstudent. When it comes to artificial intelligence, treating machine learning\\nmodels as students, the loss functions that are optimized act as perfect\\ncounterparts of the learning objective set by the teacher. In this work, we\\nexplore the possibility of imitating human teaching behaviors by dynamically\\nand automatically outputting appropriate loss functions to train machine\\nlearning models. Different from typical learning settings in which the loss\\nfunction of a machine learning model is predefined and fixed, in our framework,\\nthe loss function of a machine learning model (we call it student) is defined\\nby another machine learning model (we call it teacher). The ultimate goal of\\nteacher model is cultivating the student to have better performance measured on\\ndevelopment dataset. Towards that end, similar to human teaching, the teacher,\\na parametric model, dynamically outputs different loss functions that will be\\nused and optimized by its student model at different training stages. We\\ndevelop an efficient learning method for the teacher model that makes gradient\\nbased optimization possible, exempt of the ineffective solutions such as policy\\noptimization. We name our method as \"learning to teach with dynamic loss\\nfunctions\" (L2T-DLF for short). Extensive experiments on real world tasks\\nincluding image classification and neural machine translation demonstrate that\\nour method significantly improves the quality of various student models.\\n</summary>\\n    <author>\\n      <name>Lijun Wu</name>\\n    </author>\\n    <author>\\n      <name>Fei Tian</name>\\n    </author>\\n    <author>\\n      <name>Yingce Xia</name>\\n    </author>\\n    <author>\\n      <name>Yang Fan</name>\\n    </author>\\n    <author>\\n      <name>Tao Qin</name>\\n    </author>\\n    <author>\\n      <name>Jianhuang Lai</name>\\n    </author>\\n    <author>\\n      <name>Tie-Yan Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.12081v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.12081v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0908.0050v2</id>\\n    <updated>2010-02-11T07:33:02Z</updated>\\n    <published>2009-08-01T06:09:18Z</published>\\n    <title>Online Learning for Matrix Factorization and Sparse Coding</title>\\n    <summary>  Sparse coding--that is, modelling data vectors as sparse linear combinations\\nof basis elements--is widely used in machine learning, neuroscience, signal\\nprocessing, and statistics. This paper focuses on the large-scale matrix\\nfactorization problem that consists of learning the basis set, adapting it to\\nspecific data. Variations of this problem include dictionary learning in signal\\nprocessing, non-negative matrix factorization and sparse principal component\\nanalysis. In this paper, we propose to address these tasks with a new online\\noptimization algorithm, based on stochastic approximations, which scales up\\ngracefully to large datasets with millions of training samples, and extends\\nnaturally to various matrix factorization formulations, making it suitable for\\na wide range of learning problems. A proof of convergence is presented, along\\nwith experiments with natural images and genomic data demonstrating that it\\nleads to state-of-the-art performance in terms of speed and optimization for\\nboth small and large datasets.\\n</summary>\\n    <author>\\n      <name>Julien Mairal</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Rocquencourt</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Francis Bach</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Rocquencourt</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Jean Ponce</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Rocquencourt, LIENS</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Guillermo Sapiro</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">revised version</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research 11 (2010) 19--60</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0908.0050v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0908.0050v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1107.4080v1</id>\\n    <updated>2011-07-20T19:34:00Z</updated>\\n    <published>2011-07-20T19:34:00Z</published>\\n    <title>On the Universality of Online Mirror Descent</title>\\n    <summary>  We show that for a general class of convex online learning problems, Mirror\\nDescent can always achieve a (nearly) optimal regret guarantee.\\n</summary>\\n    <author>\\n      <name>Nathan Srebro</name>\\n    </author>\\n    <author>\\n      <name>Karthik Sridharan</name>\\n    </author>\\n    <author>\\n      <name>Ambuj Tewari</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1107.4080v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.4080v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.0239v4</id>\\n    <updated>2015-02-21T16:58:39Z</updated>\\n    <published>2013-06-02T18:46:58Z</published>\\n    <title>Deep Learning using Linear Support Vector Machines</title>\\n    <summary>  Recently, fully-connected and convolutional neural networks have been trained\\nto achieve state-of-the-art performance on a wide variety of tasks such as\\nspeech recognition, image classification, natural language processing, and\\nbioinformatics. For classification tasks, most of these \"deep learning\" models\\nemploy the softmax activation function for prediction and minimize\\ncross-entropy loss. In this paper, we demonstrate a small but consistent\\nadvantage of replacing the softmax layer with a linear support vector machine.\\nLearning minimizes a margin-based loss instead of the cross-entropy loss. While\\nthere have been various combinations of neural nets and SVMs in prior art, our\\nresults using L2-SVMs show that by simply replacing softmax with linear SVMs\\ngives significant gains on popular deep learning datasets MNIST, CIFAR-10, and\\nthe ICML 2013 Representation Learning Workshop\\'s face expression recognition\\nchallenge.\\n</summary>\\n    <author>\\n      <name>Yichuan Tang</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Contribution to the ICML 2013 Challenges in Representation Learning\\n  Workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1306.0239v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.0239v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1110.2306v1</id>\\n    <updated>2011-10-11T09:04:56Z</updated>\\n    <published>2011-10-11T09:04:56Z</published>\\n    <title>Ground Metric Learning</title>\\n    <summary>  Transportation distances have been used for more than a decade now in machine\\nlearning to compare histograms of features. They have one parameter: the ground\\nmetric, which can be any metric between the features themselves. As is the case\\nfor all parameterized distances, transportation distances can only prove useful\\nin practice when this parameter is carefully chosen. To date, the only option\\navailable to practitioners to set the ground metric parameter was to rely on a\\npriori knowledge of the features, which limited considerably the scope of\\napplication of transportation distances. We propose to lift this limitation and\\nconsider instead algorithms that can learn the ground metric using only a\\ntraining set of labeled histograms. We call this approach ground metric\\nlearning. We formulate the problem of learning the ground metric as the\\nminimization of the difference of two polyhedral convex functions over a convex\\nset of distance matrices. We follow the presentation of our algorithms with\\npromising experimental results on binary classification tasks using GIST\\ndescriptors of images taken in the Caltech-256 set.\\n</summary>\\n    <author>\\n      <name>Marco Cuturi</name>\\n    </author>\\n    <author>\\n      <name>David Avis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">32 pages, 4 figures</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research, 15, 533-564. 2014</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1110.2306v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1110.2306v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1208.3422v2</id>\\n    <updated>2013-01-08T20:26:55Z</updated>\\n    <published>2012-08-16T17:16:18Z</published>\\n    <title>Distance Metric Learning for Kernel Machines</title>\\n    <summary>  Recent work in metric learning has significantly improved the\\nstate-of-the-art in k-nearest neighbor classification. Support vector machines\\n(SVM), particularly with RBF kernels, are amongst the most popular\\nclassification algorithms that uses distance metrics to compare examples. This\\npaper provides an empirical analysis of the efficacy of three of the most\\npopular Mahalanobis metric learning algorithms as pre-processing for SVM\\ntraining. We show that none of these algorithms generate metrics that lead to\\nparticularly satisfying improvements for SVM-RBF classification. As a remedy we\\nintroduce support vector metric learning (SVML), a novel algorithm that\\nseamlessly combines the learning of a Mahalanobis metric with the training of\\nthe RBF-SVM parameters. We demonstrate the capabilities of SVML on nine\\nbenchmark data sets of varying sizes and difficulties. In our study, SVML\\noutperforms all alternative state-of-the-art metric learning algorithms in\\nterms of accuracy and establishes itself as a serious alternative to the\\nstandard Euclidean metric with model selection by cross validation.\\n</summary>\\n    <author>\\n      <name>Zhixiang Xu</name>\\n    </author>\\n    <author>\\n      <name>Kilian Q. Weinberger</name>\\n    </author>\\n    <author>\\n      <name>Olivier Chapelle</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1208.3422v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1208.3422v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1307.0414v1</id>\\n    <updated>2013-07-01T15:53:22Z</updated>\\n    <published>2013-07-01T15:53:22Z</published>\\n    <title>Challenges in Representation Learning: A report on three machine\\n  learning contests</title>\\n    <summary>  The ICML 2013 Workshop on Challenges in Representation Learning focused on\\nthree challenges: the black box learning challenge, the facial expression\\nrecognition challenge, and the multimodal learning challenge. We describe the\\ndatasets created for these challenges and summarize the results of the\\ncompetitions. We provide suggestions for organizers of future challenges and\\nsome comments on what kind of knowledge can be gained from machine learning\\ncompetitions.\\n</summary>\\n    <author>\\n      <name>Ian J. Goodfellow</name>\\n    </author>\\n    <author>\\n      <name>Dumitru Erhan</name>\\n    </author>\\n    <author>\\n      <name>Pierre Luc Carrier</name>\\n    </author>\\n    <author>\\n      <name>Aaron Courville</name>\\n    </author>\\n    <author>\\n      <name>Mehdi Mirza</name>\\n    </author>\\n    <author>\\n      <name>Ben Hamner</name>\\n    </author>\\n    <author>\\n      <name>Will Cukierski</name>\\n    </author>\\n    <author>\\n      <name>Yichuan Tang</name>\\n    </author>\\n    <author>\\n      <name>David Thaler</name>\\n    </author>\\n    <author>\\n      <name>Dong-Hyun Lee</name>\\n    </author>\\n    <author>\\n      <name>Yingbo Zhou</name>\\n    </author>\\n    <author>\\n      <name>Chetan Ramaiah</name>\\n    </author>\\n    <author>\\n      <name>Fangxiang Feng</name>\\n    </author>\\n    <author>\\n      <name>Ruifan Li</name>\\n    </author>\\n    <author>\\n      <name>Xiaojie Wang</name>\\n    </author>\\n    <author>\\n      <name>Dimitris Athanasakis</name>\\n    </author>\\n    <author>\\n      <name>John Shawe-Taylor</name>\\n    </author>\\n    <author>\\n      <name>Maxim Milakov</name>\\n    </author>\\n    <author>\\n      <name>John Park</name>\\n    </author>\\n    <author>\\n      <name>Radu Ionescu</name>\\n    </author>\\n    <author>\\n      <name>Marius Popescu</name>\\n    </author>\\n    <author>\\n      <name>Cristian Grozea</name>\\n    </author>\\n    <author>\\n      <name>James Bergstra</name>\\n    </author>\\n    <author>\\n      <name>Jingjing Xie</name>\\n    </author>\\n    <author>\\n      <name>Lukasz Romaszko</name>\\n    </author>\\n    <author>\\n      <name>Bing Xu</name>\\n    </author>\\n    <author>\\n      <name>Zhang Chuang</name>\\n    </author>\\n    <author>\\n      <name>Yoshua Bengio</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 2 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1307.0414v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.0414v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1504.04114v1</id>\\n    <updated>2015-04-16T07:26:11Z</updated>\\n    <published>2015-04-16T07:26:11Z</published>\\n    <title>Actively Learning to Attract Followers on Twitter</title>\\n    <summary>  Twitter, a popular social network, presents great opportunities for on-line\\nmachine learning research. However, previous research has focused almost\\nentirely on learning from passively collected data. We study the problem of\\nlearning to acquire followers through normative user behavior, as opposed to\\nthe mass following policies applied by many bots. We formalize the problem as a\\ncontextual bandit problem, in which we consider retweeting content to be the\\naction chosen and each tweet (content) is accompanied by context. We design\\nreward signals based on the change in followers. The result of our month long\\nexperiment with 60 agents suggests that (1) aggregating experience across\\nagents can adversely impact prediction accuracy and (2) the Twitter community\\'s\\nresponse to different actions is non-stationary. Our findings suggest that\\nactively learning on-line can provide deeper insights about how to attract\\nfollowers than machine learning over passively collected data alone.\\n</summary>\\n    <author>\\n      <name>Nir Levine</name>\\n    </author>\\n    <author>\\n      <name>Timothy A. Mann</name>\\n    </author>\\n    <author>\\n      <name>Shie Mannor</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1504.04114v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1504.04114v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1505.06378v3</id>\\n    <updated>2016-01-20T22:54:21Z</updated>\\n    <published>2015-05-23T20:57:58Z</published>\\n    <title>Monotonic Calibrated Interpolated Look-Up Tables</title>\\n    <summary>  Real-world machine learning applications may require functions that are\\nfast-to-evaluate and interpretable. In particular, guaranteed monotonicity of\\nthe learned function can be critical to user trust. We propose meeting these\\ngoals for low-dimensional machine learning problems by learning flexible,\\nmonotonic functions using calibrated interpolated look-up tables. We extend the\\nstructural risk minimization framework of lattice regression to train monotonic\\nlook-up tables by solving a convex problem with appropriate linear inequality\\nconstraints. In addition, we propose jointly learning interpretable\\ncalibrations of each feature to normalize continuous features and handle\\ncategorical or missing data, at the cost of making the objective non-convex. We\\naddress large-scale learning through parallelization, mini-batching, and\\npropose random sampling of additive regularizer terms. Case studies with\\nreal-world problems with five to sixteen features and thousands to millions of\\ntraining samples demonstrate the proposed monotonic functions can achieve\\nstate-of-the-art accuracy on practical problems while providing greater\\ntransparency to users.\\n</summary>\\n    <author>\\n      <name>Maya Gupta</name>\\n    </author>\\n    <author>\\n      <name>Andrew Cotter</name>\\n    </author>\\n    <author>\\n      <name>Jan Pfeifer</name>\\n    </author>\\n    <author>\\n      <name>Konstantin Voevodski</name>\\n    </author>\\n    <author>\\n      <name>Kevin Canini</name>\\n    </author>\\n    <author>\\n      <name>Alexander Mangylov</name>\\n    </author>\\n    <author>\\n      <name>Wojtek Moczydlowski</name>\\n    </author>\\n    <author>\\n      <name>Alex van Esbroeck</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear (with minor revisions), Journal Machine Learning Research\\n  2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1505.06378v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.06378v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6407v1</id>\\n    <updated>2012-06-27T19:59:59Z</updated>\\n    <published>2012-06-27T19:59:59Z</published>\\n    <title>Large-Scale Feature Learning With Spike-and-Slab Sparse Coding</title>\\n    <summary>  We consider the problem of object recognition with a large number of classes.\\nIn order to overcome the low amount of labeled examples available in this\\nsetting, we introduce a new feature learning and extraction procedure based on\\na factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C\\nhas not prioritized the ability to exploit parallel architectures and scale S3C\\nto the enormous problem sizes needed for object recognition. We present a novel\\ninference procedure for appropriate for use with GPUs which allows us to\\ndramatically increase both the training set size and the amount of latent\\nfactors that S3C may be trained with. We demonstrate that this approach\\nimproves upon the supervised learning capabilities of both sparse coding and\\nthe spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10\\ndataset. We use the CIFAR-100 dataset to demonstrate that our method scales to\\nlarge numbers of classes better than previous methods. Finally, we use our\\nmethod to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical\\nModels? Transfer Learning Challenge.\\n</summary>\\n    <author>\\n      <name>Ian Goodfellow</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Universite de Montreal</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Aaron Courville</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Universite de Montreal</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Yoshua Bengio</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Universite de Montreal</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with\\n  arXiv:1201.3382</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6407v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6407v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.6476v1</id>\\n    <updated>2012-06-27T19:59:59Z</updated>\\n    <published>2012-06-27T19:59:59Z</published>\\n    <title>Similarity Learning for Provably Accurate Sparse Linear Classification</title>\\n    <summary>  In recent years, the crucial importance of metrics in machine learning\\nalgorithms has led to an increasing interest for optimizing distance and\\nsimilarity functions. Most of the state of the art focus on learning\\nMahalanobis distances (requiring to fulfill a constraint of positive\\nsemi-definiteness) for use in a local k-NN algorithm. However, no theoretical\\nlink is established between the learned metrics and their performance in\\nclassification. In this paper, we make use of the formal framework of good\\nsimilarities introduced by Balcan et al. to design an algorithm for learning a\\nnon PSD linear similarity optimized in a nonlinear feature space, which is then\\nused to build a global linear classifier. We show that our approach has uniform\\nstability and derive a generalization bound on the classification error.\\nExperiments performed on various datasets confirm the effectiveness of our\\napproach compared to state-of-the-art methods and provide evidence that (i) it\\nis fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.\\n</summary>\\n    <author>\\n      <name>Aurelien Bellet</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Saint-Etienne</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Amaury Habrard</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Saint-Etienne</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Marc Sebban</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Saint-Etienne</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the 29th International Conference on\\n  Machine Learning (ICML 2012)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.6476v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.6476v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1603.06170v2</id>\\n    <updated>2018-09-28T13:16:25Z</updated>\\n    <published>2016-03-20T00:55:06Z</published>\\n    <title>Joint Stochastic Approximation learning of Helmholtz Machines</title>\\n    <summary>  Though with progress, model learning and performing posterior inference still\\nremains a common challenge for using deep generative models, especially for\\nhandling discrete hidden variables. This paper is mainly concerned with\\nalgorithms for learning Helmholz machines, which is characterized by pairing\\nthe generative model with an auxiliary inference model. A common drawback of\\nprevious learning algorithms is that they indirectly optimize some bounds of\\nthe targeted marginal log-likelihood. In contrast, we successfully develop a\\nnew class of algorithms, based on stochastic approximation (SA) theory of the\\nRobbins-Monro type, to directly optimize the marginal log-likelihood and\\nsimultaneously minimize the inclusive KL-divergence. The resulting learning\\nalgorithm is thus called joint SA (JSA). Moreover, we construct an effective\\nMCMC operator for JSA. Our results on the MNIST datasets demonstrate that the\\nJSA\\'s performance is consistently superior to that of competing algorithms like\\nRWS, for learning a range of difficult models.\\n</summary>\\n    <author>\\n      <name>Haotian Xu</name>\\n    </author>\\n    <author>\\n      <name>Zhijian Ou</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Fixing typos. Published at ICLR-2016 Workshop Track</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1603.06170v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.06170v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1510.08231v3</id>\\n    <updated>2016-11-02T14:29:29Z</updated>\\n    <published>2015-10-28T09:18:50Z</published>\\n    <title>Operator-valued Kernels for Learning from Functional Response Data</title>\\n    <summary>  In this paper we consider the problems of supervised classification and\\nregression in the case where attributes and labels are functions: a data is\\nrepresented by a set of functions, and the label is also a function. We focus\\non the use of reproducing kernel Hilbert space theory to learn from such\\nfunctional data. Basic concepts and properties of kernel-based learning are\\nextended to include the estimation of function-valued functions. In this\\nsetting, the representer theorem is restated, a set of rigorously defined\\ninfinite-dimensional operator-valued kernels that can be valuably applied when\\nthe data are functions is described, and a learning algorithm for nonlinear\\nfunctional data analysis is introduced. The methodology is illustrated through\\nspeech and audio signal processing experiments.\\n</summary>\\n    <author>\\n      <name>Hachem Kadri</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LIF</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Emmanuel Duflos</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CRIStAL</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Philippe Preux</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CRIStAL, SEQUEL</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>St\\xc3\\xa9phane Canu</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LITIS</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Alain Rakotomamonjy</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LITIS</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Julien Audiffren</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CMLA</arxiv:affiliation>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">in Journal of Machine Learning Research (JMLR), 2016</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research 17 (2016) 1-54</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1510.08231v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1510.08231v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.00876v1</id>\\n    <updated>2016-08-02T15:48:58Z</updated>\\n    <published>2016-08-02T15:48:58Z</published>\\n    <title>Relational Similarity Machines</title>\\n    <summary>  This paper proposes Relational Similarity Machines (RSM): a fast, accurate,\\nand flexible relational learning framework for supervised and semi-supervised\\nlearning tasks. Despite the importance of relational learning, most existing\\nmethods are hard to adapt to different settings, due to issues with efficiency,\\nscalability, accuracy, and flexibility for handling a wide variety of\\nclassification problems, data, constraints, and tasks. For instance, many\\nexisting methods perform poorly for multi-class classification problems, graphs\\nthat are sparsely labeled or network data with low relational autocorrelation.\\nIn contrast, the proposed relational learning framework is designed to be (i)\\nfast for learning and inference at real-time interactive rates, and (ii)\\nflexible for a variety of learning settings (multi-class problems), constraints\\n(few labeled instances), and application domains. The experiments demonstrate\\nthe effectiveness of RSM for a variety of tasks and data.\\n</summary>\\n    <author>\\n      <name>Ryan A. Rossi</name>\\n    </author>\\n    <author>\\n      <name>Rong Zhou</name>\\n    </author>\\n    <author>\\n      <name>Nesreen K. Ahmed</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">MLG16</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1608.00876v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.00876v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.03793v2</id>\\n    <updated>2016-08-16T18:36:44Z</updated>\\n    <published>2016-08-12T13:50:24Z</published>\\n    <title>Applying Deep Learning to Basketball Trajectories</title>\\n    <summary>  One of the emerging trends for sports analytics is the growing use of player\\nand ball tracking data. A parallel development is deep learning predictive\\napproaches that use vast quantities of data with less reliance on feature\\nengineering. This paper applies recurrent neural networks in the form of\\nsequence modeling to predict whether a three-point shot is successful. The\\nmodels are capable of learning the trajectory of a basketball without any\\nknowledge of physics. For comparison, a baseline static machine learning model\\nwith a full set of features, such as angle and velocity, in addition to the\\npositional data is also tested. Using a dataset of over 20,000 three pointers\\nfrom NBA SportVu data, the models based simply on sequential positional data\\noutperform a static feature rich machine learning model in predicting whether a\\nthree-point shot is successful. This suggests deep learning models may offer an\\nimprovement to traditional feature based machine learning methods for tracking\\ndata.\\n</summary>\\n    <author>\\n      <name>Rajiv Shah</name>\\n    </author>\\n    <author>\\n      <name>Rob Romijnders</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">KDD 2016, Large Scale Sports Analytic Workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1608.03793v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.03793v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.08564v1</id>\\n    <updated>2017-05-23T23:51:37Z</updated>\\n    <published>2017-05-23T23:51:37Z</published>\\n    <title>Towards Interrogating Discriminative Machine Learning Models</title>\\n    <summary>  It is oftentimes impossible to understand how machine learning models reach a\\ndecision. While recent research has proposed various technical approaches to\\nprovide some clues as to how a learning model makes individual decisions, they\\ncannot provide users with ability to inspect a learning model as a complete\\nentity. In this work, we propose a new technical approach that augments a\\nBayesian regression mixture model with multiple elastic nets. Using the\\nenhanced mixture model, we extract explanations for a target model through\\nglobal approximation. To demonstrate the utility of our approach, we evaluate\\nit on different learning models covering the tasks of text mining and image\\nrecognition. Our results indicate that the proposed approach not only\\noutperforms the state-of-the-art technique in explaining individual decisions\\nbut also provides users with an ability to discover the vulnerabilities of a\\nlearning model.\\n</summary>\\n    <author>\\n      <name>Wenbo Guo</name>\\n    </author>\\n    <author>\\n      <name>Kaixuan Zhang</name>\\n    </author>\\n    <author>\\n      <name>Lin Lin</name>\\n    </author>\\n    <author>\\n      <name>Sui Huang</name>\\n    </author>\\n    <author>\\n      <name>Xinyu Xing</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1705.08564v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.08564v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.10229v1</id>\\n    <updated>2017-05-29T15:01:44Z</updated>\\n    <published>2017-05-29T15:01:44Z</published>\\n    <title>Latent Intention Dialogue Models</title>\\n    <summary>  Developing a dialogue agent that is capable of making autonomous decisions\\nand communicating by natural language is one of the long-term goals of machine\\nlearning research. Traditional approaches either rely on hand-crafting a small\\nstate-action set for applying reinforcement learning that is not scalable or\\nconstructing deterministic models for learning dialogue sentences that fail to\\ncapture natural conversational variability. In this paper, we propose a Latent\\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\\nlearn underlying dialogue intentions in the framework of neural variational\\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\\ninterpreted as actions guiding the generation of machine responses, which can\\nbe further refined autonomously by reinforcement learning. The experimental\\nevaluation of LIDM shows that the model out-performs published benchmarks for\\nboth corpus-based and human evaluation, demonstrating the effectiveness of\\ndiscrete latent variable models for learning goal-oriented dialogues.\\n</summary>\\n    <author>\\n      <name>Tsung-Hsien Wen</name>\\n    </author>\\n    <author>\\n      <name>Yishu Miao</name>\\n    </author>\\n    <author>\\n      <name>Phil Blunsom</name>\\n    </author>\\n    <author>\\n      <name>Steve Young</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted at ICML 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.10229v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.10229v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1707.09118v3</id>\\n    <updated>2017-12-14T13:44:31Z</updated>\\n    <published>2017-07-28T06:32:47Z</published>\\n    <title>Counterfactual Learning from Bandit Feedback under Deterministic\\n  Logging: A Case Study in Statistical Machine Translation</title>\\n    <summary>  The goal of counterfactual learning for statistical machine translation (SMT)\\nis to optimize a target SMT system from logged data that consist of user\\nfeedback to translations that were predicted by another, historic SMT system. A\\nchallenge arises by the fact that risk-averse commercial SMT systems\\ndeterministically log the most probable translation. The lack of sufficient\\nexploration of the SMT output space seemingly contradicts the theoretical\\nrequirements for counterfactual learning. We show that counterfactual learning\\nfrom deterministic bandit logs is possible nevertheless by smoothing out\\ndeterministic components in learning. This can be achieved by additive and\\nmultiplicative control variates that avoid degenerate behavior in empirical\\nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU\\npoints by counterfactual learning from deterministic bandit feedback.\\n</summary>\\n    <author>\\n      <name>Carolin Lawrence</name>\\n    </author>\\n    <author>\\n      <name>Artem Sokolov</name>\\n    </author>\\n    <author>\\n      <name>Stefan Riezler</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Conference on Empirical Methods in Natural Language Processing\\n  (EMNLP), 2017, Copenhagen, Denmark</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1707.09118v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.09118v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.05584v3</id>\\n    <updated>2018-04-10T15:26:32Z</updated>\\n    <published>2017-09-17T00:19:33Z</published>\\n    <title>Representation Learning on Graphs: Methods and Applications</title>\\n    <summary>  Machine learning on graphs is an important and ubiquitous task with\\napplications ranging from drug design to friendship recommendation in social\\nnetworks. The primary challenge in this domain is finding a way to represent,\\nor encode, graph structure so that it can be easily exploited by machine\\nlearning models. Traditionally, machine learning approaches relied on\\nuser-defined heuristics to extract features encoding structural information\\nabout a graph (e.g., degree statistics or kernel functions). However, recent\\nyears have seen a surge in approaches that automatically learn to encode graph\\nstructure into low-dimensional embeddings, using techniques based on deep\\nlearning and nonlinear dimensionality reduction. Here we provide a conceptual\\nreview of key advancements in this area of representation learning on graphs,\\nincluding matrix factorization-based methods, random-walk based algorithms, and\\ngraph neural networks. We review methods to embed individual nodes as well as\\napproaches to embed entire (sub)graphs. In doing so, we develop a unified\\nframework to describe these recent approaches, and we highlight a number of\\nimportant applications and directions for future work.\\n</summary>\\n    <author>\\n      <name>William L. Hamilton</name>\\n    </author>\\n    <author>\\n      <name>Rex Ying</name>\\n    </author>\\n    <author>\\n      <name>Jure Leskovec</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in the IEEE Data Engineering Bulletin, September 2017;\\n  version with minor corrections</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1709.05584v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.05584v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1711.08679v1</id>\\n    <updated>2017-11-23T13:07:37Z</updated>\\n    <published>2017-11-23T13:07:37Z</published>\\n    <title>Markov chain Hebbian learning algorithm with ternary synaptic units</title>\\n    <summary>  In spite of remarkable progress in machine learning techniques, the\\nstate-of-the-art machine learning algorithms often keep machines from real-time\\nlearning (online learning) due in part to computational complexity in parameter\\noptimization. As an alternative, a learning algorithm to train a memory in real\\ntime is proposed, which is named as the Markov chain Hebbian learning\\nalgorithm. The algorithm pursues efficient memory use during training in that\\n(i) the weight matrix has ternary elements (-1, 0, 1) and (ii) each update\\nfollows a Markov chain--the upcoming update does not need past weight memory.\\nThe algorithm was verified by two proof-of-concept tasks (handwritten digit\\nrecognition and multiplication table memorization) in which numbers were taken\\nas symbols. Particularly, the latter bases multiplication arithmetic on memory,\\nwhich may be analogous to humans\\' mental arithmetic. The memory-based\\nmultiplication arithmetic feasibly offers the basis of factorization,\\nsupporting novel insight into the arithmetic.\\n</summary>\\n    <author>\\n      <name>Guhyun Kim</name>\\n    </author>\\n    <author>\\n      <name>Vladimir Kornijcuk</name>\\n    </author>\\n    <author>\\n      <name>Dohun Kim</name>\\n    </author>\\n    <author>\\n      <name>Inho Kim</name>\\n    </author>\\n    <author>\\n      <name>Jaewook Kim</name>\\n    </author>\\n    <author>\\n      <name>Hyo Cheon Woo</name>\\n    </author>\\n    <author>\\n      <name>Ji Hun Kim</name>\\n    </author>\\n    <author>\\n      <name>Cheol Seong Hwang</name>\\n    </author>\\n    <author>\\n      <name>Doo Seok Jeong</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">25 pages, 4 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1711.08679v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1711.08679v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1712.00310v2</id>\\n    <updated>2017-12-05T11:29:27Z</updated>\\n    <published>2017-12-01T13:30:36Z</published>\\n    <title>Deep Learning with Permutation-invariant Operator for Multi-instance\\n  Histopathology Classification</title>\\n    <summary>  The computer-aided analysis of medical scans is a longstanding goal in the\\nmedical imaging field. Currently, deep learning has became a dominant\\nmethodology for supporting pathologists and radiologist. Deep learning\\nalgorithms have been successfully applied to digital pathology and radiology,\\nnevertheless, there are still practical issues that prevent these tools to be\\nwidely used in practice. The main obstacles are low number of available cases\\nand large size of images (a.k.a. the small n, large p problem in machine\\nlearning), and a very limited access to annotation at a pixel level that can\\nlead to severe overfitting and large computational requirements. We propose to\\nhandle these issues by introducing a framework that processes a medical image\\nas a collection of small patches using a single, shared neural network. The\\nfinal diagnosis is provided by combining scores of individual patches using a\\npermutation-invariant operator (combination). In machine learning community\\nsuch approach is called a multi-instance learning (MIL).\\n</summary>\\n    <author>\\n      <name>Jakub M. Tomczak</name>\\n    </author>\\n    <author>\\n      <name>Maximilian Ilse</name>\\n    </author>\\n    <author>\\n      <name>Max Welling</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Workshop on \"Medical Imaging meets NIPS\" at NIPS 2017</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1712.00310v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1712.00310v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.04300v2</id>\\n    <updated>2018-07-30T08:41:52Z</updated>\\n    <published>2018-03-12T15:10:45Z</published>\\n    <title>Neural Conditional Gradients</title>\\n    <summary>  The move from hand-designed to learned optimizers in machine learning has\\nbeen quite successful for gradient-based and -free optimizers. When facing a\\nconstrained problem, however, maintaining feasibility typically requires a\\nprojection step, which might be computationally expensive and not\\ndifferentiable. We show how the design of projection-free convex optimization\\nalgorithms can be cast as a learning problem based on Frank-Wolfe Networks:\\nrecurrent networks implementing the Frank-Wolfe algorithm aka. conditional\\ngradients. This allows them to learn to exploit structure when, e.g.,\\noptimizing over rank-1 matrices. Our LSTM-learned optimizers outperform\\nhand-designed as well learned but unconstrained ones. We demonstrate this for\\ntraining support vector machines and softmax classifiers.\\n</summary>\\n    <author>\\n      <name>Patrick Schramowski</name>\\n    </author>\\n    <author>\\n      <name>Christian Bauckhage</name>\\n    </author>\\n    <author>\\n      <name>Kristian Kersting</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1610.05120 by other authors</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.04300v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.04300v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.10897v1</id>\\n    <updated>2018-06-28T11:48:36Z</updated>\\n    <published>2018-06-28T11:48:36Z</published>\\n    <title>Deep learning in business analytics and operations research: Models,\\n  applications and managerial implications</title>\\n    <summary>  Business analytics refers to methods and practices that create value through\\ndata for individuals, firms, and organizations. This field is currently\\nexperiencing a radical shift due to the advent of deep learning: deep neural\\nnetworks promise improvements in prediction performance as compared to models\\nfrom traditional machine learning. However, our research into the existing body\\nof literature reveals a scarcity of research works utilizing deep learning in\\nour discipline. Accordingly, the objectives of this work are as follows: (1) we\\nmotivate why researchers and practitioners from business analytics should\\nutilize deep neural networks and review potential use cases, necessary\\nrequirements, and benefits. (2) We investigate the added value to operations\\nresearch in different case studies with real data from entrepreneurial\\nundertakings. All such cases demonstrate a higher prediction performance in\\ncomparison to traditional machine learning and thus direct value gains. (3) We\\nprovide guidelines and implications for researchers, managers and practitioners\\nin operations research who want to advance their capabilities for business\\nanalytics with regard to deep learning. (4) We finally discuss directions for\\nfuture research in the field of business analytics.\\n</summary>\\n    <author>\\n      <name>Mathias Kraus</name>\\n    </author>\\n    <author>\\n      <name>Stefan Feuerriegel</name>\\n    </author>\\n    <author>\\n      <name>Asil Oztekin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1806.10897v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.10897v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.11189v1</id>\\n    <updated>2018-06-26T06:38:01Z</updated>\\n    <published>2018-06-26T06:38:01Z</published>\\n    <title>A hybrid deep learning approach for medical relation extraction</title>\\n    <summary>  Mining relationships between treatment(s) and medical problem(s) is vital in\\nthe biomedical domain. This helps in various applications, such as decision\\nsupport system, safety surveillance, and new treatment discovery. We propose a\\ndeep learning approach that utilizes both word level and sentence-level\\nrepresentations to extract the relationships between treatment and problem.\\nWhile deep learning techniques demand a large amount of data for training, we\\nmake use of a rule-based system particularly for relationship classes with\\nfewer samples. Our final relations are derived by jointly combining the results\\nfrom deep learning and rule-based models. Our system achieved a promising\\nperformance on the relationship classes of I2b2 2010 relation extraction task.\\n</summary>\\n    <author>\\n      <name>Veera Raghavendra Chikka</name>\\n    </author>\\n    <author>\\n      <name>Kamalakar Karlapalem</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 4 tables, 1 figure, 2018 KDD workshop on Machine Learning\\n  for Medicine and Healthcare</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">MLMH 2018 2018 KDD workshop on Machine Learning for Medicine and\\n  Healthcare. London, United Kingdom. August 20, 2018</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1806.11189v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.11189v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.01619v2</id>\\n    <updated>2018-07-05T14:58:54Z</updated>\\n    <published>2018-07-04T14:43:57Z</published>\\n    <title>Ensemble learning with Conformal Predictors: Targeting credible\\n  predictions of conversion from Mild Cognitive Impairment to Alzheimer\\'s\\n  Disease</title>\\n    <summary>  Most machine learning classifiers give predictions for new examples\\naccurately, yet without indicating how trustworthy predictions are. In the\\nmedical domain, this hampers their integration in decision support systems,\\nwhich could be useful in the clinical practice. We use a supervised learning\\napproach that combines Ensemble learning with Conformal Predictors to predict\\nconversion from Mild Cognitive Impairment to Alzheimer\\'s Disease. Our goal is\\nto enhance the classification performance (Ensemble learning) and complement\\neach prediction with a measure of credibility (Conformal Predictors). Our\\nresults showed the superiority of the proposed approach over a similar ensemble\\nframework with standard classifiers.\\n</summary>\\n    <author>\\n      <name>Telma Pereira</name>\\n    </author>\\n    <author>\\n      <name>Sandra Cardoso</name>\\n    </author>\\n    <author>\\n      <name>Dina Silva</name>\\n    </author>\\n    <author>\\n      <name>Manuela Guerreiro</name>\\n    </author>\\n    <author>\\n      <name>Alexandre de Mendon\\xc3\\xa7a</name>\\n    </author>\\n    <author>\\n      <name>Sara C. Madeira</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 1 figure, accepted for presentation at the KDD Workshop on\\n  Machine Learning for Medicine and Healthcare, London, UK, August 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.01619v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.01619v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"68-06\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.10406v1</id>\\n    <updated>2018-08-30T17:25:48Z</updated>\\n    <published>2018-08-30T17:25:48Z</published>\\n    <title>Towards Reproducible Empirical Research in Meta-Learning</title>\\n    <summary>  Meta-learning is increasingly used to support the recommendation of machine\\nlearning algorithms and their configurations. Such recommendations are made\\nbased on meta-data, consisting of performance evaluations of algorithms on\\nprior datasets, as well as characterizations of these datasets. These\\ncharacterizations, also called meta-features, describe properties of the data\\nwhich are predictive for the performance of machine learning algorithms trained\\non them. Unfortunately, despite being used in a large number of studies,\\nmeta-features are not uniformly described and computed, making many empirical\\nstudies irreproducible and hard to compare. This paper aims to remedy this by\\nsystematizing and standardizing data characterization measures used in\\nmeta-learning, and performing an in-depth analysis of their utility. Moreover,\\nit presents MFE, a new tool for extracting meta-features from datasets and\\nidentify more subtle reproducibility issues in the literature, proposing\\nguidelines for data characterization that strengthen reproducible empirical\\nresearch in meta-learning.\\n</summary>\\n    <author>\\n      <name>Adriano Rivolli</name>\\n    </author>\\n    <author>\\n      <name>Lu\\xc3\\xads P. F. Garcia</name>\\n    </author>\\n    <author>\\n      <name>Carlos Soares</name>\\n    </author>\\n    <author>\\n      <name>Joaquin Vanschoren</name>\\n    </author>\\n    <author>\\n      <name>Andr\\xc3\\xa9 C. P. L. F. de Carvalho</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1808.10406v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.10406v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.00175v3</id>\\n    <updated>2018-11-08T04:29:37Z</updated>\\n    <published>2018-09-01T13:33:31Z</published>\\n    <title>Hyperparameter Learning for Conditional Kernel Mean Embeddings with\\n  Rademacher Complexity Bounds</title>\\n    <summary>  Conditional kernel mean embeddings are nonparametric models that encode\\nconditional expectations in a reproducing kernel Hilbert space. While they\\nprovide a flexible and powerful framework for probabilistic inference, their\\nperformance is highly dependent on the choice of kernel and regularization\\nhyperparameters. Nevertheless, current hyperparameter tuning methods\\npredominantly rely on expensive cross validation or heuristics that is not\\noptimized for the inference task. For conditional kernel mean embeddings with\\ncategorical targets and arbitrary inputs, we propose a hyperparameter learning\\nframework based on Rademacher complexity bounds to prevent overfitting by\\nbalancing data fit against model complexity. Our approach only requires batch\\nupdates, allowing scalable kernel hyperparameter tuning without invoking kernel\\napproximations. Experiments demonstrate that our learning framework outperforms\\ncompeting methods, and can be further extended to incorporate and learn deep\\nneural network weights to improve generalization.\\n</summary>\\n    <author>\\n      <name>Kelvin Hsu</name>\\n    </author>\\n    <author>\\n      <name>Richard Nock</name>\\n    </author>\\n    <author>\\n      <name>Fabio Ramos</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Best Student Machine Learning Paper Award Winner at ECML-PKDD 2018\\n  (European Conference on Machine Learning and Principles and Practice of\\n  Knowledge Discovery in Databases)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1809.00175v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.00175v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.02442v1</id>\\n    <updated>2018-10-04T22:21:55Z</updated>\\n    <published>2018-10-04T22:21:55Z</published>\\n    <title>AutoLoss: Learning Discrete Schedules for Alternate Optimization</title>\\n    <summary>  Many machine learning problems involve iteratively and alternately optimizing\\ndifferent task objectives with respect to different sets of parameters.\\nAppropriately scheduling the optimization of a task objective or a set of\\nparameters is usually crucial to the quality of convergence. In this paper, we\\npresent AutoLoss, a meta-learning framework that automatically learns and\\ndetermines the optimization schedule. AutoLoss provides a generic way to\\nrepresent and learn the discrete optimization schedule from metadata, allows\\nfor a dynamic and data-driven schedule in ML problems that involve alternating\\nupdates of different parameters or from different loss objectives. We apply\\nAutoLoss on four ML tasks: d-ary quadratic regression, classification using a\\nmulti-layer perceptron (MLP), image generation using GANs, and multi-task\\nneural machine translation (NMT). We show that the AutoLoss controller is able\\nto capture the distribution of better optimization schedules that result in\\nhigher quality of convergence on all four tasks. The trained AutoLoss\\ncontroller is generalizable -- it can guide and improve the learning of a new\\ntask model with different specifications, or on different datasets.\\n</summary>\\n    <author>\\n      <name>Haowen Xu</name>\\n    </author>\\n    <author>\\n      <name>Hao Zhang</name>\\n    </author>\\n    <author>\\n      <name>Zhiting Hu</name>\\n    </author>\\n    <author>\\n      <name>Xiaodan Liang</name>\\n    </author>\\n    <author>\\n      <name>Ruslan Salakhutdinov</name>\\n    </author>\\n    <author>\\n      <name>Eric Xing</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19-pages manuscripts. The first two authors contributed equally</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.02442v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.02442v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.11098v1</id>\\n    <updated>2018-10-25T20:34:37Z</updated>\\n    <published>2018-10-25T20:34:37Z</published>\\n    <title>Provable Gaussian Embedding with One Observation</title>\\n    <summary>  The success of machine learning methods heavily relies on having an\\nappropriate representation for data at hand. Traditionally, machine learning\\napproaches relied on user-defined heuristics to extract features encoding\\nstructural information about data. However, recently there has been a surge in\\napproaches that learn how to encode the data automatically in a low dimensional\\nspace. Exponential family embedding provides a probabilistic framework for\\nlearning low-dimensional representation for various types of high-dimensional\\ndata. Though successful in practice, theoretical underpinnings for exponential\\nfamily embeddings have not been established. In this paper, we study the\\nGaussian embedding model and develop the first theoretical results for\\nexponential family embedding models. First, we show that, under mild condition,\\nthe embedding structure can be learned from one observation by leveraging the\\nparameter sharing between different contexts even though the data are dependent\\nwith each other. Second, we study properties of two algorithms used for\\nlearning the embedding structure and establish convergence results for each of\\nthem. The first algorithm is based on a convex relaxation, while the other\\nsolved the non-convex formulation of the problem directly. Experiments\\ndemonstrate the effectiveness of our approach.\\n</summary>\\n    <author>\\n      <name>Ming Yu</name>\\n    </author>\\n    <author>\\n      <name>Zhuoran Yang</name>\\n    </author>\\n    <author>\\n      <name>Tuo Zhao</name>\\n    </author>\\n    <author>\\n      <name>Mladen Kolar</name>\\n    </author>\\n    <author>\\n      <name>Zhaoran Wang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.11098v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.11098v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1811.06052v1</id>\\n    <updated>2018-11-14T20:44:50Z</updated>\\n    <published>2018-11-14T20:44:50Z</published>\\n    <title>Human-like machine learning: limitations and suggestions</title>\\n    <summary>  This paper attempts to address the issues of machine learning in its current\\nimplementation. It is known that machine learning algorithms require a\\nsignificant amount of data for training purposes, whereas recent developments\\nin deep learning have increased this requirement dramatically. The performance\\nof an algorithm depends on the quality of data and hence, algorithms are as\\ngood as the data they are trained on. Supervised learning is developed based on\\nhuman learning processes by analysing named (i.e. annotated) objects, scenes\\nand actions. Whether training on large quantities of data (i.e. big data) is\\nthe right or the wrong approach, is debatable. The fact is, that training\\nalgorithms the same way we learn ourselves, comes with limitations. This paper\\ndiscusses the issues around applying a human-like approach to train algorithms\\nand the implications of this approach when using limited data. Several current\\nstudies involving non-data-driven algorithms and natural examples are also\\ndiscussed and certain alternative approaches are suggested.\\n</summary>\\n    <author>\\n      <name>Georgios Mastorakis</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Preprint, 24 pages, 5 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1811.06052v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1811.06052v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1812.00050v2</id>\\n    <updated>2018-12-07T12:50:59Z</updated>\\n    <published>2018-11-30T20:48:12Z</published>\\n    <title>Learning Interpretable Rules for Multi-label Classification</title>\\n    <summary>  Multi-label classification (MLC) is a supervised learning problem in which,\\ncontrary to standard multiclass classification, an instance can be associated\\nwith several class labels simultaneously. In this chapter, we advocate a\\nrule-based approach to multi-label classification. Rule learning algorithms are\\noften employed when one is not only interested in accurate predictions, but\\nalso requires an interpretable theory that can be understood, analyzed, and\\nqualitatively evaluated by domain experts. Ideally, by revealing patterns and\\nregularities contained in the data, a rule-based theory yields new insights in\\nthe application domain. Recently, several authors have started to investigate\\nhow rule-based models can be used for modeling multi-label data. Discussing\\nthis task in detail, we highlight some of the problems that make rule learning\\nconsiderably more challenging for MLC than for conventional classification.\\nWhile mainly focusing on our own previous work, we also provide a short\\noverview of related work in this area.\\n</summary>\\n    <author>\\n      <name>Eneldo Loza Menc\\xc3\\xada</name>\\n    </author>\\n    <author>\\n      <name>Johannes F\\xc3\\xbcrnkranz</name>\\n    </author>\\n    <author>\\n      <name>Eyke H\\xc3\\xbcllermeier</name>\\n    </author>\\n    <author>\\n      <name>Michael Rapp</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-319-98131-4_4</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-319-98131-4_4\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Preprint version. To appear in: Explainable and Interpretable Models\\n  in Computer Vision and Machine Learning. The Springer Series on Challenges in\\n  Machine Learning. Springer (2018). See\\n  http://www.ke.tu-darmstadt.de/bibtex/publications/show/3077 for further\\n  information</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1812.00050v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1812.00050v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.07986v2</id>\\n    <updated>2019-04-10T20:37:11Z</updated>\\n    <published>2019-01-23T16:34:28Z</published>\\n    <title>PD-ML-Lite: Private Distributed Machine Learning from Lighweight\\n  Cryptography</title>\\n    <summary>  Privacy is a major issue in learning from distributed data. Recently the\\ncryptographic literature has provided several tools for this task. However,\\nthese tools either reduce the quality/accuracy of the learning\\nalgorithm---e.g., by adding noise---or they incur a high performance penalty\\nand/or involve trusting external authorities.\\n  We propose a methodology for {\\\\sl private distributed machine learning from\\nlight-weight cryptography} (in short, PD-ML-Lite). We apply our methodology to\\ntwo major ML algorithms, namely non-negative matrix factorization (NMF) and\\nsingular value decomposition (SVD). Our resulting protocols are communication\\noptimal, achieve the same accuracy as their non-private counterparts, and\\nsatisfy a notion of privacy---which we define---that is both intuitive and\\nmeasurable. Our approach is to use lightweight cryptographic protocols (secure\\nsum and normalized secure sum) to build learning algorithms rather than wrap\\ncomplex learning algorithms in a heavy-cost MPC framework.\\n  We showcase our algorithms\\' utility and privacy on several applications: for\\nNMF we consider topic modeling and recommender systems, and for SVD, principal\\ncomponent regression, and low rank approximation.\\n</summary>\\n    <author>\\n      <name>Maksim Tsikhanovich</name>\\n    </author>\\n    <author>\\n      <name>Malik Magdon-Ismail</name>\\n    </author>\\n    <author>\\n      <name>Muhammad Ishaq</name>\\n    </author>\\n    <author>\\n      <name>Vassilis Zikas</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.07986v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.07986v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.08755v1</id>\\n    <updated>2019-01-25T06:53:11Z</updated>\\n    <published>2019-01-25T06:53:11Z</published>\\n    <title>SecureBoost: A Lossless Federated Learning Framework</title>\\n    <summary>  The protection of user privacy is an important concern in machine learning,\\nas evidenced by the rolling out of the General Data Protection Regulation\\n(GDPR) in the European Union (EU) in May 2018. The GDPR is designed to give\\nusers more control over their personal data, which motivates us to explore\\nmachine learning frameworks with data sharing without violating user privacy.\\nTo meet this goal, in this paper, we propose a novel lossless\\nprivacy-preserving tree-boosting system known as SecureBoost in the setting of\\nfederated learning. This federated-learning system allows a learning process to\\nbe jointly conducted over multiple parties with partially common user samples\\nbut different feature sets, which corresponds to a vertically partitioned\\nvirtual data set. An advantage of SecureBoost is that it provides the same\\nlevel of accuracy as the non-privacy-preserving approach while at the same\\ntime, reveal no information of each private data provider. We theoretically\\nprove that the SecureBoost framework is as accurate as other non-federated\\ngradient tree-boosting algorithms that bring the data into one place. In\\naddition, along with a proof of security, we discuss what would be required to\\nmake the protocols completely secure.\\n</summary>\\n    <author>\\n      <name>Kewei Cheng</name>\\n    </author>\\n    <author>\\n      <name>Tao Fan</name>\\n    </author>\\n    <author>\\n      <name>Yilun Jin</name>\\n    </author>\\n    <author>\\n      <name>Yang Liu</name>\\n    </author>\\n    <author>\\n      <name>Tianjian Chen</name>\\n    </author>\\n    <author>\\n      <name>Qiang Yang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.08755v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.08755v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.10868v1</id>\\n    <updated>2019-01-30T14:51:24Z</updated>\\n    <published>2019-01-30T14:51:24Z</published>\\n    <title>Learning to Project in Multi-Objective Binary Linear Programming</title>\\n    <summary>  In this paper, we investigate the possibility of improving the performance of\\nmulti-objective optimization solution approaches using machine learning\\ntechniques. Specifically, we focus on multi-objective binary linear programs\\nand employ one of the most effective and recently developed criterion space\\nsearch algorithms, the so-called KSA, during our study. This algorithm computes\\nall nondominated points of a problem with p objectives by searching on a\\nprojected criterion space, i.e., a (p-1)-dimensional criterion apace. We\\npresent an effective and fast learning approach to identify on which projected\\nspace the KSA should work. We also present several generic features/variables\\nthat can be used in machine learning techniques for identifying the best\\nprojected space. Finally, we present an effective bi-objective optimization\\nbased heuristic for selecting the best subset of the features to overcome the\\nissue of overfitting in learning. Through an extensive computational study over\\n2000 instances of tri-objective Knapsack and Assignment problems, we\\ndemonstrate that an improvement of up to 12% in time can be achieved by the\\nproposed learning method compared to a random selection of the projected space.\\n</summary>\\n    <author>\\n      <name>Alvaro Sierra-Altamiranda</name>\\n    </author>\\n    <author>\\n      <name>Hadi Charkhgard</name>\\n    </author>\\n    <author>\\n      <name>Iman Dayarian</name>\\n    </author>\\n    <author>\\n      <name>Ali Eshragh</name>\\n    </author>\\n    <author>\\n      <name>Sorna Javadi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.10868v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.10868v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.07958v1</id>\\n    <updated>2019-02-21T10:50:44Z</updated>\\n    <published>2019-02-21T10:50:44Z</published>\\n    <title>Deep Learning Multidimensional Projections</title>\\n    <summary>  Dimensionality reduction methods, also known as projections, are frequently\\nused for exploring multidimensional data in machine learning, data science, and\\ninformation visualization. Among these, t-SNE and its variants have become very\\npopular for their ability to visually separate distinct data clusters. However,\\nsuch methods are computationally expensive for large datasets, suffer from\\nstability problems, and cannot directly handle out-of-sample data. We propose a\\nlearning approach to construct such projections. We train a deep neural network\\nbased on a collection of samples from a given data universe, and their\\ncorresponding projections, and next use the network to infer projections of\\ndata from the same, or similar, universes. Our approach generates projections\\nwith similar characteristics as the learned ones, is computationally two to\\nthree orders of magnitude faster than SNE-class methods, has no complex-to-set\\nuser parameters, handles out-of-sample data in a stable manner, and can be used\\nto learn any projection technique. We demonstrate our proposal on several\\nreal-world high dimensional datasets from machine learning.\\n</summary>\\n    <author>\\n      <name>Mateus Espadoto</name>\\n    </author>\\n    <author>\\n      <name>Nina S. T. Hirata</name>\\n    </author>\\n    <author>\\n      <name>Alexandru C. Telea</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.07958v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.07958v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.06793v3</id>\\n    <updated>2017-04-06T02:40:23Z</updated>\\n    <published>2016-06-22T00:26:59Z</published>\\n    <title>Scalable Semi-supervised Learning with Graph-based Kernel Machine</title>\\n    <summary>  Acquiring labels are often costly, whereas unlabeled data are usually easy to\\nobtain in modern machine learning applications. Semi-supervised learning\\nprovides a principled machine learning framework to address such situations,\\nand has been applied successfully in many real-word applications and\\nindustries. Nonetheless, most of existing semi-supervised learning methods\\nencounter two serious limitations when applied to modern and large-scale\\ndatasets: computational burden and memory usage demand. To this end, we present\\nin this paper the Graph-based semi-supervised Kernel Machine (GKM), a method\\nthat leverages the generalization ability of kernel-based method with the\\ngeometrical and distributive information formulated through a spectral graph\\ninduced from data for semi-supervised learning purpose. Our proposed GKM can be\\nsolved directly in the primal form using the Stochastic Gradient Descent method\\nwith the ideal convergence rate $O(\\\\frac{1}{T})$. Besides, our formulation is\\nsuitable for a wide spectrum of important loss functions in the literature of\\nmachine learning (e.g., Hinge, smooth Hinge, Logistic, L1, and\\n{\\\\epsilon}-insensitive) and smoothness functions (i.e., $l_p(t) = |t|^p$ with\\n$p\\\\ge1$). We further show that the well-known Laplacian Support Vector Machine\\nis a special case of our formulation. We validate our proposed method on\\nseveral benchmark datasets to demonstrate that GKM is appropriate for the\\nlarge-scale datasets since it is optimal in memory usage and yields superior\\nclassification accuracy whilst simultaneously achieving a significant\\ncomputation speed-up in comparison with the state-of-the-art baselines.\\n</summary>\\n    <author>\\n      <name>Trung Le</name>\\n    </author>\\n    <author>\\n      <name>Khanh Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Van Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Vu Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Dinh Phung</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.06793v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.06793v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.08823v2</id>\\n    <updated>2019-02-01T23:39:08Z</updated>\\n    <published>2018-03-23T14:53:05Z</published>\\n    <title>A high-bias, low-variance introduction to Machine Learning for\\n  physicists</title>\\n    <summary>  Machine Learning (ML) is one of the most exciting and dynamic areas of modern\\nresearch and application. The purpose of this review is to provide an\\nintroduction to the core concepts and tools of machine learning in a manner\\neasily understood and intuitive to physicists. The review begins by covering\\nfundamental concepts in ML and modern statistics such as the bias-variance\\ntradeoff, overfitting, regularization, generalization, and gradient descent\\nbefore moving on to more advanced topics in both supervised and unsupervised\\nlearning. Topics covered in the review include ensemble models, deep learning\\nand neural networks, clustering and data visualization, energy-based models\\n(including MaxEnt models and Restricted Boltzmann Machines), and variational\\nmethods. Throughout, we emphasize the many natural connections between ML and\\nstatistical physics. A notable aspect of the review is the use of Python\\nJupyter notebooks to introduce modern ML/statistical packages to readers using\\nphysics-inspired datasets (the Ising Model and Monte-Carlo simulations of\\nsupersymmetric decays of proton-proton collisions). We conclude with an\\nextended outlook discussing possible uses of machine learning for furthering\\nour understanding of the physical world as well as open problems in ML where\\nphysicists may be able to contribute. (Notebooks are available at\\nhttps://physics.bu.edu/~pankajm/MLnotebooks.html )\\n</summary>\\n    <author>\\n      <name>Pankaj Mehta</name>\\n    </author>\\n    <author>\\n      <name>Marin Bukov</name>\\n    </author>\\n    <author>\\n      <name>Ching-Hao Wang</name>\\n    </author>\\n    <author>\\n      <name>Alexandre G. R. Day</name>\\n    </author>\\n    <author>\\n      <name>Clint Richardson</name>\\n    </author>\\n    <author>\\n      <name>Charles K. Fisher</name>\\n    </author>\\n    <author>\\n      <name>David J. Schwab</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Notebooks have been updated. 122 pages, 78 figures, 20 Python\\n  notebooks</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1803.08823v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.08823v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.00503v4</id>\\n    <updated>2018-10-17T18:59:37Z</updated>\\n    <published>2018-06-01T18:35:50Z</published>\\n    <title>Machine learning materials physics: Surrogate optimization and\\n  multi-fidelity algorithms predict precipitate morphology in an alternative to\\n  phase field dynamics</title>\\n    <summary>  Machine learning has been effective at detecting patterns and predicting the\\nresponse of systems that behave free of natural laws. Examples include learning\\ncrowd dynamics, recommender systems and autonomous mobility. There also have\\nbeen applications to the search for new materials that bear relations to big\\ndata classification problems. However, when it comes to physical systems\\ngoverned by conservation laws, the role of machine learning has been more\\nlimited. Here, we present our recent work in exploring the role of machine\\nlearning methods in discovering, or aiding, the search for physics.\\nSpecifically, we focus on using machine learning algorithms to represent\\nhigh-dimensional free energy surfaces with the goal of identifying precipitate\\nmorphologies in alloy systems. Traditionally, this problem has been approached\\nby combining phase field models, which impose first-order dynamics, with\\nelasticity, to traverse a free energy landscape in search of minima.\\nEquilibrium precipitate morphologies occur at these minima. Here, we exploit\\nthe machine learning methods to represent high-dimensional data, combined with\\nsurrogate optimization, sensitivity analysis and multifidelity modelling as an\\nalternate framework to explore phenomena controlled by energy extremization.\\nThis combination of data-driven methods offers an alternative to the imposition\\nof first-order dynamics via phase field methods, and represents one approach to\\nmachine learning materials physics.\\n</summary>\\n    <author>\\n      <name>Gregory Teichert</name>\\n    </author>\\n    <author>\\n      <name>Krishna Garikipati</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.cma.2018.10.025</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.cma.2018.10.025\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1806.00503v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.00503v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"physics.comp-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1808.00525v2</id>\\n    <updated>2018-08-03T02:59:12Z</updated>\\n    <published>2018-07-30T14:29:27Z</published>\\n    <title>The impact of imbalanced training data on machine learning for author\\n  name disambiguation</title>\\n    <summary>  In supervised machine learning for author name disambiguation, negative\\ntraining data are often dominantly larger than positive training data. This\\npaper examines how the ratios of negative to positive training data can affect\\nthe performance of machine learning algorithms to disambiguate author names in\\nbibliographic records. On multiple labeled datasets, three classifiers -\\nLogistic Regression, Na\\\\\"ive Bayes, and Random Forest - are trained through\\nrepresentative features such as coauthor names, and title words extracted from\\nthe same training data but with various positive-negative training data ratios.\\nResults show that increasing negative training data can improve disambiguation\\nperformance but with a few percent of performance gains and sometimes degrade\\nit. Logistic Regression and Na\\\\\"ive Bayes learn optimal disambiguation models\\neven with a base ratio (1:1) of positive and negative training data. Also, the\\nperformance improvement by Random Forest tends to quickly saturate roughly\\nafter 1:10 ~ 1:15. These findings imply that contrary to the common practice\\nusing all training data, name disambiguation algorithms can be trained using\\npart of negative training data without degrading much disambiguation\\nperformance while increasing computational efficiency. This study calls for\\nmore attention from author name disambiguation scholars to methods for machine\\nlearning from imbalanced data.\\n</summary>\\n    <author>\\n      <name>Jinseok Kim</name>\\n    </author>\\n    <author>\\n      <name>Jenna Kim</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s11192-018-2865-9</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s11192-018-2865-9\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 3 figures, and 3 tables</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Kim, J. &amp; Kim, J. (2018). The impact of imbalanced training data\\n  on machine learning for author name disambiguation. Scientometrics</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/1808.00525v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1808.00525v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.00249v1</id>\\n    <updated>2019-02-01T09:43:50Z</updated>\\n    <published>2019-02-01T09:43:50Z</published>\\n    <title>ProteinNet: a standardized data set for machine learning of protein\\n  structure</title>\\n    <summary>  Rapid progress in deep learning has spurred its application to bioinformatics\\nproblems including protein structure prediction and design. In classic machine\\nlearning problems like computer vision, progress has been driven by\\nstandardized data sets that facilitate fair assessment of new methods and lower\\nthe barrier to entry for non-domain experts. While data sets of protein\\nsequence and structure exist, they lack certain components critical for machine\\nlearning, including high-quality multiple sequence alignments and insulated\\ntraining / validation splits that account for deep but only weakly detectable\\nhomology across protein space. We have created the ProteinNet series of data\\nsets to provide a standardized mechanism for training and assessing data-driven\\nmodels of protein sequence-structure relationships. ProteinNet integrates\\nsequence, structure, and evolutionary information in programmatically\\naccessible file formats tailored for machine learning frameworks. Multiple\\nsequence alignments of all structurally characterized proteins were created\\nusing substantial high-performance computing resources. Standardized data\\nsplits were also generated to emulate the difficulty of past CASP (Critical\\nAssessment of protein Structure Prediction) experiments by resetting protein\\nsequence and structure space to the historical states that preceded six prior\\nCASPs. Utilizing sensitive evolution-based distance metrics to segregate\\ndistantly related proteins, we have additionally created validation sets\\ndistinct from the official CASP sets that faithfully mimic their difficulty.\\nProteinNet thus represents a comprehensive and accessible resource for training\\nand assessing machine-learned models of protein structure.\\n</summary>\\n    <author>\\n      <name>Mohammed AlQuraishi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 6 figures, 1 table</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1902.00249v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.00249v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"q-bio.QM\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1405.7292v2</id>\\n    <updated>2014-06-05T15:47:26Z</updated>\\n    <published>2014-05-28T16:08:32Z</published>\\n    <title>An Easy to Use Repository for Comparing and Improving Machine Learning\\n  Algorithm Usage</title>\\n    <summary>  The results from most machine learning experiments are used for a specific\\npurpose and then discarded. This results in a significant loss of information\\nand requires rerunning experiments to compare learning algorithms. This also\\nrequires implementation of another algorithm for comparison, that may not\\nalways be correctly implemented. By storing the results from previous\\nexperiments, machine learning algorithms can be compared easily and the\\nknowledge gained from them can be used to improve their performance. The\\npurpose of this work is to provide easy access to previous experimental results\\nfor learning and comparison. These stored results are comprehensive -- storing\\nthe prediction for each test instance as well as the learning algorithm,\\nhyperparameters, and training set that were used. Previous results are\\nparticularly important for meta-learning, which, in a broad sense, is the\\nprocess of learning from previous machine learning results such that the\\nlearning process is improved. While other experiment databases do exist, one of\\nour focuses is on easy access to the data. We provide meta-learning data sets\\nthat are ready to be downloaded for meta-learning experiments. In addition,\\nqueries to the underlying database can be made if specific information is\\ndesired. We also differ from previous experiment databases in that our\\ndatabases is designed at the instance level, where an instance is an example in\\na data set. We store the predictions of a learning algorithm trained on a\\nspecific training set for each instance in the test set. Data set level\\ninformation can then be obtained by aggregating the results from the instances.\\nThe instance level information can be used for many tasks such as determining\\nthe diversity of a classifier or algorithmically determining the optimal subset\\nof training instances for a learning algorithm.\\n</summary>\\n    <author>\\n      <name>Michael R. Smith</name>\\n    </author>\\n    <author>\\n      <name>Andrew White</name>\\n    </author>\\n    <author>\\n      <name>Christophe Giraud-Carrier</name>\\n    </author>\\n    <author>\\n      <name>Tony Martinez</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 1 figure, 6 tables</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1405.7292v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1405.7292v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1303.0448v2</id>\\n    <updated>2013-09-25T19:21:54Z</updated>\\n    <published>2013-03-03T01:49:56Z</published>\\n    <title>Learning Stable Multilevel Dictionaries for Sparse Representations</title>\\n    <summary>  Sparse representations using learned dictionaries are being increasingly used\\nwith success in several data processing and machine learning applications. The\\navailability of abundant training data necessitates the development of\\nefficient, robust and provably good dictionary learning algorithms. Algorithmic\\nstability and generalization are desirable characteristics for dictionary\\nlearning algorithms that aim to build global dictionaries which can efficiently\\nmodel any test data similar to the training samples. In this paper, we propose\\nan algorithm to learn dictionaries for sparse representations from large scale\\ndata, and prove that the proposed learning algorithm is stable and\\ngeneralizable asymptotically. The algorithm employs a 1-D subspace clustering\\nprocedure, the K-hyperline clustering, in order to learn a hierarchical\\ndictionary with multiple levels. We also propose an information-theoretic\\nscheme to estimate the number of atoms needed in each level of learning and\\ndevelop an ensemble approach to learn robust dictionaries. Using the proposed\\ndictionaries, the sparse code for novel test data can be computed using a\\nlow-complexity pursuit procedure. We demonstrate the stability and\\ngeneralization characteristics of the proposed algorithm using simulations. We\\nalso evaluate the utility of the multilevel dictionaries in compressed recovery\\nand subspace learning applications.\\n</summary>\\n    <author>\\n      <name>Jayaraman J. Thiagarajan</name>\\n    </author>\\n    <author>\\n      <name>Karthikeyan Natesan Ramamurthy</name>\\n    </author>\\n    <author>\\n      <name>Andreas Spanias</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1303.0448v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.0448v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1511.06429v5</id>\\n    <updated>2016-02-10T11:57:18Z</updated>\\n    <published>2015-11-19T22:39:35Z</published>\\n    <title>Patterns for Learning with Side Information</title>\\n    <summary>  Supervised, semi-supervised, and unsupervised learning estimate a function\\ngiven input/output samples. Generalization of the learned function to unseen\\ndata can be improved by incorporating side information into learning. Side\\ninformation are data that are neither from the input space nor from the output\\nspace of the function, but include useful information for learning it. In this\\npaper we show that learning with side information subsumes a variety of related\\napproaches, e.g. multi-task learning, multi-view learning and learning using\\nprivileged information. Our main contributions are (i) a new perspective that\\nconnects these previously isolated approaches, (ii) insights about how these\\nmethods incorporate different types of prior knowledge, and hence implement\\ndifferent patterns, (iii) facilitating the application of these methods in\\nnovel tasks, as well as (iv) a systematic experimental evaluation of these\\npatterns in two supervised learning tasks.\\n</summary>\\n    <author>\\n      <name>Rico Jonschkowski</name>\\n    </author>\\n    <author>\\n      <name>Sebastian H\\xc3\\xb6fer</name>\\n    </author>\\n    <author>\\n      <name>Oliver Brock</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The first two authors contributed equally to this work</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1511.06429v5\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1511.06429v5\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1501.04413v1</id>\\n    <updated>2015-01-19T07:24:21Z</updated>\\n    <published>2015-01-19T07:24:21Z</published>\\n    <title>Statistical-mechanical analysis of pre-training and fine tuning in deep\\n  learning</title>\\n    <summary>  In this paper, we present a statistical-mechanical analysis of deep learning.\\nWe elucidate some of the essential components of deep learning---pre-training\\nby unsupervised learning and fine tuning by supervised learning. We formulate\\nthe extraction of features from the training data as a margin criterion in a\\nhigh-dimensional feature-vector space. The self-organized classifier is then\\nsupplied with small amounts of labelled data, as in deep learning. Although we\\nemploy a simple single-layer perceptron model, rather than directly analyzing a\\nmulti-layer neural network, we find a nontrivial phase transition that is\\ndependent on the number of unlabelled data in the generalization error of the\\nresultant classifier. In this sense, we evaluate the efficacy of the\\nunsupervised learning component of deep learning. The analysis is performed by\\nthe replica method, which is a sophisticated tool in statistical mechanics. We\\nvalidate our result in the manner of deep learning, using a simple iterative\\nalgorithm to learn the weight vector on the basis of belief propagation.\\n</summary>\\n    <author>\\n      <name>Masayuki Ohzeki</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.7566/JPSJ.84.034003</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.7566/JPSJ.84.034003\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages and 2 figures, to appear in JPSJ</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1501.04413v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1501.04413v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.dis-nn\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.stat-mech\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.00241v1</id>\\n    <updated>2016-05-01T11:56:01Z</updated>\\n    <published>2016-05-01T11:56:01Z</published>\\n    <title>Common-Description Learning: A Framework for Learning Algorithms and\\n  Generating Subproblems from Few Examples</title>\\n    <summary>  Current learning algorithms face many difficulties in learning simple\\npatterns and using them to learn more complex ones. They also require more\\nexamples than humans do to learn the same pattern, assuming no prior knowledge.\\nIn this paper, a new learning framework is introduced that is called\\ncommon-description learning (CDL). This framework has been tested on 32 small\\nmulti-task datasets, and the results show that it was able to learn complex\\nalgorithms from a few number of examples. The final model is perfectly\\ninterpretable and its depth depends on the question. What is meant by depth\\nhere is that whenever needed, the model learns to break down the problem into\\nsimpler subproblems and solves them using previously learned models. Finally,\\nwe explain the capabilities of our framework in discovering complex relations\\nin data and how it can help in improving language understanding in machines.\\n</summary>\\n    <author>\\n      <name>Basem G. El-Barashy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">32 pages, 13 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1605.00241v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.00241v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.08696v3</id>\\n    <updated>2017-01-18T04:41:17Z</updated>\\n    <published>2016-10-27T10:50:55Z</published>\\n    <title>Learning Bound for Parameter Transfer Learning</title>\\n    <summary>  We consider a transfer-learning problem by using the parameter transfer\\napproach, where a suitable parameter of feature mapping is learned through one\\ntask and applied to another objective task. Then, we introduce the notion of\\nthe local stability and parameter transfer learnability of parametric feature\\nmapping,and thereby derive a learning bound for parameter transfer algorithms.\\nAs an application of parameter transfer learning, we discuss the performance of\\nsparse coding in self-taught learning. Although self-taught learning algorithms\\nwith plentiful unlabeled data often show excellent empirical performance, their\\ntheoretical analysis has not been studied. In this paper, we also provide the\\nfirst theoretical learning bound for self-taught learning.\\n</summary>\\n    <author>\\n      <name>Wataru Kumagai</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper was accepted at NIPS 2016 as a poster presentation</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.08696v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.08696v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1612.02879v2</id>\\n    <updated>2017-04-27T14:53:00Z</updated>\\n    <published>2016-12-09T00:56:42Z</published>\\n    <title>Learning Representations by Stochastic Meta-Gradient Descent in Neural\\n  Networks</title>\\n    <summary>  Representations are fundamental to artificial intelligence. The performance\\nof a learning system depends on the type of representation used for\\nrepresenting the data. Typically, these representations are hand-engineered\\nusing domain knowledge. More recently, the trend is to learn these\\nrepresentations through stochastic gradient descent in multi-layer neural\\nnetworks, which is called backprop. Learning the representations directly from\\nthe incoming data stream reduces the human labour involved in designing a\\nlearning system. More importantly, this allows in scaling of a learning system\\nfor difficult tasks. In this paper, we introduce a new incremental learning\\nalgorithm called crossprop, which learns incoming weights of hidden units based\\non the meta-gradient descent approach, that was previously introduced by Sutton\\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\\nequation introduces an additional memory parameter for each of these weights\\nand generalizes the backprop update equation. From our experiments, we show\\nthat crossprop learns and reuses its feature representation while tackling new\\nand unseen tasks whereas backprop relearns a new feature representation.\\n</summary>\\n    <author>\\n      <name>Vivek Veeriah</name>\\n    </author>\\n    <author>\\n      <name>Shangtong Zhang</name>\\n    </author>\\n    <author>\\n      <name>Richard S. Sutton</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1612.02879v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1612.02879v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1705.11159v1</id>\\n    <updated>2017-05-31T15:58:35Z</updated>\\n    <published>2017-05-31T15:58:35Z</published>\\n    <title>Reinforcement Learning for Learning Rate Control</title>\\n    <summary>  Stochastic gradient descent (SGD), which updates the model parameters by\\nadding a local gradient times a learning rate at each step, is widely used in\\nmodel training of machine learning algorithms such as neural networks. It is\\nobserved that the models trained by SGD are sensitive to learning rates and\\ngood learning rates are problem specific. We propose an algorithm to\\nautomatically learn learning rates using neural network based actor-critic\\nmethods from deep reinforcement learning (RL).In particular, we train a policy\\nnetwork called actor to decide the learning rate at each step during training,\\nand a value network called critic to give feedback about quality of the\\ndecision (e.g., the goodness of the learning rate outputted by the actor) that\\nthe actor made. The introduction of auxiliary actor and critic networks helps\\nthe main network achieve better performance. Experiments on different datasets\\nand network architectures show that our approach leads to better convergence of\\nSGD than human-designed competitors.\\n</summary>\\n    <author>\\n      <name>Chang Xu</name>\\n    </author>\\n    <author>\\n      <name>Tao Qin</name>\\n    </author>\\n    <author>\\n      <name>Gang Wang</name>\\n    </author>\\n    <author>\\n      <name>Tie-Yan Liu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1705.11159v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1705.11159v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1706.03100v1</id>\\n    <updated>2017-06-09T19:34:03Z</updated>\\n    <published>2017-06-09T19:34:03Z</published>\\n    <title>Decoupling Learning Rules from Representations</title>\\n    <summary>  In the artificial intelligence field, learning often corresponds to changing\\nthe parameters of a parameterized function. A learning rule is an algorithm or\\nmathematical expression that specifies precisely how the parameters should be\\nchanged. When creating an artificial intelligence system, we must make two\\ndecisions: what representation should be used (i.e., what parameterized\\nfunction should be used) and what learning rule should be used to search\\nthrough the resulting set of representable functions. Using most learning\\nrules, these two decisions are coupled in a subtle (and often unintentional)\\nway. That is, using the same learning rule with two different representations\\nthat can represent the same sets of functions can result in two different\\noutcomes. After arguing that this coupling is undesirable, particularly when\\nusing artificial neural networks, we present a method for partially decoupling\\nthese two decisions for a broad class of learning rules that span unsupervised\\nlearning, reinforcement learning, and supervised learning.\\n</summary>\\n    <author>\\n      <name>Philip S. Thomas</name>\\n    </author>\\n    <author>\\n      <name>Christoph Dann</name>\\n    </author>\\n    <author>\\n      <name>Emma Brunskill</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1706.03100v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.03100v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.01231v1</id>\\n    <updated>2017-09-05T04:10:44Z</updated>\\n    <published>2017-09-05T04:10:44Z</published>\\n    <title>Discriminative Similarity for Clustering and Semi-Supervised Learning</title>\\n    <summary>  Similarity-based clustering and semi-supervised learning methods separate the\\ndata into clusters or classes according to the pairwise similarity between the\\ndata, and the pairwise similarity is crucial for their performance. In this\\npaper, we propose a novel discriminative similarity learning framework which\\nlearns discriminative similarity for either data clustering or semi-supervised\\nlearning. The proposed framework learns classifier from each hypothetical\\nlabeling, and searches for the optimal labeling by minimizing the\\ngeneralization error of the learned classifiers associated with the\\nhypothetical labeling. Kernel classifier is employed in our framework. By\\ngeneralization analysis via Rademacher complexity, the generalization error\\nbound for the kernel classifier learned from hypothetical labeling is expressed\\nas the sum of pairwise similarity between the data from different classes,\\nparameterized by the weights of the kernel classifier. Such pairwise similarity\\nserves as the discriminative similarity for the purpose of clustering and\\nsemi-supervised learning, and discriminative similarity with similar form can\\nalso be induced by the integrated squared error bound for kernel density\\nclassification. Based on the discriminative similarity induced by the kernel\\nclassifier, we propose new clustering and semi-supervised learning methods.\\n</summary>\\n    <author>\\n      <name>Yingzhen Yang</name>\\n    </author>\\n    <author>\\n      <name>Feng Liang</name>\\n    </author>\\n    <author>\\n      <name>Nebojsa Jojic</name>\\n    </author>\\n    <author>\\n      <name>Shuicheng Yan</name>\\n    </author>\\n    <author>\\n      <name>Jiashi Feng</name>\\n    </author>\\n    <author>\\n      <name>Thomas S. Huang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1709.01231v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.01231v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1709.05412v2</id>\\n    <updated>2018-02-20T21:37:12Z</updated>\\n    <published>2017-09-15T21:24:26Z</published>\\n    <title>Multi-Agent Distributed Lifelong Learning for Collective Knowledge\\n  Acquisition</title>\\n    <summary>  Lifelong machine learning methods acquire knowledge over a series of\\nconsecutive tasks, continually building upon their experience. Current lifelong\\nlearning algorithms rely upon a single learning agent that has centralized\\naccess to all data. In this paper, we extend the idea of lifelong learning from\\na single agent to a network of multiple agents that collectively learn a series\\nof tasks. Each agent faces some (potentially unique) set of tasks; the key idea\\nis that knowledge learned from these tasks may benefit other agents trying to\\nlearn different (but related) tasks. Our Collective Lifelong Learning Algorithm\\n(CoLLA) provides an efficient way for a network of agents to share their\\nlearned knowledge in a distributed and decentralized manner, while preserving\\nthe privacy of the locally observed data. Note that a decentralized scheme is a\\nsubclass of distributed algorithms where a central server does not exist and in\\naddition to data, computations are also distributed among the agents. We\\nprovide theoretical guarantees for robust performance of the algorithm and\\nempirically demonstrate that CoLLA outperforms existing approaches for\\ndistributed multi-task learning on a variety of data sets.\\n</summary>\\n    <author>\\n      <name>Mohammad Rostami</name>\\n    </author>\\n    <author>\\n      <name>Soheil Kolouri</name>\\n    </author>\\n    <author>\\n      <name>Kyungnam Kim</name>\\n    </author>\\n    <author>\\n      <name>Eric Eaton</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1709.05412v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.05412v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1801.05558v3</id>\\n    <updated>2018-06-14T12:33:23Z</updated>\\n    <published>2018-01-17T05:34:08Z</published>\\n    <title>Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace</title>\\n    <summary>  Gradient-based meta-learning methods leverage gradient descent to learn the\\ncommonalities among various tasks. While previous such methods have been\\nsuccessful in meta-learning tasks, they resort to simple gradient descent\\nduring meta-testing. Our primary contribution is the {\\\\em MT-net}, which\\nenables the meta-learner to learn on each layer\\'s activation space a subspace\\nthat the task-specific learner performs gradient descent on. Additionally, a\\ntask-specific learner of an {\\\\em MT-net} performs gradient descent with respect\\nto a meta-learned distance metric, which warps the activation space to be more\\nsensitive to task identity. We demonstrate that the dimension of this learned\\nsubspace reflects the complexity of the task-specific learner\\'s adaptation\\ntask, and also that our model is less sensitive to the choice of initial\\nlearning rates than previous gradient-based meta-learning methods. Our method\\nachieves state-of-the-art or comparable performance on few-shot classification\\nand regression tasks.\\n</summary>\\n    <author>\\n      <name>Yoonho Lee</name>\\n    </author>\\n    <author>\\n      <name>Seungjin Choi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to ICML 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1801.05558v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1801.05558v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1802.05313v1</id>\\n    <updated>2018-02-14T20:37:38Z</updated>\\n    <published>2018-02-14T20:37:38Z</published>\\n    <title>Reinforcement Learning from Imperfect Demonstrations</title>\\n    <summary>  Robust real-world learning should benefit from both demonstrations and\\ninteractions with the environment. Current approaches to learning from\\ndemonstration and reward perform supervised learning on expert demonstration\\ndata and use reinforcement learning to further improve performance based on the\\nreward received from the environment. These tasks have divergent losses which\\nare difficult to jointly optimize and such methods can be very sensitive to\\nnoisy demonstrations. We propose a unified reinforcement learning algorithm,\\nNormalized Actor-Critic (NAC), that effectively normalizes the Q-function,\\nreducing the Q-values of actions unseen in the demonstration data. NAC learns\\nan initial policy network from demonstrations and refines the policy in the\\nenvironment, surpassing the demonstrator\\'s performance. Crucially, both\\nlearning from demonstration and interactive refinement use the same objective,\\nunlike prior approaches that combine distinct supervised and reinforcement\\nlosses. This makes NAC robust to suboptimal demonstration data since the method\\nis not forced to mimic all of the examples in the dataset. We show that our\\nunified reinforcement learning algorithm can learn robustly and outperform\\nexisting baselines when evaluated on several realistic driving games.\\n</summary>\\n    <author>\\n      <name>Yang Gao</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Harry</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name> Huazhe</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Harry</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name> Xu</name>\\n    </author>\\n    <author>\\n      <name>Ji Lin</name>\\n    </author>\\n    <author>\\n      <name>Fisher Yu</name>\\n    </author>\\n    <author>\\n      <name>Sergey Levine</name>\\n    </author>\\n    <author>\\n      <name>Trevor Darrell</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1802.05313v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1802.05313v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1803.08089v1</id>\\n    <updated>2018-03-21T18:50:18Z</updated>\\n    <published>2018-03-21T18:50:18Z</published>\\n    <title>Incremental Learning-to-Learn with Statistical Guarantees</title>\\n    <summary>  In learning-to-learn the goal is to infer a learning algorithm that works\\nwell on a class of tasks sampled from an unknown meta distribution. In contrast\\nto previous work on batch learning-to-learn, we consider a scenario where tasks\\nare presented sequentially and the algorithm needs to adapt incrementally to\\nimprove its performance on future tasks. Key to this setting is for the\\nalgorithm to rapidly incorporate new observations into the model as they\\narrive, without keeping them in memory. We focus on the case where the\\nunderlying algorithm is ridge regression parameterized by a positive\\nsemidefinite matrix. We propose to learn this matrix by applying a stochastic\\nstrategy to minimize the empirical error incurred by ridge regression on future\\ntasks sampled from the meta distribution. We study the statistical properties\\nof the proposed algorithm and prove non-asymptotic bounds on its excess\\ntransfer risk, that is, the generalization performance on new tasks from the\\nsame meta distribution. We compare our online learning-to-learn approach with a\\nstate of the art batch method, both theoretically and empirically.\\n</summary>\\n    <author>\\n      <name>Giulia Denevi</name>\\n    </author>\\n    <author>\\n      <name>Carlo Ciliberto</name>\\n    </author>\\n    <author>\\n      <name>Dimitris Stamos</name>\\n    </author>\\n    <author>\\n      <name>Massimiliano Pontil</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1803.08089v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1803.08089v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.00852v1</id>\\n    <updated>2018-06-03T19:16:50Z</updated>\\n    <published>2018-06-03T19:16:50Z</published>\\n    <title>On the Importance of Attention in Meta-Learning for Few-Shot Text\\n  Classification</title>\\n    <summary>  Current deep learning based text classification methods are limited by their\\nability to achieve fast learning and generalization when the data is scarce. We\\naddress this problem by integrating a meta-learning procedure that uses the\\nknowledge learned across many tasks as an inductive bias towards better natural\\nlanguage understanding. Based on the Model-Agnostic Meta-Learning framework\\n(MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML)\\nalgorithm for text classification. The essential difference between MAML and\\nATAML is in the separation of task-agnostic representation learning and\\ntask-specific attentive adaptation. The proposed ATAML is designed to encourage\\ntask-agnostic representation learning by way of task-agnostic parameterization\\nand facilitate task-specific adaptation via attention mechanisms. We provide\\nevidence to show that the attention mechanism in ATAML has a synergistic effect\\non learning performance. In comparisons with models trained from random\\ninitialization, pretrained models and meta trained MAML, our proposed ATAML\\nmethod generalizes better on single-label and multi-label classification tasks\\nin miniRCV1 and miniReuters-21578 datasets.\\n</summary>\\n    <author>\\n      <name>Xiang Jiang</name>\\n    </author>\\n    <author>\\n      <name>Mohammad Havaei</name>\\n    </author>\\n    <author>\\n      <name>Gabriel Chartrand</name>\\n    </author>\\n    <author>\\n      <name>Hassan Chouaib</name>\\n    </author>\\n    <author>\\n      <name>Thomas Vincent</name>\\n    </author>\\n    <author>\\n      <name>Andrew Jesson</name>\\n    </author>\\n    <author>\\n      <name>Nicolas Chapados</name>\\n    </author>\\n    <author>\\n      <name>Stan Matwin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 4 figures, submitted to NIPS</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.00852v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.00852v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.01265v2</id>\\n    <updated>2018-07-08T12:53:13Z</updated>\\n    <published>2018-06-01T21:54:18Z</published>\\n    <title>Equivalence Between Wasserstein and Value-Aware Loss for Model-based\\n  Reinforcement Learning</title>\\n    <summary>  Learning a generative model is a key component of model-based reinforcement\\nlearning. Though learning a good model in the tabular setting is a simple task,\\nlearning a useful model in the approximate setting is challenging. In this\\ncontext, an important question is the loss function used for model learning as\\nvarying the loss function can have a remarkable impact on effectiveness of\\nplanning. Recently Farahmand et al. (2017) proposed a value-aware model\\nlearning (VAML) objective that captures the structure of value function during\\nmodel learning. Using tools from Asadi et al. (2018), we show that minimizing\\nthe VAML objective is in fact equivalent to minimizing the Wasserstein metric.\\nThis equivalence improves our understanding of value-aware models, and also\\ncreates a theoretical foundation for applications of Wasserstein in model-based\\nreinforcement~learning.\\n</summary>\\n    <author>\\n      <name>Kavosh Asadi</name>\\n    </author>\\n    <author>\\n      <name>Evan Cater</name>\\n    </author>\\n    <author>\\n      <name>Dipendra Misra</name>\\n    </author>\\n    <author>\\n      <name>Michael L. Littman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted at the FAIM workshop \"Prediction and Generative Modeling in\\n  Reinforcement Learning\", Stockholm, Sweden, 2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.01265v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.01265v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1806.06253v2</id>\\n    <updated>2019-02-02T19:57:26Z</updated>\\n    <published>2018-06-16T15:32:32Z</published>\\n    <title>DynMat, a network that can learn after learning</title>\\n    <summary>  To survive in the dynamically-evolving world, we accumulate knowledge and\\nimprove our skills based on experience. In the process, gaining new knowledge\\ndoes not disrupt our vigilance to external stimuli. In other words, our\\nlearning process is \\'accumulative\\' and \\'online\\' without interruption. However,\\ndespite the recent success, artificial neural networks (ANNs) must be trained\\noffline, and they suffer catastrophic interference between old and new\\nlearning, indicating that ANNs\\' conventional learning algorithms may not be\\nsuitable for building intelligent agents comparable to our brain. In this\\nstudy, we propose a novel neural network architecture (DynMat) consisting of\\ndual learning systems, inspired by the complementary learning system (CLS)\\ntheory suggesting that the brain relies on short- and long-term learning\\nsystems to learn continuously. Our experiments show that 1) DynMat can learn a\\nnew class without catastrophic interference and 2) it does not strictly require\\noffline training.\\n</summary>\\n    <author>\\n      <name>Jung H. Lee</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">40 pages and 9 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1806.06253v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1806.06253v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1807.11459v1</id>\\n    <updated>2018-07-30T17:34:24Z</updated>\\n    <published>2018-07-30T17:34:24Z</published>\\n    <title>Improving Transferability of Deep Neural Networks</title>\\n    <summary>  Learning from small amounts of labeled data is a challenge in the area of\\ndeep learning. This is currently addressed by Transfer Learning where one\\nlearns the small data set as a transfer task from a larger source dataset.\\nTransfer Learning can deliver higher accuracy if the hyperparameters and source\\ndataset are chosen well. One of the important parameters is the learning rate\\nfor the layers of the neural network. We show through experiments on the\\nImageNet22k and Oxford Flowers datasets that improvements in accuracy in range\\nof 127% can be obtained by proper choice of learning rates. We also show that\\nthe images/label parameter for a dataset can potentially be used to determine\\noptimal learning rates for the layers to get the best overall accuracy. We\\nadditionally validate this method on a sample of real-world image\\nclassification tasks from a public visual recognition API.\\n</summary>\\n    <author>\\n      <name>Parijat Dube</name>\\n    </author>\\n    <author>\\n      <name>Bishwaranjan Bhattacharjee</name>\\n    </author>\\n    <author>\\n      <name>Elisabeth Petit-Bois</name>\\n    </author>\\n    <author>\\n      <name>Matthew Hill</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 11 figures, 2 tables, Workshop on Domain Adaptation for\\n  Visual Understanding (Joint IJCAI/ECAI/AAMAS/ICML 2018 Workshop) Keywords:\\n  deep learning, transfer learning, finetuning, deep neural network,\\n  experimental</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1807.11459v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1807.11459v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1809.06719v2</id>\\n    <updated>2018-11-04T19:40:31Z</updated>\\n    <published>2018-09-16T17:07:33Z</published>\\n    <title>Improvements on Hindsight Learning</title>\\n    <summary>  Sparse reward problems are one of the biggest challenges in Reinforcement\\nLearning. Goal-directed tasks are one such sparse reward problems where a\\nreward signal is received only when the goal is reached. One promising way to\\ntrain an agent to perform goal-directed tasks is to use Hindsight Learning\\napproaches. In these approaches, even when an agent fails to reach the desired\\ngoal, the agent learns to reach the goal it achieved instead. Doing this over\\nmultiple trajectories while generalizing the policy learned from the achieved\\ngoals, the agent learns a goal conditioned policy to reach any goal. One such\\napproach is Hindsight Experience replay which uses an off-policy Reinforcement\\nLearning algorithm to learn a goal conditioned policy. In this approach, a\\nreplay of the past transitions happens in a uniformly random fashion. Another\\napproach is to use a Hindsight version of the policy gradients to directly\\nlearn a policy. In this work, we discuss different ways to replay past\\ntransitions to improve learning in hindsight experience replay focusing on\\nprioritized variants in particular. Also, we implement the Hindsight Policy\\ngradient methods to robotic tasks.\\n</summary>\\n    <author>\\n      <name>Ameet Deshpande</name>\\n    </author>\\n    <author>\\n      <name>Srikanth Sarma</name>\\n    </author>\\n    <author>\\n      <name>Ashutosh Jha</name>\\n    </author>\\n    <author>\\n      <name>Balaraman Ravindran</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1809.06719v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1809.06719v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.04303v1</id>\\n    <updated>2018-10-10T00:02:55Z</updated>\\n    <published>2018-10-10T00:02:55Z</published>\\n    <title>Batch Active Preference-Based Learning of Reward Functions</title>\\n    <summary>  Data generation and labeling are usually an expensive part of learning for\\nrobotics. While active learning methods are commonly used to tackle the former\\nproblem, preference-based learning is a concept that attempts to solve the\\nlatter by querying users with preference questions. In this paper, we will\\ndevelop a new algorithm, batch active preference-based learning, that enables\\nefficient learning of reward functions using as few data samples as possible\\nwhile still having short query generation times. We introduce several\\napproximations to the batch active learning problem, and provide theoretical\\nguarantees for the convergence of our algorithms. Finally, we present our\\nexperimental results for a variety of robotics tasks in simulation. Our results\\nsuggest that our batch active learning algorithm requires only a few queries\\nthat are computed in a short amount of time. We then showcase our algorithm in\\na study to learn human users\\' preferences.\\n</summary>\\n    <author>\\n      <name>Erdem B\\xc4\\xb1y\\xc4\\xb1k</name>\\n    </author>\\n    <author>\\n      <name>Dorsa Sadigh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the 2nd Conference on Robot Learning (CoRL), October\\n  2018</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1810.04303v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.04303v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.05347v1</id>\\n    <updated>2018-10-12T04:03:20Z</updated>\\n    <published>2018-10-12T04:03:20Z</published>\\n    <title>Optimal Hierarchical Learning Path Design with Reinforcement Learning</title>\\n    <summary>  E-learning systems are capable of providing more adaptive and efficient\\nlearning experiences for students than the traditional classroom setting. A key\\ncomponent of such systems is the learning strategy, the algorithm that designs\\nthe learning paths for students based on information such as the students\\'\\ncurrent progresses, their skills, learning materials, and etc. In this paper,\\nwe address the problem of finding the optimal learning strategy for an\\nE-learning system. To this end, we first develop a model for students\\'\\nhierarchical skills in the E-learning system. Based on the hierarchical skill\\nmodel and the classical cognitive diagnosis model, we further develop a\\nframework to model various proficiency levels of hierarchical skills. The\\noptimal learning strategy on top of the hierarchical structure is found by\\napplying a model-free reinforcement learning method, which does not require\\ninformation on students\\' learning transition process. The effectiveness of the\\nproposed framework is demonstrated via numerical experiments.\\n</summary>\\n    <author>\\n      <name>Xiao Li</name>\\n    </author>\\n    <author>\\n      <name>Hanchen Xu</name>\\n    </author>\\n    <author>\\n      <name>Jinming Zhang</name>\\n    </author>\\n    <author>\\n      <name>Hua-hua Chang</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.05347v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.05347v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1810.11910v2</id>\\n    <updated>2018-12-02T19:23:28Z</updated>\\n    <published>2018-10-29T00:13:50Z</published>\\n    <title>Learning to Learn without Forgetting By Maximizing Transfer and\\n  Minimizing Interference</title>\\n    <summary>  Lack of performance when it comes to continual learning over non-stationary\\ndistributions of data remains a major challenge in scaling neural network\\nlearning to more human realistic settings. In this work we propose a new\\nconceptualization of the continual learning problem in terms of a temporally\\nsymmetric trade-off between transfer and interference that can be optimized by\\nenforcing gradient alignment across examples. We then propose a new algorithm,\\nMeta-Experience Replay (MER), that directly exploits this view by combining\\nexperience replay with optimization based meta-learning. This method learns\\nparameters that make interference based on future gradients less likely and\\ntransfer based on future gradients more likely. We conduct experiments across\\ncontinual lifelong supervised learning benchmarks and non-stationary\\nreinforcement learning environments demonstrating that our approach\\nconsistently outperforms recently proposed baselines for continual learning.\\nOur experiments show that the gap between the performance of MER and baseline\\nalgorithms grows both as the environment gets more non-stationary and as the\\nfraction of the total experiences stored gets smaller.\\n</summary>\\n    <author>\\n      <name>Matthew Riemer</name>\\n    </author>\\n    <author>\\n      <name>Ignacio Cases</name>\\n    </author>\\n    <author>\\n      <name>Robert Ajemian</name>\\n    </author>\\n    <author>\\n      <name>Miao Liu</name>\\n    </author>\\n    <author>\\n      <name>Irina Rish</name>\\n    </author>\\n    <author>\\n      <name>Yuhai Tu</name>\\n    </author>\\n    <author>\\n      <name>Gerald Tesauro</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1810.11910v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1810.11910v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1901.09895v1</id>\\n    <updated>2019-01-27T05:06:30Z</updated>\\n    <published>2019-01-27T05:06:30Z</published>\\n    <title>Modularization of End-to-End Learning: Case Study in Arcade Games</title>\\n    <summary>  Complex environments and tasks pose a difficult problem for holistic\\nend-to-end learning approaches. Decomposition of an environment into\\ninteracting controllable and non-controllable objects allows supervised\\nlearning for non-controllable objects and universal value function approximator\\nlearning for controllable objects. Such decomposition should lead to a shorter\\nlearning time and better generalisation capability. Here, we consider\\narcade-game environments as sets of interacting objects (controllable,\\nnon-controllable) and propose a set of functional modules that are specialized\\non mastering different types of interactions in a broad range of environments.\\nThe modules utilize regression, supervised learning, and reinforcement learning\\nalgorithms. Results of this case study in different Atari games suggest that\\nhuman-level performance can be achieved by a learning agent within a human\\namount of game experience (10-15 minutes game time) when a proper decomposition\\nof an environment or a task is provided. However, automatization of such\\ndecomposition remains a challenging problem. This case study shows how a model\\nof a causal structure underlying an environment or a task can benefit learning\\ntime and generalization capability of the agent, and argues in favor of\\nexploiting modular structure in contrast to using pure end-to-end learning\\napproaches.\\n</summary>\\n    <author>\\n      <name>Andrew Melnik</name>\\n    </author>\\n    <author>\\n      <name>Sascha Fleer</name>\\n    </author>\\n    <author>\\n      <name>Malte Schilling</name>\\n    </author>\\n    <author>\\n      <name>Helge Ritter</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1901.09895v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1901.09895v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1902.06239v1</id>\\n    <updated>2019-02-17T10:34:18Z</updated>\\n    <published>2019-02-17T10:34:18Z</published>\\n    <title>A new Potential-Based Reward Shaping for Reinforcement Learning Agent</title>\\n    <summary>  Potential-based reward shaping (PBRS) is a particular category of machine\\nlearning methods which aims to improve the learning speed of a reinforcement\\nlearning agent by extracting and utilizing extra knowledge while performing a\\ntask. There are two steps in the process of transfer learning: extracting\\nknowledge from previously learned tasks and transferring that knowledge to use\\nit in a target task. The latter step is well discussed in the literature with\\nvarious methods being proposed for it, while the former has been explored less.\\nWith this in mind, the type of knowledge that is transmitted is very important\\nand can lead to considerable improvement. Among the literature of both the\\ntransfer learning and the potential-based reward shaping, a subject that has\\nnever been addressed is the knowledge gathered during the learning process\\nitself. In this paper, we presented a novel potential-based reward shaping\\nmethod that attempted to extract knowledge from the learning process. The\\nproposed method extracts knowledge from episodes\\' cumulative rewards. The\\nproposed method has been evaluated in the Arcade learning environment and the\\nresults indicate an improvement in the learning process in both the single-task\\nand the multi-task reinforcement learner agents.\\n</summary>\\n    <author>\\n      <name>Babak Badnava</name>\\n    </author>\\n    <author>\\n      <name>Nasser Mozayani</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1902.06239v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1902.06239v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1903.04566v1</id>\\n    <updated>2019-03-11T19:50:38Z</updated>\\n    <published>2019-03-11T19:50:38Z</published>\\n    <title>Complementary Learning for Overcoming Catastrophic Forgetting Using\\n  Experience Replay</title>\\n    <summary>  Despite huge success, deep networks are unable to learn effectively in\\nsequential multitask learning settings as they forget the past learned tasks\\nafter learning new tasks. Inspired from complementary learning systems theory,\\nwe address this challenge by learning a generative model that couples the\\ncurrent task to the past learned tasks through a discriminative embedding\\nspace. We learn an abstract level generative distribution in the embedding that\\nallows the generation of data points to represent the experience. We sample\\nfrom this distribution and utilize experience replay to avoid forgetting and\\nsimultaneously accumulate new knowledge to the abstract distribution in order\\nto couple the current task with past experience. We demonstrate theoretically\\nand empirically that our framework learns a distribution in the embedding that\\nis shared across all task and as a result tackles catastrophic forgetting.\\n</summary>\\n    <author>\\n      <name>Mohammad Rostami</name>\\n    </author>\\n    <author>\\n      <name>Soheil Kolouri</name>\\n    </author>\\n    <author>\\n      <name>Praveen K. Pilly</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1903.04566v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1903.04566v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/0906.5151v1</id>\\n    <updated>2009-06-28T17:47:22Z</updated>\\n    <published>2009-06-28T17:47:22Z</published>\\n    <title>Unsupervised Search-based Structured Prediction</title>\\n    <summary>  We describe an adaptation and application of a search-based structured\\nprediction algorithm \"Searn\" to unsupervised learning problems. We show that it\\nis possible to reduce unsupervised learning to supervised learning and\\ndemonstrate a high-quality unsupervised shift-reduce parsing model. We\\nadditionally show a close connection between unsupervised Searn and expectation\\nmaximization. Finally, we demonstrate the efficacy of a semi-supervised\\nextension. The key idea that enables this is an application of the predict-self\\nidea for unsupervised learning.\\n</summary>\\n    <author>\\n      <name>Hal Daum\\xc3\\xa9 III</name>\\n    </author>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the International Conference on Machine Learning,\\n  2009</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/0906.5151v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.5151v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1005.0437v1</id>\\n    <updated>2010-05-04T06:05:51Z</updated>\\n    <published>2010-05-04T06:05:51Z</published>\\n    <title>A Unifying View of Multiple Kernel Learning</title>\\n    <summary>  Recent research on multiple kernel learning has lead to a number of\\napproaches for combining kernels in regularized risk minimization. The proposed\\napproaches include different formulations of objectives and varying\\nregularization strategies. In this paper we present a unifying general\\noptimization criterion for multiple kernel learning and show how existing\\nformulations are subsumed as special cases. We also derive the criterion\\'s dual\\nrepresentation, which is suitable for general smooth optimization algorithms.\\nFinally, we evaluate multiple kernel learning in this framework analytically\\nusing a Rademacher complexity bound on the generalization error and empirically\\nin a set of experiments.\\n</summary>\\n    <author>\\n      <name>Marius Kloft</name>\\n    </author>\\n    <author>\\n      <name>Ulrich R\\xc3\\xbcckert</name>\\n    </author>\\n    <author>\\n      <name>Peter L. Bartlett</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1005.0437v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1005.0437v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1202.3765v1</id>\\n    <updated>2012-02-14T16:41:17Z</updated>\\n    <published>2012-02-14T16:41:17Z</published>\\n    <title>Learning mixed graphical models from data with p larger than n</title>\\n    <summary>  Structure learning of Gaussian graphical models is an extensively studied\\nproblem in the classical multivariate setting where the sample size n is larger\\nthan the number of random variables p, as well as in the more challenging\\nsetting when p&gt;&gt;n. However, analogous approaches for learning the structure of\\ngraphical models with mixed discrete and continuous variables when p&gt;&gt;n remain\\nlargely unexplored. Here we describe a statistical learning procedure for this\\nproblem based on limited-order correlations and assess its performance with\\nsynthetic and real data.\\n</summary>\\n    <author>\\n      <name>Inma Tur</name>\\n    </author>\\n    <author>\\n      <name>Robert Castelo</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1202.3765v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1202.3765v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1205.2608v1</id>\\n    <updated>2012-05-09T18:38:39Z</updated>\\n    <published>2012-05-09T18:38:39Z</published>\\n    <title>Temporal-Difference Networks for Dynamical Systems with Continuous\\n  Observations and Actions</title>\\n    <summary>  Temporal-difference (TD) networks are a class of predictive state\\nrepresentations that use well-established TD methods to learn models of\\npartially observable dynamical systems. Previous research with TD networks has\\ndealt only with dynamical systems with finite sets of observations and actions.\\nWe present an algorithm for learning TD network representations of dynamical\\nsystems with continuous observations and actions. Our results show that the\\nalgorithm is capable of learning accurate and robust models of several noisy\\ncontinuous dynamical systems. The algorithm presented here is the first fully\\nincremental method for learning a predictive representation of a continuous\\ndynamical system.\\n</summary>\\n    <author>\\n      <name>Christopher M. Vigorito</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\\n  in Artificial Intelligence (UAI2009)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1205.2608v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1205.2608v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1306.0543v2</id>\\n    <updated>2014-10-27T11:49:08Z</updated>\\n    <published>2013-06-03T19:16:26Z</published>\\n    <title>Predicting Parameters in Deep Learning</title>\\n    <summary>  We demonstrate that there is significant redundancy in the parameterization\\nof several deep learning models. Given only a few weight values for each\\nfeature it is possible to accurately predict the remaining values. Moreover, we\\nshow that not only can the parameter values be predicted, but many of them need\\nnot be learned at all. We train several different architectures by learning\\nonly a small number of weights and predicting the rest. In the best case we are\\nable to predict more than 95% of the weights of a network without any drop in\\naccuracy.\\n</summary>\\n    <author>\\n      <name>Misha Denil</name>\\n    </author>\\n    <author>\\n      <name>Babak Shakibi</name>\\n    </author>\\n    <author>\\n      <name>Laurent Dinh</name>\\n    </author>\\n    <author>\\n      <name>Marc\\'Aurelio Ranzato</name>\\n    </author>\\n    <author>\\n      <name>Nando de Freitas</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1306.0543v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.0543v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.1167v1</id>\\n    <updated>2014-08-06T02:45:51Z</updated>\\n    <published>2014-08-06T02:45:51Z</published>\\n    <title>Boosted Markov Networks for Activity Recognition</title>\\n    <summary>  We explore a framework called boosted Markov networks to combine the learning\\ncapacity of boosting and the rich modeling semantics of Markov networks and\\napplying the framework for video-based activity recognition. Importantly, we\\nextend the framework to incorporate hidden variables. We show how the framework\\ncan be applied for both model learning and feature selection. We demonstrate\\nthat boosted Markov networks with hidden variables perform comparably with the\\nstandard maximum likelihood estimation. However, our framework is able to learn\\nsparse models, and therefore can provide computational savings when the learned\\nmodels are used for classification.\\n</summary>\\n    <author>\\n      <name>Truyen Tran</name>\\n    </author>\\n    <author>\\n      <name>Hung Bui</name>\\n    </author>\\n    <author>\\n      <name>Svetha Venkatesh</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Intelligent Sensors, Sensor Networks and\\n  Information Processing (ISSNIP)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.1167v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.1167v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1408.2032v1</id>\\n    <updated>2014-08-09T05:26:02Z</updated>\\n    <published>2014-08-09T05:26:02Z</published>\\n    <title>Bayesian Multitask Learning with Latent Hierarchies</title>\\n    <summary>  We learn multiple hypotheses for related tasks under a latent hierarchical\\nrelationship between tasks. We exploit the intuition that for domain\\nadaptation, we wish to share classifier structure, but for multitask learning,\\nwe wish to share covariance structure. Our hierarchical model is seen to\\nsubsume several previously proposed multitask learning models and performs well\\non three distinct real-world data sets.\\n</summary>\\n    <author>\\n      <name>Hal Daume III</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\\n  in Artificial Intelligence (UAI2009)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1408.2032v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1408.2032v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1504.04054v1</id>\\n    <updated>2015-04-15T21:31:58Z</updated>\\n    <published>2015-04-15T21:31:58Z</published>\\n    <title>A Generative Model for Deep Convolutional Learning</title>\\n    <summary>  A generative model is developed for deep (multi-layered) convolutional\\ndictionary learning. A novel probabilistic pooling operation is integrated into\\nthe deep model, yielding efficient bottom-up (pretraining) and top-down\\n(refinement) probabilistic learning. Experimental results demonstrate powerful\\ncapabilities of the model to learn multi-layer features from images, and\\nexcellent classification results are obtained on the MNIST and Caltech 101\\ndatasets.\\n</summary>\\n    <author>\\n      <name>Yunchen Pu</name>\\n    </author>\\n    <author>\\n      <name>Xin Yuan</name>\\n    </author>\\n    <author>\\n      <name>Lawrence Carin</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, 1 figure, ICLR workshop</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1504.04054v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1504.04054v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1602.04621v3</id>\\n    <updated>2016-07-04T17:11:52Z</updated>\\n    <published>2016-02-15T10:54:20Z</published>\\n    <title>Deep Exploration via Bootstrapped DQN</title>\\n    <summary>  Efficient exploration in complex environments remains a major challenge for\\nreinforcement learning. We propose bootstrapped DQN, a simple algorithm that\\nexplores in a computationally and statistically efficient manner through use of\\nrandomized value functions. Unlike dithering strategies such as epsilon-greedy\\nexploration, bootstrapped DQN carries out temporally-extended (or deep)\\nexploration; this can lead to exponentially faster learning. We demonstrate\\nthese benefits in complex stochastic MDPs and in the large-scale Arcade\\nLearning Environment. Bootstrapped DQN substantially improves learning times\\nand performance across most Atari games.\\n</summary>\\n    <author>\\n      <name>Ian Osband</name>\\n    </author>\\n    <author>\\n      <name>Charles Blundell</name>\\n    </author>\\n    <author>\\n      <name>Alexander Pritzel</name>\\n    </author>\\n    <author>\\n      <name>Benjamin Van Roy</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1602.04621v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.04621v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.01128v2</id>\\n    <updated>2016-09-05T08:12:15Z</updated>\\n    <published>2016-06-03T15:07:52Z</published>\\n    <title>Difference of Convex Functions Programming Applied to Control with\\n  Expert Data</title>\\n    <summary>  This paper reports applications of Difference of Convex functions (DC)\\nprogramming to Learning from Demonstrations (LfD) and Reinforcement Learning\\n(RL) with expert data. This is made possible because the norm of the Optimal\\nBellman Residual (OBR), which is at the heart of many RL and LfD algorithms, is\\nDC. Improvement in performance is demonstrated on two specific algorithms,\\nnamely Reward-regularized Classification for Apprenticeship Learning (RCAL) and\\nReinforcement Learning with Expert Demonstrations (RLED), through experiments\\non generic Markov Decision Processes (MDP), called Garnets.\\n</summary>\\n    <author>\\n      <name>Bilal Piot</name>\\n    </author>\\n    <author>\\n      <name>Matthieu Geist</name>\\n    </author>\\n    <author>\\n      <name>Olivier Pietquin</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1606.01128v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.01128v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.01885v1</id>\\n    <updated>2016-06-06T19:50:47Z</updated>\\n    <published>2016-06-06T19:50:47Z</published>\\n    <title>Learning to Optimize</title>\\n    <summary>  Algorithm design is a laborious process and often requires many iterations of\\nideation and validation. In this paper, we explore automating algorithm design\\nand present a method to learn an optimization algorithm, which we believe to be\\nthe first method that can automatically discover a better algorithm. We\\napproach this problem from a reinforcement learning perspective and represent\\nany particular optimization algorithm as a policy. We learn an optimization\\nalgorithm using guided policy search and demonstrate that the resulting\\nalgorithm outperforms existing hand-engineered algorithms in terms of\\nconvergence speed and/or the final objective value.\\n</summary>\\n    <author>\\n      <name>Ke Li</name>\\n    </author>\\n    <author>\\n      <name>Jitendra Malik</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.01885v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.01885v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1606.08660v2</id>\\n    <updated>2016-06-29T11:29:35Z</updated>\\n    <published>2016-06-28T11:41:03Z</published>\\n    <title>Theory reconstruction: a representation learning view on predicate\\n  invention</title>\\n    <summary>  With this positional paper we present a representation learning view on\\npredicate invention. The intention of this proposal is to bridge the relational\\nand deep learning communities on the problem of predicate invention. We propose\\na theory reconstruction approach, a formalism that extends autoencoder approach\\nto representation learning to the relational settings. Our intention is to\\nstart a discussion to define a unifying framework for predicate invention and\\ntheory revision.\\n</summary>\\n    <author>\\n      <name>Sebastijan Dumancic</name>\\n    </author>\\n    <author>\\n      <name>Wannes Meert</name>\\n    </author>\\n    <author>\\n      <name>Hendrik Blockeel</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 pages, StaRAI 2016 submission</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1606.08660v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1606.08660v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1106.2429v4</id>\\n    <updated>2013-09-11T10:55:26Z</updated>\\n    <published>2011-06-13T12:30:05Z</published>\\n    <title>Efficient Transductive Online Learning via Randomized Rounding</title>\\n    <summary>  Most traditional online learning algorithms are based on variants of mirror\\ndescent or follow-the-leader. In this paper, we present an online algorithm\\nbased on a completely different approach, tailored for transductive settings,\\nwhich combines \"random playout\" and randomized rounding of loss subgradients.\\nAs an application of our approach, we present the first computationally\\nefficient online algorithm for collaborative filtering with trace-norm\\nconstrained matrices. As a second application, we solve an open question\\nlinking batch learning and transductive online learning\\n</summary>\\n    <author>\\n      <name>Nicol\\xc3\\xb2 Cesa-Bianchi</name>\\n    </author>\\n    <author>\\n      <name>Ohad Shamir</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in a Festschrift in honor of V.N. Vapnik. Preliminary\\n  version presented in NIPS 2011</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1106.2429v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1106.2429v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1206.3721v1</id>\\n    <updated>2012-06-17T04:40:09Z</updated>\\n    <published>2012-06-17T04:40:09Z</published>\\n    <title>Constraint-free Graphical Model with Fast Learning Algorithm</title>\\n    <summary>  In this paper, we propose a simple, versatile model for learning the\\nstructure and parameters of multivariate distributions from a data set.\\nLearning a Markov network from a given data set is not a simple problem,\\nbecause Markov networks rigorously represent Markov properties, and this rigor\\nimposes complex constraints on the design of the networks. Our proposed model\\nremoves these constraints, acquiring important aspects from the information\\ngeometry. The proposed parameter- and structure-learning algorithms are simple\\nto execute as they are based solely on local computation at each node.\\nExperiments demonstrate that our algorithms work appropriately.\\n</summary>\\n    <author>\\n      <name>Kazuya Takabatake</name>\\n    </author>\\n    <author>\\n      <name>Shotaro Akaho</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 11 figures, submitted to UAI2012</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1206.3721v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.3721v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1704.07938v1</id>\\n    <updated>2017-04-26T00:33:36Z</updated>\\n    <published>2017-04-26T00:33:36Z</published>\\n    <title>An ensemble-based online learning algorithm for streaming data</title>\\n    <summary>  In this study, we introduce an ensemble-based approach for online machine\\nlearning. The ensemble of base classifiers in our approach is obtained by\\nlearning Naive Bayes classifiers on different training sets which are generated\\nby projecting the original training set to lower dimensional space. We propose\\na mechanism to learn sequences of data using data chunks paradigm. The\\nexperiments conducted on a number of UCI datasets and one synthetic dataset\\ndemonstrate that the proposed approach performs significantly better than some\\nwell-known online learning algorithms.\\n</summary>\\n    <author>\\n      <name>Tien Thanh Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Thi Thu Thuy Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Xuan Cuong Pham</name>\\n    </author>\\n    <author>\\n      <name>Alan Wee-Chung Liew</name>\\n    </author>\\n    <author>\\n      <name>James C. Bezdek</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages, 3 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1704.07938v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.07938v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1305.6568v1</id>\\n    <updated>2013-05-28T17:47:08Z</updated>\\n    <published>2013-05-28T17:47:08Z</published>\\n    <title>Reinforcement Learning for the Soccer Dribbling Task</title>\\n    <summary>  We propose a reinforcement learning solution to the \\\\emph{soccer dribbling\\ntask}, a scenario in which a soccer agent has to go from the beginning to the\\nend of a region keeping possession of the ball, as an adversary attempts to\\ngain possession. While the adversary uses a stationary policy, the dribbler\\nlearns the best action to take at each decision point. After defining\\nmeaningful variables to represent the state space, and high-level macro-actions\\nto incorporate domain knowledge, we describe our application of the\\nreinforcement learning algorithm \\\\emph{Sarsa} with CMAC for function\\napproximation. Our experiments show that, after the training period, the\\ndribbler is able to accomplish its task against a strong adversary around 58%\\nof the time.\\n</summary>\\n    <author>\\n      <name>Arthur Carvalho</name>\\n    </author>\\n    <author>\\n      <name>Renato Oliveira</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/CIG.2011.6031994</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/CIG.2011.6031994\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/1305.6568v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.6568v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1407.0754v1</id>\\n    <updated>2014-07-03T00:48:34Z</updated>\\n    <published>2014-07-03T00:48:34Z</published>\\n    <title>Structured Learning via Logistic Regression</title>\\n    <summary>  A successful approach to structured learning is to write the learning\\nobjective as a joint function of linear parameters and inference messages, and\\niterate between updates to each. This paper observes that if the inference\\nproblem is \"smoothed\" through the addition of entropy terms, for fixed\\nmessages, the learning objective reduces to a traditional (non-structured)\\nlogistic regression problem with respect to parameters. In these logistic\\nregression problems, each training example has a bias term determined by the\\ncurrent set of messages. Based on this insight, the structured energy function\\ncan be extended from linear factors to any function class where an \"oracle\"\\nexists to minimize a logistic loss.\\n</summary>\\n    <author>\\n      <name>Justin Domke</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Advances in Neural Information Processing Systems 2013</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1407.0754v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1407.0754v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1411.3128v2</id>\\n    <updated>2014-12-10T15:55:12Z</updated>\\n    <published>2014-11-12T10:40:52Z</published>\\n    <title>Deep Multi-Instance Transfer Learning</title>\\n    <summary>  We present a new approach for transferring knowledge from groups to\\nindividuals that comprise them. We evaluate our method in text, by inferring\\nthe ratings of individual sentences using full-review ratings. This approach,\\nwhich combines ideas from transfer learning, deep learning and multi-instance\\nlearning, reduces the need for laborious human labelling of fine-grained data\\nwhen abundant labels are available at the group level.\\n</summary>\\n    <author>\\n      <name>Dimitrios Kotzias</name>\\n    </author>\\n    <author>\\n      <name>Misha Denil</name>\\n    </author>\\n    <author>\\n      <name>Phil Blunsom</name>\\n    </author>\\n    <author>\\n      <name>Nando de Freitas</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/1411.3128v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1411.3128v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1506.07477v1</id>\\n    <updated>2015-06-24T17:27:28Z</updated>\\n    <published>2015-06-24T17:27:28Z</published>\\n    <title>Efficient Learning for Undirected Topic Models</title>\\n    <summary>  Replicated Softmax model, a well-known undirected topic model, is powerful in\\nextracting semantic representations of documents. Traditional learning\\nstrategies such as Contrastive Divergence are very inefficient. This paper\\nprovides a novel estimator to speed up the learning based on Noise Contrastive\\nEstimate, extended for documents of variant lengths and weighted inputs.\\nExperiments on two benchmarks show that the new estimator achieves great\\nlearning efficiency and high accuracy on document retrieval and classification.\\n</summary>\\n    <author>\\n      <name>Jiatao Gu</name>\\n    </author>\\n    <author>\\n      <name>Victor O. K. Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by ACL-IJCNLP 2015 short paper. 6 pages</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1506.07477v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1506.07477v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1605.05212v1</id>\\n    <updated>2016-05-17T15:37:19Z</updated>\\n    <published>2016-05-17T15:37:19Z</published>\\n    <title>Multimodal Sparse Coding for Event Detection</title>\\n    <summary>  Unsupervised feature learning methods have proven effective for\\nclassification tasks based on a single modality. We present multimodal sparse\\ncoding for learning feature representations shared across multiple modalities.\\nThe shared representations are applied to multimedia event detection (MED) and\\nevaluated in comparison to unimodal counterparts, as well as other feature\\nlearning methods such as GMM supervectors and sparse RBM. We report the\\ncross-validated classification accuracy and mean average precision of the MED\\nsystem trained on features learned from our unimodal and multimodal settings\\nfor a subset of the TRECVID MED 2014 dataset.\\n</summary>\\n    <author>\\n      <name>Youngjune Gwon</name>\\n    </author>\\n    <author>\\n      <name>William Campbell</name>\\n    </author>\\n    <author>\\n      <name>Kevin Brady</name>\\n    </author>\\n    <author>\\n      <name>Douglas Sturim</name>\\n    </author>\\n    <author>\\n      <name>Miriam Cha</name>\\n    </author>\\n    <author>\\n      <name>H. T. Kung</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Multimodal Machine Learning Workshop at NIPS 2015</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1605.05212v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1605.05212v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1608.00220v1</id>\\n    <updated>2016-07-31T14:28:15Z</updated>\\n    <published>2016-07-31T14:28:15Z</published>\\n    <title>Learning Robust Features using Deep Learning for Automatic Seizure\\n  Detection</title>\\n    <summary>  We present and evaluate the capacity of a deep neural network to learn robust\\nfeatures from EEG to automatically detect seizures. This is a challenging\\nproblem because seizure manifestations on EEG are extremely variable both\\ninter- and intra-patient. By simultaneously capturing spectral, temporal and\\nspatial information our recurrent convolutional neural network learns a general\\nspatially invariant representation of a seizure. The proposed approach exceeds\\nsignificantly previous results obtained on cross-patient classifiers both in\\nterms of sensitivity and false positive rate. Furthermore, our model proves to\\nbe robust to missing channel and variable electrode montage.\\n</summary>\\n    <author>\\n      <name>Pierre Thodoroff</name>\\n    </author>\\n    <author>\\n      <name>Joelle Pineau</name>\\n    </author>\\n    <author>\\n      <name>Andrew Lim</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Presented at 2016 Machine Learning and Healthcare Conference (MLHC\\n  2016), Los Angeles, CA</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1608.00220v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1608.00220v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1609.09580v1</id>\\n    <updated>2016-09-30T03:20:52Z</updated>\\n    <published>2016-09-30T03:20:52Z</published>\\n    <title>Referential Uncertainty and Word Learning in High-dimensional,\\n  Continuous Meaning Spaces</title>\\n    <summary>  This paper discusses lexicon word learning in high-dimensional meaning spaces\\nfrom the viewpoint of referential uncertainty. We investigate various\\nstate-of-the-art Machine Learning algorithms and discuss the impact of scaling,\\nrepresentation and meaning space structure. We demonstrate that current Machine\\nLearning techniques successfully deal with high-dimensional meaning spaces. In\\nparticular, we show that exponentially increasing dimensions linearly impact\\nlearner performance and that referential uncertainty from word sensitivity has\\nno impact.\\n</summary>\\n    <author>\\n      <name>Michael Spranger</name>\\n    </author>\\n    <author>\\n      <name>Katrien Beuls</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published as Spranger, M. and Beuls, K. (2016). Referential\\n  uncertainty and word learning in high-dimensional, continuous meaning spaces.\\n  In Hafner, V. and Pitti, A., editors, Development and Learning and Epigenetic\\n  Robotics (ICDL-Epirob), 2016 Joint IEEE International Conferences on, 2016.\\n  IEEE</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1609.09580v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1609.09580v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.01132v3</id>\\n    <updated>2016-12-27T20:59:01Z</updated>\\n    <published>2016-10-04T19:22:44Z</published>\\n    <title>A Non-generative Framework and Convex Relaxations for Unsupervised\\n  Learning</title>\\n    <summary>  We give a novel formal theoretical framework for unsupervised learning with\\ntwo distinctive characteristics. First, it does not assume any generative model\\nand based on a worst-case performance metric. Second, it is comparative, namely\\nperformance is measured with respect to a given hypothesis class. This allows\\nto avoid known computational hardness results and improper algorithms based on\\nconvex relaxations. We show how several families of unsupervised learning\\nmodels, which were previously only analyzed under probabilistic assumptions and\\nare otherwise provably intractable, can be efficiently learned in our framework\\nby convex optimization.\\n</summary>\\n    <author>\\n      <name>Elad Hazan</name>\\n    </author>\\n    <author>\\n      <name>Tengyu Ma</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2016</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.01132v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.01132v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n  <entry>\\n    <id>http://arxiv.org/abs/1610.04578v3</id>\\n    <updated>2017-08-07T15:26:04Z</updated>\\n    <published>2016-10-14T18:51:25Z</published>\\n    <title>Improved Strongly Adaptive Online Learning using Coin Betting</title>\\n    <summary>  This paper describes a new parameter-free online learning algorithm for\\nchanging environments. In comparing against algorithms with the same time\\ncomplexity as ours, we obtain a strongly adaptive regret bound that is a factor\\nof at least $\\\\sqrt{\\\\log(T)}$ better, where $T$ is the time horizon. Empirical\\nresults show that our algorithm outperforms state-of-the-art methods in\\nlearning with expert advice and metric learning scenarios.\\n</summary>\\n    <author>\\n      <name>Kwang-Sung Jun</name>\\n    </author>\\n    <author>\\n      <name>Francesco Orabona</name>\\n    </author>\\n    <author>\\n      <name>Rebecca Willett</name>\\n    </author>\\n    <author>\\n      <name>Stephen Wright</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">fixed a few typos</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/1610.04578v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1610.04578v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
     ]
    }
   ],
   "source": [
    "data = urllib.request.urlopen(url).read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = feedparser.parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'links': [{'href': 'http://arxiv.org/api/query?search_query%3Dall%3Amachine%20learning%26id_list%3D%26start%3D0%26max_results%3D1000',\n",
       "   'rel': 'self',\n",
       "   'type': 'application/atom+xml'}],\n",
       " 'title': 'ArXiv Query: search_query=all:machine learning&id_list=&start=0&max_results=1000',\n",
       " 'title_detail': {'type': 'text/html',\n",
       "  'language': None,\n",
       "  'base': '',\n",
       "  'value': 'ArXiv Query: search_query=all:machine learning&id_list=&start=0&max_results=1000'},\n",
       " 'id': 'http://arxiv.org/api/cJ94CoIB99FKowsEQVatCUfLlTs',\n",
       " 'guidislink': True,\n",
       " 'link': 'http://arxiv.org/api/cJ94CoIB99FKowsEQVatCUfLlTs',\n",
       " 'updated': '2019-04-12T00:00:00-04:00',\n",
       " 'updated_parsed': time.struct_time(tm_year=2019, tm_mon=4, tm_mday=12, tm_hour=4, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=102, tm_isdst=0),\n",
       " 'opensearch_totalresults': '77475',\n",
       " 'opensearch_startindex': '0',\n",
       " 'opensearch_itemsperpage': '1000'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['feed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Optimal Control View of Adversarial Machine Learning'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['entries'][0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I describe an optimal control view of adversarial machine learning, where the\\ndynamical system is the machine learner, the input are adversarial actions, and\\nthe control costs are defined by the adversary's goals to do harm and be hard\\nto detect. This view encompasses many types of adversarial machine learning,\\nincluding test-item attacks, training-data poisoning, and adversarial reward\\nshaping. The view encourages adversarial machine learning researcher to utilize\\nadvances in control theory and reinforcement learning.\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['entries'][0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d['entries'], columns=['title', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Optimal Control View of Adversarial Machine...</td>\n",
       "      <td>I describe an optimal control view of adversar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Minimax deviation strategies for machine learn...</td>\n",
       "      <td>The article is devoted to the problem of small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to Machine Learning: Class Notes ...</td>\n",
       "      <td>Introduction to Machine learning covering Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Unified Analytical Framework for Trustable M...</td>\n",
       "      <td>Traditional machine learning algorithms use da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLBench: How Good Are Machine Learning Clouds ...</td>\n",
       "      <td>We conduct an empirical study of machine learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AutoCompete: A Framework for Machine Learning ...</td>\n",
       "      <td>In this paper, we propose AutoCompete, a highl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Proceedings of the 2016 ICML Workshop on #Data...</td>\n",
       "      <td>This is the Proceedings of the ICML Workshop o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Joint Training of Deep Boltzmann Machines</td>\n",
       "      <td>We introduce a new method for training deep Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Primer on PAC-Bayesian Learning</td>\n",
       "      <td>Generalized Bayesian learning algorithms are i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal-related Convolutional-Restricted-Bolt...</td>\n",
       "      <td>In this article, we extend the conventional fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Proceedings of the 29th International Conferen...</td>\n",
       "      <td>This is an index to the papers that appear in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Distributed Multi-Task Learning with Shared Re...</td>\n",
       "      <td>We study the problem of distributed multi-task...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>On-the-Fly Learning in a Perpetual Learning Ma...</td>\n",
       "      <td>Despite the promise of brain-inspired machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Distributed Multitask Learning</td>\n",
       "      <td>We consider the problem of distributed multi-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Distributed Stochastic Multi-Task Learning wit...</td>\n",
       "      <td>We propose methods for distributed graph-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>An Aggregate and Iterative Disaggregate Algori...</td>\n",
       "      <td>We propose a clustering-based iterative algori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bayesian Optimization for Machine Learning : A...</td>\n",
       "      <td>The engineering of machine learning systems is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Towards A Rigorous Science of Interpretable Ma...</td>\n",
       "      <td>As machine learning systems become ubiquitous,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Infrastructure for Usable Machine Learning: Th...</td>\n",
       "      <td>Despite incredible recent advances in machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>This paper investigates to what extent cogniti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Techniques for Interpretable Machine Learning</td>\n",
       "      <td>Interpretable machine learning tackles the imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Optimization Models for Machine Learning: A Su...</td>\n",
       "      <td>This paper surveys the machine learning litera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Meta-Learning: A Survey</td>\n",
       "      <td>Meta-learning, or learning to learn, is the sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Introduction to intelligent computing unit 1</td>\n",
       "      <td>This brief note highlights some basic concepts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Compressive Classification (Machine Learning w...</td>\n",
       "      <td>Compressive learning is a framework where (so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Machine Learning Interpretability: A Science r...</td>\n",
       "      <td>The term \"interpretability\" is oftenly used by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A Survey on Resilient Machine Learning</td>\n",
       "      <td>Machine learning based system are increasingly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Discussion on Mechanical Learning and Learning...</td>\n",
       "      <td>Mechanical learning is a computing system that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Information Theory and its Relation to Machine...</td>\n",
       "      <td>In this position paper, I first describe a new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>An Introduction to MM Algorithms for Machine L...</td>\n",
       "      <td>MM (majorization--minimization) algorithms are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>Batch Active Preference-Based Learning of Rewa...</td>\n",
       "      <td>Data generation and labeling are usually an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Optimal Hierarchical Learning Path Design with...</td>\n",
       "      <td>E-learning systems are capable of providing mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Learning to Learn without Forgetting By Maximi...</td>\n",
       "      <td>Lack of performance when it comes to continual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>Modularization of End-to-End Learning: Case St...</td>\n",
       "      <td>Complex environments and tasks pose a difficul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>A new Potential-Based Reward Shaping for Reinf...</td>\n",
       "      <td>Potential-based reward shaping (PBRS) is a par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Complementary Learning for Overcoming Catastro...</td>\n",
       "      <td>Despite huge success, deep networks are unable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Unsupervised Search-based Structured Prediction</td>\n",
       "      <td>We describe an adaptation and application of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>A Unifying View of Multiple Kernel Learning</td>\n",
       "      <td>Recent research on multiple kernel learning ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>Learning mixed graphical models from data with...</td>\n",
       "      <td>Structure learning of Gaussian graphical model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Temporal-Difference Networks for Dynamical Sys...</td>\n",
       "      <td>Temporal-difference (TD) networks are a class ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Predicting Parameters in Deep Learning</td>\n",
       "      <td>We demonstrate that there is significant redun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>Boosted Markov Networks for Activity Recognition</td>\n",
       "      <td>We explore a framework called boosted Markov n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Bayesian Multitask Learning with Latent Hierar...</td>\n",
       "      <td>We learn multiple hypotheses for related tasks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>A Generative Model for Deep Convolutional Lear...</td>\n",
       "      <td>A generative model is developed for deep (mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>Deep Exploration via Bootstrapped DQN</td>\n",
       "      <td>Efficient exploration in complex environments ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>Difference of Convex Functions Programming App...</td>\n",
       "      <td>This paper reports applications of Difference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>Learning to Optimize</td>\n",
       "      <td>Algorithm design is a laborious process and of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Theory reconstruction: a representation learni...</td>\n",
       "      <td>With this positional paper we present a repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Efficient Transductive Online Learning via Ran...</td>\n",
       "      <td>Most traditional online learning algorithms ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Constraint-free Graphical Model with Fast Lear...</td>\n",
       "      <td>In this paper, we propose a simple, versatile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>An ensemble-based online learning algorithm fo...</td>\n",
       "      <td>In this study, we introduce an ensemble-based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Reinforcement Learning for the Soccer Dribblin...</td>\n",
       "      <td>We propose a reinforcement learning solution t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Structured Learning via Logistic Regression</td>\n",
       "      <td>A successful approach to structured learning i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Deep Multi-Instance Transfer Learning</td>\n",
       "      <td>We present a new approach for transferring kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Efficient Learning for Undirected Topic Models</td>\n",
       "      <td>Replicated Softmax model, a well-known undirec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Multimodal Sparse Coding for Event Detection</td>\n",
       "      <td>Unsupervised feature learning methods have pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Learning Robust Features using Deep Learning f...</td>\n",
       "      <td>We present and evaluate the capacity of a deep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Referential Uncertainty and Word Learning in H...</td>\n",
       "      <td>This paper discusses lexicon word learning in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>A Non-generative Framework and Convex Relaxati...</td>\n",
       "      <td>We give a novel formal theoretical framework f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Improved Strongly Adaptive Online Learning usi...</td>\n",
       "      <td>This paper describes a new parameter-free onli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    An Optimal Control View of Adversarial Machine...   \n",
       "1    Minimax deviation strategies for machine learn...   \n",
       "2    Introduction to Machine Learning: Class Notes ...   \n",
       "3    A Unified Analytical Framework for Trustable M...   \n",
       "4    MLBench: How Good Are Machine Learning Clouds ...   \n",
       "5    AutoCompete: A Framework for Machine Learning ...   \n",
       "6    Proceedings of the 2016 ICML Workshop on #Data...   \n",
       "7            Joint Training of Deep Boltzmann Machines   \n",
       "8                    A Primer on PAC-Bayesian Learning   \n",
       "9    Temporal-related Convolutional-Restricted-Bolt...   \n",
       "10   Proceedings of the 29th International Conferen...   \n",
       "11   Distributed Multi-Task Learning with Shared Re...   \n",
       "12   On-the-Fly Learning in a Perpetual Learning Ma...   \n",
       "13                      Distributed Multitask Learning   \n",
       "14   Distributed Stochastic Multi-Task Learning wit...   \n",
       "15   An Aggregate and Iterative Disaggregate Algori...   \n",
       "16   Bayesian Optimization for Machine Learning : A...   \n",
       "17   Towards A Rigorous Science of Interpretable Ma...   \n",
       "18   Infrastructure for Usable Machine Learning: Th...   \n",
       "19   A review of possible effects of cognitive bias...   \n",
       "20       Techniques for Interpretable Machine Learning   \n",
       "21   Optimization Models for Machine Learning: A Su...   \n",
       "22                             Meta-Learning: A Survey   \n",
       "23        Introduction to intelligent computing unit 1   \n",
       "24   Compressive Classification (Machine Learning w...   \n",
       "25   Machine Learning Interpretability: A Science r...   \n",
       "26              A Survey on Resilient Machine Learning   \n",
       "27   Discussion on Mechanical Learning and Learning...   \n",
       "28   Information Theory and its Relation to Machine...   \n",
       "29   An Introduction to MM Algorithms for Machine L...   \n",
       "..                                                 ...   \n",
       "970  Batch Active Preference-Based Learning of Rewa...   \n",
       "971  Optimal Hierarchical Learning Path Design with...   \n",
       "972  Learning to Learn without Forgetting By Maximi...   \n",
       "973  Modularization of End-to-End Learning: Case St...   \n",
       "974  A new Potential-Based Reward Shaping for Reinf...   \n",
       "975  Complementary Learning for Overcoming Catastro...   \n",
       "976    Unsupervised Search-based Structured Prediction   \n",
       "977        A Unifying View of Multiple Kernel Learning   \n",
       "978  Learning mixed graphical models from data with...   \n",
       "979  Temporal-Difference Networks for Dynamical Sys...   \n",
       "980             Predicting Parameters in Deep Learning   \n",
       "981   Boosted Markov Networks for Activity Recognition   \n",
       "982  Bayesian Multitask Learning with Latent Hierar...   \n",
       "983  A Generative Model for Deep Convolutional Lear...   \n",
       "984              Deep Exploration via Bootstrapped DQN   \n",
       "985  Difference of Convex Functions Programming App...   \n",
       "986                               Learning to Optimize   \n",
       "987  Theory reconstruction: a representation learni...   \n",
       "988  Efficient Transductive Online Learning via Ran...   \n",
       "989  Constraint-free Graphical Model with Fast Lear...   \n",
       "990  An ensemble-based online learning algorithm fo...   \n",
       "991  Reinforcement Learning for the Soccer Dribblin...   \n",
       "992        Structured Learning via Logistic Regression   \n",
       "993              Deep Multi-Instance Transfer Learning   \n",
       "994     Efficient Learning for Undirected Topic Models   \n",
       "995       Multimodal Sparse Coding for Event Detection   \n",
       "996  Learning Robust Features using Deep Learning f...   \n",
       "997  Referential Uncertainty and Word Learning in H...   \n",
       "998  A Non-generative Framework and Convex Relaxati...   \n",
       "999  Improved Strongly Adaptive Online Learning usi...   \n",
       "\n",
       "                                               summary  \n",
       "0    I describe an optimal control view of adversar...  \n",
       "1    The article is devoted to the problem of small...  \n",
       "2    Introduction to Machine learning covering Stat...  \n",
       "3    Traditional machine learning algorithms use da...  \n",
       "4    We conduct an empirical study of machine learn...  \n",
       "5    In this paper, we propose AutoCompete, a highl...  \n",
       "6    This is the Proceedings of the ICML Workshop o...  \n",
       "7    We introduce a new method for training deep Bo...  \n",
       "8    Generalized Bayesian learning algorithms are i...  \n",
       "9    In this article, we extend the conventional fr...  \n",
       "10   This is an index to the papers that appear in ...  \n",
       "11   We study the problem of distributed multi-task...  \n",
       "12   Despite the promise of brain-inspired machine ...  \n",
       "13   We consider the problem of distributed multi-t...  \n",
       "14   We propose methods for distributed graph-based...  \n",
       "15   We propose a clustering-based iterative algori...  \n",
       "16   The engineering of machine learning systems is...  \n",
       "17   As machine learning systems become ubiquitous,...  \n",
       "18   Despite incredible recent advances in machine ...  \n",
       "19   This paper investigates to what extent cogniti...  \n",
       "20   Interpretable machine learning tackles the imp...  \n",
       "21   This paper surveys the machine learning litera...  \n",
       "22   Meta-learning, or learning to learn, is the sc...  \n",
       "23   This brief note highlights some basic concepts...  \n",
       "24   Compressive learning is a framework where (so ...  \n",
       "25   The term \"interpretability\" is oftenly used by...  \n",
       "26   Machine learning based system are increasingly...  \n",
       "27   Mechanical learning is a computing system that...  \n",
       "28   In this position paper, I first describe a new...  \n",
       "29   MM (majorization--minimization) algorithms are...  \n",
       "..                                                 ...  \n",
       "970  Data generation and labeling are usually an ex...  \n",
       "971  E-learning systems are capable of providing mo...  \n",
       "972  Lack of performance when it comes to continual...  \n",
       "973  Complex environments and tasks pose a difficul...  \n",
       "974  Potential-based reward shaping (PBRS) is a par...  \n",
       "975  Despite huge success, deep networks are unable...  \n",
       "976  We describe an adaptation and application of a...  \n",
       "977  Recent research on multiple kernel learning ha...  \n",
       "978  Structure learning of Gaussian graphical model...  \n",
       "979  Temporal-difference (TD) networks are a class ...  \n",
       "980  We demonstrate that there is significant redun...  \n",
       "981  We explore a framework called boosted Markov n...  \n",
       "982  We learn multiple hypotheses for related tasks...  \n",
       "983  A generative model is developed for deep (mult...  \n",
       "984  Efficient exploration in complex environments ...  \n",
       "985  This paper reports applications of Difference ...  \n",
       "986  Algorithm design is a laborious process and of...  \n",
       "987  With this positional paper we present a repres...  \n",
       "988  Most traditional online learning algorithms ar...  \n",
       "989  In this paper, we propose a simple, versatile ...  \n",
       "990  In this study, we introduce an ensemble-based ...  \n",
       "991  We propose a reinforcement learning solution t...  \n",
       "992  A successful approach to structured learning i...  \n",
       "993  We present a new approach for transferring kno...  \n",
       "994  Replicated Softmax model, a well-known undirec...  \n",
       "995  Unsupervised feature learning methods have pro...  \n",
       "996  We present and evaluate the capacity of a deep...  \n",
       "997  This paper discusses lexicon word learning in ...  \n",
       "998  We give a novel formal theoretical framework f...  \n",
       "999  This paper describes a new parameter-free onli...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
